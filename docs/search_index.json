[["working-with-text.html", "18 working with text 18.1 overview of key topics in text analysis 18.2 a case study 18.3 exercise: what would you do next?", " 18 working with text This chapter includes two parts: A (brief) overview and a (longer) case study which serves as an example. 18.1 overview of key topics in text analysis We can represent a body of text (a corpus) as a string of words (or, more generally, tokens), using the tidytext package. See the “introduction to tidytext” for more. (https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html). [Jane Austen, Sentiment analysis, Word frequencies, bag-of-words, stop words]. There is more in the Silge &amp; Robinson book. Dictionaries are an extension of sentiment analysis. Word clouds can be built from tables of word frequencies. Hamilton project Challenges in text analysis include problems in character encoding. Decisions must be made about stop-words (whether or not to exclude them), and units of analysis (words, bigrams, etc. as units of speech; chapters, speakers, etc. as ‘corpora’ which you might compare). Limitations of this approach include negation. Another example looks at Topic modeling in Taylor Swift lyrics. In my own work, I’ve looked at such things as comparing scholarly texts, looking at words associated with stages of personality development, and analyzing the contents of Reddit posts. The nature of LLMs, inferring meaning from context. 18.2 a case study In this chapter, I present a rudimentary case study in which a contemporary issue is explored using R. I examine Reddit1 data, create comparison word-clouds from single words and two-word strings (bigrams), and consider the results of an external proprietary tool for examining categories of language. The chapter is based on the tidy approach to text analysis, for which a clear and thorough introduction has been provided in two books by Silge and her colleagues, each of which is available online Hvitfeldt and Silge (2021). A word of warning: This chapter is light on the grunt work of data wrangling and preprocessing, which were introduced in Chapter 14. Keep in mind that that work is typically very time-consuming. 18.2.1 federal workers On March 19, 2025, the New York Times ran an article with the title “Will I Lose My Job?’ Federal Workers Flock to Reddit for Answers.” The gist of the article was that US Federal workers, facing job insecurity in the opening days of President Trump’s second term, were using the r/fedworkers subreddit to connect with others and communicate their anxieties, concerns, and strategies. But did they, though? Is there evidence that the r/fedworkers subreddit was being used in these ways? In the analysis that follows, I will examine posts2 to the community for the period from January 1, 2024 through March 24, 2025. We’ll consider the following: What specific words differentiate Trump era posts (since his second inauguration in January 2025) from those from the Biden era (prior to the November 2024 election)? For this, we’ll rely primarily on visualizations, in particular, differential word clouds of single words and bigrams. What categories of speech differentiate the two epochs? For this, we’ll use a proprietary tool, Linguistic Inquiry and Word Count. LIWC is a widely used measure for extracting grammatical, social, and psychological variables from text [LIWC; Boyd et al. (2022)]. We’ll generate tables for the LIWC categories and examine the effect size and statistical significance of these differences. This approach can optionally be extended to additional comparisons, including: Are similar results obtained when we examine comments rather than posts? (Here, we could compare Trump-era words and categories with Biden-era words and categories). How are comments and posts different from each other? (Here, we could compare words and categories for r/fedworkers posts with words and categories for r/fedworkers comments) Does the content of posts (or comments) between the election and the inaugural more closely resemble that of the Biden era (as Biden was still president) or the coming Trump administration? In order to address these questions, we’ll briefly touch on some auxiliary technical questions as well, including: Should we ‘stem’ words prior to analysis? That is, should we treat terms such as (donut and donuts) as the same word or as different? How about (go and going)? A similar question can be asked about case - should all characters be set to lowercase? That is, should (I’m and i’m) be treated as the same? How about (DOGE and doge)? Should we include common stop words (a, and, if, or, me) in our analysis, or disregard them? 18.2.2 finding Reddit data Although Reddit data became less accessible beginning in 2023, archives of Reddit posts and comments remain readily available following some simple Web sleuthing. In this block, I begin with a set of posts that I have already downloaded. We initially look at a very small set of data - 100 lines - both to make sure the code works and, if it does, to make an initial determination about what fields are of interest. The file is large, so it will ultimately be read in through streaming rather than all-at-once, hence the readLines -&gt; stream_in syntax. posts &lt;- read_lines(&quot;data/r_fednews_posts.jsonl&quot;, n_max = 100) a1 &lt;- stream_in(textConnection( gsub(&quot;\\\\n&quot;,&quot;&quot;, posts))) ## Found 100 records... Imported 100 records. Simplifying... glimpse(a1) ## Rows: 100 ## Columns: 113 ## $ `_meta` &lt;df[,1]&gt; &lt;data.frame[26 x 1]&gt; ## $ all_awardings &lt;list&gt; [], [], [], [], [], [], [], [], [], [… ## $ allow_live_comments &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALS… ## $ approved_at_utc &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ approved_by &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ archived &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ author &lt;chr&gt; &quot;Unlikely_Story1424&quot;, &quot;lakepirate1775&quot;, … ## $ author_flair_background_color &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ author_flair_css_class &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ author_flair_richtext &lt;list&gt; [], [], [], [], [], [], [], [], [], [], … ## $ author_flair_template_id &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ author_flair_text &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ author_flair_text_color &lt;chr&gt; NA, NA, NA, NA, &quot;dark&quot;, NA, NA, NA, NA, … ## $ author_flair_type &lt;chr&gt; &quot;text&quot;, &quot;text&quot;, &quot;text&quot;, &quot;text&quot;, &quot;text&quot;, … ## $ author_fullname &lt;chr&gt; &quot;t2_dvmjwod3g&quot;, &quot;t2_3z3p2fng&quot;, &quot;t2_r0erp… ## $ author_is_blocked &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ author_patreon_flair &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ author_premium &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ awarders &lt;list&gt; [], [], [], [], [], [], [], [], [], [], … ## $ banned_at_utc &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ banned_by &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ can_gild &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ can_mod_post &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ category &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ clicked &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ content_categories &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ contest_mode &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ created &lt;int&gt; 1704074487, 1704075982, 1704083189, 1704… ## $ created_utc &lt;int&gt; 1704074487, 1704075982, 1704083189, 1704… ## $ discussion_type &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ distinguished &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ domain &lt;chr&gt; &quot;self.fednews&quot;, &quot;self.fednews&quot;, &quot;self.fe… ## $ downs &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ edited &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ gilded &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ gildings &lt;df[,0]&gt; &lt;data.frame[26 x 0]&gt; ## $ hidden &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ hide_score &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T… ## $ id &lt;chr&gt; &quot;18vmmmu&quot;, &quot;18vn1fo&quot;, &quot;18vox52&quot;, &quot;18voyk… ## $ is_created_from_ads_ui &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ is_crosspostable &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, … ## $ is_meta &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ is_original_content &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ is_reddit_media_domain &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ is_robot_indexable &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, … ## $ is_self &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE… ## $ is_video &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ likes &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ link_flair_background_color &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, NA, &quot;&quot;, &quot;&quot;, … ## $ link_flair_css_class &lt;chr&gt; NA, NA, &quot;e&quot;, NA, &quot;c&quot;, &quot;c&quot;, NA, &quot;c&quot;, NA, … ## $ link_flair_richtext &lt;list&gt; [&lt;data.frame[0 x 0]&gt;], [&lt;data.frame[0 x … ## $ link_flair_text &lt;chr&gt; NA, NA, &quot;Misc&quot;, NA, &quot;Pay &amp; Benefits&quot;, &quot;P… ## $ link_flair_text_color &lt;chr&gt; &quot;dark&quot;, &quot;dark&quot;, &quot;dark&quot;, &quot;dark&quot;, &quot;dark&quot;,… ## $ link_flair_type &lt;chr&gt; &quot;text&quot;, &quot;text&quot;, &quot;richtext&quot;, &quot;text&quot;, &quot;ric… ## $ locked &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ media &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ media_embed &lt;df[,0]&gt; &lt;data.frame[26 x 0]&gt; ## $ media_only &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ mod_note &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ mod_reason_by &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mod_reason_title &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ mod_reports &lt;list&gt; [], [], [], [], [], [], [], [], [], [], … ## $ name &lt;chr&gt; &quot;t3_18vmmmu&quot;, &quot;t3_18vn1fo&quot;, &quot;t3_18vox52&quot;… ## $ no_follow &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE… ## $ num_comments &lt;int&gt; 1, 15, 1, 1, 1, 19, 1, 61, 220, 95, 1, … ## $ num_crossposts &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ num_reports &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ over_18 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ parent_whitelist_status &lt;chr&gt; &quot;all_ads&quot;, &quot;all_ads&quot;, &quot;all_ads&quot;, &quot;all_ad… ## $ permalink &lt;chr&gt; &quot;/r/fednews/comments/18vmmmu/historical_… ## $ pinned &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ previous_selftext &lt;chr&gt; &quot;[removed]&quot;, NA, NA, NA, NA, NA, NA, NA,… ## $ pwls &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6… ## $ quarantine &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ removal_reason &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ removed_by &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ removed_by_category &lt;chr&gt; &quot;automod_filtered&quot;, NA, &quot;moderator&quot;, &quot;mo… ## $ report_reasons &lt;list&gt; [], [], [], [], [], [], [], [], [], [], … ## $ retrieved_on &lt;int&gt; 1704074505, 1704075998, 1704083206, 1704… ## $ saved &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ score &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 19, 1, 1, 1, 1, 1,… ## $ secure_media &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ secure_media_embed &lt;df[,0]&gt; &lt;data.frame[26 x 0]&gt; ## $ selftext &lt;chr&gt; &quot;Anyone else here use Mint to track thei… ## $ send_replies &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE… ## $ spoiler &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ stickied &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FA… ## $ subreddit &lt;chr&gt; &quot;fednews&quot;, &quot;fednews&quot;, &quot;fednews&quot;, &quot;fednew… ## $ subreddit_id &lt;chr&gt; &quot;t5_2xy8z&quot;, &quot;t5_2xy8z&quot;, &quot;t5_2xy8z&quot;, &quot;t5_… ## $ subreddit_name_prefixed &lt;chr&gt; &quot;r/fednews&quot;, &quot;r/fednews&quot;, &quot;r/fednews&quot;, &quot;… ## $ subreddit_subscribers &lt;int&gt; 72988, 72990, 73009, 73010, 73027, 73027… ## $ subreddit_type &lt;chr&gt; &quot;public&quot;, &quot;public&quot;, &quot;public&quot;, &quot;public&quot;, … ## $ suggested_sort &lt;chr&gt; &quot;confidence&quot;, &quot;confidence&quot;, &quot;confidence&quot;… ## $ thumbnail &lt;chr&gt; &quot;self&quot;, &quot;self&quot;, &quot;self&quot;, &quot;self&quot;, &quot;self&quot;, … ## $ thumbnail_height &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ thumbnail_width &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ title &lt;chr&gt; &quot;Historical TSP Data + Mint Shutdown&quot;, &quot;… ## $ top_awarded_type &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ total_awards_received &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ treatment_tags &lt;list&gt; [], [], [], [], [], [], [], [], [], [], … ## $ ups &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 19, 1, 1, 1, 1, 1, … ## $ upvote_ratio &lt;dbl&gt; 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00… ## $ url &lt;chr&gt; &quot;https://www.reddit.com/r/fednews/commen… ## $ user_reports &lt;list&gt; [], [], [], [], [], [], [], [], [], [],… ## $ view_count &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ visited &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ whitelist_status &lt;chr&gt; &quot;all_ads&quot;, &quot;all_ads&quot;, &quot;all_ads&quot;, &quot;all_ad… ## $ wls &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, … ## $ link_flair_template_id &lt;chr&gt; NA, NA, &quot;998fe156-1729-11e4-9ea3-12313b0… ## $ media_metadata &lt;df[,1]&gt; &lt;data.frame[26 x 1]&gt; ## $ post_hint &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ preview &lt;df[,2]&gt; &lt;data.frame[26 x 2]&gt; ## $ url_overridden_by_dest &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … Only a few columns are of interest to us. We select those columns in the next chunk, for a sample of 10000 posts. We also take note of how long it takes to read a sample of this size by noting start-time, end.time, and the difference between these. Finally we make the date-time field (created_utc) field “human readable.” posts &lt;- read_lines(&quot;data/r_fednews_posts.jsonl&quot;, n_max = 10000) start.time &lt;- Sys.time() a1 &lt;- stream_in(textConnection( gsub(&quot;\\\\n&quot;,&quot;&quot;, posts))) |&gt; select(author, title, selftext, id, link_flair_text, num_comments, ups, downs, subreddit, created_utc) |&gt; mutate (date = (as_datetime(created_utc))) |&gt; select(-created_utc) ## Found 500 records... Found 1000 records... Found 1500 records... Found 2000 records... Found 2500 records... Found 3000 records... Found 3500 records... Found 4000 records... Found 4500 records... Found 5000 records... Found 5500 records... Found 6000 records... Found 6500 records... Found 7000 records... Found 7500 records... Found 8000 records... Found 8500 records... Found 9000 records... Found 9500 records... Found 10000 records... Imported 10000 records. Simplifying... end.time &lt;- Sys.time() (end.time - start.time) ## Time difference of 7.929319 secs Looks good - the date/time field is clean, the columns are those that we want, and it only took a few seconds to read in 10,000 posts. Let’s read them all in, then take a look at a small random sample - 10 - of the texts of the posts. posts &lt;- read_lines(&quot;data/r_fednews_posts.jsonl&quot;) start.time &lt;- Sys.time() a1 &lt;- stream_in(textConnection( gsub(&quot;\\\\n&quot;,&quot;&quot;, posts))) |&gt; select(author, title, selftext, id, link_flair_text, num_comments, score, subreddit, created_utc) |&gt; mutate (date = (as_datetime(created_utc))) |&gt; select(-created_utc) ## Found 500 records... Found 1000 records... Found 1500 records... Found 2000 records... Found 2500 records... Found 3000 records... Found 3500 records... Found 4000 records... Found 4500 records... Found 5000 records... Found 5500 records... Found 6000 records... Found 6500 records... Found 7000 records... Found 7500 records... Found 8000 records... Found 8500 records... Found 9000 records... Found 9500 records... Found 10000 records... Found 10500 records... Found 11000 records... Found 11500 records... Found 12000 records... Found 12500 records... Found 13000 records... Found 13500 records... Found 14000 records... Found 14500 records... Found 15000 records... Found 15500 records... Found 16000 records... Found 16500 records... Found 17000 records... Found 17500 records... Found 18000 records... Found 18500 records... Found 19000 records... Found 19500 records... Found 20000 records... Found 20500 records... Found 21000 records... Found 21500 records... Found 22000 records... Found 22500 records... Found 23000 records... Found 23500 records... Found 24000 records... Found 24500 records... Found 25000 records... Found 25500 records... Found 26000 records... Found 26500 records... Found 27000 records... Found 27500 records... Found 28000 records... Found 28500 records... Found 29000 records... Found 29500 records... Found 30000 records... Found 30500 records... Found 31000 records... Found 31500 records... Found 32000 records... Found 32500 records... Found 33000 records... Found 33500 records... Found 34000 records... Found 34500 records... Found 35000 records... Found 35500 records... Found 36000 records... Found 36500 records... Found 37000 records... Found 37500 records... Found 38000 records... Found 38500 records... Found 39000 records... Found 39500 records... Found 40000 records... Found 40500 records... Found 41000 records... Found 41500 records... Found 42000 records... Found 42500 records... Found 43000 records... Found 43500 records... Found 44000 records... Found 44500 records... Found 45000 records... Found 45500 records... Found 46000 records... Found 46500 records... Found 47000 records... Found 47500 records... Found 48000 records... Found 48500 records... Found 49000 records... Found 49500 records... Found 50000 records... Found 50500 records... Found 51000 records... Found 51500 records... Found 52000 records... Found 52500 records... Found 53000 records... Found 53500 records... Found 54000 records... Found 54500 records... Found 55000 records... Found 55500 records... Found 56000 records... Found 56500 records... Found 57000 records... Found 57500 records... Found 58000 records... Found 58500 records... Found 59000 records... Found 59500 records... Found 59651 records... Imported 59651 records. Simplifying... end.time &lt;- Sys.time() (end.time - start.time) ## Time difference of 2.785413 mins set.seed(33458) a1 |&gt; select (selftext) |&gt; slice_sample(n=10) ## selftext ## 1 ACA and IRA provided alternate funding sources for several agencies to hire new FTEs. In my agency even when there was a hiring freeze from the general “congress appropriations” hiring budget, we were allowed to bring new people on through the IRA as direct hires. The ACA is also very contentious. If those laws get repealed what happens to those employees? ## 2 [removed] ## 3 [removed] ## 4 I am a supervisor in USDA, more specifically FPAC. I have asked the higher ups for HR guidance on a problem employee (this has gone on for 3-4 years) and have received none. I have sent in numerous emails, detailing running records - the only documentation they have or considered doing anything on said employee was 3 years ago when she assaulted another employee. \\n\\nI have become excellent writing running records, I’ve learned valuable leadership and management skills when they said “maybe I am the one who needed to change.” They continue to fail to take any action. I asked for training on issues that pop up with her and nothing. I felt like I was crazy but visiting with EAP helped, the therapist was able to spot her out and personality quickly. I felt relieved. I try to stay positive but she brings down the whole office. She started bullying the other employees in another department (we’re open concept so they sit near each other) and I’m fed up. \\n\\nCan anyone point me to specific policies/ procedures from FPAC that I can utilize to track her performance and conduct? \\nYes, I keep detailed running records but I don’t know what “counts” that can be written up. \\n\\n*Please note* I am not looking for things to write her up on, employee goes through “cycles” her mood in the office is like a roller coaster, and after working with her for 4 years and 5 years of managing her, I can predict with precision what she will do next. So this outburst she had with the other employees is just the start of what’s to come. ## 5 [removed] ## 6 ## 7 If the Supreme Court overturns Humphrey’s Executor, would all civil service protections be at risk? I understand that principal officers would no longer be protected from presidential whims, but what about the rank and file? Would the overturning of Humphrey’s Executor mean that the Pendleton Act and all subsequent civil service protections are unconstitutional and void? ## 8 Other than sharing how we lost our jobs because of the illegal firings, what should we demand them to do? Argue that we should have a government shutdown next month? What emergency countermeasures, if any, do Democrats have right now that they can use? ## 9 [removed] ## 10 We are NTEU 335, the union representing 1,100 CFPB employees, and we will be rallying at HQ today to protest the attack on the CFPB and its workers, who were locked out of their offices over the weekend. Please join us!\\n\\nWhere: CFPB Headquarters, 1700 G St NW, Washington DC 20552\\n\\nWhen: 4:15 pm, TODAY, February 10\\n\\nWho: YOU!!! These 59651 posts include all of those which were available for the period from January 1, 2024 through March 24, 2025. 18.2.3 some initial observations There are a few interesting properties of the data: One is that it appears that many of the posts have been removed, including 4 out of this (very small) sample of 10. It would be interesting to compare the proportion of deleted posts to this subreddit with those of other subreddits during the same period. (I would anticipate that this proportion is much higher in this subreddit, likely because of fears of retribution that may or may not be warranted). We should keep in mind that the removed or deleted posts are unlikely to be a random sample of posts on the subreddit - they may be more angry, more polarized, and/or more easily de-anonymized. We should return to this when we consider our results. For now, we will filter out the removed posts. One way to examine this empirically would be to examine the ‘titles’ field. The removed posts still include titles, so the nature of the removed posts in comparison with the non-removed posts could be examined by using the “compare words and categories” approach on the title fields of the two sets. The second is that there are, at least in this small sample, a lot of abbreviations and acronyms - including ACA, IRA, USDA, FPAC, NTEU, CFPB, HQ, and EAP. Often, at this point in an analysis of text, we would reformat the text so that all words were lowercase (with the tolower) function. We’ll instead modify this a bit - retaining capitalization when a token is in ALL CAPS (and is more than a single character like “I” or “A”) - and making into lowercase otherwise. 18.2.4 preprocessing In this next chunk, we create a new variable called ‘epoch,’ with three values, Biden (during the Biden presidency, but before the November election, Lameduck (the period between the election and Trump’s second inauguration), and Trump. We treat the ‘lameduck’ period separately because, although Biden was President, Trump, and concerns about the coming Trump presidency, was dominant on the airwaves. In these initial comparisons, we’ll exclude that lameduck period from analysis We also take the content of the posts and unravel them, with one line per word (so that a post which had 100 words would be 100 lines in the new, tidy datafile). Call this file tidyCorpus0 (a corpus is a body of text). We then selectively set the text to lowercase (retaining uppercase only when a multi-character token, or word, is in ALL CAPS). This gives us tidyCorpus1. We also remove stopwords from the text in order to simplify and shorten the texts; acknowledging that there are often good reasons not to do this Pennebaker et al. (2014). We’ll remove numbers from the text in the same step. Call this file tidyCorpus2. Finally, we stem the text, reducing each word to its common stem, using the SnowballC library. This gives is tidyCorpus3. If we were to stem the text, we would move ahead with this file rather than tidyCorpus2. For more on stemming, please consider Hvitfeldt and Slige(2021), which is available online. set.seed(33458) a2 &lt;- a1 |&gt; filter (selftext != &quot;[removed]&quot;) |&gt; mutate (epoch = case_when ( date &lt; &quot;2024-11-06 00:00:00&quot; ~ &quot;Biden&quot;, date &gt; &quot;2025-01-20 12:00:00&quot; ~ &quot;Trump&quot;, TRUE ~ &quot;Lameduck&quot; )) tidyCorpus0 &lt;- a2 |&gt; unnest_tokens(word, selftext, to_lower = FALSE) tidyCorpus1 &lt;- tidyCorpus0 |&gt; mutate (word = case_when( # if changing a word to uppercase has no effect ... (toupper(word) == word) &amp; # amd the word is more than 1 character long, keep it as is nchar(word) &gt; 1 ~ word, # else change it to lowercase TRUE ~ tolower(word))) # tidyCorpus2 gets rid of numbers and removes stop words tidyCorpus2 &lt;- tidyCorpus1 |&gt; anti_join(stop_words) |&gt; mutate(word = gsub(x = word, pattern = &quot;[0-9]+|[[:punct:]]|\\\\(.*\\\\)&quot;, replacement = &quot;&quot;)) |&gt; filter (word != &quot;&quot;) # tidyCorpus3 takes the additional step of stemming the data tidyCorpus3 &lt;- tidyCorpus2 |&gt; mutate(word = wordStem(word)) 18.2.5 comparing the words and stems We reduce each of the the three tidy corpora (bodies of text) to simple frequency tables nwordsCorpus0 &lt;- tidyCorpus0 |&gt; count (word) |&gt; nrow() nwordsCorpus1 &lt;- tidyCorpus1 |&gt; count (word) |&gt; nrow() nwordsCorpus2 &lt;- tidyCorpus2 |&gt; count (word) |&gt; nrow() nwordsCorpus3 &lt;- tidyCorpus3 |&gt; count (word) |&gt; nrow() Whereas there were 58154 words in the initial data, this drops to 49447 when we (mostly) represent all words as lower case. It drops further to 41586 when we remove numbers and stop words, and finally to 29338 when we stem the data. 18.2.6 Construction of differential word clouds We first take the corpus and reshape it to a matrix with row names (words) and counts (one column for each group). wideCorpus2 &lt;- tidyCorpus2 |&gt; filter (epoch != &quot;Lameduck&quot;) |&gt; count(epoch, word, sort = TRUE) |&gt; pivot_wider(names_from = epoch, values_from = n, values_fill = 0) |&gt; column_to_rownames(var = &quot;word&quot;) |&gt; # filter out words that don&#39;t appear at least a few times in each group # and also removes words that are too rare to be useful for comparison mutate (mincount = pmin(Biden, Trump)) |&gt; filter (mincount &gt; 4) |&gt; select (-mincount) |&gt; as.matrix(rownames = TRUE) head(wideCorpus2,5) ## Trump Biden ## federal 7200 2346 ## employees 6351 1008 ## people 4930 1246 ## https 4801 762 ## im 3975 3281 The comparison.cloud function from the wordcloud package is used to show the greatest relative proportions of words in each group (in that function, the scale parameters will need to be adjusted to get the best visual representation). comparison.cloud(wideCorpus2, scale = c(1.5,.5), max.words=100, random.order=FALSE, rot.per = 0, use.r.layout=FALSE, family=&quot;sans&quot;, title.size=1) Many of the words which appear disproportionately in the Trump corpus (employees, https, federal, government) appear relatively neutral in emotional tone, although there are also terms such as fired and resignation here. On the Biden side, the abbreviation GS (for General Schedule, describing the level of various positions in civil service), together with position, job, HR, step, hours, and supervisor indicates that most posts refer to employment issues. Interesting, but perhaps less than we might have expected. Will an analysis of two-word strings (bigrams) provide more insight? 18.2.7 bigrams The comparison cloud illustrates differences in single-word occurrences between the Biden and Trump epochs in r/fedworkers. Here, we repeat the analysis, investigating two-word strings. Note that in order to remove stopwords and numbers, we fist combine adjacent words (tokens) into bigrams, then separate these into pairs of adjacent words, filter on each of these adjacent words, and rejoin them. tidyBigrams &lt;- a2 |&gt; filter (epoch != &quot;Lameduck&quot;) |&gt; unnest_tokens(bigram, selftext, token = &quot;ngrams&quot;, n = 2) |&gt; separate(bigram,c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) |&gt; anti_join(stop_words, by = join_by(&quot;word1&quot; == &quot;word&quot;)) |&gt; anti_join(stop_words, by = join_by(&quot;word2&quot; == &quot;word&quot;)) |&gt; mutate(word1 = gsub(x = word1, pattern = &quot;[0-9]+|[[:punct:]]|\\\\(.*\\\\)&quot;, replacement = &quot;&quot;)) |&gt; mutate(word2 = gsub(x = word2, pattern = &quot;[0-9]+|[[:punct:]]|\\\\(.*\\\\)&quot;, replacement = &quot;&quot;)) |&gt; drop_na() |&gt; filter (word1 != &quot;&quot;) |&gt; filter (word2 != &quot;&quot;) |&gt; unite(bigram, word1, word2, sep = &quot; &quot;) bigramCounts &lt;- tidyBigrams |&gt; count(epoch, bigram, sort = TRUE) |&gt; pivot_wider(names_from = epoch, values_from = n, values_fill = 0) |&gt; column_to_rownames(var = &quot;bigram&quot;) |&gt; as.matrix(rownames = TRUE) comparison.cloud(bigramCounts, scale = c(1.5,.5), max.words=100, random.order=FALSE, rot.per = 0, use.r.layout=FALSE, family=&quot;sans&quot;, title.size=1) Here, the language of the two epochs comes into clearer focus, with phrases such as deferred resignation characterizing the Trump epoch and the relatively benign pay period characterizing the Biden period. 18.2.8 categories of words In my own research, I rely on a proprietary tool, Linguistic Inquiry and Word Count. Though LIWC can be run within R on a machine that has the license for the software, here I run the program externally, then reimport it into R. write_csv(a2, &quot;data/fedworkersposts.csv&quot;) # we run LIWC outside of R here. # the code is run on the raw data files fedworkersLIWC &lt;- read_csv(&quot;data/fedworkersLIWC.csv&quot;) 18.2.9 Code for assessing LIWC effect sizes between 2 groups There are many measures, or categories, in LIWC. In order to assess the statistical significance of differecnces between the Biden and Trump epochs on LIWC, the simplest approach is to adjust for the number of comparisons using the Bonferroni correction. The number of LIWC categories is 117, so the Bonferroni p-value should be .05/117 = .000427. This chunk uses three different libraries describe function from the psych package to get means, standard deviations, and sample sizes for each LIWC category. Then it computes the effect sizes and associated statistics using the mes function from the compute.es package and makes tables for the LIWC variables with the largest effects. BonferroniP &lt;- (.05/117) # 117 is the number of LIWC categories# simple descriptives BidenEpoch &lt;- fedworkersLIWC |&gt; filter (epoch == &quot;Biden&quot;) |&gt; select(WC:OtherP) |&gt; describe() |&gt; # from the psych package select(BidenN=n, BidenM = mean, BidenSD = sd) |&gt; round(2) TrumpEpoch &lt;- fedworkersLIWC |&gt; filter (epoch == &quot;Trump&quot;) |&gt; select(WC:OtherP) |&gt; describe() |&gt; select(TrumpN=n, TrumpM = mean, TrumpSD = sd) |&gt; round(2) poststats &lt;- BidenEpoch |&gt; bind_cols (TrumpEpoch) # from the compare.es package. Effects &lt;- mes(poststats[,2], poststats[,5], poststats[,3], poststats[,6], poststats[,1],poststats[,4], level = BonferroniP, verbose = FALSE) |&gt; select(d, l.d, u.d, pval.d) LIWCSummary &lt;- poststats |&gt; bind_cols(Effects) |&gt; select(-BidenN, -TrumpN) |&gt; write_csv(&quot;data/LIWCeffects.csv&quot;) Nsignificant &lt;- Effects |&gt; mutate (sig = case_when (pval.d &lt; BonferroniP ~ 1, TRUE ~ 0)) |&gt; summarise(sum = sum(sig)) |&gt; as.numeric() BidenN &lt;- min(poststats$BidenN) TrumpN &lt;- min(poststats$TrumpN) LIWCSummary |&gt; slice_max(d, n = 10) |&gt; kable(caption = &quot;LIWC categories in r/fedworkers most associated with the Biden era&quot;) |&gt; kable_styling() %&gt;% add_footnote(paste0( &quot;Note: BidenNs &gt;= &quot;, min(poststats$BidenN), &quot; TrumpNs &gt;= &quot;, min(poststats$TrumpN))) Table 18.1: Table 18.2: LIWC categories in r/fedworkers most associated with the Biden era BidenM BidenSD TrumpM TrumpSD d l.d u.d pval.d Authentic 72.81 31.07 52.05 36.62 0.59 0.59 0.59 0 i 6.33 4.44 3.68 4.64 0.58 0.58 0.58 0 Tone 41.36 29.32 31.28 28.63 0.35 0.35 0.35 0 Dic 89.69 7.90 86.08 12.16 0.33 0.33 0.33 0 function 54.47 10.41 50.53 15.62 0.28 0.28 0.28 0 money 1.55 2.50 0.91 2.23 0.28 0.28 0.28 0 Linguistic 69.16 10.82 65.14 16.25 0.27 0.27 0.27 0 conj 6.46 3.26 5.52 3.79 0.26 0.26 0.26 0 time 5.33 4.21 4.23 4.36 0.26 0.26 0.26 0 achieve 1.67 2.02 1.23 1.87 0.23 0.23 0.23 0 Lifestyle 6.82 4.34 5.75 4.81 0.23 0.23 0.23 0 a Note: BidenNs &gt;= 9010 TrumpNs &gt;= 19068 LIWCSummary |&gt; slice_min(d, n = 10) |&gt; kable(caption = &quot;LIWC categories in r/fedworkers most associated with the Trump era&quot;) |&gt; kable_styling() %&gt;% add_footnote(paste0( &quot;Note: BidenNs &gt;= &quot;, min(poststats$BidenN), &quot; TrumpNs &gt;= &quot;, min(poststats$TrumpN))) Table 18.1: Table 18.1: LIWC categories in r/fedworkers most associated with the Trump era BidenM BidenSD TrumpM TrumpSD d l.d u.d pval.d Clout 23.33 28.67 42.22 33.36 -0.59 -0.59 -0.59 0 Culture 1.26 2.39 2.70 4.10 -0.40 -0.40 -0.40 0 we 0.46 1.29 1.14 2.34 -0.33 -0.33 -0.33 0 tone_neg 1.03 2.05 2.05 3.73 -0.31 -0.31 -0.31 0 tech 0.56 1.68 1.44 3.23 -0.31 -0.31 -0.31 0 Social 8.76 5.96 10.92 7.60 -0.30 -0.30 -0.30 0 socrefs 4.93 4.43 6.47 5.37 -0.30 -0.30 -0.30 0 affiliation 1.13 2.31 1.91 3.02 -0.28 -0.28 -0.28 0 power 1.63 2.55 2.48 3.29 -0.28 -0.28 -0.28 0 emo_neg 0.56 1.47 1.28 3.13 -0.27 -0.27 -0.27 0 a Note: BidenNs &gt;= 9010 TrumpNs &gt;= 19068 LIWCSummary |&gt; arrange(desc(d)) |&gt; head(10) |&gt; kable() BidenM BidenSD TrumpM TrumpSD d l.d u.d pval.d Authentic 72.81 31.07 52.05 36.62 0.59 0.59 0.59 0 i 6.33 4.44 3.68 4.64 0.58 0.58 0.58 0 Tone 41.36 29.32 31.28 28.63 0.35 0.35 0.35 0 Dic 89.69 7.90 86.08 12.16 0.33 0.33 0.33 0 function 54.47 10.41 50.53 15.62 0.28 0.28 0.28 0 money 1.55 2.50 0.91 2.23 0.28 0.28 0.28 0 Linguistic 69.16 10.82 65.14 16.25 0.27 0.27 0.27 0 conj 6.46 3.26 5.52 3.79 0.26 0.26 0.26 0 time 5.33 4.21 4.23 4.36 0.26 0.26 0.26 0 achieve 1.67 2.02 1.23 1.87 0.23 0.23 0.23 0 There are just over 19000 posts in the (roughly four month long) Trump epoch, and just over 9000 in the (roughly ten months long) Biden epoch. Judging from the number of posts, the r/fedworkers subreddit has been substantially more active since the Trump presidency began. In comparisons of the two “epochs,” the LIWC categories which were particularly characteristic of the Biden posts included Authentic, a summary variable indicating apparent honesty or genuineness, and I, including first-person singular pronouns (Boyd et al. 2022). For each of these, the effect size (difference in standard deviations between the two distributions), was more than .5, indicating a moderate effect. The LIWC categories which were particularly characteristic of the Trump posts included Clout, a summary category of words associated with leadership or status, followed by Culture, which includes language about politics, technology, and ethnicity, and we, second-person singular pronouns). The shift from the first-person singular to plural is what we might expect in a community of individuals subjectively under siege. Altogether, of the 117 LIWC Categories, 98 showed a statistically significant difference between the two epochs. These analyses support the argument presented in the Times article referenced above, including that the r/fedworkers subreddit had grown, in the Trump epoch, into a supportive community (shift from i to we), in which workers were sharing anxieties (LIWC categories such as emo_neg and tone_neg). 18.3 exercise: what would you do next? This chapter has provided a brief example of how to work with text from Reddit. If you were to work with these data, what would you do next? If you were to work with another dataset, what would it look like, what would you expect to find, and how would you study it? References Boyd, Ryan L, Ashwini Ashokkumar, Sarah Seraj, and James W Pennebaker. 2022. “The Development and Psychometric Properties of LIWC-22.” Austin, TX: University of Texas at Austin 10: 1–47. Hvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. Chapman and Hall/CRC. Pennebaker, James W., Cindy K. Chung, Joey Frazee, Gary M. Lavergne, and David I. Beaver. 2014. “When Small Words Foretell Academic Success: The Case of College Admissions Essays.” Edited by Qiyong Gong. PLoS ONE 9 (12): e115844. https://doi.org/10/f6z8q5. Reddit is an electronic discussion board or website which is structured as a set of communities or subreddits; each subreddit includes posts, and, typically, submitted by users (redditors).↩︎ The selection of posts rather than comments is somewhat arbitrary; one reason for examining posts is that the simple number of comments in the period under study would be relatively unwieldy.↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
