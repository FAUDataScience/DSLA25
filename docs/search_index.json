[["index.html", "Data science for the liberal arts ", " Data science for the liberal arts Kevin Lanning 2021-08-29 "],["preface.html", "preface some features of the text the book is for you", " preface This work-in-progress will ultimately serve as a textbook for introductory undergraduate courses in data sciences. No prior knowledge of computer programming is presumed, though, ideally, students will have had college algebra (or its equivalent) and an introductory course in statistics, methods, or data analysis. Data science is still a new field of study, and there are multiple approaches to teaching it and to its place in the college curriculum. This book is intended to serve courses such as the Introduction to Data Science at the Wilkes Honors College of Florida Atlantic University which, in turn, has drawn from data science classes at the universities of North Carolina, British Columbia, Duke, Maryland, Wisconsin, Stanford, BYU, Harvard, and UC Berkeley At each of these schools, the Introduction to Data Science appears, to my eyes at least, closer to Statistics than to Computer Science. But if our approach is closer to statistics than to programming, it is particularly close to statistics in its most applied and pragmatic form. The choice of statistical methods should follow from the data and problem at hand - that is, statistics should serve the needs of the user rather than dictate them (Loevinger 1957). This pragmatic focus is driving the growth of data science in industry, and it is reflected in the way data science has been taught at still other schools including Chicago, Georgia Tech, UC Santa Barbara, Princeton, UC Berkeley, at Berlins Hertie School of Governance, and in Columbias School of Journalism. some features of the text There are a number of different approaches to teaching data science. The present text includes several distinguishing features. R In a recent informal survey of introductory data science courses, I saw a pretty even split between those which begin with Python and those which begin with the statistical programming language R. This difference corresponds, very loosely, to the split noted above: Computer science based approaches to data science are frequently grounded in Python, while statistics-based approaches are generally grounded in R. Our course, like those for most of the syllabi and courses linked above, will be based in R. Reproducible science The course will provide an introduction to some of the methods and tools of reproducible science. We will consider the replication crisis in the natural and social sciences, and then consider three distinct approaches which serve as partial solutions to the crisis. The first of these is training in a notebook-based approach to writing analyses, reports and projects (using R markdown documents). The second is using public repositories (such as the Open Science Framework and, to a limited extent, GitHub) to provide snapshots of projects over time. Finally, the third is to consider the place of significance testing in the age of Big Data, and to provide training in the use of descriptive, exploratory techniques of data analysis. Good visualizations Part of Type C data science is communication, and this includes not just writing up results, but also designing data displays that incisively convey the key ideas or features in a flood of data. Well examine and develop data visualizations such as plots, networks and text clouds. More advanced topics may include maps, interactive displays, and animations. All Some of the data Its been claimed that in the last fifteen years, humans have produced more than 60 times as much information as existed in the entire previous history of humankind. (It sounds like hyperbole, but even if its off by an order of magnitude its still amazing). There are plenty of data sources for us to examine, and well consider existing datasets from disciplines ranging from literature to economics to public health, with sizes ranging from a few dozen to millions of data points. We will also clean and create new datasets. All A few of the latest tools One feature of Data Science is that it is changing rapidly. The tools, methods, data sources, and ethical concerns that face us in 2021 are different from those which shaped the field just one or two years ago. In fields that are undergoing rapid change, there is some trade-off between building expertise with existing (older) tools and trying the newer approaches. Partly because I want to equip you with skills which will not be obsolete, partly because some of these new approaches promise more accessibility, elegance, and/or power, and partly because of my own interest in staying current, well be using some of the latest packages and programs. In the last few years, Ive shifted the class from the standard R dialect (as I learned it from the Johns Hopkins-Coursera Data Science Specialization) to the Tidyverse. This year, for the first time, much of our work with the R language will take place online, using RStudio cloud, for this should help overcome some hiccups that arise when students are using different machines, operating systems, etc., and should facilitate collaboration among us as well. Well also be experimenting for the first time with the learnr package, and emphasizing other approaches to learning R, including Swirl, less. In the past, Ive used the Slack platform for messaging, communication and collaboration; this year, Ive somewhat reluctantly moved to the Canvas LMS. And, in the past, Ive recommended using dedicated markdown editors such as Typora. While I still think that these are great for some text-editing and note-taking, well do our work instead with the editor in the latest variant of RStudio on our laptops, as this allows WYSIWIG (what you see is what you get) formatting of documents - such as this one - that are intended as publication-ready texts. Well continue to use spreadsheets such as Excel or Google Sheets as well. the book is for you Its my intention that this text should serve every college student, regardless of concentration or college major. The skills and insights that you will gain in this course will help you in graduate and professional schools, will help you in your careers, and will help you in your goal of making a better world. And it will help you train the next generation of data scientists as well. References "],["data-science-for-the-liberal-arts.html", "1 data science for the liberal arts 1.1 type C data science = data science for the liberal arts 1.2 the incompleteness of the data science Venn diagram 1.3 a dimension of depth 1.4 Google and the liberal arts 1.5 data sci and TMI 1.6 discussion: what will you do with data science?", " 1 data science for the liberal arts Hochster, in (Hicks and Irizarry 2018), describes two broad types of data scientists: Type A (Analysis) data scientists, whose skills are like those of an applied statistician, and Type B (Building) data scientists, whose skills lie in problem solving or coding, using the skills of the computer scientist. This view arguably omits a critical component of the field, as data science is driven not just by statistics and computer science, but also by domain expertise: Fig 1.1 - The iconic data science Venn diagram 1.1 type C data science = data science for the liberal arts The iconic Venn diagram model of data science shown above suggests what we will call Type C data science. It begins with domain expertise in your concentration in the arts, humanities, social and/or natural sciences, it both informs and can be informed by new methods and tools of data analysis, and it includes such things as communication (including writing and the design and display of quantitative data), collaboration (making use of the tools of team science), and citizenship (serving the public good, overcoming the digital divide, furthering social justice, increasing public health, diminishing human suffering, and making the world a more beautiful place). Its shaped, too, by an awareness of the fact that the world and workforce are undergoing massive change: This puts the classic liberal arts focus of learning how to learn (as opposed to memorization) at center stage. And Type C data science is shaped, not least, by the creepiness of living increasingly in a measured, observed world. Type C data science does not merely integrate domain expertise with statistics and computing, it places content squarely at the center. We can appreciate the compelling logic and power of statistics as well as the elegance of well-written code, but for our purposes these are means to an end. Programming and statistics are tools in the service of social and scientific problems and cultural concerns. Type C data science aims for work which is not merely cool, efficient, or elegant but responsible and meaningful. 1.2 the incompleteness of the data science Venn diagram Data visualizations are starting points which can provide insights, typically highlighting big truths or effects by obscuring other, presumably smaller ones. The Venn diagram model of data science is no exception: As with other graphs, figures, and maps, it allows us to see by showing only part of the picture. What does it omit? That is, beyond statistics, computing/hacking, and domain expertise, what other skills contribute to the success of the data scientist? The complexity of data science is such that individuals typically have expertise in some but not all facets of the area. Consequently, problem solving requires collaboration. Collaboration, even more than statistical and technical sophistication, is arguably the most distinctive feature of contemporary scholarship in the natural and social sciences as well as in the private sector (Isaacson 2014). Communication is central to data science because results are inconsequential unless they are recognized, understood, and built upon; facets of communication include oral presentations, written texts and, too, clear data visualizations. Reproducibility is related to both communication and collaboration. There has been something of a crisis in recent years in the social and natural sciences as many results initially characterized as statistically significant have been found not to replicate. The reasons for this are multiple and presently contentious, but one path towards better science includes the public sharing of methods and data, ideally before experiments are undertaken. Reproducible methods are a key feature of contemporary data science. Pragmatism refers to the relevance of work towards real-world goals. These real-world goals should be informed by ethical concerns including a respect for the privacy and autonomy of our fellow humans. 1.3 a dimension of depth Cutting across these various facets (statistics, computing, domain expertise, collaboration, communication, reproducibility, pragmatism, and ethics), a second dimension can be articulated. No one of us can excel in all of these domains, rather, we might aim towards goals ranging from literacy (can understand) through proficiency (can get by) to fluency (can practice) to leadership (can create new solutions or methods). That is, we can think of a continuum of knowledge, skills, interests, and goals, ranging from that which characterizes the data consumer to the data citizen to the data science contributor. A Type C data science includes this dimension of depth as well. 1.4 Google and the liberal arts Data science is at its core empirical, and all of this rhetoric would be meaningless if not grounded in real world findings. Although it was reported in late 2017 that soft skills rather than STEM training were the most important predictors of success among Google employees, its difficult to know whether these results would generalize to a less select group. Nonetheless, there is a clear need for individuals with well-rounded training in the liberal arts in data science positions and, conversely, learning data science is arguably a key part of a contemporary liberal arts education. 1.5 data sci and TMI One difference between traditional statistics and data science is that the former is typically concerned with making inferences from datasets that are too small, while the latter is concerned with extracting a signal from data that is or are too big (D. Donoho 2017). The struggle to extract meaning from a sea of information - of finding needles in haystacks, of finding faint signals in a cacophony of overstimulation - is arguably the question of the age. It is a question we deal with as individuals on a moment-by-moment basis. It is a challenge I face as I wade through the many things that I could include in this class and these notes. The primacy of editing or selection lies at the essence of human perception and the creation of art forms ranging from novels to film. And it is a key challenge that the data scientist faces as well. 1.6 discussion: what will you do with data science? Imagine it is ten years from today. You are working in a cool job (yay). How, ideally, would data science inform your professional contributions? More proximally (closer to today) - what are your own goals for progress in data science, in terms of the model described above? References "],["getting-started.html", "2 getting started 2.1 are you already a programmer and statistician? 2.2 some best practices for spreadsheets 2.3 setting up your machine: some basic tools 2.4 a modified 15-minute rule 2.5 discussion: who deserves a good grade?", " 2 getting started We begin with a brief self-assessment, asking you to reflect on your own knowledge of data science, including the necessary-but-not-sufficient areas of computer programming and statistics. We then move to a description of some rudimentary tools that we will be using. 2.1 are you already a programmer and statistician? Regarding programming, you may know more than you think you do. Heres a simple program - a set of instructions - for producing a cup of coffee: add water to the kettle and turn it on if its morning, put regular coffee in the French press, otherwise use decaf if the water has boiled, add it to the French press, else keep waiting if the coffee has steeped for four minutes, depress (smash) piston/plunger, else keep waiting pour coffee into cup enjoy As a post-millennial student from a WEIRD culture, or Western, Educated, Industrialized, Rich Democracy (Henrich, Heine, and Norenzayan 2010), youve programmed computers, too, if only to enter a password, open an app, and upload a photo on your cell phone. Statistics is of fundamental importance, not just for understanding abstract trends, but for making decisions about everyday life. Consider the case of Susie, a college senior: Exercise 2_1 Susie is applying to two med schools. At School A, 25% of students are accepted, and at School B, 25% are accepted as well. You are Susie. Are you going to get in to at least one of these programs? What is the probability? Does your estimate depend upon any assumptions? Questions such as these are important for us. If the combined probability is low, it will likely (another probability concept) make sense for Susie to spend time, money, and energy to apply to additional programs. If the probability is higher, it may not. But problems like this are hard - our estimates of probability are frequently poorly calibrated, and combining probability estimates is challenging (see, e.g., (Tversky and Kahneman 1974), and consider taking a course in Behavioral Economics or Thinking and Decision Making to learn more). You may have worked with data in spreadsheets such as Excel or Google Sheets. Exercise 2_2 Open the Google Sheet at http://bit.ly/dslaX2_1. Save a copy and edit it, entering the following in cell B7: =SUM (B2:B6) What is the result? Now copy cell B7 to C7 What happens? Is this the result you expected? Would another approach be more useful? 2.2 some best practices for spreadsheets Spreadsheets are great tools - the first one, Visi-Calc, was the first killer app to usher in the personal computer revolution. But they have limitations as well. Best practices have been proposed for using spreadsheets in data science such as, for example, include only data (and not calculations) in spreadsheets, use what we will recognize as a tidy format in which data are in a simple rectangle (avoid combining cells and using multi-line headers), and save spreadsheets as simple text files, typically in comma-delimited or CSV format (Broman and Woo 2018). Typically, this means that each worksheet will be in a separate file, rather than in workbooks with multiple tabs. There are good reasons for these recommendations: For example, when we sort data in spreadsheets, we risk chaos, for example, only certain columns may be sorted. When we manipulate data in spreadsheets, we typically will not have a record of what was (and wasnt) changed, and this compromises the reproducibility of our work. The bottom line is that spreadsheets should generally be used to store data rather than to analyze it. But dont be a slave to this - if you are making some quick calculations that are inconsequential and/or to be used on just one occasion, working in Excel or Google Sheets is often the way to go. 2.3 setting up your machine: some basic tools Collaboration and communication are integral to data science. In the world beyond universities, the most important messaging and collaboration platform is Slack. Slack is a commercial app, but it has a free tier. Slack is handy for group work of all forms, but most students in full-time programs will be expected to use a Learning Management System such as Canvas so that all of their classes are on the same platform. Documents produced in word processors typically include invisible characters used for formatting, security, etc. These characters, when fed into a computer program, lead to unpredictable results. Markdown is a simple syntax for producing documents that are easily read by both humans and machines. Most of the documents you will produce and work with this term, including the present chapter, are and will be written in some version of Markdown. You can find an introduction to markdown syntax in Chapter 3 of (Freeman and Ross 2017). I use Typora (currently free for both Windows and Mac), but there are many alternatives. Install this or another Markdown editor on your laptop and play with it. Google Docs is free and is convenient for collaborative work. One other important feature of Google Docs is that it provides a framework for version control, a critical skill in information management. You can learn more about how to see and revert to prior versions of a project in Google Docs here. Version control can help you avoid the chaos and confusion of having a computer (or several computers) full of files that look like Chams (2012) comic: Fig 2.1: Never call anything final.doc. Version control is an important concept in data science. Collaboratively built programs and platforms, including most of the add-ons (libraries, packages) which make R so powerful, are open-source projects built by many individuals over time. For projects such as these, both the current build and its associated history are typically maintained on GitHub, a website for hosting code. When we contribute to these projects, we will first mirror the web-based GitHub site using a program on our own Macs or Windows PCs called Git, then upload our proposed changes. Keeping remote and local branches of files in sync can be challenging, however, and you will not be expected to use this technology in this class. But if you are curious, or want to learn more, an introduction to using Git and R together may be found here. 2.4 a modified 15-minute rule You will run into problems, if not here, then elsewhere. An important determinant of your success will be the balance you maintain between persistence and help-seeking. The 15-minute rule is one guideline for this balance: It has been cleverly summarized as You must try, and then you must ask. That is, if you get stuck, keep trying for 15 minutes, then reach out to others. I think that this rule is basically sound, particularly if it is applied with cognitive flexibility, social sensitivity, and reciprocity. So when you get stuck, make a note of the problem, then move to another part of your project (thats the cognitive flexibility part): This allows your problem to percolate and still make progress. When you ask others for help, ask in a way that shows an awareness of the demands on their time (social sensitivity): Part of this means that you should explain your problem in as detailed a fashion as possible - in technical terms, a reprex or reproducible example. Finally, you should be willing to provide as well as give help (reciprocity). 2.5 discussion: who deserves a good grade? In an introductory class in data science, students invariably come to class with different backgrounds. Should this be taken into account in assigning grades? That is, would it be possible (and desirable) to assign grades in a class based not just on what students know at the end of the term, but also on how much they have learned? A formal, statistical approach to this could use regression analysis. That is, one could predict final exam scores from pretest scores, and use the residuals - the extent to which students did better or worse than expected - as a contributor to final exam grades. Interestingly, there would be an unusual incentive for students on this pretest to do, seemingly perversely, as poorly as possible. How could this be addressed? Another problem with this approach is that there may be ceiling effects - students who are the strongest coming in to the class cant improve as much as those who have more room to grow. Again, how might this be addressed? Should it? References "],["welcome-to-r-world.html", "3 welcome to R world 3.1 Using RStudio cloud 3.2 Using R Studio on your laptop", " 3 welcome to R world In this chapter, well learn how to install and use R in two ways - in the cloud and on your own laptop. For each of these, well use RStudio as a front end (an integrated development environment, or IDE). 3.1 Using RStudio cloud Using the link provided to you by your instructor, set up a new account in RStudio (it is free for you). Then click on the Join Space button. It will bring you to the Welcome page. (Not much to see here as yet). Click on the hamburger menu. You should see, on the left side of the screen, a menu with three sections: Spaces, Learn, and Help. The Spaces menu will likely include three options. The first will be Your Workspace, the second will be the workspace for this class (e.g., IntroDataSciSpring21), and the third will allow you to create a New Space. Click on the class workspace, then on the first assignment. After a few seconds, youll see a screen that looks something like the following: Fig 3.1 - R Studio Cloud. Click on the assignment, and youll see that there are now four windows on the screen (note that these windows are fully customizable, so that the locations and characterizations I am giving here are just a starting point): In the upper left is the source window - the code you are working on.. This is where we will be doing much of our work, typically in R markdown documents like this one. R markdown is, not surprisingly, a flavor of markdown. Other tabs in this quadrant may include displays of your data and other scripts that you may be working on. The console, by default in the lower left quadrant, can be used to execute single lines of code. You may occasionally use the terminal tab here as well. The environment, in the upper right, includes datasets, variables, and functions that are available to us. The Build and Git tabs are useful for producing documents and version control/collaboration, respectively. The history tab is useful to go back and look at the syntax you used in a prior chunk of code. The files in our directory are listed in the lower right; additional tabs include a list of available packages (libraries) and some resources for help. Now read the code in the console, and follow the instructions to complete your first homework assignment. Congratulations, youve just run your first code in R. 3.2 Using R Studio on your laptop Install the latest version of R on your own Windows or Mac laptop. Then install RStudio Desktop. Feeling ambitious? Once this is loaded, (a) set up a directory for this class, then (b) a subdirectory you can call Assignment01. Go back to your Rstudio.cloud / Assignment01, and check the boxes next to Assiignment01.Rmd, movies.Rdata, and project.rproj. Then click on More, then Export and put these in your new subdirectory. Now go back to your desktop environment, and try to run the same code on your home machine. (Note that to be successful, you will need to first execute the command on your laptop). install.packages(tidyverse) Get as far as you can, but remember the 15 minute rule. How far did you get? Be prepared to share your frustrations, discoveries, and accomplishments with your classmates. "],["r-stands-for.html", "4 R stands for  4.1 a few characteristics of R 4.2 finding help 4.3 Wickham and R for Data Science", " 4 R stands for  Historically, R grew out of S which could stand for Statistics. But what does R stand for? R is a system for Reproducible analysis, and reproducibility is essential. R markdown documents, like Jupyter notebooks in the Python world, facilitate reproducible work, as they include comments or explanations, code, links to data, and results. R is for Research. Research is not just an end-product, not just a published paper or book:  these documents are not the research [rather] these documents are the advertising. The research is the full software environment, code, and data that produced the results (Buckheit and Donoho 1995; D. L. Donoho 2010, 385). When we separate the research from its advertisement we are making it difficult for others to verify the findings by reproducing them (Gandrud 2013). R is a system for Representing data in cool, insight-facilitating ways, a tool for creating (reproducible) data visualizations which can provide insights and communicate results. R is Really popular, and this matters, because learning R will make you a more attractive candidate for many graduate programs as well as jobs in the private sector. Because R is popular, there are many Resources, including, for example - Online resources include the simple (and less simple) lessons of SwirlR, which offers the possibility of learning R in R, as well as DataCamp, the Data Science Certificate Program at Johns Hopkins, and other MOOCs. Books include (Peng 2014) - which includes not only videos of his lectures in the program at Hopkins, but also a brief list of still more resources - and (Wickham and Grolemund 2016). Youll also learn (more directly) from people, including your classmates, as well as the broader community of people around the world. There are hundreds if not thousands of people, young and old, who are on the road with you. I am as well, just a step or two (hopefully) ahead. R might stand for Relatively high level. Programming languages can be described along a continuum from high to low level, the former (like R) are more accessible to humans, the latter (like assembly language) more accessible to machines. Python, Java, and C++ are all more towards the middle of this continuum. R does not stand for [arggh](https://www.urbandictionary.com/define.php?term=ARGH), although you may proclaim this in frustration (arggh, why cant I get this to work?) or, perhaps, in satisfaction (arggh, matey, that be a clever way of doing this). But R does stand for Rewarding. A language is a way of thinking about the world, and this is true for computer languages as well. Youll be challenged by its complexity, its idiosyncracy, its alien logic. But you will succeed, and you will find that you can do things that you did not believe possible. 4.1 a few characteristics of R R includes the base together with packages. These packages (libraries) are customized add-ons which simplify certain tasks, such as text analysis. There are, at this writing, 15,373 available packages on the CRAN package repository - and though there is not yet an R package for ordering pizza (Peng 2014), there are many for most data tasks, including, for example, over 50 different packages for text analysis. So how do you choose, and where do you begin? We will start with the curated list of packages which jointly comprise the tidyverse (Wickham and Grolemund 2016), which is effectively a dialect of R. R is an object-oriented language - one conceptually organized around objects and data rather than actions and logic. In R, at the atomic level, objects include characters, real numbers, integers, complex numbers, and logical. These atoms are combined into vectors, which generally include objects of the same type [one kind of object, lists, is an exception to this; Peng (2014)]. Vectors can be further combined into data frames, which are two-dimensional tables or arrays. A tibble is a particular type of data frame which is used in the tidyverse. It is, in some ways, handier to work with than other data frames. Well be working extensively with data frames in general, and tibbles in particular, as we move forward. Objects have attributes. Attributes of R include such things as name, dimensions (for vectors and arrays), class (thats the type of object described in the previous paragraph), length, etc. Real world data sets are messy, and frequently have missing values. In R, missing values may be characterized by NA (not available) or NaN (not a number, implying an undefined or impossible value). RStudio, the environment we will use to write, test, and run R code, is a commercial enterprise whose business model, judged from afar, is an important one in the world of technology. Most of what RStudio offers is free (97% according to Garrett Grolemund in the video below). The commercial product they offer makes sense for a relative few, but it is sufficiently lucrative to fund the enterprise. The free product helps to drive the popularity of Rstudio; this widespread use, in turn, makes it increasingly essential for businesses to use. This mixed free/premium, or freemium, model characterizes Slack as well, but while the ratio of free to paid users of Slack is on the order of 3:1, for R it is, I am guessing, an order of magnitude higher than this. 4.2 finding help One does not simply learn R. Unlike, say, learning to ride a bicycle, fry an egg, or drive a car with a manual transmission, learning R is not a discrete accomplishment that one can be said to have mastered and from which one then moves on. Rather, R is an evolving, open system of applications and tools which is so vast that there is always more that one can achieve, new lessons that one can learn. And, the complexity of R syntax is such that, for almost all of us, we will need help for coding on any non-trivial task. For us, the key ideas in looking for help will include not just the tools on the RStudio IDE, but also (a) using Google searches wisely, and (b) reaching out to your classmates on Slack. Here, as in the real world, there is an etiquette for help-seeking which is based on consideration. Your search for help should begin by making sure that others will encounter the same result, then by stripping the problem down to its essence. Once you have reduced the problem to this minimal, reproducible essence, you will often be able to spot the problem yourself - and, if not, you will make it easier for others to help you. There is an R package (reprex) which will likely facilitate this, but I havent tried it yet. Here is a good introduction. Again, remember the 15 minute rule - we all need somebody to lean on. Finally, to get a sense of some of the ways you can get help in RStudio (and to see how a master uses the R Studio interface), consider this video: video 4.1: Garrett Grolemund of RStudio 4.3 Wickham and R for Data Science The first chapter of the Wickham text (Wickham and Grolemund 2016) provides both a framework for his approach and a brief introduction to the tidyverse which will be the dialect of R we will study in the weeks ahead. Please read it now. References "],["now-draw-the-rest-of-the-owl.html", "5 now draw the rest of the owl 5.1 Review the RStudio cloud primers 5.2 Play with RStudio on your laptop 5.3 Take a DataCamp class 5.4 Swirl (Swirlstats) 5.5 Read Pengs text and/or watch the associated videos 5.6 Code along with a tidy webinar, or attend a virtual conference 5.7 Something else", " 5 now draw the rest of the owl Fig 5.1: Draw the rest of the owl. There are many sources for learning the basics of R. A few of these follow. Please spend at least 90 mins exploring at least two of the following. Be prepared to discuss your progress next class (you will be asked which source(s) you used, what you struggled with, and whether you would recommend it to your classmates. (Note that all of these are free, though you may choose to make a donation to the author if you use the Peng text). Hint: If you find the material too challenging - if you feel like you are drawing the rest of the owl - take a break away from your machine and other screens, clear your head, then try a different approach. 5.1 Review the RStudio cloud primers A series of up-to-date, interactive tutorials may be found for working with RStudio.cloud here. There are two basic tutorials - one on data visualization, the other on programming. 5.2 Play with RStudio on your laptop Last week, we began exploring Mine Çetinkaya-Rundels movies dataset in RStudio.cloud. Were you successful in getting this to run on your laptop as well? 5.2.1 Create a new R Markdown document and knit it to a PDF or a Word doc In literate programming, comments, code, and results are integrated in a clear and reproducible way - they document our work. R markdown is built around this idea. To create an R markdown (Rmd) document in Rstudio (on your laptop) or Rstudio.cloud, begin with (File -&gt; New File -&gt; R Markdown). A window will open up with a file that begins with some YAML (Yet Another Markdown Language). You can edit this as needed: --- title: &quot;Your movies in R Markdown Document&quot; author: &quot;Frankie McFrank Frank&quot; date: &quot;1/27/2020&quot; output: html_document --- Go ahead and click on the clever knit icon in the bar just above the source window to create a sample document. Youll need to save the file with a new name, then R will finally create an HTML (Hyper Text Markdown Language - see the pattern?) page. Compare the R Markdown document (your code) with the result (the HTML). Now, try to knit the document into different output formats (click on the down arrow next to knit, and see if you can knit to PDF and Word). It may take a while the first time you do this, as you may need to load some additional formatting tools or packages (e.g., TinyTex) to successfully render these files. Can you knit directly to a PDF document on your laptop? A Word document? If you struggle, you are not alone here - an advantage of RStudio.cloud over RStudio on the desktop is that, on your own machine, you may run into system-specific problems associated with the pandoc document converter. If, after spending 15 minutes on this problem, you cannot knit directly to a PDF, stop and think about the problem. 5.2.2 Play with and explore the movies data Iain Carmichael used these data to introduce R for his students several years ago; you may wish to use his code as a starting point. Continue to ask interesting questions about the data - can you see any way in which movies appear to have changed over time? What does spilled mean? Write up your results as an R markdown document. 5.3 Take a DataCamp class Many folks swear by (and others, I presume, at) DataCamp, which kind of gamifies learning software. As a student in this class, you have access to all of their stuff free. You can even do lessons on your phone. Use the link given to you in class to enroll. 5.4 Swirl (Swirlstats) I, like thousands of others, learned R in the process of completing the Johns Hopkins Data Science Specialization offered through Coursera. The sequence can be challenging, but their introduction to R used an accessible, interactive R package called Swirl. You can read about swirl (learn R in R) at https://swirlstats.com/. Using Swirl. After loading R (and opening R studio), you will get to the Swirl lessons with the following steps: Install the Swirl package on your computer (you only need to do this once). Type the following into your console window in R studio (typically left hand side of your screen or lower left) install.packages(swirl) Then load the package into your workspace (youll need to do this at the beginning of every session you use Swirl) library (swirl) Then run it! swirl () Swirl will ask a few questions then give you the option of choosing one of several courses. Youll choose the R Programming option, which leads to 15 separate lessons. At the end of each lesson, youll be asked Would you like to receive credit for completing this course on Coursera.org? Answer no then do another lesson. 5.5 Read Pengs text and/or watch the associated videos Finally, consider the text and videos from the Coursera R class. Most of the material from that class can be found in the Peng text (2014). A slightly updated version of the text can be found at https://bookdown.org/rdpeng/rprogdatascience/, and the videos in the series may be found by clicking on the following: . Video 5.2: Roger Peng introducing R 5.6 Code along with a tidy webinar, or attend a virtual conference Among these resources for R studio beginners, you will find Thomas Mocks introduction to Tidy statistics in R. It includes a link to the code used in the seminar at the bottom of the launch screen. The second chapter of Healys online book about Data visualizations provides an introduction to R and R studio which largely parallels the discussions here and in the Wickham and Grolemund text (Healy 2017; Wickham and Grolemund 2016) The Grolemund video linked in the prior chapter is part of a series of videos on programming in RStudio. Its a little out of date now (from 2015), but still likely to be useful. Most of the offerings at the Rstudio::global conference are aimed at more experienced R users, some are aimed at (relative) noobs, including Learning R with Humorous Side Projects. The 2021 conference, to be held on January 21, 2021, is free; you can enroll and explore here 5.7 Something else The something else category includes Datacarpentry.org, which is aimed at fostering data literacy and provides free lessons for learning and applying R in areas such as Ecology, Genomics and Geospatial data analysis. Of particular interest are the social science lessons, as well as a workshop in data science based on the \"Studying African Farmer-led Irrigation (SAFI)\" dataset. References "],["principles-of-data-visualization.html", "6 principles of data visualization 6.1 some opening thoughts 6.2 some early graphs 6.3 Tukey and EDA 6.4 approaches to graphs 6.5 Tufte: first principles 6.6 the politics of data visualization 6.7 the psychology of data visualization 6.8 further reading and resources", " 6 principles of data visualization 6.1 some opening thoughts Graphs arent just to inform, but to make you reflect. We are concerned not just with the design of graphs, but with our role in society as leaders, as arbiters of facts, as educators. We want to foster data literacy. How do people understand graphs? How can you use graphs to tell people the truth in a way that they understand? How can you convey uncertainty in a graph? When you see a graph, what do you notice, what do you wonder, and what is the story? Is story telling what visualizations should be about? A graph might begin with perception and understanding (the consumer), with knowledge and design values (the producer), but it also reflects the truth of the data. How much is each? 6.2 some early graphs Visual displays of information reach back to prehistoric times; modern graphs date back, arguably, to Playfairs 1786 Political Atlas - in which  spatial dimensions were used to represent nonspatial, quantitative, idiographic, empirical data. Although it now seems natural to represent, for example, rising and falling imports over time as a rising and falling line, it does not seem to have been done before that time and was quite an accomplishment. Notably, in addition to the statistical line chart, Playfair at one fell swoop single-handedly invented most of the remaining forms of the statistical graphic repertoire used today-the bar chart and histogram, the surface chart, and the circle diagram orpie chart\" (H. Wainer and Thissen 1981). Fig 6.1: Playfairs 1786 analysis of trade deficits The most celebrated early graph is that of Minard: Fig 6.2: Minards display of Napoleons catastrophic assault on Moscow, 1812 The visualization depicts the size, latitude, and longitude of Napoleons army as they moved towards (tan line) then away (black line) from Moscow; temperature during the retreat is plotted as well. Further, vertical lines linking the temperature display to the number of troops indicate the often perilous river crossings which further decimated Napoleons troops). Cheng (2014) decomposes the graph and provides some simpler visualizations; she also provides the following background: \"Czar Alexander of Russia sees that Napoleon was becoming too powerful, so he refuses to participate in this embargo [against the UK]. Angry at Czar Alexanders decision, Napoleon gathers a massive army of over 400,000 to attack Russia in June of 1812. While Russias troops are not as numerous as Frances, Russia has a plan. Russian troops keep retreating as Napoleons troops move forward, burning everything they pass, ensuring that the French forces could not take anything from their environment. Eventually the French army follows the Russian army all the way to Moscow during October, suffering major losses from lack of food. By the time Napoleon gets to Moscow, he knows he has to retreat. As winter settles into Europe and the temperature drops, Napoleons troops suffer even more losses, returning to France from lack of food, disease, and weather conditions.\" Of course, the casualties and retreat of Napoleons army are immortalized not just in this graph, but also in Russian literature (Tolstoys War and Peace) and music (Tchaikovskys 1812 overture, in which five cannon shots mark the battle of Borodino and eleven more mark the arrival of Napoleon in the now-razed city of Moscow). 6.3 Tukey and EDA For (D. Donoho 2017), the publication of John Tukeys Future of Data Analysis (Tukey 1962) arguably marks the beginning of data science. As the first data scientist, Tukey embraced a descriptive and exploratory approach to data analysis, particularly in his publication of Exploratory Data Analysis (Tukey 1977). In EDA, Tukey presented an idiosyncratic, coherent approach to looking at data, beginning with tallying. The distributions of small counts of single variables, for Tukey, could best be presented in stem and leaf displays. Comparisons between groups can be presented in box plots. To examine relationships between variables and the adequacy of simple models, he argues for thoughtfully transforming data to uncover linear trends, then to examine residuals to find where these trends do not hold. 6.4 approaches to graphs A graph might begin with perception and understanding (the consumer), with knowledge and design values (the producer), but it also reflects the truth of the data. How much is each? In thinking about how to design graphs, we can begin with abstract theory, with principles of design informed by our understanding of perception, or with empirical analyses of understanding and memory. 6.5 Tufte: first principles (Tufte 2001) describes Graphical Excellence. Graphs should, among other things, Induce the viewer to think about the substance, rather than about methodology, graphic design, the technology of graphic productions, or something else. Graphs should Present many numbers in a small space, make large data sets coherent, and encourage the eye to compare different pieces of data. Graphs should serve a reasonably clear purpose: description, exploration, tabulation, or decoration [and] be closely integrated with the statistical and verbal descriptions of a data set. Tufte concludes with the following Principles of Graphical Excellence, which I quote verbatim: Graphical excellence is the well-designed presentation of interesting dataa matter of substance, of statistics, and of design. Graphical excellence consists of complex ideas communicated with clarity, precision and efficiency. Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space. Graphical excellence is nearly always multivariate. And graphical excellence requires telling the truth. 6.6 the politics of data visualization On a cold morning in January, 1986, Space Shuttle Challenger lifted off from Cape Canaveral. Because of the cold weather, engineers at Morton Thiokol, who designed the rocket boosters, considered the possibility that the O-rings which sealed joints on the rockets would be too hard and brittle to prevent the release of potentially explosive fuel. They examined the relation between temperature and o-ring damage on prior flights, using the following visualization: Figure 6.3: What Morton Thiokol engineers saw prior to deciding to launch the Challenger Space Shuttle in January, 1986 What, if anything, can we see here? The graphic was chaotic and poorly designed; if the engineers had wanted to systematically explore the relation between o-ring damage and temperature they could have removed the rockets and arranged them on a single axis by liftoff-temperature rather on several rows by liftoff-date. Heres what they would have seen: Figure 6.4: What the engineers could have seen, perhaps, with a better graph. The forecast for the morning of the launch was far colder than that for prior launches, and within the set of prior launches a clear relationship between temperature and o-ring damage is evident. But the Challenger did launch, exploding shortly after takeoff, killing the seven astronauts on board. With a clearer display of the data, the Challenger launch would likely have been postponed (Tufte 2001). 6.6.1 poor design leads to an uninformed or misinformed world In the flood of information that surrounds us, our peripheral and sometimes our focal attention will be drawn to pictorial summaries of consequential events and effects. But while data visualizations and data journalism has become more responsible, poorly designed graphs - what Tufte has described as chartjunk - are still common. Consider the following graph, which recently appeared in a number of newspapers or feeds in the United States (including the Palm Beach Post, on January 12, 2019). Figure 6.5: Which smartphone manufacturers are doing well? Exercise 6.1 Examine the graph shown above. Look at it for just a moment, as if you might while reading something else on the screen, or as your ten-year old little sister might. What does the graph tell you? Look at it more closely. What does it actually say? How could you improve it to make the content of the graph more informative about its title? Why was the graph designed in this way? Does this matter? Poorly designed graphs dont just confuse rocket scientists. They reach us, mislead us, and make fools of all of us as well. With better graphs, we become a better informed, better educated citizenry. 6.6.2 poor design can be a tool to deceive Figure 6.6: Trump as the first datavis President (Meeks, 2019). The apps on our smartphones and electronic devices are meticulously designed to maintain our attention. They are largely successful, but, for many of us, made it more difficult to read in depth (Wu, The Attention Merchants). With information all around us demanding our attention, visual representations of data have become particularly important. The data visualization is, increasingly, not just a supplement to the story, but the story itself. When this premise is coupled with the idea that the truth is malleable, the product of what our leaders say rather than empirical statements of fact, we approach a dangerous situation, one in which real threats are misrepresented and lives are again endangered. Presenting information in self-promoting ways includes so-called Sharpie-gate, where President Trump simply altered a hurricane prediction map in defense of a misstatement. It also includes more subtle misrepresentations: Does stand-your-ground really make us safer? Figure 6.7: Stand your ground makes us all safer. Or not. 6.7 the psychology of data visualization Speaking of America, consider the following: Figure 6.8: Chernoffs too-clever faces In this figure, from (H. Wainer and Thissen 1981), (Chernoffs) faces are used to represent multivariate data. The display is loosely based on the (psychological) premise that, from infancy, we are inordinately sensitive to facial features. Consequently, the most powerful, effective, or informative data displays should manipulate facial features. Does this graph succeed? Why or why not? Would an updated model, based on richer, more complex, and more realistic simulations of facial expressions (Thies et al. 2015) be more successful? 6.7.1 the power of animation Animated data displays bring the dimension of time into data visualization. Here are two brief (&lt; 5 minutes) animated data displays, each powerful in its own way, and each of which gives the viewer an appreciation of an important social phenomenon. The first is from the late Hans Rosling, and summarizes a chunk of the Gapminder data. There is a Gapminder package in R if you would like to explore the data further, and perhaps become the next Rosling: Video 6.9: Rosling and social progress The second is from Kim Rees and her ex-colleagues at Periscopic (Rees is now at CapitalOne). For me, its an important graphic because it tries to overcome what has been called psychic numbing - we are sensitive to the individual case, but lose our perspective when faced with large-scale tragedies, so that effectively the more lives lost the less we care (Slovic et al. 2013). Video 6.10: Rees and stolen years 6.7.2 telling the truth when the truth is unclear We live in an uncertain world, and a major challenge in data visualization is how to convey this uncertainty in a way that people understand readily. Examples of this are familiar. Here in Florida, projected hurricane paths are frequently represented by a cone of uncertainty surrounding a single track. An alternative approach is to use a display which shows a distribution of possible tracks. Figure 6.11: Two approaches to displaying hurricane paths 6.7.3 visualizing uncertainty To further illustrate the idea of uncertainty, data visualizations may be animated. To display hurricane outcomes, for example, individual lines in a spaghetti plot might fade in and out (cox2013a?). Another use of animation is suggested by (Hullman, Resnick, and Adar 2015) who use hypothetical outcome plots rather than simple error bars to illustrate sampling distributions. Empirically, subjects have a better understanding of the meaning of statistical effects when these are represented by dynamic displays. During the 2016 US Presidential Election, the NY Times graphics presented the range of likely possibilities (here, between the 25th and 75th percentiles) as a dynamically changing jittery gauge . Following the election, there was some criticism of these gauges, as readers claimed that they tended to make them anxious. However, it appears that at least some of the anxiety was attributable not to the gauges themselves, but to the electoral outcome itself. The gauges were back in 2018, and will likely be used again in the future. Finally, animation may be used not just to display uncertainty or a possible range, but also data displays which both carry a great deal of information and unfold over time, such as the language of personality development in my own work. 6.8 further reading and resources If youd like to learn more, (Tufte 2001) and his other books are beautiful and thought provoking. (Cleveland and McGill 1985) examines graphs from a more rigorous psychological and empirical viewpoint. The Data Stories podcasts are often excellent, despite the challenge of an auditory medium for visual display (especially the episode on Hans Rosling). And (Healy 2018) provides a comprehensive introduction that, in essence, provides the basis for a semester-long course based in the ggplot2 package in R that we will be working with in the next chapter (Wickham and Grolemund 2016). Finally, several years ago Alberto Cairo of the University of Miami offered an online MOOC (massive open online course) on data visualizations. You can find the course materials here. References "],["visualization-in-r-with-ggplot.html", "7 visualization in R with ggplot 7.1 a picture &gt; (words, numbers)? 7.2 Read Hadley ggplots 7.3 exploring more data", " 7 visualization in R with ggplot In the last chapter, we introduced data visualization, citing vision-aries including Edward Tufte and Hans Rosling, inspired works such as Minards Carte Figurative and Periscopics stolen years, as well as a few cautionary tales of misleading and confusing graphs. Here, in playing with and learning the R package ggplot, we begin to move from consumers to creators of data visualizations. As the first visualization in (Wickham and Grolemund 2016) reminds us, data visualization is at the core of exploratory data analysis: Fig 7.1: Data visualization is at the core of data analysis ((Wickham and Grolemund 2016)) In the world of data science, statistical programming is about discovering and communicating truths within your data. This exploratory data analysis is the corner of science, particularly at a time in which confirmatory studies are increasingly found to be unreproducible. Most of your reading will be from Chapter 3 of (Wickham and Grolemund 2016), this is intended only as a supplement. 7.1 a picture &gt; (words, numbers)? The chapter begins with a quote from John Tukey about the importance of graphs. Yet there is a tendency among some statisticians and scientists to discount graphs, to consider graphic representations of data as less valuable than statistical ones. It is true that, because there are many ways to graph data, and because scientists and data journalists are humans with pre-existing beliefs and values, a graphical displays should not be assumed to simply depict a singular reality. But the same can be said about statistical analyses (see Chapter 8). To consider the value of statistical versus graphical displays, consider Anscombes quartet (screenshot below, live at http://bit.ly/anscombe2019): Table 7.1: An adaptation of Anscombes quartet (Anscombe 1973) Exercise 7_1 Consider the spreadsheet chunk presented above, which I am characterizing as data collected on a sample of ten primary school children at recess on four consecutive days. Working with your classmates, compute the mean, standard deviation, and correlation between the two measures for one day. Share your results with the class. The four pairs of variables in (Anscombe 1973) appear statistically the same, yet the data suggest something else. Later, well try to plot these. Perhaps graphs can reveal truths that statistics can hide. Exercise 7_2 The anscombe data is included as a library in R. Can you find, load, and explore it? 7.2 Read Hadley ggplots In class, we will review and recreate the plots in section 3.2 of (Wickham and Grolemund 2016) and exercises through 3.4. Savor this section, reading slowly, and playing around with the RStudio interface. For example, read about the mpg data in the help panel, pull up the mpg data in a view window, and sort through it by clicking on various columns. Fig. 7.2: A screenshot from RStudio, showing the mpg dataset 7.3 exploring more data Explore the Gapminder data https://cran.r-project.org/web/packages/gapminder/README.html Choose one of the datasets in R, pull out a few variables, and explore these. Try to make a cool graph - one that informs the viewer, and, to paraphrase Tukey, helps us see what we dont expect. Try several different displays. Which fail? Which succeed? Be prepared to share your efforts. Dont be afraid to screw up. Each mistake you wisdom. References "],["examining-local-covid-data-in-r.html", "8 examining local COVID data in R 8.1 tracking the Novel Coronavirus (from Feb 2020) 8.2 plotting confirmed cases (Feb-Mar 2020) 8.3 status (Feb 2021) 8.4 how to create new knowledge", " 8 examining local COVID data in R In Winter 2020, as the COVID-19 pandemic began to take hold, I began chronicling the course of the virus. What follows is an archive, followed by a consideration of some possible new directions. 8.1 tracking the Novel Coronavirus (from Feb 2020) Here, I want to consider a timely (but challenging) dataset. The Novel Coronavirus is an emerging health crisis, particularly in Wuhan (a Chinese city larger than New York City) and the surrounding province of Hubei. It is not yet a threat in the United States - there have, at this writing (02/11/20) been zero cases in Florida. Still, tracking the spread of the virus - the unfolding number of people infected and recovered, as well as the number of deaths - is a fascinating exercise. This is an educational script for students learning R with the Tidyverse. It reads data provided by the Johns Hopkins Center for Systems Science and Engineering (JHU/CSSE). It was modified February 3 because of a new GoogleSheet link and altered variable names, on Feb 5 because of a new URL for the data and additional changes in the variable name for date, and Feb 7 to (a) remove need for OAuth and (b) separate Wuhan from other China. On Feb 9, additional data cleaning was performed and interactive plots were added. On February 11, the code was rewritten to read files from a Github repo rather than Google Sheets. Consequently, this does not use an API or require authorization from Github. # get list of files filelist &lt;- GET(&quot;https://api.github.com/repos/CSSEGISandData/2019-nCoV/git/trees/master?recursive=1&quot;) %&gt;% content() %&gt;% # there is probably a more efficient way to reduce this # list to a set of filenames flatten() %&gt;% map (&quot;path&quot;) %&gt;% flatten() %&gt;% tibble() %&gt;% rename(filename = 1) %&gt;% filter(str_detect(filename,&quot;.csv&quot;) &amp; str_detect(filename,&quot;daily&quot;)) nsheets &lt;- nrow(filelist) rawGitFiles &lt;- &quot;https://raw.githubusercontent.com/CSSEGISandData/2019-nCoV/master/&quot; 8.1.1 reading the data (Feb 2020) The Novel Coronavirus data consists of a series of csv files in a Github repository. This combines them into a single sheet in R. # variables to retain or create numvars &lt;- c(&quot;Confirmed&quot;, &quot;Deaths&quot;, &quot;Recovered&quot;) varlist &lt;- c(&quot;Province/State&quot;, &quot;Country/Region&quot;, &quot;Last Update&quot;, numvars) # one cool trick to initialize a tibble coronaData &lt;- varlist %&gt;% map_dfr( ~tibble(!!.x := logical() ) ) # add data from files to tibble for (i in 1:nsheets) { j &lt;- read_csv(paste0(rawGitFiles,filelist$filename[i])) # if a variable doesn&#39;t exist in sheet, add it j[setdiff(varlist,names(j))] &lt;- NA # datetime is formatted inconsistently # across files, this must be done before merging j %&lt;&gt;% mutate(`Last Update` = parse_date_time(`Last Update`, c(&#39;mdy hp&#39;,&#39;mdy HM&#39;, &#39;mdy HMS&#39;,&#39;ymd HMS&#39;))) %&gt;% select(varlist) coronaData &lt;- rbind(coronaData, j) } head(coronaData) ## # A tibble: 6 x 6 ## `Province/State` `Country/Region` `Last Update` Confirmed Deaths ## &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Anhui Mainland China 2020-01-21 22:00:00 NA NA ## 2 Beijing Mainland China 2020-01-21 22:00:00 10 NA ## 3 Chongqing Mainland China 2020-01-21 22:00:00 5 NA ## 4 Guangdong Mainland China 2020-01-21 22:00:00 17 NA ## 5 Guangxi Mainland China 2020-01-21 22:00:00 NA NA ## 6 Guizhou Mainland China 2020-01-21 22:00:00 NA NA ## # ... with 1 more variable: Recovered &lt;dbl&gt; str(coronaData) ## tibble [2,062,514 x 6] (S3: tbl_df/tbl/data.frame) ## $ Province/State: chr [1:2062514] &quot;Anhui&quot; &quot;Beijing&quot; &quot;Chongqing&quot; &quot;Guangdong&quot; ... ## $ Country/Region: chr [1:2062514] &quot;Mainland China&quot; &quot;Mainland China&quot; &quot;Mainland China&quot; &quot;Mainland China&quot; ... ## $ Last Update : POSIXct[1:2062514], format: &quot;2020-01-21 22:00:00&quot; &quot;2020-01-21 22:00:00&quot; ... ## $ Confirmed : num [1:2062514] NA 10 5 17 NA NA NA NA 1 NA ... ## $ Deaths : num [1:2062514] NA NA NA NA NA NA NA NA NA NA ... ## $ Recovered : num [1:2062514] NA NA NA NA NA NA NA NA NA NA ... 8.1.2 cleaning (wrangling, munging) the data (Feb 2020) Cleaning the data includes not just finding errors, but adapting it for our own use. Its generally time consuming, as was the case here. The following letters refer to sections of the code below. a - fix a few missing values outside of China for province and country b - the earliest cases, all in China, did not include country c - because province/state is included inconsistently, an unambiguous place variable is created d - reportdate is standardized (above) and renamed e - in some cases, multiple reports are issued for each day. only the last of these is used for each place. f - for dates where no data was supplied, the most recent (previous) data are used g - values of NA for Deaths, Confirmed, and Recovered cases are replaced by zero. h - Prior to Feb 1, 2020 reporting for US included only state, since then, city and state. This drops the (duplicated) province/state-only values beginning Feb 1. coronaData &lt;- coronaData %&gt;% # a mutate (`Province/State` = case_when( (is.na(`Province/State`) &amp; (`Country/Region` == &quot;Australia&quot;)) ~ &quot;New South Wales&quot;, (is.na(`Province/State`) &amp; (`Country/Region` == &quot;Germany&quot;)) ~ &quot;Bavaria&quot;, TRUE ~ `Province/State`)) %&gt;% mutate (`Country/Region` = case_when( `Province/State` == &quot;Hong Kong&quot; ~ &quot;Hong Kong&quot;, `Province/State` == &quot;Taiwan&quot; ~ &quot;Taiwan&quot;, `Province/State` == &quot;Washington&quot; ~ &quot;US&quot;, # b is.na (`Country/Region`) ~ &quot;Mainland China&quot;, TRUE ~ `Country/Region`)) %&gt;% # c mutate(place = ifelse(is.na(`Province/State`), `Country/Region`, paste0(`Province/State`,&quot;, &quot;, `Country/Region`))) %&gt;% mutate(reportDate = date(`Last Update`)) %&gt;% group_by(place,reportDate) %&gt;% # e slice(which.max(`Last Update`)) %&gt;% ungroup() %&gt;% # fill in missing dates for each place for time series # f group_by(place) %&gt;% complete(reportDate = seq.Date(min(reportDate), today(), by=&quot;day&quot;)) %&gt;% fill(c(Confirmed,Deaths,Recovered, `Country/Region`,`Province/State`)) %&gt;% # g ungroup() %&gt;% mutate_if(is.numeric, ~replace_na(., 0)) %&gt;% # h mutate(dropcase = ((!str_detect(`Province/State`,&quot;,&quot;)) &amp; (reportDate &gt; &quot;2020-01-31&quot;) &amp; (`Country/Region` == &quot;Canada&quot; | `Country/Region` == &quot;US&quot;))) %&gt;% # dplyr called explicitly here because plotly has taken over &#39;filter&#39; dplyr::filter (!dropcase) %&gt;% select(-c(`Last Update`,`Province/State`,`Country/Region`,dropcase)) head(coronaData) ## # A tibble: 6 x 5 ## place reportDate Confirmed Deaths Recovered ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 2020-02-24 1 0 0 ## 2 Afghanistan 2020-02-25 1 0 0 ## 3 Afghanistan 2020-02-26 1 0 0 ## 4 Afghanistan 2020-02-27 1 0 0 ## 5 Afghanistan 2020-02-28 1 0 0 ## 6 Afghanistan 2020-02-29 1 0 0 write_csv(coronaData,&quot;coronaData.csv&quot;) 8.1.3 eleven months later: the code still runs! The above code runs without apparent error, and leads to a dataset of 245095 rows by 5 columns. Heres a quick peek (Note that I use the group_by and summarize functions to collapse the file to one line per date): coronaData %&gt;% group_by(reportDate) %&gt;% summarize(Confirmed = sum(Confirmed)) %&gt;% head() ## # A tibble: 6 x 2 ## reportDate Confirmed ## &lt;date&gt; &lt;dbl&gt; ## 1 2020-01-21 332 ## 2 2020-01-22 557 ## 3 2020-01-23 654 ## 4 2020-01-24 941 ## 5 2020-01-25 1771 ## 6 2020-01-26 2795 8.1.4 shall we graph it? (Feb 2021) So far, so good. Lets plot the whole range. What do we see? This will generate a quick graph in ggPlot which shows the global incidence of confirmed cases. coronaData %&gt;% group_by(reportDate) %&gt;% summarize(Confirmed = sum(Confirmed)) %&gt;% ggplot(aes(x=reportDate)) + geom_line(aes(y=Confirmed)) 8.1.5 too bad According to the above graph, there have been no additional COVID cases since mid-March or so. Too bad that the data arent right here - for us and especially the world. Exercise 8_1 How would we find the exact date when the file stopped updating? In class, well consider this question (well use pre-downloaded data to save time and computer resources). Use whatever method you like for this - kludgy or clever - but you should describe your results in explicit language or code so that anyone else would get the same results. 8.1.6 what now? I want to make sure that the rest of my code still works, so Ill look more closely at the valid data. Ill generate a new variable (firstbaddate, which sounds like a corny romcom), and filter by it. When the code was written a year ago, the COVID outbreak was largely contained to the Hubei province (which includes the city of Wuhan). So I tried breaking down the data into three locations, breaking down China into Hubei, other China, and the rest of the world. And I generated summaries for three measures - Confirmed, Deaths, and Recovered cases firstbaddate &lt;- &#39;2020-03-22&#39; # This line is added in 2021 coronaDataSimple &lt;- coronaData %&gt;% filter(reportDate &lt; firstbaddate) %&gt;% # added in 2021 mutate(country = case_when( str_detect(place,&quot;China&quot;) ~ &quot;China&quot;, TRUE ~ &quot;Other countries&quot;)) %&gt;% mutate(location = case_when( place == &quot;Hubei, Mainland China&quot; ~ &quot;Hubei (Wuhan)&quot;, country == &quot;China&quot; ~ &quot;Other China&quot;, # what happens when this line is not commented out? # why is it written this way? # str_detect(place, &quot;ruise&quot;) ~ &quot;Cruise Ship&quot;, TRUE ~ &quot;Elsewhere&quot;)) %&gt;% group_by(location,reportDate) %&gt;% summarize(Confirmed = sum(Confirmed), Deaths = sum(Deaths), Recovered = sum(Recovered)) %&gt;% ungroup() ## `summarise()` has grouped output by &#39;location&#39;. You can override using the `.groups` argument. 8.1.7 an initial plot (Feb 2020) The first plot is simple, including data for only deaths. A caption is added to show the source of the data. Heres what the data looked like last February 11: myCaption &lt;- &quot; Data courtesy JHU/CSSE http://bit.ly/ncvdata&quot; oldData &lt;- coronaDataSimple %&gt;% filter(reportDate &lt; &#39;2020-02-12&#39;) coronaPlot0 &lt;- oldData %&gt;% # filter(reportDate &lt; &#39;2020-02-12&#39;) ggplot(aes(x=reportDate)) + geom_line(aes(y=Deaths, color = location)) + labs(caption = myCaption) coronaPlot0 8.1.8 five weeks later (Mar 2020) About five weeks later our data would stop updating. But the world had already changed: Heres the same graph, through March 21, 2020: coronaPlot0 &lt;- coronaDataSimple %&gt;% ggplot(aes(x=reportDate)) + geom_line(aes(y=Deaths, color = location)) + labs(caption = myCaption) coronaPlot0 Its apparent that the coding for Wuhan versus the rest of China is off for some of the newer data, as one of these increases while the other reaches a plateau. Still, the overall picture is clear. 8.1.9 adding recovered cases (code from Feb, data through Mar 2020) Similar results are obtained if we examine two other populations of interest, recovered and confirmed cases. Here, recovered cases and deaths are included on a single plot, as these are roughly on the same scale. Additional changes to the graph are self-evident. mySubtitle &lt;- paste0( &quot;Recovered cases (solid line) and deaths (dotted) by region through &quot;, firstbaddate) # (month(today())), &quot;/&quot;, # (day(today())),&quot;/&quot;, # (year(today())),&quot;.&quot;) myCaption &lt;- &quot; Data courtesy JHU/CSSE http://bit.ly/ncvdata&quot; coronaPlot1 &lt;- coronaDataSimple %&gt;% ggplot(aes(x=reportDate)) + geom_line(aes(y=Recovered, color = location), linetype = &quot;solid&quot;) + geom_line(aes(y=Deaths, color = location), linetype = &quot;dotted&quot;) + theme(axis.title.y = element_text(angle = 90, vjust = 1,size = 14), legend.position = (c(.2,.8))) + labs(title = &quot;Novel coronavirus&quot;, subtitle = mySubtitle, y = &quot;Cases&quot;, caption = myCaption) coronaPlot1 8.2 plotting confirmed cases (Feb-Mar 2020) Finally, confirmed cases are plotted on a different scale: mySubtitle &lt;- paste0( &quot;Confirmed cases (solid line) through &quot;, firstbaddate) # (month(today())), &quot;/&quot;, # (day(today())),&quot;/&quot;, # (year(today())),&quot;.&quot;) myCaption &lt;- &quot; Data courtesy JHU/CSSE http://bit.ly/ncvdata&quot; coronaPlot1 &lt;- coronaDataSimple %&gt;% ggplot(aes(x=reportDate)) + geom_line(aes(y=Confirmed, color = location), linetype = &quot;solid&quot;) + theme(axis.title.y = element_text(angle = 90, vjust = 1,size = 14), legend.position = (c(.2,.8))) + labs(title = &quot;Novel coronavirus&quot;, subtitle = mySubtitle, y = &quot;Cases&quot;, caption = myCaption) coronaPlot1 8.3 status (Feb 2021) The code above works ok, but the March 2020 data need further cleaning, and no new data have been added in 11 months. In that time, as the pandemic has spread, numerous other resources for tracking COVID have been developed, most of which are far more sophisticated and less cumbersome than the code above. There are new datasets, new R packages, and, perhaps most importantly, new questions that we might consider about variables such as vaccination rates, test-positivity rates, and hospital capacity. 8.3.1 an assignment for Wednesday, please study and be prepared to report on at least one of the following packages for studying COVID data using R: If the last digit of your Z number is 1 or 2, begin with covid19datahub/COVID19: Unified dataset for a better understanding of COVID-19 (github.com) If the last digit of your Z number is 3 or 4, begin with aedobbyn/covid19us: R package for the COVID Tracking Project API providing US COVID-19 data (github.com) If the last digit of your Z number is 5 or 6, begin with RamiKrispin/coronavirus: The coronavirus dataset (github.com) If the last digit of your Z number is 7 or 8, begin with Covid19R/covid19nytimes: Pulls the covid-19 data from the New York Times Public Data Source (github.com) If the last digit of your Z number is 9 or 0, begin with joachim-gassen/tidycovid19: {tidycovid19}: An R Package to Download, Tidy and Visualize Covid-19 Related Data (github.com) Review the GitHub page, especially the README.md section to get a sense of what is available in the data. How current is it (when was the code last modified?). How popular is it (look at the number of watches stars and forks in the upper right hand corner of the webpage)? How buggy is it (look at the issues tab, then look at Open and Closed issues? Is it worth looking further at this library)? If you get this far, consider looking at a second package, Googling R COVID data, and/or installing and playing with a package on your home machine. Well discuss your findings and work together on this in class on Wednesday. 8.4 how to create new knowledge One thing to keep in mind as we move forward is that, despite the fact that hundreds if not thousands of statisticians, data scientists, epidemiologists, policy makers, and journalists have looked at COVID data, there are still unanswered questions. For example, here in Florida, there was recently some controversy as the Governor chose to disperse COVID vaccinations at Publix pharmacies; the controversy arose, in large part, because poorer communities (including those in the Western part of Palm Beach County) appear less likely to have a Publix supermarket. Is it the case that there is a relationship between COVID disease rates and the number of Publix/per capita by city or zip code (Florida data by Zip code is available at https://covid19-usflibrary.hub.arcgis.com/datasets/)??) If so, what would this tell us? In data science, we can often create new knowledge by putting together two data sources which havent been combined before, together with an interesting and potentially important question. "],["on-probability-and-statistics.html", "9 on probability and statistics 9.1 on probability 9.2 the rules of probability 9.3 continuous probability distributions 9.4 the most dangerous equation", " 9 on probability and statistics Last week, we considered (Anscombe 1973) and his quartet, and how visualizing data is valuable. This week, we move to a brief discussion of principles of statistics. 9.1 on probability Discrete probability is used to understand the likelihood of categorical events. We can think of initial estimates of probability as subjective or personal. For some events (what is the probability this plane will crash?), an estimate of probability can be drawn from a base rate or relative frequency (e.g., p(this plane will crash) = (number of flights with crashes/ number of flights)). For other events (what is the probability that a US President will resign or be impeached before completing his term of office?), it may be hard to arrive at a suitable base rate. Here, a number of subjective beliefs or principles may be combined to arrive at a subjective or personal probability. In a sense, all probability estimates begin with a personal belief such as this, in part because the choice of the most informative base rate is often not self-evident - in the plane crash example, maybe we should consider a reference group rates such as for this airline etc. (Lanning 1987). The personal origins of probability estimates should become less important as we are exposed to data and revise our estimates in accordance with Bayes theorem. But over the last 45 years, a substantial body of evidence has demonstrates that, under at least some circumstances, we dont make estimates of probability in this way. (This material is discussed in the Thinking and Decision Making/Behavioral Economics class). 9.2 the rules of probability Heres an introduction to the principles of probability. These are presented, with examples and code, in this R markdown document at Harvards datasciencelabs repository: I. For any event A, 0 &lt;= P (A) &lt;= 1 II. Let S be the sample space, or set of all possible outcomes. Then P(S) = 1, and P (not S) = 0. III. If P (A and B) = 0, then P (A or B) = P (A) + P (B). IV. P (A|B) = P (A and B)/ P (B) Principle III applies for mutually exclusive events, such as A = you are here this morning, B = you are at the beach this morning. For mutually exclusive (disjoint, disjunctive) events, the union is the sum of the two events. This is called the addition rule for disjoint events. A different rule applies for events that are mutually independent, such as (A = I toss a coin and it lands on Heads) and (B = it will rain tomorrow). What we mean by independent is that our estimates of the probability of one dont change based on the state of the other - your estimate of the likelihood of rain shouldnt depend on my coin flip. Here, you multiply rather than add: If P (A|B) = P (A), then P (A and B) = P (A) P (B). In words - if the probability of A given B equals the probability of A, then the probability of both A and B equals the probability of A times the probability of B. Ask yourself - are mutually exclusive events independent? Come up with your own examples. This multiplication rule is handy for estimating the probability of an outcome that happens following a chain of independent events, such as the probability that the next eight times I toss a coin it will land on tails every time: P (TTTTTTTT) = P (T) P (T) P (T) P (T) P (T) P (T) P (T) P (T). = .58 = 1/256. Many sets of events are neither disjoint nor independent, so we need more general ways of thinking about pairs of events. For most of us, Venn diagrams are useful to think about combining probabilities. The union or P (A U B) describes the probability that A, B, or both of these will occur. Here, you will use the general addition rule: P (A or B) = P (A) + P (B) - P (A and B) (the probability of A or B is the probability of A plus the probability of B minus the probability of both A and B). For the intersection or P (A  B), we need to consider conditional probabilities. Think of the probability of two events sequentially: First, whats the probability of A? Second, whats the probability of B, given that A has occurred? Multiply these to get the likelihood of A and B: P (A and B) = P (A) P (B|A). Example: The probability of you and your roommate both getting mononucleosis equals the probability of your getting mono times the probability that your roommate gets it, given that you have it also. This is the general multiplication rule. In this abstract example, the order is irrelevant. To estimate the likelihood of A and B, we could as easily take the probability of B, and multiply it by the conditional probability of A given B P (A and B) = P (B) P (A|B). Use the mono example again. What are A and B here? Does it still make sense? When might P (B|A) make more sense than P (A|B)? We are often interested in estimating conditional probabilities, in which case well use the same equation, but solve instead for P (A|B). This leads us back to principle IV: IV. P (A|B) = P (A and B)/ P (B) What is the likelihood of getting in an accident (A), given that one is driving on I-95 (B)? How would you estimate this? If there are fewer accidents on Military Trail, does this mean that you would be safer there? 9.2.1 keeping conditional probabilities straight In general, P (B|A) and P (A|B) are not equivalent. Moores (2000) Basic Practice of Statistics gives the example of P (Police car | Crown Victoria) = .7, and P (Crown Vic | Police car) = .85. Here, one could use a Venn diagram to model this asymmetry. There are several R packages for doing this, including Venn and VennDiagram. Consider exploring these, or at the very least make a rough sketch that can help you answer the following question: &gt; In general, if P (A|B) &lt; P (B|A), what must be true of the relationship of P (A) to P (B)? 9.3 continuous probability distributions We can also use probability with continuous variables such as systolic blood pressure (thats the first one), which has a mean of approximately 120 and a standard deviation of 15. Continuous probability distributions are handy tools for thinking about the meaning of scores, particularly when we express scores in standard deviations from the mean (z scores). More to the point, this way of thinking about probability is widely used in questions of scientific inference, as, for example, in testing hypotheses such that the average systolic blood pressure among a group of people studying at Crux (hence caffeinated) will be significantly greater than that of the population as a whole. This is part of the logic of Null Hypothesis Significance Testing (NHST) - if the result in my Crux sample is sufficiently high, then I say that I have rejected the null hypothesis, and found data which support the hypothesis of interest. 9.4 the most dangerous equation Just as (Tufte 2001) demonstrated that poor data visualizations can be dangerous, leading, for example, to the loss of life in the Challenger disaster, (Howard Wainer 2007) shows that a lack of statistical literacy is also dangerous. He argues that deMoivres equation is the most dangerous equation - this equation (for the standard error) shows that variability decreases with the square root of sample size. Other nominees include the linear regression equation (and, in particular, how coefficients may change or reverse when new variables are added) and regression to the mean. Regarding linear regression, Simpsons paradox shows how the direction of regression coefficients may change when additional variables are added to a model. I would argue that, from the standpoint of psychology, ignorance of regression to the mean was arguably more dangerous than ignorance about the central limit theorem and standard error, in particular because regression effects contribute to an overestimate of the effectiveness of punishment and an under-appreciation of the effectiveness of positive reinforcement as tools for behavior change (Hastie and Dawes 2010). References "],["reproducibility-and-the-replication-crisis.html", "10 reproducibility and the replication crisis 10.1 Answers to the reproducibility crisis 10.2 answers to the reproducibility crisis III: Pre-registration 10.3 further readings", " 10 reproducibility and the replication crisis Probability theory is elegant, and the logic of Null Hypothesis Significance Testing (NHST) is compelling. But philosophers of science have long recognized that this is not really how science works (Lakatos 1969). That is, science is not primarily built by the testing and rejecting of null hypotheses. (Think, for example, of how you might describe an experiment for testing the hypothesis that gravity exists, and whether you would ever reject this hypothesis). The problem is a multifaceted one. It arises partly because, in any experiment, there is a large (infinite?) number of conditions which might be invoked to explain-away results of a failure of our work. It arises because we are human, and prone to various biases of decision making and information integration (that is, we are poor Bayesians). It arises, too, because scientific institutions such as journals, funding agencies, and universities are competitive environments which incentivize positive findings. In recent years, the tension between the false ideal of NHST and the real world of science has become increasingly evident. Within psychology, experimental studies have often - even typically - failed to replicate well-known results (Open Science Collaboration 2015). Its not just psychology (Baker 2016). One of the first important papers to shine light in the area (Ioannidis 2005) came from medicine; it suggested six contributing factors, which I quote verbatim here: The smaller the studies conducted in a scientific field, the less likely the research findings are to be true. This stems directly from our discussion of the central limit theorem and the instability of results from small samples. The smaller the effect sizes in a scientific field, the less likely the research findings are to be true Well talk about effect size below. The greater the number and the lesser the selection of tested relationships in a scientific field, the less likely the research findings are to be true. (and) The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true. The problem of analytic flexibility leads to p-hacking The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true and The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true. Positive findings rise, and negative ones are ignored. And scientists are human, and subject to incentives. Heres a video which provides some more context for the crisis: . *Video 10.1: On the reproducibility crisis (12 mins) 10.1 Answers to the reproducibility crisis For scientific progress to occur, there needs to be more than a simple rejection of what is wrong with current methods and research programs: Here, as elsewhere, criticism must be constructive to be valuable. There have been a number of solutions proposed to the reproducibility crisis. 10.1.1 Tweak or abandon NHST The first cluster of responses to the reproducibility crisis is concerned with statistics, specifically problems with Null Hypothesis Significance Testing (NHST). These include (a) justifying ones alpha - making it more stringent, for example, for counter-intuitive claims (Grange et al. 2018), (b) changing the default p value from .05 to .005 (Benjamin et al. 2018), and (c) abandoning significance testing altogether (McShane et al. 2017). (Szucs and Ioannidis 2017) goes into some of these issues in more detail and discusses other limitations of significance testing, including the dichotomous, all-or-none silliness of the accept/reject decision. (If you play the NHST game, there is no almost significant, approached significance, highly significant, etc.). (Leek and Peng 2015) argue that the problems are not merely with NHST, but with the whole of data analysis. They maintain that better training in data science - courses like ours, perhaps, are part of the answer. (figure) (Munafò et al. 2017) also argue that threats to reproducible science occur at a number of places in science, not just with the evaluation of hypotheses. munafo2017 10.1.2 keep a log of every step of every analysis in R markdown or Jupyter notebooks A second cluster of responses is concerned with keeping good records. Lets say that you are running a study, say, which looks at the hypothesis of differential variation of females and males in a cognitive measure; your interest is to critically examine the hypothesis discussed by (Howard Wainer 2007) that males show more variability. There have been a lot of studies related to this over the years, so that rather than collect new data you decide that you will work with existing data from several online archives. You find and download spreadsheets from two studies: In the first, gender is coded 1 for male, 2 for female. In the second, gender is coded 1 for female, 2 for male, and 3 for other. There are, in essence, two ways that you can combine the variables into a common format: The first would be to take one of the spreadsheets and do a few find-and-replace commands on the appropriate column of the data. This is quick and easy - but when someone else, or even future you, returns to this data, you will not remember if you have recoded it. The alternative is to keep a record of your work in R markdown. This is more time consuming, and can sometimes be clumsy. But it is virtuous useful and clear - and when you screw up, you will have a full record of what happened. Part of the problem of scientific reproducibility is to keep comprehensive records. This record-keeping and research transparency is at the heart of R markdown documents. 10.2 answers to the reproducibility crisis III: Pre-registration The third answer to the reproducibility crisis is the most comprehensive; it involves not merely keeping a record of what you have done, but preregistering your work, that is, fully specifying your planned analyses beforehand (Miguel et al. 2014). The author, an economist, outlines his argument in a five-minute video here. For randomized controlled trials, consider socialscienceregistry.org, and for more general use, use the open science framework page. 10.3 further readings Finally, if you would like to learn more about the reproducibility crisis, there is a collection of papers in Nature here. References "],["literate-programming-with-r-markdown.html", "11 literate programming with R markdown 11.1 scripts are files of code 11.2 projects are directories containing related scripts 11.3 R markdown documents integrate rationale, script, and results 11.4 What to do when you are stuck", " 11 literate programming with R markdown Showing your work, to (future) you as well as others, is a key part of reproducible science. R Markdown documents facilitate this, as they allow you to include comments, code, and results in a single place. But before we consider R markdown, we begin with two more elemental ideas: scripts (R4DS, Chapter 6) and projects (Chapter 8). 11.1 scripts are files of code We begin with R4DS Chapter 6, which shows the R studio interface and encourages you to save your work using scripts, written in the source (editor) window in the upper left quadrant of the default R studio screen. Note the recommendations - for example, include packages (libraries) at the beginning of your code. One more thing - in setting up R studio, consider adjusting the insert spaces for tab setting to something more than 2. This will allow you to more easily see the nested structure of functions, loops, etc. - and will create a modest disincentive against making these nested structures too deep or complex: Fig 11.1: I use 4 spaces here. YMMV. Note, too, the code diagnostics in R. Consider enabling all of these, including the R style diagnostics, to help you keep your code readable: Fig 11.2: Enable code diagnostics 11.1.1 some elements of coding style Good coding is often a combination of several skills ranging from puzzle-solving to communication. I cant claim that these are the elements of coding style (apologies to Strunk &amp; White), but rather that these are merely some of the elements. Good coding is clear and thus commented. You are writing for your future self as well as others, so be explicit about the purpose of each chunk of code. Good coding is concise. When you can write code in 3 lines instead of 30, your code may be more clear and efficient. Take pleasure in writing parsimonious, efficient code. But where efficiency and clarity conflict, choose the latter. Good code should be complete, including all steps from reading the data to producing output. Where appropriate, comment-out (rather than delete) informative errors, again for the future you. Good code may be creative. The coolest solutions are those which pull from and synthesize a number of ideas. Creativity often requires walking away from a problem in order to ultimately arrive at a solution (Wertheimers Productive Thinking). Finally, good code should be considered. Reflect on the impacts of your work - just because you can analyze something doesnt mean that you should. 11.2 projects are directories containing related scripts You will save your work in projects - which isolate your data and scripts into different directories. (See r4ds, Chapter 8). To reinforce the idea that your unit of analysis in R is the project rather than the script, consider associating your Rmd filetype (see next section) with your markdown editor, and only your Rproj filetype with R studio. Soon, it is likely that you will soon be working on R for different things in parallel - for this and another class, for this class and your thesis, or perhaps for two distinct types of analysis within your thesis. When you open up an R project, youll be in the right directory, with the relevant files (and only the relevant files) at your fingertips in the files pane. 11.3 R markdown documents integrate rationale, script, and results R Markdown documents allow you to include comments, scripts, and results in a single place. The basics of R markdown are presented in Chapter 27 of R4DS. I encourage you to use R markdown for nearly everything you do in R. Within R studio, open up a new R markdown document. There are as many as four parts of an R markdown document: A YAML (yet another markdown language) header Text formatted in markdown R code (chunks) surrounded by code fences and, occasionally, inline code There is a handy R Markdown cheat sheet which can give you a sense of what R markdown is about. It describes eight steps, from workflow to publish (and a ninth, learn more). Dont worry about all of the detail here, but do get a sense of how it works. Exercise 11.1: Working in groups, do the exercises in section 27.4.7 of R4DS. Begin with the R markdown file that is included at the beginning of Chapter 27. You can download it here. Study the code, and annotate it so that you have a better sense of how it works. For example, this block loads needed libraries, then takes the _____ dataset and ___________ . Play with the graph. Change one or more parameters of it to make it more useful. Again, annotate your changes. 11.4 What to do when you are stuck Google. pay attention to your error messages Ask for help, make your questions clear and reproducible (see R4DS Chapter 1) Take a break, think outside the box and kludge something together if you have to Document your struggle and your cleverness for a future you "],["the-tidyverse.html", "12 the tidyverse 12.1 some simple principles", " 12 the tidyverse The tidyverse is an opinionated collection of R packages designed for data science - https://www.tidyverse.org/ R had its origins in S, a system designed for engineers at Bell Labs. This audience meant that R would be more accessible to those with programming backgrounds, more aimed at developers than users approaching data science from an applied or statistical perspective than one in programming. As the popularity of R increased, it would become more flexible and versatile for these power users, but there was less progress in making R accessible to and tailored for data scientists. To this day, base-R is, for most users, more challenging than SPSS or Stata. The tidyverse was born partly to address these issues (Peng 2018). The tidyverse is a growing set of interconnected packages which share a common syntax; it can be seen as a dialect of R. More precisely, the tidyverse is a lucid collection of R packages offering data science solutions in the areas of data manipulation, exploration, and visualization that share a common design philosophy. It was created by R industry luminary Hadley Wickham, the chief scientist behind RStudio. R packages in the tidyverse are intended to make statisticians and data scientists more productive. Packages guide them through workflows that facilitate communication and result in reproducible work products. The tidyverse essentially focuses on the interconnections of the tools that make the workflow possible (Gutierrez 2018). The workflow is one that you have seen here and in R4DS. In this 2017 slide, the main processes of data analysis are accompanied by the packages in the tidyverse. (As of 2019, there have been a few small changes in the packages associated with modeling). All of these are installed on your computer with install.packages(tidyverse), but only those in bold are loaded into memory when you issue the command library(tidyverse): Fig 10.1: Schematic of the tidyverse. From Wickhams 2017 rstudio:conf keynote 12.1 some simple principles search for tidyverse solutions. When you have a problem in your code, for example, how do I compute the mean for different groups of a variable, search for R mean groups tidyverse, not just R mean groups. This will get you in the habit of working with tidy solutions where they can be found. library(tidyverse) mtcars %&gt;% group_by(cyl) %&gt;% summarise(mean = mean(disp), n = n()) (Another suggestion: Because R and the tidyverse are constantly evolving, consider looking at recent pages first. In your Google search bar, click on Tools -&gt; Past year). talk the talk. Recognize that %&gt;% (the pipe) means then. Statements with pipes begin with data, may include queries (extract, combine, arrange), and finish with a command. annotate your work. Assume that you will come back to it at a later date, while working on a different project, and use portions of your current code. Your R markdown documents should be a log. When you run in to a significant, challenging problem, dont delete your mistakes, but ## comment them out. library(gapminder) b &lt;- gapminder %&gt;% ## when should you comment out an error ## instead of deleting it? for me, I&#39;ll ## comment out errors that took me a long time ## to solve, and/or that I&#39;ll learn from. ## Probably not here, in other words... ## filter(lifeExp) &gt; 70 bad parens filter(lifeExp &gt; 70) work with tidy data. Make each row an observation, and each column a variable. Complex data sets, such as samples of text, become much more manageable when reduced to simple arrays. write functions. If you repeat a section of code, rewrite it as a function. (Well come back to this later). adhere to good coding style. Well-written code is reasonably parsimonious, readable, and easily debugged. There are a few style manuals for R, including one from Hadley, and this [Rchaeological Commentary] (https://cran.r-project.org/web/packages/rockchalk/vignettes/Rstyle.pdf). but maintain perspective. Your need to solve problems (how to analyze x, etc.) should not take a back seat to your desire to write the best code. There is almost always a better way to do things. Strive reasonably to accomplish this, but be prepared to kludge. References "],["finding-exploring-and-cleaning-data.html", "13 finding, exploring, and cleaning data 13.1 Data in R libraries 13.2 Other prepared datasets 13.3 Make/extract/combine your own data 13.4 exploring data 13.5 messy data: Cleaning and curation", " 13 finding, exploring, and cleaning data Data science, oddly enough, begins not with R but with data. There is no shortage of datasets available to analyze, and each can give rise to a host of interesting analyses and insights. What do you want to study? 13.1 Data in R libraries A few weeks back, we explored five different R packages (libraries) for looking at COVID data. There are a number of other R packages which provide easy access to data - for example, consider the babynames package. Install it on your machine, load the library, and View it. Exercise 13.1 Come up with an interesting question about the babynames dataset. How would you go about examining it? Libraries in R with multiple datasets include the built-in R dataset library, which includes, at this writing, 101 different datasets. Another is the R dslabs library, with 42 additional datasets. And the R fivethirtyeight library provides access to 198 clever, clean, and largely manageable datasets, each of which underlies the empirical analyses and reports of Nate Silver and his team (You can learn more at https://data.fivethirtyeight.com/). Less tidy, somewhat more ambitious, and more far-ranging datasets include these Nineteen datasets and sources, ranging from Census data to Yelp reviews. 13.2 Other prepared datasets The datasets in the libraries described above should be relatively easy to work with, requiring minimal munging prior to analysis. If you are up for something a little more ambitious, read on. Kaggleis a noun (a community, a website, a challenge), and a verb (to kaggle is to participate in a data challenge) which describes a crowdsourced competition to improve on a problem in prediction. Perhaps the first and best known example of this was the Netflix prize (Jackson 2017), which, in 2006, promised one million dollars to the first team to improve the algorithm by which that company recommended movies to its customer base. The competition took several years, and inspired substantial improvements in machine learning as well as in crowdsourced science. At this writing, Kaggle hosts many active competitions - including a $1,500,000 award offered by the United States Department of Homeland Security known as the passenger screening algorithm challenge. (Good luck!) Kaggle also hosts hundreds if not thousands of datasets. A good place to start is with their datasets stored in comma separated value format (.csv); you can find them here. Kaggling is an important feature of data science culture. If you are into psychology and behavioral science, the Open Science Framework (OSF) provides a system for hosting and sharing code and data from research articles. One OSF page is a compilation of many datasets from prominent papers in psychology and psychiatry Incidentally, almost all of the data and code from papers I have published is on the OSF as well. Beyond OSF, there is some structured personality test data available, too. Outside of psychology, repositories of data from many disciplines may be found at Re3data https://www.re3data.org/. There are many datasets about music - songs, artists, lyrics, etc. - at millionsongdataset. Note that many of these are quite large, but there are smaller files of ~ 10000 songs that are available. Github is the primary site for coders to share and improve upon their work. Git is a system in which one can upload (push) ones work from a local computer to the cloud in a repository (repo), share this with collaborators who copy (fork) the repo, pull it down to their computers, and possibly make changes which will appear as a separate branch of the repo. Each change is time-stamped, and efficiently stored as only its difference from the prior edit (or commit). There are, in all of these pushes and pulls, opportunities for collisions and problems, but learning Git remains a critical part of the data scientists toolkit. You can set up an account on Github if you like, but even without this you can access some of the datasets that are stored there, including a set of curated datasets on topics such as economics, demographics, air quality, flights and house prices. Perhaps the easiest way to access these is to click through repos until you find a data directory, open the files up as raw files, and paste them into a spreadsheet or notepad program of your choice. Github also hosts the awesome public datasets (many of which probably are). You can work with R repositories straight from R studio. Or just Google datasets. 13.2.1 Keep it manageable Proceed with caution - many of these datasets are likely to be quite large (for example, analyses of images) and/or in formats that for now are too challenging (JSON). I encourage you to stick with data that are available in a .csv format and that dont have more than, say, a million data points (e.g., 50,000 observations * 20 variables). And probably avoid, for the time being, datasets consisting primarily of natural language samples or networks - for those who are interested, we will look at these in the Computational Social Science class in the Fall. 13.3 Make/extract/combine your own data Despite the petabytes (exabytes? zettabytes? yottabytes?) of data in the datasets described above, its possible that the dataset that you want to examine does not yet exist. But you may be able to create it, for example, by scraping data from the Web. Typically, you would use an Application Programming Interface (API) to pull data down from platforms such as Twitter or Reddit. For these and other major social media and news platforms, there are R packages which will walk you through the process of getting the data from webpage to tidy dataset. (Be aware, though, that the methods for data access on these platforms frequently changes, so that code that worked a year ago might not work today). Another source of data is  your own life. If you wear a pedometer or sleep tracker, are a calorie counter or keep other logs as a member of the quantified self movement, consider how such data might relate to aspects of the physical environment (such as temperature, or the time between sunrise and sunset) and/or the broader social and cultural context (a measure, perhaps of the sentiment, or mood, of news articles from papers like the NY Times). Finally, you might want to combine multiple datasets, such as county-level home pricing data from Zillow (https://www.zillow.com/research/data/), county-level elections data from, for example, here: https://github.com/tonmcg/US_County_Level_Election_Results_08-16, and the boundaries of Woodards 11 American Nations (see Lanning). In joining different datasets, or data from different sources, we can go beyond a pedagogical exercise (learning about learning) and contribute new and meaningful knowledge. 13.4 exploring data To look at the datasets in the prior section, remember that there are a few key commands, which are here applied to the gapminder dataset: str(gapminder) head(gapminder) summary(gapminder) To simplify your data, youll want to select certain columns or rows, or possibly create new variables based on existing scores: gapminder %&gt;% filter(gdpPercap &lt; 100 &amp; year &gt; 2000) gapminder %&gt;% select(-(c(continent,pop))) gapminder %&gt;% mutate(size = ifelse(pop &lt; 10e06, &quot;small&quot;, &quot;large&quot;)) 13.5 messy data: Cleaning and curation Between 50 and 80% of the work of the data scientist consists of the compiling, cleaning and curation of data, or what is called data munging or wrangling. One part of data wrangling is looking for and dealing with encoding inconsistencies, missing values, and errors. Consider the following: Exercise 13.2 Run the following code in an R markdown document. Youll need to add a library beforehand. car2019 &lt;- tibble(model = c(Corolla, Prius, Camry, Avalon), price = c(22.5, about 25K , 24762, 33000-34000)) Inspect the data frame. Add and annotate code to fix any problems that you believe exist. Summarize the results. Another part of data wrangling is about data rectangling (Bryan 2017), that is, getting diverse types of data into a data frame (specifically, a tibble). This is likely to be particularly challenging when you are combining data from different sources, including Web APIs. Well consider this further down the road when we talk about lists. A third part of data wrangling occurs when we join data from different sources. There are many ways to do this, but attention must be paid to insure that the observations line up correctly, that the same metrics are used for different datasets (for example, inflation adjusted dollars vs raw), that dates are interpreted as dates, that missing values are recognized as missing and not scored as zero, and so forth. Well talk about this in the weeks ahead, particularly when we consider relational data. library(tidyverse) library(maps) # maps must be loaded after tidyverse, else purrr::map takes over library(maptools) References "],["transforming-and-joining-data.html", "14 transforming and joining data 14.1 from data on the web to data in R 14.2 working with geodata: a function to get US states from latitude/longitude 14.3 drowning in the sea of songs (with apologies to Artist #ARIVOIM1187B990643) 14.4 review of munging tools 14.5 more about joining", " 14 transforming and joining data Once you have your data, you will almost invariably need to transform it - to sort it, to select observations or variables from it, to create new variables, to partition it into groups, or to summarize it. In R, there is a general purpose tool (ok, package) that exists for this called dplyr (d-plier). Dplyr is a core part of the tidyverse and hence is loaded automatically when you load the tidyverse ensemble of libraries. The versatility of dplyr is demonstrated in Chapter 5 of R4DS, which shows how to do many basic, and some not so basic, operations on your data. Read it closely if you havent already done so. 14.1 from data on the web to data in R Lets consider a dataset consisting of 10,000 songs (observations) and 35 measures (variables including artist). The first step here (and for your class project and the data science challenge) will be to get the data into R. How do you this? Method 1: Download the file to your computer as a csv file. Some advantages of this include (a) it will allow the code to run even if the website changes or disappears, (b) in the likely event that I will run the code several times, it is faster to get data from my own machine than from the cloud, and (c) if there are any anomalies in the data I can look at them locally (e.g., in Notepad++). If you use this approach, make sure that you know where your data is on your computer. The easiest way to do this is to work with an R project. Within R studio, create a new R project (or use an existing one). Move the csv file into this directory, so that data, code, and results will all be in the same place. Method 2: Import the data from the cloud directly into R. You can do this either by specifying the musicURL separately (Method 2a) or call it directly within the read_csv statement (2b). Here, too, I would encourage you to set up and work within a project. ### Method 1 # music1 &lt;- read_csv(&quot;music.csv&quot;) ### Method 2a musicURL &lt;- &quot;https://think.cs.vt.edu/corgis/datasets/csv/music/music.csv?forcedownload=1&quot; music1 &lt;- read_csv(musicURL) ### Method 2b # music1 &lt;- read_csv( # &quot;https://think.cs.vt.edu/corgis/csv/music/music.csv?forcedownload=1&quot; # ) What can you do with the data in its existing form? What questions do you wish you could ask about these songs, artists, and the places they come from? 14.2 working with geodata: a function to get US states from latitude/longitude When I first ran this code in 2018, the variables included states and countries. The current dataset doesnt have these, but it does have longitude and latitude. To get state data from this, reverse geocoding is needed. A Google search turned up the following function. Note that it requires two additional libraries, maps and maptools. # source is https://github.com/abresler # The single argument to this function, pointsDF, is a data.frame in which: # - column 1 contains the longitude in degrees (negative in the US) # - column 2 contains the latitude in degrees latlong2state &lt;- function(pointsDF) { # Prepare SpatialPolygons object with one SpatialPolygon # per state (plus DC, minus HI &amp; AK) states &lt;- map(&#39;state&#39;, fill=TRUE, col=&quot;transparent&quot;, plot=FALSE) IDs &lt;- sapply(strsplit(states$names, &quot;:&quot;), function(x) x[1]) states_sp &lt;- map2SpatialPolygons(states, IDs=IDs, proj4string=CRS(&quot;+proj=longlat +datum=WGS84&quot;)) # Convert pointsDF to a SpatialPoints object pointsSP &lt;- SpatialPoints(pointsDF, proj4string=CRS(&quot;+proj=longlat +datum=WGS84&quot;)) # Use &#39;over&#39; to get _indices_ of the Polygons object containing each point indices &lt;- over(pointsSP, states_sp) # Return the state names of the Polygons object containing each point stateNames &lt;- sapply(states_sp@polygons, function(x) x@ID) stateNames[indices] } 14.2.1 applying the function to the music data If you have run the chunk above, the latlong2state function will now exists in your environment. You still need to apply the function to your data. We do this using three of the essential tools in the mungers toolbox: select (to choose several columns from the data), mutate (to create a new variable), and finally drop_na (to drop cases with missing data - here, artists from outside of the US). # create a new dataset with just latitude and longitude tempLatLong &lt;- music1 %&gt;% select (artist.longitude, artist.latitude) music1$state &lt;- latlong2state(tempLatLong) USartists &lt;- music1 %&gt;% mutate(state = latlong2state(tempLatLong)) %&gt;% drop_na(state) rm(tempLatLong) 14.3 drowning in the sea of songs (with apologies to Artist #ARIVOIM1187B990643) At this point, we have a list of songs and artists and some measures of hotttness, restricted to US artists and now including states as well as latitude and longitude. There is a field called song-id, but it doesnt have song titles. (several hours later) I found the titles on the net. They were part of a file millionsongsubset, which is an archive of many files, and which is altogether about 2 GB in size. Within this, there is a file that looks right - lets try it. The file is delimited by the characters &lt;SEP&gt;, but read_delim requires that data fields be separated by a single character (like a comma or a tab). So the file is read as a single variable, which R assigns the default name X1. I use mutate to replace each &lt;SEP&gt; by a tab, then the separate command to split X1 up into its constituent variables - the first is a mysterious ID, the second looks like the song.id in the original file, the third is an artist name, and the fourth is a song title. 14.3.1 combining the song titles with our US artists Finally, we can join this to the USartist data, as each row of the two files have a common key (song.id). Join is discussed in more detail below. Here, we use a right_join - implicitly, maybeTitles, which is at the head of the pipe, is on the left. The right_join will link the left (maybeTitles) file with the right (USartists), by what appears to be a common key: # again, several different ways to load the data maybeTitles &lt;- read_delim(&quot;https://raw.githubusercontent.com/xbucchiotty/xpua/master/src/main/resources/AdditionalFiles/subset_unique_tracks.txt&quot;,&quot;~&quot;, col_names = FALSE) %&gt;% #maybeTitles &lt;- read_delim(&quot;data/subset_unique_tracks.txt&quot;,&quot;~&quot;, # col_names = FALSE) %&gt;% mutate(X1 = str_replace_all(X1, &quot;&lt;SEP&gt;&quot;, &quot;\\t&quot;)) %&gt;% separate(X1, sep = &quot;\\t&quot;, into = c(&quot;ID1&quot;, &quot;song.id&quot;, &quot;artistFromMaybeTitles&quot;, &quot;SongTitle&quot;)) %&gt;% right_join(USartists, key = &quot;song.id&quot;) ## Rows: 10000 Columns: 1 ## -- Column specification -------------------------------------------------------- ## Delimiter: &quot;~&quot; ## chr (1): X1 ## ## i Use `spec()` to retrieve the full column specification for this data. ## i Specify the column types or set `show_col_types = FALSE` to quiet this message. ## Joining, by = &quot;song.id&quot; So far, so good. Do things line up correctly? If the match is perfect, the two artist fields should be identical. We can use the [not] oddly-titled logical function identical to check this: identical(maybeTitles$artistFromMaybeTitles, maybeTitles$artist.name) ## [1] FALSE Whoops. The two variables arent the same - or are they? Run the following commands, which includes two more of the essential munging tools - select and arrange. Here, note that the filter function is used to select cases where the two variables are different. maybeTitles %&gt;% filter (artistFromMaybeTitles != artist.name) %&gt;% select (artistFromMaybeTitles, artist.name) %&gt;% arrange(artistFromMaybeTitles) %&gt;% head() ## # A tibble: 6 x 2 ## artistFromMaybeTitles artist.name ## &lt;chr&gt; &lt;chr&gt; ## 1 Alvin Youngblood Hart Alvin YoungbloodA Hart ## 2 Andraé Crouch &amp; Solid Gospel AndraA(c) Crouch &amp; Solid Gospel ## 3 Angélica María AngA(c)lica MarAa ## 4 Fabián FabiA!n ## 5 Ill Niño Ill NiA+-o ## 6 Los Muñequitos De Matanzas Los MuA+-equitos De Matanzas 14.3.2 exercises What happened with the artist names? How would you move forward with these data? Do something interesting with the data - use filter, arrange, and select in your piped code. Work with the babynames data. Use mutate to create an interesting new variable. most importantly, make progress on your class project. You will be presenting this two weeks from today. 14.4 review of munging tools The primary tools, at least in my experience, are these: If you want to pull out only certain columns, use select. If you want to pull out only certain rows, use filter. If you want to sort, you arrange. If you want to create, you mutate. If you want to combine data, you join. 14.5 more about joining There are many different approaches to joining datasets. We might want to retain only cases which are present in both datasets: A political consultant might want to target calls to people who were on both a list of Democrats and a list of people who voted in 2020. We might want to include all members of one Dataset who are absent in a second: An investigative journalist might want to interview People who worked for Amazon in November 2020, but not January 2021. We might even want to join two datasets in which people appear in either of two lists - a private investigator might look for any individuals who rented cars from Hertz or Avis. These three approaches - an inner join, a left join, and a full join - begin to illustrate the richness and challenges of combining datasets. Joins are at the heart of Structured Query Language (or SQL, which is pronounced es-que-ell or sequel). Joins of multiple databases can become very complex - think about links in a company between sales data, salary, hours worked, seniority, and a client base, or, better, links in a University between lists of students, classes, faculties, grades, and parking violations). Relational Database Management Systems (RDBMS) are linked networks of datasets such as these, and a rudimentary understanding of the ideas behind these is an essential part of proficiency in data science. There are two sources which I will refer you to for now. The first is chapter 13 of R4DS. The second is what Jenny Bryan describes as a cheatsheet, but is I think more than this (https://stat545.com/bit001_dplyr-cheatsheet.html). This shows how the various joins of dplyr work in combining two tibbles (superheroes and publishers). If you want to get a feel for how combining data works, it is a great start. 14.5.1 more about munging Weve also used separate to split a column into several; the complement of this is unite. You may want to add new measures for all cases without a common key. Here, you can use the bind_cols function. If you instead want to add observations to an existing dataframe, you may bind_rows. We havent yet considered the tools used to reshape data frames from short-and-wide to long-and-narrow, and vice-versa. In our music data, we might, for example, want to think about the data by artist (with possibly multiple songs on each line), or song-titles (with possibly covers by different artists on each line). More typically, consider repeated-measures data in which each score reflects a measure taken on one of ten subjects and one of three occasions - for example, scores on the Beck Depression Inventory (BDI) before, during, and after treatment. Here, we may wish to structure the data in a long format with thirty rows (one for each subject X occasion) and three columns (person, occasion, and BDI), or in a wide format with just ten rows (one for each subject), and four columns (person, BDI-before, etc). To move from long to wide, use the pivot_longer spread command, and to move instead from wide to long, use pivot_wider gather. (These changes were introduced in ~ 2020). "],["strings-factors-dates-and-times.html", "15 strings, factors, dates, and times 15.1 strings 15.2 factors 15.3 dates 15.4 times", " 15 strings, factors, dates, and times This chapter discusses some of the types of data other than numeric and logical, in particular strings, factors, and dates/times. In this chapter, as in the last few, I refer primarily to three of the chapters in R4DS. Consider these notes supplementary. 15.1 strings Strings are sets of characters which may include 123 as well as why *DID* the chicken cross the road? Samples of text, from names to novels, are the most interesting type of string. Among the tools that are used in examining texts are searches (do these tweets include language associated with hate speech?), validity checks (does the string correspond to a valid zip code?), and reformatting (to lower case so that BOB, Bob, and bob are all coded as identical). These ideas are simple, but quickly become challenging when, for example, the strings in which we are interested include characters that R usually interprets as code - such as commas, quotes, and slashes. See the section on string basics (14.2) for how to escape these characters, for example, how to treat a hashtag (#) as just a character as opposed to the beginning of a comment. These rules are codified as regular expressions (regex, sometimes regexp). Regex are not unique to R, but are shared with other languages as well. In R, particularly in the tidyverse package, regex are typically implicit, represented within commands that are part of the stringr package and that typically begin with str_. For example, str_detect returns a set of logical values: donuts &lt;- c(&quot;glazed&quot;, &quot;cakes&quot;, &quot;Pink sprinkled&quot;, &quot;cream filled&quot;, &quot;day-old frosted&quot;, &quot;chocolates&quot;) donuts %&gt;% str_detect(&quot; &quot;) ## [1] FALSE FALSE TRUE TRUE TRUE FALSE Most of the str_ functions are straightforward, but remember that str_sub provides a subset, not a substitution; to change a string, use str_replace. As Hadley points out in R4DS, the autocomplete function in R_studio is very handy for helping you explore the different functions - in your console, type str_  then scroll through the possibilities. donuts %&gt;% str_sub(1,5) ## [1] &quot;glaze&quot; &quot;cakes&quot; &quot;Pink &quot; &quot;cream&quot; &quot;day-o&quot; &quot;choco&quot; donuts %&gt;% str_replace(&quot; &quot;,&quot;_&quot;) ## [1] &quot;glazed&quot; &quot;cakes&quot; &quot;Pink_sprinkled&quot; &quot;cream_filled&quot; ## [5] &quot;day-old_frosted&quot; &quot;chocolates&quot; As you work with texts, simple problems sometimes require sophisticated codes. The regex that are used to solve these problems quickly become dense and challenging. One tool that can help you is the str_view command, which returns (when used in R studio) highlighted text showing corresponding passages. For example: dumbSlashMovieTitles &lt;- c(&quot;Craslash&quot;, &quot;Revenge o&#39; \\&#39;McSlashy&#39;&quot;, &quot;Star Wars: Episode \\\\slash&quot;) dumbSlashMovieTitles %&gt;% str_view (&quot;sh&quot;) Note the effects of the backslash: For example, the Star Wars movie has two slashes in the input, but only one in the output, as the backslash is the escape character that tells R to take the next character literally (as in the Revenge movie). So. How would you use str_view to find strings which include the backslash character in our dumbSlashMovieTitles? [Experiment. Google. Ask your classmates.] The density of regex statements can make them challenging. The backslash case is an unusual one, but other special characters are more common, and can be useful, including (for example) indicators for the beginning (^) or end ($) of a string: # get rid of suspected plural donuts %&gt;% str_replace (&quot;s$&quot;, &quot;&quot;) ## [1] &quot;glazed&quot; &quot;cake&quot; &quot;Pink sprinkled&quot; &quot;cream filled&quot; ## [5] &quot;day-old frosted&quot; &quot;chocolate&quot; A particularly useful function in stringr is str_split, which can be used to quickly break a text into discrete words. Note that using a space and the explicit word boundary give different results. str_split(donuts, &quot; &quot;, simplify = TRUE) ## [,1] [,2] ## [1,] &quot;glazed&quot; &quot;&quot; ## [2,] &quot;cakes&quot; &quot;&quot; ## [3,] &quot;Pink&quot; &quot;sprinkled&quot; ## [4,] &quot;cream&quot; &quot;filled&quot; ## [5,] &quot;day-old&quot; &quot;frosted&quot; ## [6,] &quot;chocolates&quot; &quot;&quot; str_split(donuts, boundary (&quot;word&quot;), simplify = TRUE) ## [,1] [,2] [,3] ## [1,] &quot;glazed&quot; &quot;&quot; &quot;&quot; ## [2,] &quot;cakes&quot; &quot;&quot; &quot;&quot; ## [3,] &quot;Pink&quot; &quot;sprinkled&quot; &quot;&quot; ## [4,] &quot;cream&quot; &quot;filled&quot; &quot;&quot; ## [5,] &quot;day&quot; &quot;old&quot; &quot;frosted&quot; ## [6,] &quot;chocolates&quot; &quot;&quot; &quot;&quot; The output of str_split is generally a list (more on that soon), but here the lists are simplified into tibbles. In the tidyverse, str_split is typically one of the first steps in preparing text. The tidytext package (https://www.tidytextmining.com/), which is discussed at length in the computational social science course, builds on this foundation and is a powerful set of tools for all sorts of problems in formal text analysis. 15.2 factors Conditions (experimental vs control), categories (male or female), types (scorpio, hates astrology) and other nominal measures are categorical variables or factors. In the tidyverse, the r package for dealing with this type of measure is forcats, one of the core parts of the tidyverse. Heres an example of a categorical variable. Why is it set up like this, and what does it do? # Example of a factor eyes &lt;- factor(x = c(&quot;blue&quot;, &quot;green&quot;, &quot;green&quot;, &quot;zombieRed&quot;), levels = c(&quot;blue&quot;, &quot;brown&quot;, &quot;green&quot;)) eyes ## [1] blue green green &lt;NA&gt; ## Levels: blue brown green In base R, string variables (donut, anti-Brexit, and yellow) are generally treated as factors by default. In the tidyverse, string variables are treated as strings until they are explicitly declared as factors. The syntax for working with factors-as-categories is given in Chapter 15 of R4DS. I will not duplicate that here, but I will point out that factors are represented internally in R as numbers, and converting (coercing) factors to other data types can occasionally lead to nasty surprises. Sections 15.4 and 15.5 describe how factors can be cleanly reordered and modified. 15.2.1 types of babies In the babynames data, babys gender is a categorical variable, which is treated (because tidyverse) as a character or string. Here, we make it into a factor. We create two other factors as well. # adding third level for non-binary babies sexlevels &lt;- c(&quot;M&quot;, &quot;F&quot;, &quot;O&quot;) babynames2 &lt;- babynames %&gt;% mutate(sex = factor(sex, levels = sexlevels)) %&gt;% mutate(beginVowel = case_when( substr(name,1,1) %in% c(&quot;A&quot;,&quot;E&quot;,&quot;I&quot;,&quot;O&quot;,&quot;U&quot;) ~ &quot;Vowel&quot;, TRUE ~ &quot;Consonant&quot;)) %&gt;% mutate(beginVowel = factor(beginVowel)) %&gt;% mutate (century = case_when( year &lt; 1900 ~ &quot;19th&quot;, year &lt; 2000 ~ &quot;20th&quot;, year &gt; 1999 ~ &quot;21st&quot;)) %&gt;% mutate(century = factor(century)) Use the syntax above to create types of names for different generations (boomers, gen x, Millenials, gen z). Use https://www.kasasa.com/articles/generations/gen-x-gen-y-gen-z to determine your groupings. Say something interesting about the data - names, genders, etc. Plot this. 15.2.2 types of grown-ups If you would instead like to examine survey data, the forcats package includes a set of categorical variables. Using the discussion in Chapter 15 of R4DS as your guide, examine the relationship between two or more of these categorical variables. Again, plot these gss_cat ## # A tibble: 21,483 x 9 ## year marital age race rincome partyid relig denom tvhours ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 2000 Never married 26 White $8000 to 9999 Ind,nea~ Prote~ South~ 12 ## 2 2000 Divorced 48 White $8000 to 9999 Not str~ Prote~ Bapti~ NA ## 3 2000 Widowed 67 White Not applicable Indepen~ Prote~ No de~ 2 ## 4 2000 Never married 39 White Not applicable Ind,nea~ Ortho~ Not a~ 4 ## 5 2000 Divorced 25 White Not applicable Not str~ None Not a~ 1 ## 6 2000 Married 25 White $20000 - 24999 Strong ~ Prote~ South~ NA ## 7 2000 Never married 36 White $25000 or more Not str~ Chris~ Not a~ 3 ## 8 2000 Divorced 44 White $7000 to 7999 Ind,nea~ Prote~ Luthe~ NA ## 9 2000 Married 44 White $25000 or more Not str~ Prote~ Other 0 ## 10 2000 Married 47 White $25000 or more Strong ~ Prote~ South~ 3 ## # ... with 21,473 more rows 15.3 dates The challenges of combining time-demarcated data (Chapter 16) are significant. For dates, a variety of different formats (3-April, October 23, 1943, 10/12/92) must be made sense of. Sometimes we are concerned with durations (how many days, etc.); on other occasions, we are concerned with characteristics of particular dates (as in figuring out the day of the week on which you were born). And dont forget about leap years. In R, the lubridate package (a non-core part of the tidyverse, i.e., one that you must load separately) helps to handle dates and times smoothly. It anticipates many of the problems we might encounter in extracting date and time information from strings. Lubridate generally works well to simplify files with dates and times, and can be used to help in data munging. For example, in my analyses of the Corona data, dates and times were reported in four different ways. The code below decodes these transparently and combines them into a common date/time format . 2/3/20 6 PM 2/3/20 18:00 2/3/20 18:00:00 2020-02-03 18:00:00 # not run coronaData2 &lt;- coronaData %&gt;% mutate (`FixedDate = parse_date_time(`Last Update`, c(&#39;mdy hp&#39;,&#39;mdy HM&#39;, &#39;mdy HMS&#39;,&#39;ymd HMS&#39;))) 15.4 times Working with temporal data is often challenging. The existence of, for example, 12 versus 24 hour clocks, time zones, and daylight savings, can make a simple question about duration quite challenging. Imagine that Fred was born in Singapore at the exact moment of Y2K. He now lives in NYC. How many hours has he been alive as of right now? How would you solve this? # find timezones for Singapore and NYC # a = get datetime for Y2K in Singapore in UTC # b = get datetime for now in NYC in UTC # compute difference and express in sensible metric "],["lists.html", "16 lists", " 16 lists Up until now, we have we have thought about data structures as matrices (rectangles, two-dimensional arrays), in which columns typically correspond to variables and rows to observations, and in which each variable has a particular type, such as numeric or character (or, in the last chapter, special types such as factors and dates/times). In base R, data matrices are typically represented as data frames (type = df). In the Tidyverse, we have been using a special type of data frame, the tibble (type = df and tbl_df). The diamonds dataset, for example, is a tibble: babynames2 &lt;- babynames %&gt;% mutate(generation = case_when( (1944 &lt;= year) &amp; (year &lt;= 1964) ~ &quot;boomer&quot;, (1965 &lt;= year) &amp; (year &lt;= 1979) ~ &quot;genX&quot;, (1980 &lt;= year) &amp; (year &lt;= 1994) ~ &quot;millenial&quot;, (1995 &lt;= year) &amp; (year &lt;= 2019) ~ &quot;genZ&quot;)) %&gt;% mutate(generation = factor(generation)) str(babynames2$generation) ## Factor w/ 4 levels &quot;boomer&quot;,&quot;genX&quot;,..: NA NA NA NA NA NA NA NA NA NA ... babynames3 &lt;- babynames2 %&gt;% # count(generation) %&gt;% na.omit(generation) Within the diamonds tibble, we can examine the types of each variable. This uses the sapply function to SIMPLY APPLY a function (class) to the columns of a data frame or tibble. Here, each column (such as $carat) is a vector, and each vector is homogeneous (of one particular type): diamonds %&gt;% sapply(class) ## $carat ## [1] &quot;numeric&quot; ## ## $cut ## [1] &quot;ordered&quot; &quot;factor&quot; ## ## $color ## [1] &quot;ordered&quot; &quot;factor&quot; ## ## $clarity ## [1] &quot;ordered&quot; &quot;factor&quot; ## ## $depth ## [1] &quot;numeric&quot; ## ## $table ## [1] &quot;numeric&quot; ## ## $price ## [1] &quot;integer&quot; ## ## $x ## [1] &quot;numeric&quot; ## ## $y ## [1] &quot;numeric&quot; ## ## $z ## [1] &quot;numeric&quot; Beyond these atomic vectors, data can take more complex forms, such as hierarchical or tree-like structures such as the following. Honors College Courses Area: Psychology Name: Personality and Social Development Term: Spring 2021 Instructors: Lanning Students: Al Year: Freshman Concentration: Psychology Barb etc. Name: Political Psychology Term: Fall 2020 Instructors: Lanning etc. Nested data sets such as these are common across the Internet. They describe the structure of the webpage you are looking at (which you can see, depending upon your browser, by clicking on something like developer tools). Data formats for representing nested structures include XML (Extensible Markup Language) and JSON (Java Script Object Notation). Many datasets of interest, such as this set of ratings of 10,000 books on Goodreads are structured as XML as well. In R, XML and JSON files will (after some likely massaging), be represented as lists. Lists are recursive, that is, they may include other lists. In addition to external data sources, the results of many procedures within R may also be represented as lists. Consider the following code. What does it do? What is in mod? Why is it stored like this? mod &lt;- lm(price ~ carat, data = diamonds) In R studio, you can inspect the structure of the list by clicking on it in the global environment window, by using the View tab, or with the command str(mod). You can extract rows of your list by including them in single brackets (which will return another list), or double brackets (which will return a vector or data frame). Compare the structure of the following data sets: b1 &lt;- mod[&#39;coefficients&#39;] b2 &lt;- mod[[&#39;coefficients&#39;]] c1 &lt;- mod[&#39;model&#39;] c2 &lt;- mod[[&#39;model&#39;]] Lists are, in a sense, containers (see the example of the pepper shakers in Chapter 20). The single bracket gives us the wrapper as well as what is inside; the double bracket extracts only the inner element. Lists can be challenging, but they are necessary in a world where data is complexly structured. The R package purrr, a core part of the tidyverse, includes functions which simplify working with lists; to learn more, there is a tutorial here, and an overview of the package (with a cheatsheet) here. "],["loops-functions-and-beyond.html", "17 loops, functions, and beyond 17.1 loops 17.2 from loop to apply to purrr::map 17.3 some examples of functions 17.4 how many bottles of what?", " 17 loops, functions, and beyond In one of the most important contemporary theoretical models of intelligence, (Sternberg 1999) has argued that the ability to automatize, that is, to work efficiently on repeated or habitual tasks, is a key component of intelligent behavior. Solving habitual problems efficiently - whether it is making a cup of coffee, finding the shortest path to complete a shopping list in a supermarket or a series of errands across town - allows us to focus our limited resources on other challenging tasks that, in turn, may determine whether we survive, or at least prosper. In programming, loops and functions are essential tools for making repetitive tasks simple. Simplifying your code is one of the more intellectually satisfying aspects of working in R or in any programming language. In R, loops are supplemented by additional tools for simplifying and avoiding repetition in code, including the apply family and the map function in purrr. Functions (and, beyond this, custom libraries) can further streamline your work. 17.1 loops Consider the task of printing out a series of numbers. Heres a simple example of how this could be done in a loop in R. It prints numbers between 1995 and 1998 (inclusive). for (i in 1995:1998) { # i is an index code print(i) # print the ith value in the sequence } # go to the next one until the range is complete ## [1] 1995 ## [1] 1996 ## [1] 1997 ## [1] 1998 Lets expand on this a little, connecting back to the babynames data. This will print counts of the number of babies for each year in GenerationZ, which includes years from 1995-2015. genZ &lt;- (1995:2015) # this reduces the (big) babynames to a simple file of years and counts babyCounts &lt;- babynames %&gt;% group_by(year) %&gt;% filter(year %in% genZ) %&gt;% summarize(nbabies = sum(n)) # and this uses a for loop to print each row in turn for (i in (1:nrow(babyCounts))) { # filename[i,j] == ith row and jth column print(paste((babyCounts[i,1]), babyCounts[i,2])) } ## [1] &quot;1995 3661351&quot; ## [1] &quot;1996 3646362&quot; ## [1] &quot;1997 3624799&quot; ## [1] &quot;1998 3677107&quot; ## [1] &quot;1999 3692537&quot; ## [1] &quot;2000 3778079&quot; ## [1] &quot;2001 3741451&quot; ## [1] &quot;2002 3736042&quot; ## [1] &quot;2003 3799971&quot; ## [1] &quot;2004 3818361&quot; ## [1] &quot;2005 3842004&quot; ## [1] &quot;2006 3952932&quot; ## [1] &quot;2007 3994007&quot; ## [1] &quot;2008 3926358&quot; ## [1] &quot;2009 3815638&quot; ## [1] &quot;2010 3690700&quot; ## [1] &quot;2011 3651914&quot; ## [1] &quot;2012 3650462&quot; ## [1] &quot;2013 3637310&quot; ## [1] &quot;2014 3696311&quot; ## [1] &quot;2015 3688687&quot; The syntax of loops, including where to put parentheses in index statements, can be tricky. Expect to refer to Google and StackExchange often in order to get your code running. Another good source is Chapter 21 of R4DS. There, Wickham goes in to additional detail, including a description of seq_along(df) as a tool for creating an index corresponding to 1:ncol(df). Heres an example with the diamonds dataset: diamonds2 &lt;- diamonds %&gt;% select_if(., is.numeric) # for (i in (1:ncol(diamonds2))) { for (i in seq_along(diamonds2)) { print(paste(names(diamonds2[i]), round(mean(diamonds2[[i]]),2))) } ## [1] &quot;carat 0.8&quot; ## [1] &quot;depth 61.75&quot; ## [1] &quot;table 57.46&quot; ## [1] &quot;price 3932.8&quot; ## [1] &quot;x 5.73&quot; ## [1] &quot;y 5.73&quot; ## [1] &quot;z 3.54&quot; Chapter 21 of R4DS also considers some extensions to related problems such as loops of indefinite length, which can often be addressed using the while command. 17.2 from loop to apply to purrr::map Understanding for loops is fundamental in programming, but in R they should often be sidestepped. If the order of iteration isnt important (if, for example, it doesnt matter which of the diamonds variables we take the mean of first), then using one of the measures from the apply family can generally be used to make your code simpler and more efficient. The logic is that one takes a dataframe (df or tibble), then applies a function to its rows or columns: # apply = apply the function (mean) to the columns(2) of the df diamonds2 %&gt;% apply(2,mean) ## carat depth table price x y ## 0.7979397 61.7494049 57.4571839 3932.7997219 5.7311572 5.7345260 ## z ## 3.5387338 # sapply = simply apply - guesses that you are looking for col. means diamonds2 %&gt;% sapply(mean) ## carat depth table price x y ## 0.7979397 61.7494049 57.4571839 3932.7997219 5.7311572 5.7345260 ## z ## 3.5387338 The many variants of the apply family, including lapply [list apply] and tapply [table apply] as well as sapply, each have their own uses and can be quite efficient but, again, can be syntactically challenging. In the evolving tidyverse, the map family of commands is supplementing if not supplanting apply; these commands (part of the purrr package in the core tidyverse) may prove to be more convenient and clear. For example, the map_df function will apply a function and return a dataframe (tibble), which can be handy for further analysis. diamonds2 %&gt;% map_df(mean) ## # A tibble: 1 x 7 ## carat depth table price x y z ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.798 61.7 57.5 3933. 5.73 5.73 3.54 17.3 some examples of functions If you repeat a series of lines of code several times in your program, it is often best to wrap this into a function. The first example is from a preregistered study I recently started of language and politics. For the preregistration, I ran analyses using simulated data, both to increase the likelihood that the code will run without error on real data and to help anticipate the analyses which are to be run on real data. I began by getting a real body of text from the net and scrambling it, then constructing fake Republican and Democratic texts from this. Here, I illustrate this by constructing 50 sample documents, each consisting of between 5 and 20 words. The project is given in four steps: 17.3.1 preliminaries Here is the preliminary stuff, where I pull the data off the net and initialize the variables sampledata &lt;- read_csv(&quot;data/sentiment-words-DFE-785960.csv&quot;)#url( # &quot;https://www.crowdflower.com/wp-content/uploads/2016/03/sentiment-words-DFE-785960.csv&quot;)) #&quot;https://raw.githubusercontent.com/totalgood/hope/master/data/corpora/sentiment-words-DFE-785960.csv&quot;)) # pulls off four words sampledata &lt;- sampledata[22:25] %&gt;% na.omit() ndocs &lt;- 50 minDocLength &lt;- 5 maxDocLength &lt;- 20 doc &lt;- vector(mode = &quot;character&quot;, length = ndocs) 17.3.2 the function Heres the simple function which pulls a random word out of the matrix of sampledata. set.seed(33458) # a random seed is used to allow reproducible results getword &lt;- function() { rowid &lt;- sample(1:nrow(sampledata), 1) colid &lt;- sample(1:ncol(sampledata), 1) sampledata[rowid,colid] } 17.3.3 applying the function The function is applied, first, to extract one word, then, in successive loops, to build up one phrase and then many. # combine words into docs # establish length of first phrase docLength &lt;- sample(minDocLength:maxDocLength,1) # initialize with one word sampleCorpus &lt;- getword() # loop to build up first phrase for (i in 1:docLength) { addWord &lt;- getword() sampleCorpus &lt;- paste(sampleCorpus, addWord) } #add additional simulated documents for (j in 2:ndocs) { docLength &lt;- sample(minDocLength:maxDocLength,1) newdoc &lt;- getword() for (i in 1:docLength - 1) { addWord &lt;- getword() newdoc &lt;- paste(newdoc, addWord) } sampleCorpus &lt;- rbind(newdoc,sampleCorpus) } Finally, the results are combined with a vector of alternating labels of Dem and Rep: row.names(sampleCorpus) &lt;- NULL evenOdd &lt;- rep(c(&quot;Dem&quot;,&quot;Rep&quot;),length.out = nrow(sampleCorpus)) workingCorpus &lt;- as_tibble(cbind(evenOdd,sampleCorpus)) head(workingCorpus,5) ## # A tibble: 5 x 2 ## evenOdd V2 ## &lt;chr&gt; &lt;chr&gt; ## 1 Dem wild at heart nobody loves me lost art miss the sun need a doctor kin~ ## 2 Rep great player awfully nice back hurts not paying pretty rubbish found ~ ## 3 Dem yahooo make no promises horrible daughter ew crazy evening fit ## 4 Rep can&#39;t sleep lazy afternoon sucks #fedup great shame positive break fa~ ## 5 Dem problems nice funniest hurt not necessarily sad nose eat no problem d~ 17.4 how many bottles of what? To put the fun back into function, heres a solution to the 99 bottles of beer function described in r4DS 21.2.1. Study the code. Ask or answer a question about it in class or on Slack. beerSong &lt;- function(liquid = &quot;beer&quot;, count = 99, surface = &quot;wall&quot;) { songtext &lt;- &quot;&quot; for (i in (count:1)) { thisLine = (paste0(i, &quot; bottles of &quot;, liquid, &quot; on the &quot;, surface, &quot;, you take one down and pass it around,\\n&quot;)) songtext = c(songtext, thisLine) } songtext = c(songtext, (paste0(&quot;no more bottles of &quot;, liquid,&quot; on the &quot;, surface,&quot;...&quot;))) cat(songtext) # cat prints without line numbers } #beerSong() And here are solutions proposed by some of your classmates over the last few years. How do they differ from each other, and from the solution given above? beersng &lt;- function(n) { if (n == 1) { cat(&quot;\\n&quot;,n,&quot; bottle of beer on the wall, &quot;,n, &quot; bottles of beer.\\nTake one down and pass it around,&quot;, &quot; no more bottles of beer on the wall.\\n&quot;, sep = &quot;&quot;) cat(&quot;\\nNo more bottles of beer on the wall, no more bottles of beer.\\n&quot;, &quot;Go to the store and buy some more, 99 bottles of beer on the wall.\\n&quot;, sep=&quot;&quot;) } else { cat(&quot;\\n&quot;,n,&quot; bottles of beer on the wall, &quot;,n, &quot; bottles of beer.\\nTake one down and pass it around, &quot;, n-1, &quot; more bottles of beer on the wall.\\n&quot;, sep=&quot;&quot;) return(beersng(n-1)) } } #beersng(99) moreBeer &lt;- function () { for (i in 0:100){ starting_number &lt;- 100 if (starting_number - i == 0) { print(&quot;No more bottles of beer on the wall, no more bottles of beer, Go to the store and buy some more, 99 bottles of beer on the wall.&quot;) break } print(paste(starting_number - i, &quot;bottles of beer on the wall&quot;, &quot;Take one down and pass it around,&quot;, starting_number - i - 1, &quot;bottles of beer on the wall.&quot;)) } } # moreBeer() song &lt;- function(bottlesofbeer){ for(i in bottlesofbeer:1){ cat(bottlesofbeer,&quot; bottles of beer on the wall \\n&quot;, bottlesofbeer,&quot; bottles of beer \\nTake one down, pass it around \\n&quot;, bottlesofbeer-1, &quot; bottles of beer on the wall \\n&quot;,&quot; \\n&quot;) bottlesofbeer = bottlesofbeer - 1 } } #song(99) Which code is best? Good code is clear, but it is also efficient. We probably shouldnt expect much in the way of differences between these functions in terms of speed (as each includes 99 iterations of a simple print command), but heres a simple way to compare. Note that Ive used the sink command to write my output to files rather than consoles: start_time &lt;- Sys.time() sink (file = &quot;f1.txt&quot;) beerSong(&quot;beer&quot;,99) sink() end_time &lt;- Sys.time() beerSongtime &lt;- end_time - start_time start_time &lt;- Sys.time() sink (file = &quot;f2.txt&quot;) beersng(99) sink() end_time &lt;- Sys.time() beersngtime &lt;- end_time - start_time start_time &lt;- Sys.time() sink (file = &quot;f3.txt&quot;) song(99) sink() end_time &lt;- Sys.time() songtime &lt;- end_time - start_time start_time &lt;- Sys.time() sink (file = &quot;f4.txt&quot;) moreBeer() sink() end_time &lt;- Sys.time() moreBeertime &lt;- end_time - start_time Here is the first line of output from each function, together with table showing the elapsed times: readLines(&quot;f1.txt&quot;,1) ## character(0) readLines(&quot;f2.txt&quot;,2) ## character(0) readLines(&quot;f3.txt&quot;,4) ## character(0) readLines(&quot;f4.txt&quot;,1) ## character(0) tibble(functionName = c(&quot;beerSongtime&quot;, &quot;beersngtime&quot;, &quot;songtime&quot;, &quot;moreBeertime&quot;), time = c(beerSongtime,beersngtime, songtime, moreBeertime)) %&gt;% kable(digits = 2) functionName time beerSongtime 0.03 secs beersngtime 0.03 secs songtime 0.02 secs moreBeertime 0.02 secs References "],["from-correlation-to-regression.html", "18 from correlation to regression 18.1 correlation 18.2 correlations based on small samples are unstable: A Monte Carlo demonstration 18.3 multiple regression 18.4 Swiss fertility data 18.5 marital affairs data", " 18 from correlation to regression In Chapter 22 of R4DS, Wickham introduces modeling as a complement to data visualization. At the beginning of Chapter 23, he points out that: The goal of a model is to provide a simple low-dimensional summary of a dataset. In the remainder of his section on modeling (Chapters 23 - 25), he presents an innovative approach to introducing models. Feel free to explore these. Our treatment, however, will deviate from this. 18.1 correlation (This section is excerpted directly from From https://github.com/datasciencelabs) Francis Galton, a polymath and cousin of Charles Darwin was interested, among other things, in how well can we predict a sons height based on the parents heights. We have access to Galtons family height data through the HistData package. We will create a dataset with the heights of fathers and the first son of each family. Here are the key summary statistics for the two variables of father and son height taken alone: library(HistData) data(&quot;GaltonFamilies&quot;) galton_heights &lt;- GaltonFamilies %&gt;% filter(childNum == 1 &amp; gender == &quot;male&quot;) %&gt;% select(father, childHeight) %&gt;% rename(son = childHeight) galton_heights %&gt;% summarise(mean(father), sd(father), mean(son), sd(son)) ## mean(father) sd(father) mean(son) sd(son) ## 1 69.09888 2.546555 70.45475 2.557061 This univariate description fails to capture the key characteristic of the data, namely, the idea that there is a relationship between the two variables: galton_heights %&gt;% ggplot(aes(father, son)) + geom_point(col = &#39;forestgreen&#39;) Figure 18.1: Heights of father and son pairs plotted against each other. The correlation summarizes this relationship. In these data, the correlation (r) is about .50. galton_heights %&gt;% summarize(cor(father, son)) %&gt;% round(3) ## cor(father, son) ## 1 0.501 18.2 correlations based on small samples are unstable: A Monte Carlo demonstration Correlations based on small samples can bounce around quite a bit. Consider what happens when, for example, we sample just 25 cases from Galtons data, and compute the correlation from this. We repeat this 1000 times, then plot the distribution of these sample rs: set.seed(33458) # why do I do this? nTrials &lt;- 1000 nPerTrial &lt;- 25 replications &lt;- replicate(nTrials, { sample_n(galton_heights, nPerTrial, replace = TRUE) %&gt;% # we sample with replacement here summarize(r=cor(father, son)) %&gt;% .$r }) replications %&gt;% as_tibble() %&gt;% ggplot(aes(replications)) + geom_histogram(binwidth = 0.05, col = &quot;blue&quot;, fill = &quot;cyan&quot;) These sample correlations range from -0.002 to 0.882. Their average, however, at 0.503 is almost exactly that of the overall population. Often in data science, we will estimate population parameters in this way - by repeated sampling, and by studying the extent to which results are consistent across samples. More on that later. 18.2.1 The regression line If we are predicting a random variable \\(Y\\) knowing the value of another \\(X=x\\) using a regression line, then we predict that for every standard deviation, \\(\\sigma_X\\), that \\(x\\) increases above the average \\(mu_X\\), \\(Y\\) increases \\(\\rho\\) standard deviations \\(\\sigma_Y\\) above the average \\(\\mu_Y\\), where \\(\\rho\\) is the correlation between \\(X\\) and \\(Y\\). (Confusing, yes? Make sense of it by making it concrete: Let \\(Y\\) = College GPA, \\(X\\) = HS GPA, \\(X = x\\) = 3.2  ) To manually add a regression line to a plots, we will need the means and standard deviations of each variable together with their correlation: mu_x &lt;- mean(galton_heights$father) mu_y &lt;- mean(galton_heights$son) s_x &lt;- sd(galton_heights$father) s_y &lt;- sd(galton_heights$son) r &lt;- cor(galton_heights$father, galton_heights$son) m &lt;- r * s_y / s_x b &lt;- mu_y - m*mu_x galton_heights %&gt;% ggplot(aes(father, son)) + geom_point(alpha = 0.5) + geom_abline(intercept = b, slope = m ) If we first standardize the variables, then the regression line has intercept 0 and slope equal to the correlation \\(\\rho\\). Here, the slope, regression line, and correlation are all equal (Ive made the plot square to better indicate this). galton_heights %&gt;% ggplot(aes(scale(father), scale(son))) + geom_point(alpha = 0.5) + geom_abline(intercept = 0, slope = r) 18.2.2 Warning: there are two regression lines In general, when describing bivariate relationships, we talk of predicting y from x. But what if we want to reverse this? Here, for example, what if we want to predict the fathers height based on the sons? For this, the intercept and slope of the lines will differ # son from father m_1 &lt;- r * s_y / s_x b_1 &lt;- mu_y - m_1*mu_x # vice-versa m_2 &lt;- r * s_x / s_y b_2 &lt;- mu_x - m_2*mu_y We will still see regression to the mean (the prediction for the father is closer to the father average than the son heights \\(y\\) is to the son average). Here is a plot showing the two regression lines, with blue for the predicting son heights with father heights and red for predicting father heights with son heights. galton_heights %&gt;% ggplot(aes(father, son)) + geom_point(alpha = 0.5) + geom_abline(intercept = b_1, slope = m_1, col = &quot;blue&quot;) + geom_abline(intercept = -b_2/m_2, slope = 1/m_2, col = &quot;red&quot;) 18.3 multiple regression We extend regression (predicting one continuous variable from another) to the multivariate case (predicting one variable from many). We extend this logic to the cases where our variables are not normally distributed, as in the case of dichotomous variables (yes/no, true/false), as well as counts, which often are skewed. (This section is drawn from Peng, Caffo, and Leeks treatment from Coursera/ the Johns Hopkins Data Science Program. You may need to install the packages UsingR, GGally and/orHmisc) library(broom) # an older package for unlisting regression results library(modelr) library(GGally) # for scatterplot matrix library(Ecdat) # Econometric data incl. affairs dataset 18.4 Swiss fertility data To consider the multivariate case, we examine a second dataset. data(swiss) str(swiss) ## &#39;data.frame&#39;: 47 obs. of 6 variables: ## $ Fertility : num 80.2 83.1 92.5 85.8 76.9 76.1 83.8 92.4 82.4 82.9 ... ## $ Agriculture : num 17 45.1 39.7 36.5 43.5 35.3 70.2 67.8 53.3 45.2 ... ## $ Examination : int 15 6 5 12 17 9 16 14 12 16 ... ## $ Education : int 12 9 5 7 15 7 7 8 7 13 ... ## $ Catholic : num 9.96 84.84 93.4 33.77 5.16 ... ## $ Infant.Mortality: num 22.2 22.2 20.2 20.3 20.6 26.6 23.6 24.9 21 24.4 ... Heres a scatterplot matrix of the Swiss data. Look at the first column of plots (or first row of the correlations). What is the relationship between fertility and each of the other variables? ggpairs (swiss, lower = list( continuous = &quot;smooth&quot;), axisLabels =&quot;none&quot;, switch = &#39;both&#39;) Here, we predict fertility from all of the remaining variables together in a single regression analysis, using the lm (linear model) command. As we saw in Chapter 15, the result of this analysis is a list. We can pull out the key features of the data using the summary() command. How do you interpret this? swissReg &lt;- lm(Fertility ~ ., data=swiss) summary(swissReg) ## ## Call: ## lm(formula = Fertility ~ ., data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.2743 -5.2617 0.5032 4.1198 15.3213 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66.91518 10.70604 6.250 1.91e-07 *** ## Agriculture -0.17211 0.07030 -2.448 0.01873 * ## Examination -0.25801 0.25388 -1.016 0.31546 ## Education -0.87094 0.18303 -4.758 2.43e-05 *** ## Catholic 0.10412 0.03526 2.953 0.00519 ** ## Infant.Mortality 1.07705 0.38172 2.822 0.00734 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.165 on 41 degrees of freedom ## Multiple R-squared: 0.7067, Adjusted R-squared: 0.671 ## F-statistic: 19.76 on 5 and 41 DF, p-value: 5.594e-10 18.5 marital affairs data Heres another dataset. Well apply ggpairs here, but for clarity will show only half of the data at a time. The dependent variable of interest (nbaffairs) will be included in both plots: Fair1 &lt;- Fair %&gt;% select(sex:child, nbaffairs) ggpairs(Fair1, # if you wanted to jam all 9 vars onto one page you could do this # upper = list(continuous = wrap(ggally_cor, size = 10)), lower = list(continuous = &#39;smooth&#39;), axisLabels = &quot;none&quot;, switch = &#39;both&#39;) Fair2 &lt;- Fair %&gt;% select(religious:nbaffairs) ggpairs(Fair2, lower = list(continuous = &#39;smooth&#39;), axisLabels = &quot;none&quot;, switch = &#39;both&#39;) Exercise Study the data. Learn about the measures by looking at Fair in the help tab of R studio. What do the scatterplot matrices shown above tell us? Can you just run a regression on it as is? Try predicting nbaffairs from the remaining 8 variables in Fair (not Fair1 or Fair2), using the same syntax in the Swiss data. Does it work? What do the results suggest? affairReg &lt;- lm(nbaffairs ~ ., data=Fair) summary(affairReg) ## ## Call: ## lm(formula = nbaffairs ~ ., data = Fair) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.0503 -1.7226 -0.7947 0.2101 12.7036 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.87201 1.13750 5.162 3.34e-07 *** ## sexmale 0.05409 0.30049 0.180 0.8572 ## age -0.05098 0.02262 -2.254 0.0246 * ## ym 0.16947 0.04122 4.111 4.50e-05 *** ## childyes -0.14262 0.35020 -0.407 0.6840 ## religious -0.47761 0.11173 -4.275 2.23e-05 *** ## education -0.01375 0.06414 -0.214 0.8303 ## occupation 0.10492 0.08888 1.180 0.2383 ## rate -0.71188 0.12001 -5.932 5.09e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.095 on 592 degrees of freedom ## Multiple R-squared: 0.1317, Adjusted R-squared: 0.12 ## F-statistic: 11.23 on 8 and 592 DF, p-value: 7.472e-15 "],["cross-validation.html", "19 cross-validation 19.1 revisiting the affairs data 19.2 avoiding capitalizing on chance 19.3 an example of cross-validated linear regression", " 19 cross-validation The problem of capitalizing on chance is a significant one in prediction, and one should always be skeptical of models which are untested beyond the sample from which they were derived. 19.1 revisiting the affairs data Lets go back and do what we should have done earlier, that is, examine and think about the number of affairs variable. What does the distribution look like? data(Fair) Fair %&gt;% count(nbaffairs) ## nbaffairs n ## 1 0 451 ## 2 1 34 ## 3 2 17 ## 4 3 19 ## 5 7 42 ## 6 12 38 We note that the distribution is skewed - and we realize that perhaps we should think about it differently: The meaning of the difference between 0 and 1 is not the same as that between 1 and 2, or perhaps even 1 and 12. We can transform the data in several ways. We might perform a root transform, which we could then round to an integer: Fair &lt;- Fair %&gt;% mutate(rootAffair = as.integer(sqrt(nbaffairs))) Fair %&gt;% count(rootAffair) ## rootAffair n ## 1 0 451 ## 2 1 70 ## 3 2 42 ## 4 3 38 Or we could simply distinguish between those that do and dont have affairs. Fair &lt;- Fair %&gt;% mutate(affairYN = ifelse (nbaffairs &gt; 0,1,0)) Fair %&gt;% count(affairYN) ## affairYN n ## 1 0 451 ## 2 1 150 Well examine each of these in the following paragraphs. 19.2 avoiding capitalizing on chance One lesson from the last class was that correlations (and regression coefficients) drawn from small samples were not stable. In regression analysis, as progressively smaller samples were drawn from the Fair data, the ability to predict the outcome increased. In the limiting case, when the number of predictors (variables) was equal to the number of cases in the sample (rows), prediction became perfect. 19.2.1 splitting the data into training and test subsamples The most basic solution to this problem is to split the data into two groups, a training sample from which we extract our model, and a test sample on which you will assess it. (Often, the logic of this will be extended to include a third group, a validation sample which would be used to tune or select the results of the training run before the test data are examined). Here, we will consider the simpler approach, splitting the Fair data into training and test samples. The critical feature of this analysis is that we will hold out the test data, and not even look at it until after our model building is complete. # establish a seed for your data-split # so that your results will be reproducible set.seed(33458) n &lt;- nrow(Fair) # create a set of line numbers # of size corresponding to the # desired training sample trainIndex &lt;- sample(1:n, size = round(0.6*n), replace=FALSE) # create training and test samples trainFair &lt;- Fair[trainIndex ,] testFair &lt;- Fair[-trainIndex ,] 19.3 an example of cross-validated linear regression We first predict the variable rootAffair, using just the training data: trainFair2 &lt;- trainFair %&gt;% select(-nbaffairs, -affairYN) testFair2 &lt;- testFair %&gt;% select(-nbaffairs, -affairYN) model2 &lt;- lm(rootAffair ~ ., data=trainFair2) summary(model2) ## ## Call: ## lm(formula = rootAffair ~ ., data = trainFair2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.6655 -0.4923 -0.2180 0.1444 2.7593 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.974758 0.385662 5.120 5.03e-07 *** ## sexmale -0.045067 0.106127 -0.425 0.6713 ## age -0.014193 0.007835 -1.812 0.0709 . ## ym 0.038914 0.013901 2.799 0.0054 ** ## childyes 0.027497 0.116926 0.235 0.8142 ## religious -0.090619 0.038503 -2.354 0.0191 * ## education -0.018241 0.021963 -0.831 0.4068 ## occupation 0.061293 0.030904 1.983 0.0481 * ## rate -0.265213 0.042161 -6.291 9.39e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8224 on 352 degrees of freedom ## Multiple R-squared: 0.1783, Adjusted R-squared: 0.1597 ## F-statistic: 9.549 on 8 and 352 DF, p-value: 5.595e-12 To assess the effectiveness of this model on an independent sample, we write a simple function, which assesses R2, then applies it to both the training data and the test data: R2.model.dep.data &lt;- function(myModel,myDep,myData) { errorscores &lt;- myDep - predict(myModel,myData) SS.error &lt;- sum(errorscores^2) deviations &lt;- scale(myDep,scale = FALSE)#) SS.total &lt;- sum(deviations^2) 1 - (SS.error/SS.total) } R2Train &lt;- R2.model.dep.data(model2,trainFair2$rootAffair,trainFair2) (R2Test &lt;- R2.model.dep.data(model2,testFair2$rootAffair,testFair2)) ## [1] 0.003453435 The R2 on the training sample is 0.1783303, on the test sample it is 0.0034534. The difference between these is sometimes referred to as shrinkage. Shrinkage will be problematic particularly when there are a small number of observations, a large number of predictors (and consequently a complex model), or both of these. 19.3.1 applying logistic regression analysis to the training data Lets go back to the number of affairs variable and consider a second way of thinking about it, that is, simply comparing those who do and dont have affairs: Fair &lt;- Fair %&gt;% mutate(affairYN = ifelse (nbaffairs &gt; 0,1,0)) Fair %&gt;% count(affairYN) ## affairYN n ## 1 0 451 ## 2 1 150 With this change, the regression problem becomes more clearly a problem in classification. How can we best predict which type (affair-ers vs. not) a given person falls in to? With this change, we have moved from a variable which is essentially continuous (nbaffairs) to one which is dichotomous and therefore distributed binomially. The desired regression is now a logistic one. We begin by running this analysis using only the training data. model2 &lt;- glm(affairYN ~ ., data = trainFair, family = &quot;binomial&quot;) summary (model2) ## ## Call: ## glm(formula = affairYN ~ ., family = &quot;binomial&quot;, data = trainFair) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.457e-06 -2.789e-06 -2.564e-06 2.110e-08 1.164e-05 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.601e+01 1.434e+05 0.000 1.000 ## sexmale 3.818e-02 3.891e+04 0.000 1.000 ## age -1.303e-02 3.176e+03 0.000 1.000 ## ym 1.490e-02 5.403e+03 0.000 1.000 ## childyes 2.288e-02 4.479e+04 0.000 1.000 ## religious 2.225e-02 1.508e+04 0.000 1.000 ## education 3.888e-02 8.532e+03 0.000 1.000 ## occupation -4.976e-02 1.113e+04 0.000 1.000 ## rate -1.357e-01 1.591e+04 0.000 1.000 ## nbaffairs -3.191e+01 4.714e+04 -0.001 0.999 ## rootAffair 1.455e+02 1.626e+05 0.001 0.999 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 4.0981e+02 on 360 degrees of freedom ## Residual deviance: 3.2775e-09 on 350 degrees of freedom ## AIC: 22 ## ## Number of Fisher Scoring iterations: 25 Our predicted scores are continuous, corresponding to the probability that a given person will have an affair. Heres how they are distributed (still on the training data here): predictTrain &lt;- predict(model2, trainFair, type = &quot;response&quot;) summary (predictTrain) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.0000 0.0000 0.2548 1.0000 1.0000 As scores on predictTrain increase, the predicted likelihood of an affair increases. If you are considering marriage to a potential partner, what score would be too high? How do we distinguish ok from not? "],["prediction-and-classification.html", "20 prediction and classification 20.1 from regression to classification: selection of a threshold 20.2 another approach to classification: k-nearest neighbor", " 20 prediction and classification If we want to classify people, we will need to create a decision threshold at which we will change our prediction from no to yes. 20.1 from regression to classification: selection of a threshold We will continue to work with the affairs data; heres the relevant code from last chapter: # code from last chapter on Fair data Fair &lt;- Fair %&gt;% mutate(rootAffair = as.integer(sqrt(nbaffairs))) %&gt;% mutate(affairYN = ifelse (nbaffairs &gt; 0,1,0)) set.seed(33458) n &lt;- nrow(Fair) trainIndex &lt;- sample(1:n, size = round(0.6*n), replace=FALSE) trainFair &lt;- Fair[trainIndex ,] testFair &lt;- Fair[-trainIndex ,] trainFair2 &lt;- trainFair %&gt;% select(-nbaffairs, -affairYN) testFair2 &lt;- testFair %&gt;% select(-nbaffairs, -affairYN) model2 &lt;- lm(rootAffair ~ ., data = trainFair2) predictTrain &lt;- predict(model2, trainFair, type = &quot;response&quot;) To maximise overall prediction, we will create a threshold equal to the actual proportion of people who dont have affairs in our sample: (threshold &lt;- mean(trainFair$affairYN)) ## [1] 0.2548476 This is equal to both the mean of our predicted scores (above) and the mean of our actual scores, and, because this is a dichotomous variable, the proportion of people in the sample who have affairs. Well predict that if a person has a predicted score more than this well predict that s/he will be unfaithful, else we will PredictOK. Then we will create a confusion matrix, to compare our correct predictions (PredictOK and affairYN = 1, Predictunfaithful and affairYN = 0) with the remainder. classification &lt;- ifelse(predictTrain &gt; threshold, &quot;Predictunfaithful&quot;, &quot;PredictOK&quot;) (b &lt;- table(classification, trainFair$affairYN)) ## ## classification 0 1 ## PredictOK 107 18 ## Predictunfaithful 162 74 (correct &lt;- b[1,1] + b[2,2]) ## [1] 181 (errors &lt;- b[1,2] + b[2,1]) ## [1] 180 (accuracy &lt;- correct/(correct + errors)) ## [1] 0.501385 20.1.1 applying the model to the test data We shouldnt trust these results, though, because the model is based on the same data that it is tested upon. Now we will apply the model to the test data. With the linear regression model above, we examined shrinkage in the overall R-square. Here, we can assess shrinkage in terms of the percent of erroneous classifications. Typically (but not invariably), the percent of accurate classifications will decline, especially if the model is a complex one with many variables or if the number of observations is low. predictTest &lt;- predict(model2, testFair, type = &quot;response&quot;) classification &lt;- ifelse(predictTest &gt; threshold, &quot;Predictunfaithful&quot;, &quot;PredictOK&quot;) (b &lt;- table(classification, testFair$affairYN)) ## ## classification 0 1 ## PredictOK 73 9 ## Predictunfaithful 109 49 (correct &lt;- b[1,1] + b[2,2]) ## [1] 122 (errors &lt;- b[1,2] + b[2,1]) ## [1] 118 (accuracy &lt;- correct/(correct + errors)) ## [1] 0.5083333 On the test data, we are correct 51 percent of the time. This describes our out of sample risk. 20.1.2 changing our decision threshold In many decision problems, there is an asymmetry in the cost of different types of errors: if you are foraging for mushrooms, for example, an error of the form (you decide its safe and it is poisonous) is more costly than the converse (you decide its poisonous and it is safe). This may be true in the present example as well. Consider someone who is looking for a spouse, but is really averse to the idea of getting hurt by an affair. That person might feel like the cost of marrying an unfaithful person is much greater than the cost of not marrying a faithful one. So we adjust the threshold downwards: threshold &lt;- .05 classification &lt;- ifelse(predictTest &gt; threshold, &quot;Predictunfaithful&quot;, &quot;PredictOK&quot;) (b &lt;- table(classification, testFair$affairYN)) ## ## classification 0 1 ## PredictOK 27 3 ## Predictunfaithful 155 55 (correct &lt;- b[1,1] + b[2,2]) ## [1] 82 (errors &lt;- b[1,2] + b[2,1]) ## [1] 158 (accuracy &lt;- correct/(correct + errors)) ## [1] 0.3416667 The overall accuracy - that is, number of correct classifications - drops. But thats not what we are really interested in, rather, we are interested in minimizing hurt. Heres another example: Someone who is very lonely might feel the opposite, and be willing to accept greater substantially greater risk. threshold &lt;- .5 classification &lt;- ifelse(predictTest &gt; threshold, &quot;Predictunfaithful&quot;, &quot;PredictOK&quot;) (b &lt;- table(classification, testFair$affairYN)) ## ## classification 0 1 ## PredictOK 111 21 ## Predictunfaithful 71 37 (correct &lt;- b[1,1] + b[2,2]) ## [1] 148 (errors &lt;- b[1,2] + b[2,1]) ## [1] 92 (accuracy &lt;- correct/(correct + errors)) ## [1] 0.6166667 Prediction is higher here - but not much higher than it would be if we raised the threshold even further, and just assumed that everyone can be trusted. Then, our error rate would be 25 percent. When overall predictability is low, its often the case that you can maximize correct predictions by simply predicting that everyone will be in the most popular category. Predicting rare events, such as suicides, is particularly difficult. 20.1.3 more confusion There is a nice shortcut to generating confusion matrices such as those above using the caret package. This function describes outcomes in several ways, as there are many languages for describing outcomes in 2 x 2 tables, including Type I vs. Type II errors, Hits vs. False Alarms/False Positives, and Sensitivity vs. Specificity. In these data, its been set up so that hit rate ~ sensitivity ~ (no affair | no affair) correct rejection ~ specificity ~ (affair | affair) threshold &lt;- .5 # syntax for classification in caret is a little different # (the labels for the actual and predicted scores have to be the same) #classification &lt;- ifelse(predictTest &gt; threshold, # &quot;Predictunfaithful&quot;, &quot;PredictOK&quot;) classification &lt;- ifelse(predictTest &gt; threshold, 1,0) # caret package (newest) requires explicit matching of factors classification &lt;- as.factor(classification) testFair$affairYN &lt;- as.factor(testFair$affairYN) confusionMatrix(classification, testFair$affairYN) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 111 21 ## 1 71 37 ## ## Accuracy : 0.6167 ## 95% CI : (0.5519, 0.6785) ## No Information Rate : 0.7583 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : 0.1916 ## ## Mcnemar&#39;s Test P-Value : 3.245e-07 ## ## Sensitivity : 0.6099 ## Specificity : 0.6379 ## Pos Pred Value : 0.8409 ## Neg Pred Value : 0.3426 ## Prevalence : 0.7583 ## Detection Rate : 0.4625 ## Detection Prevalence : 0.5500 ## Balanced Accuracy : 0.6239 ## ## &#39;Positive&#39; Class : 0 ## 20.1.4 ROCs and AUC Each of these decision thresholds describes the performance of a model at a particular point. We can combine the thresholds and plot them in Receiver Operating Characteristic (ROC) curves. The area under the curve (AUC) is a great measure of model accuracy, in that it summarizes how effective a classifier is across all possible thresholds. # fig.width and fig.height specified to get square plots # colAUC function gets stats etc AUCModel &lt;- colAUC(predictTest, testFair[[&quot;affairYN&quot;]], plotROC = TRUE) AUCModel ## [,1] ## 0 vs. 1 0.6662562 20.2 another approach to classification: k-nearest neighbor Real-life social predictions are often guided not by induction or the (optimized) combination of a set of predictor variables. Rather, we often reason by analogy - we might think, for example, that I wont go out with Fred because he reminds me of Larry, and Larry was kind of a jerk. If regression analysis is an approach to prediction based in our set of variables, k-nearest neighbor analysis instead makes predictions based on observations. Formally, as described in the documentation for the knn package, For each row of the test set, the k nearest (in Euclidean distance) training set vectors are found, and the classification is decided by majority vote, with ties broken at random. (ref). In the simplest form of this analysis, we find the nearest thing to a doppelganger (a look-alike or near double) for a given observation. So, in the affairs data, if a person is most like someone else in the dataset who has had an affair, we predict an affair, else not. 20.2.1 application: the affairs data Begin by loading the affairs data from last time. Using the same seed (33458) means that the same set of training and test cases will be extracted as in the prior analysis data(Fair) # one change here: Note the bidirectional pipe to simplify code # use only when you are sure that your file update is ok Fair %&lt;&gt;% # &lt;- Fair %&gt;% mutate(affairYN = # nbaffairs is set up as a factor # to allow confusionmatrix to run as.factor(ifelse(nbaffairs &gt; 0,1,0))) %&gt;% # unlike the lm and glm commands, knn will not automatically create our dummy # variables for us. so we need to do this manually. mutate(sexMale = ifelse(sex == &quot;female&quot;, 0, 1)) %&gt;% mutate(childyes = ifelse(child == &quot;no&quot;, 0, 1)) %&gt;% select(-(c(sex,child,nbaffairs))) set.seed(33458) n &lt;- nrow(Fair) trainIndex &lt;- sample(1:n, size = round(0.6*n), replace = FALSE) # create training and test samples trainFair &lt;- Fair[trainIndex ,] testFair &lt;- Fair[-trainIndex ,] To run a k-nearest neighbor analysis, we need three inputs: our predictors in the training data, our predictors in the test data, and our outcome/classes in the training data. Here, as in the regression analysis in the last chapter, we can generate a confusion matrix to assess the accuracy of prediction: set.seed(33458) trainPredictors &lt;- trainFair %&gt;% select(-affairYN) testPredictors &lt;- testFair %&gt;% select(-affairYN) knnAffair &lt;- knn(trainPredictors, # training data testPredictors, # test data trainFair$affairYN, # class 1 # number of neighbors ) b&lt;-confusionMatrix(knnAffair, testFair$affairYN) b ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 143 32 ## 1 39 26 ## ## Accuracy : 0.7042 ## 95% CI : (0.6421, 0.7611) ## No Information Rate : 0.7583 ## P-Value [Acc &gt; NIR] : 0.9772 ## ## Kappa : 0.2247 ## ## Mcnemar&#39;s Test P-Value : 0.4764 ## ## Sensitivity : 0.7857 ## Specificity : 0.4483 ## Pos Pred Value : 0.8171 ## Neg Pred Value : 0.4000 ## Prevalence : 0.7583 ## Detection Rate : 0.5958 ## Detection Prevalence : 0.7292 ## Balanced Accuracy : 0.6170 ## ## &#39;Positive&#39; Class : 0 ## b[[&quot;overall&quot;]][[&quot;Accuracy&quot;]] ## [1] 0.7041667 20.2.2 from one doppelganger to many In the above code, we used a k-nearest neighbor analysis based on a single neighbor (k = 1). Can we improve prediction by considering more than 1 neighbor? 20.2.3 the Bayesian classifier Lisa is a doctor whose patients often have post-surgical pain. Their suffering is real, but effective pain medications such as OxyContin have a high likelihood of leading to abuse and addiction. She has two bits of information about a patient, his or her age (say that we have this in ten levels, corresponding to decades of age, so that the first level is under 20 and that the last level is over 100), and his or her self-report of pain, also on a 10 point scale. Assume that Lisa wants to prescribe the medication to all patients who would not abuse it, and not prescribe the medication to anyone who would. Assume that Lisa knew the entire matrix of conditional probabilities, P (addiction | age &lt; 20 &amp; pain = 1) = .34 P (addiction | age &lt; 20 &amp; pain = 2) = .26  P (addiction | age &gt; 100 &amp; pain = 10) = .09 In this case she would prescribe the drug for every case where the conditional probability was greater than .5, and never otherwise. This is called the Bayesian classifier, and if we have the entire matrix of conditional probabilities we could do no better. In real world problems, we are typically dealing with many predictors, and we dont have the full matrix of conditional probabilities. But this two predictor case sets up the illustration drawn from James et al. (2013). Figure 10.1: Comparing two values of k. From James et al. (2013). In the figure above, assume that the horizontal and vertical axes correspond to scores on the two predictors (age and pain). The orange and blue colored dots correspond to cases of abuse and non-abuse in the training data. The dashed line is the Bayesian classifier. The solid line is the k-nn decision boundary, which distinguishes the regions in which we will predict abuse and non abuse in the test data. We see that when k is small (a single neighbor), prediction is flexible, non-linear, and that as k increases, the boundary differentiating the decision to prescribe and not prescribe becomes more nearly linear. But what value of k is optimal? 20.2.4 Back to the affairs data To test a range of values, we can first set up our knn analysis as a function (compare this code with the code in the prior section). trainPredictors &lt;- trainFair %&gt;% select(-affairYN) testPredictors &lt;- testFair %&gt;% select(-affairYN) knnFairdata &lt;- function (k) { set.seed(33458) knnAffair &lt;- knn(trainPredictors, # training data testPredictors, # test data trainFair$affairYN, # class k # number of neighbors ) b&lt;-confusionMatrix(knnAffair, testFair$affairYN) b[[&quot;overall&quot;]][[&quot;Accuracy&quot;]] } We run the function on k = 1 and k = 2 to test it: knnFairdata(1) ## [1] 0.7041667 knnFairdata(2) ## [1] 0.6958333 Now we can apply it to as many as 100 values of k, using the purrr::map command: kAccuracy &lt;- (map_dbl(1:100,knnFairdata)) %&gt;% # map the knnFunction onto numbers 1-100 as_tibble() %&gt;% # then a tibble so we can do a quick plot rename(Accuracy = 1) %&gt;% mutate(k = seq_along(Accuracy)) We can graph this, using the syntax from the beginning of the class: kAccuracy %&gt;% ggplot(aes(k, Accuracy)) +#%&gt;% geom_point() + ggtitle(&quot;Overall accuracy for varying levels of k&quot;) # This pulls out the maximum accuracy, and the value of k for which it occurs: (ka &lt;- which.max(kAccuracy$Accuracy)) ## [1] 33 (kb &lt;- max(kAccuracy$Accuracy)) ## [1] 0.7583333 20.2.5 avoiding capitalization on chance (again) In these data, with this split of training and test (and this initial seed) the maximum predictability occurs at k = 33, with an overall accuracy of 0.7583333. Would this hold if we used a different random split? Remember, here, we have tested not one model, but 100 of them, then chosen the best one. The peak in the curve at 33 may well be due to chance characteristics of the test data. We could address this empirically using one of several techniques. One approach is to have a third independent sample on which to test the accuracy of prediction at k = 33. This would require the separate validation sample that was introduced in the last chapter. In the following block, I resplit the Fair data, using proportions of 60%, 30%, 10%. These values are likely not optimal given the (relatively small) size of the Fair data, but will work to illustrate the approach: set.seed(33458) threeWaySplit &lt;- sample(1:3, size = nrow(Fair), prob = c(0.6,0.3,0.1), replace = TRUE) trainFair2 &lt;- Fair[threeWaySplit == 1,] testFair2 &lt;- Fair[threeWaySplit == 2,] validFair2 &lt;- Fair[threeWaySplit == 3,] I tweak my function here to use the new data, then run it 100 times as before. trainPredictors &lt;- trainFair2 %&gt;% select(-affairYN) testPredictors &lt;- testFair2 %&gt;% select(-affairYN) knnFairdata2 &lt;- function (k) { set.seed(33458) knnAffair &lt;- knn(trainPredictors, # training data testPredictors, # test data trainFair2$affairYN, # class k # number of neighbors ) b&lt;-confusionMatrix(knnAffair, testFair2$affairYN) b[[&quot;overall&quot;]][[&quot;Accuracy&quot;]] } kAccuracy &lt;- (map_dbl(1:100,knnFairdata2)) %&gt;% # map the knnFunction onto numbers 1-100 as_tibble() %&gt;% # then a tibble so we can do a quick plot rename(Accuracy=value) %&gt;% mutate (k = seq_along(Accuracy)) (ka2 &lt;- which.max(kAccuracy$Accuracy)) ## [1] 15 (kb2 &lt;- max(kAccuracy$Accuracy)) ## [1] 0.7802198 So here, on the (contaminated) test data, the maximum predictability occurs at k = 15, with an overall accuracy of 0.7802198. We apply this back to the validation data by pulling the knn code out of the function, and running it just once against the validation data: knnAffair &lt;- knn(trainFair2[,-7], # training data validFair2[,-7], # VALIDATION data trainFair2$affairYN, # class ka2 # number of neighbors ) b &lt;- confusionMatrix(knnAffair, validFair2$affairYN) (kb3 &lt;- b[[&quot;overall&quot;]][[&quot;Accuracy&quot;]]) ## [1] 0.703125 The overall predictability using the k-nearest neighbor analysis on the clean validation data, is now 0.703125. You might note that in our two splits of the Fair data - the two-way split of 60% Training and 40% Test, and the three-way split of 60% Training, 30% Test, 10% Validation, we got two different solutions to the question of the optimal k (i.e., 33 and 15). With larger sample sizes, these values would be more stable. 20.2.6 the multinomial case A final comment on the k-nearest neighbors approach: You can extend this to classification problems in which we are predicting not just a dichotomous outcome, but a multinomial one - such as a personality type or college concentration. References "],["machine-learning-chihuahuas-vs-muffins-and-other-distinctions-and-ideas.html", "21 machine learning: chihuahuas vs muffins, and other distinctions and ideas 21.1 supervised versus unsupervised 21.2 prediction versus classification 21.3 understanding versus prediction 21.4 bias versus variability 21.5 compensatory versus non-compensatory problems 21.6 a postscript: The Tidymodels packages", " 21 machine learning: chihuahuas vs muffins, and other distinctions and ideas In the last few chapters, we have considered linear and logistic regression and k-nearest neighbor analysis as tools for prediction and classification. Weve shown how to split the data into training, test and (in some cases) validation samples, then how to assess the robustness or accuracy of a model on these new datasets. Weve also considered measures such as R-squared, overall accuracy, and area under the ROC as measures of validity. These ideas and techniques form a starting point for the study of machine learning. My approach is drawn largely from James et al. (2013), which is available freely on the web and includes links to additional materials and R-based exercises for those who would like to study this further. 21.1 supervised versus unsupervised One problem that we havent yet considered is the distinction between supervised and unsupervised problems, arguably the most fundamental distinction in machine learning. In both the Swiss and the Fair problems, we had a known outcome (fertility, infidelity) which we were trying to predict from a set of independent variables. In these problems, we have an a priori split of the variables into two sets (outcomes and predictors). These are considered supervised problems. In these problems, we can think of the known outcome or criterion as guiding (supervising) the work of model-building. There is a second type of problem in which we dont have an outcome, which would guide or supervise our model. Without an outcome or criterion, we must rely on the internal structure of the data. These are considered unsupervised problems. Methods used to address unsupervised problems include cluster analysis (of which there are many subtypes), component analysis, and exploratory factor analysis. In the unsupervised approach, objectives include finding unknown patterns, developing a set of types (or a taxonomy), and assessing the dimensionality of a latent set of variables. In psychology, a focal problems involves assessing the factor structure of personality (if you have taken introductory psychology, you are likely familiar with the five-factor or Big Five model of personality). The unsupervised approach is also used to solve problems of community detection in the study of social and scientific networks (see Figure 20.1, from Lanning (2017)). I think that questions about dimensionality and internal structure can be compelling (Lanning 1994, 1996; Lanning and Rosenberg 2009), but I will not consider them further here. Figure 20.1. Part of the structure of personality research. From Lanning (2017) 21.2 prediction versus classification Within the category of supervised problems, we can distinguish problems in classification (those in which the outcome is nominal or discrete) from problems where the outcome is ordered or numeric. We examined the Fair data in both these ways, first treating the outcome as the rescaled number of reported affairs, then as a distinction between those who did and did not have affairs. Somewhere in between these two approaches are problems in which the criterion is an ordered set of categories - such as small, medium, and large pizza sizes or, to consider a problem in my own area of study, a sequence of seven levels of social maturity or personality development. Working with several colleagues, I constructed dictionaries which empirically distinguished these levels; the object is to allow assessment of the level of maturity or ego development of a given text. In the diagram below, the words in these initial dictionaries are arranged clockwise, with those characteristic of the earliest (Impulsive) level in the upper right quadrant (i.e., at 1 oclock), then moving through the middle stages at the bottom, etc.(Lanning et al. (2018)). 21.3 understanding versus prediction In machine learning, we are in general concerned with the problem of finding the function which relates an outcome (Y) to a set of predictors (X). This general principle spans two opposing, but overlapping, use-cases: understanding and prediction. For example, in the Fair data, we were initially concerned with the question of what predicts infidelity?, which involves or suggests an interest in understanding. With the k-nn analysis, our focus increasingly shifted away from this towards the more pragmatic goal of increasing our hit rate or overall accuracy, away from a concern with inference and understanding to a position where we were satisfied to treat the algorithm as a black box from which we were only concerned with the accuracy of outputs. These two objectives of understanding and thinking in terms of equations and models and prediction and thinking only in terms of optimization can be thought of as two points on a continuum of interpretability. Some techniques used in machine learning give results that are quite interpretable (including multiple regression, particularly restricted regression techniques such as the lasso). Others, including support vector machines and, in particular, deep neural networks sacrifice interpretability in the service of prediction. For complex approaches in image recognition, such as the chihuahua versus muffin problem, deep neural networks provide the best solutions, but are particularly challenging to understand (Kumar, Wong, and Taylor 2017). Fig 21.1: The chihuahua-muffin problem. Exercise 21.1: Consider the Chihuahua-Muffin problem illustrated above. What is the outcome variable Y? What are some of the predictors X? (Note that in image recognition and natural language processing, predictors are typically called features). Why is this problem interesting? That is, are there problems similar to this that have important social uses? Can you describe a simple algorithm or decision rule which works more than 50% of the time on the (training) data above (i.e., if X thenChihuahua\" else Muffin)? What might an effective algorithm on new (test) data look like? finally, what do you think goes on on your mind as you evaluate the photos in the chihuahua-muffin problem? 21.4 bias versus variability If you read further about machine learning, you are likely to encounter the phrase bias-variability tradeoff. You may remember that, in our initial analyses of the Swiss fertility data, we discussed how reducing the number of observations increases the fit of the model on the (training) data - and that, ultimately, the fit of the model would become perfect when we reduce the number of observations to the number of variables in the model. Here, we construct two independent samples, each with 9 subjects. data(Fair) set.seed(33458) FairSample1 &lt;- Fair %&gt;% sample_n(9) set.seed(94611) FairSample2 &lt;- Fair %&gt;% sample_n(9) In the following chunk, we run a regression analysis witin each of these samples, predicting the number of affairs from the 9 predictors (including the intercept). affairReg1 &lt;- lm(nbaffairs ~ ., data=FairSample1) affairReg2 &lt;- lm(nbaffairs ~ ., data=FairSample2) R21 &lt;- summary(affairReg1)$r.squared R22 &lt;- summary(affairReg2)$r.squared t(c(sample1 = R21, sample2 = R22)) %&gt;% kable(caption = &quot;R^2 from two small samples&quot;) Table 21.1: R^2 from two small samples sample1 sample2 1 1 names(affairReg1[[&quot;coefficients&quot;]]) %&gt;% cbind(round(affairReg1[[&quot;coefficients&quot;]], 2)) %&gt;% cbind(round(affairReg2[[&quot;coefficients&quot;]], 2)) %&gt;% as_tibble() %&gt;% rename(variable = 1, sample1 = 2, sample2 = 3) %&gt;% kable(caption = &quot;Regression coefficients from two small samples&quot;) Table 21.1: Regression coefficients from two small samples variable sample1 sample2 (Intercept) 19.93 87.53 sexmale -3.7 49.39 age -0.16 -2.96 ym 0.35 2.5 childyes 0.72 -4.45 religious -0.79 -4.55 education -0.49 -0.88 occupation 2.33 2.95 rate -3.81 -1.5 Note that in each case we have perfect predictability, but with a very different set of predictors. The difference between these two sets of coefficients is an illustration of how coefficients in overfit models will vary from one sample to another. At the opposite extreme are underfit models, which are likely to provide relatively stable coefficients across samples, but which arent very effective at prediction. This is the bias-variability trade-off, and it occurs not only in tiny data sets such as these, but in bigger datasets where there is a large number of predictor variables, as is often the case in, for example, artificial intelligence (including image recognition) and bioinformatics (including statistical genetics). The problem of too many predictors can be addressed, in part, by preprocessing, or trying to restructure the data to effectively increase the number of rows in the data (e.g., by imputing missing values), or decrease the number of columns (that is, using component or factor analysis as an initial step in data analysis). Another approach is to use resampling. 21.4.1 resampling: beyond test, training, and validation samples Weve considered one approach to avoiding chance-inflated models and prediction estimates, and that is the approach of holding out test (and possibly validation) samples. An extension of this approach is k-fold cross validation, in which the sample is divided into k (e.g., ten) parts, each of which is used as a validation sample in k different analyses. To assess the overall performance of the model, the results of these are averaged. This is a sophisticated and relatively easy to implement approach which can be used, for example, to assess the relative performance of different models, such as linear versus non-linear models. Another approach to resampling is bootstrapping, in which model parameters (e.g., regression coefficients) are taken as the average of many (for example 1,000) analyses of subsamples of the data. In the bootstrap, sampling is done with replacement, so that the same case may appear in many samples. The averaged coefficients arrived at using bootstrapping are less variable than the results of a single analysis. 21.5 compensatory versus non-compensatory problems Consider two simple hypothetical real world problems. In the first, you are deciding where you should apply to graduate school. Assume that, in the sample of schools under consideration, only two variables matter to you, say program quality and program cost, and that you weight the first of these positively and the second negatively. We might anticipate that these would be related in a compensatory way, so that you would be willing to pay more for a better program. In the second problem, you are looking to hire a group of commercial jet pilots. Again, there are only two predictors; here, they are eyesight and responsibility. Unlike in the first problem, these are related in a noncompensatory way - for example, no matter how good someones eyesight is, if they are irresponsible you will not want to hire them. Similarly, even if they are very responsible, poor eyesight might make them ineligible. Exercise 21.2: For the grad school and airline pilot problems, draw a pair of coordinate axes, together with points representing a few dozen positive (apply or hire) and unacceptable (dont apply, dont hire) observations. What can you say about the boundary between these two regions in each problem? We can think of problems such as the grad school problem in a regression framework, while the pilot problem is instead a decision tree or multiple cutoff model. Very simple decision trees, including fast and frugal trees, can be useful tools both to describe how people decide and how they might make better decisions (Phillips et al. 2017). In machine learning problems, there may be hundreds or thousands of predictors (features). The logic of decision trees is extended in techniques including bootstrap aggregation or bagging (in which a large number of trees are considered, then averaged), and random forests (which sequentially examines subsets of predictors in an effort to increase the reliability of predictions). These models are often predictively useful, but can be complex and difficult to interpret. Chapter 8 of (James et al. 2013) goes into more detail on these methods. Two additional methods bear mention: Support vector machines (SVM) are classifiers which attempt to find optimal margins (hyperplanes) between two or more sets of criteria. Finally, neural networks are models which typically include multiple layers (akin to biological neurons) each of which can be activated based on the sum of the inputs of the prior layer. The chain of prediction from input (pixel) to output (chihuahua) is not a simple forward path, rather, errors of prediction are used to tune weights of intermediate layers in a process known as backpropagation. Artificial neural networks are, at present, the most important set of techniques in artificial intelligence problems including image, speech, and even taste recognition. 21.6 a postscript: The Tidymodels packages The new Tidymodels packages provide a handful of new, tidyverse-compliant approaches to problems in prediction, modeling, and machine learning. The website, particularly the Get Started page, is straightforward, and I urge you to review it systematically. Heres a quick summary of some key points from the first few pages of that section: The parsnip package presents a tidyverse-compliant approach to writing syntax of models; it appears to be particularly useful for examining multiple models on the same data sets. The broom package includes the tidy() function, which can be used to present the results of your analyses in data frames - this is handy for making tables of your data. Results of these models can be represented as data frames, which can then be easily tabled or used to create figures. You can see how these are used on the Build a model page. Perhaps the most useful tool introduced in these pages is the Skimr package. Skimr, though not part of the Tidymodels, is handy for getting a quick set of summary statistics - its substantially more convenient than the summarise function in the Tidyverse and, unlike the describe package in the psych package, is Tidyverse-compliant. The preprocessing of data includes several concepts that we have previously considered, including splitting data into training and test subsamples, the creation of dummy variables, and how we can treat time data. The recipes package include several functions that make it more convenient to work with only subsets of our variables, that is, to assign roles to include or exclude variables in our models. This section also includes an introduction to the yardstick package, which gives an easier way to implement ROC analysis than the code I used in the prior chapters. Resampling is a process through which we can estimate model parameters by inspecting a series of subsets of the data. It is particularly useful when we want to compare a set of competing models on a small-ish dataset (Lanning et al. 2018). This section also includes an application of the random forests model introduced above. References "],["some-ethical-concerns-for-the-data-scientist.html", "22 some ethical concerns for the data scientist 22.1 ethics and personality harvesting 22.2 the law of unintended consequences 22.3 your privacy is my concern 22.4 who should hold the digital keys? 22.5 contact-tracing and COVID-19 22.6 the digital divide 22.7 still more case studies 22.8 some potential remedies", " 22 some ethical concerns for the data scientist The Latin word scientia is commonly translated as both science and knowledge. Modern science or modern knowledge has led not only to dramatic improvements in public health and human life expectancy, but also to weapons of war that allow the slaughter of millions at a distance. The idea that science or knowledge is ethically fraught is not new: Adam and Eve are said to have been cast out of the Garden of Eden after tasting the forbidden fruit from the tree of knowledge (Shattuck 1997). And, as we have seen in our discussion of the most dangerous equation, the lack of knowledge can also be dangerous - perhaps particularly today, as we are in the grips of a pandemic with leaders who govern by intuition rather than science, by hunch rather than data (Howard Wainer 2007). In this final chapter, I consider a few case studies which exemplify some of the ethical issues and concerns that frame data science. Note the word frame - this is little more than the beginning of a scaffolding. But I hope that it is sufficient to bolster the claim that ethical concerns are or should be at the foundation of data science, and that individuals with training in the liberal arts (and not just math and engineering) should play an important role. 22.1 ethics and personality harvesting In personality psychology, concerns about how personality tests might invade privacy have been raised for over fifty years. Today, these concerns are newly relevant. One issue is measurement and experimentation without consent (Gleibs 2014). Here, the most dramatic and arguably consequential example of this occurred when Cambridge Analytica scraped the online activity of individuals whose friends had participated in a study of personality, then leveraged this further to assess the personality of all or nearly all American adults. Cambridge Analytica, Facebook, and other data-rich firms such as Experian engage in practices such as shadow profiling, which is the nonconsensual profiling of individuals through their social connections (Garcia et al. 2018). This profiling leads to the selective tailoring of messages (ads, political appeals) which, at the very least, attenuates freedom of choice. At a societal level, it can lead to manipulations of consumer and political behavior, including voting decisions (Bond et al. 2012; Matz, Gladstone, and Stillwell 2016). Tailored messaging can not only harm our social fabric, deepening partisan divides among us, but can have consequences for social institutions as well. In a recent paper, several of us expressed our concern that the harvesting of personality information is likely to become more common in the years to come (Boyd, Pasca, and Lanning 2020). This argument derives from a series of claims, which are listed below: It takes very little data to identify someone. We can often discover the identity of an individual from a very small number of data points (Sweeney 2005). There is a great deal of data available. For example, credit card purchases, Uber trips, exercise routes tracked on platforms such as Strava, turnpike itineraries, medical records - are becoming increasingly digital, and so the potential for linking data increases. Combining data leads to new value. As we have seen this term, there can be meaningful value, new knowledge, when different datasets are joined. There are few constraints on data collection and synthesis. The collection of digital data is typically undertaken by private companies motivated by profit, who are often unconstrained by ethical concerns that might be raised by, for example, human subjects review boards. Information is valuable to a range of parties - from potential employers to insurance companies to potential romantic partners. For these and other reasons, we anticipate that there will be a growing market for data about our personalities (Wu 2019). 22.2 the law of unintended consequences One of the recurring issues in digital ethics has been the so-called law of unintended consequences (Merton 1936). When we initiate a new policy, or collect a new dataset, or give permission to a social media application to share our information, or investigate our family tree using a service such as Ancestry.com or 23andMe.com, we do not know what will happen with the information that we share or create. For example, assume that Fred signs up to learn about his family and health on 23andMe. He finds that he has a health vulnerability with an already known, or even a soon-to-be-discovered, genetic predisposition. That information has consequences not just for him but, potentially, for his offspring. If that genetic information is shared, it takes little in the way of a dystopian imagination to consider that Fred s grandchildren might have to pay higher health insurance premiums, be prohibited from migrating to certain other countries, be seen as less desirable partners, etc. These examples might seem extreme, but unintended consequences are typically unforeseeable. For example, when the good folks at Netflix shared some of their movie-preference data and offered a million-dollar prize to anyone who could substantially improve on their recommendation-engine, they did not intend to out individuals including a closeted lesbian mom who was identified by several data scientists (Narayanan and Shmatikov 2008). 22.3 your privacy is my concern It is not paradoxical that I should be concerned with your (right to) privacy. Im invested in your ability to choose what to reveal about yourself, when, and to whom, in part because I want you to be able to live in and contribute to the world. Even if I am not that concerned about having details of my own life revealed, I must be sympathetic to your concerns. The concern for privacy is not a fetish, but is critical if we are to live and work in a just and decent society. 22.4 who should hold the digital keys? Lets shift gears somewhat, and consider some of the issues surrounding autonomous vehicles. In a data-dependent world, who should be the guardians of the code that connects us? To consider just one example, the computer systems in modern cars typically run millions of lines of code. As cars become increasingly autonomous, this complexity will only increase. (Incidentally, the Society of Automotive Engineers, or SAE, describes 6 levels of auto autonomy. At this writing, the most sophisticated systems available to consumers, such as Tesla Autopilot, are at level 2. What lies ahead are cars which are self-driving on carefully selected, geo-fenced roads, and ultimately cars which can operate on any road a human driver could negotiate). Our roads and highways will become an Internet of Vehicles (IOV), which will include not just connections between cars and an intelligent cloud above us but also direct links between a distributed system of intelligent cars, stoplights, and road sensors in a fog around us (Bonomi et al. 2012). Fog computing and the IOV will reduce travel times and increase both fuel efficiency and automotive safety. Obviously, there are cybersecurity concerns. While the prospects for a chaotic, choreographed hack of hundreds of vehicles on the streets of Manhattan, such as that in the 2017 movie The Fate of the Furious, are remote at best (or worst), there have been examples of white-hat hackers who have successfully infiltrated (and thereby helped secure) car information systems. As the IOV develops, there will be vulnerabilities to privacy as well as safety, and the security of the system will be paramount. Different car manufacturers are taking different approaches to developing secure information systems, with many using a closed or proprietary approach. But the scope of the problem is so large that there is a movement towards pooling resources and encouraging collaboration among industry partners, academics, and citizen scientists in the development of an open-source autonomous driving platform, such as Apollo. Perhaps counterintuitively, there may be significant security advantages to using source code that is open to all (Clarke, Dorwin, and Nash 2009; FitzGerald, Levin, and Parziale 2016). 22.5 contact-tracing and COVID-19 On April 10, 2020, Google and Apple announced that they would collaborate on a contact-tracing system to try and slow the COVID-19 pandemic. Keeping in mind the ethical issues of (a) the law of unintended consequences, (b) prior failures to maintain anonymity, (c) arguments that data should be best secured by industry, government, or crowdsourcing, as well as the tech issues of (d) the potential methods for contact tracing, and (e) the costs of the pandemic to public health and to world economies, to what extent should digital tracking be used to assess and limit the spread of the novel coronavirus? You can learn more about the Google-Apple collaboration at their FAQ and this piece at techcrunch. Consider some of the ethical issues raised in this essay at fivethirtyeight.com, and in this more recent piece at the Verge. 22.6 the digital divide Not all are benefiting equally from the birth of the digital age. Unfortunately, the Matthew-effect, by which resources flow to those who have them most and need them less, is a fundamental property of networked, complex systems. As we become more interconnected, the gap between rich and poor is accelerating. The primary path to a successful life is to find a scalable occupation - that is, one in which you can serve many people with little effort. But that means that fewer will serve. So yes, spend your summer trying to become a social media influencer. Better still, make a commitment to working to try to reduce the growing digital divide, and more broadly to address issues of equality and social justice in your own applications of data science. Google, which once had the mantra dont be evil in its core of conduct, has now moved to a set of positive (dos) as well as negative (donts) guidelines for their work in Artificial Intelligence. Information can be empowering (for those who have it). 22.7 still more case studies As we face ubiquitous observation as our world becomes an Internet of Things, and live in a world in which decisions will increasingly be made for us by applications of machine learning and artifical intelligence, new questions will be raised about the social impact of our science. Some of these are illustrated in these six hypothetical case studies proposed by an interdisciplinary team at Princeton. They warrant your consideration. 22.8 some potential remedies In the European Union, the General Data Protection Regulation (GDPR) has been in effect since 2018, and provides guidelines for protecting people and personal data in the digital age. In a related piece, (Loukides, Mason, and Patil 2018) provided a briefer set of guidelines. They argued that it is not enough that people provide consent, but also that there must be clarity: That it, its not enough that people agree to share their data, they must also understand what they are agreeing to. Other issues that they highlight include the need for data security (as stolen data often include, for example, SSNs and passwords), and protection of vulnerable populations such as children. Finally, they provide a checklist for those developing data products:  Have we listed how this technology can be attacked or abused?  Have we tested our training data to ensure it is fair and representative?  Have we studied and understood possible sources of bias in our data?  Does our team reflect diversity of opinions, backgrounds, and kinds of thought?  What kind of user consent do we need to collect to use the data?  Do we have a mechanism for gathering consent from users?  Have we explained clearly what users are consenting to?  Do we have a mechanism for redress if people are harmed by the results?  Can we shut down this software in production if it is behaving badly?  Have we tested for fairness with respect to different user groups?  Have we tested for disparate error rates among different user groups?  Do we test and monitor for model drift to ensure our software remains fair over time?  Do we have a plan to protect and secure user data? References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
