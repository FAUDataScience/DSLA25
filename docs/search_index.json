[["index.html", "Data science for the liberal arts ", " Data science for the liberal arts Kevin Lanning 2021-02-07 "],["preface.html", "preface some features of the text the book is for you", " preface This work-in-progress will ultimately serve as a textbook for introductory undergraduate courses in data sciences. No prior knowledge of computer programming is presumed, though, ideally, students will have had college algebra (or its equivalent) and an introductory course in statistics, methods, or data analysis. Data science is still a new field of study, and there are multiple approaches to teaching it and to its place in the college curriculum. This book is intended to serve courses such as the Introduction to Data Science at the Wilkes Honors College of Florida Atlantic University which, in turn, has drawn from data science classes at the universities of North Carolina, British Columbia, Duke, Maryland, Wisconsin, Stanford, BYU, Harvard, and UC Berkeley At each of these schools, the Introduction to Data Science appears, to my eyes at least, closer to Statistics than to Computer Science. But if our approach is closer to statistics than to programming, it is particularly close to statistics in its most applied and pragmatic form. The choice of statistical methods should follow from the data and problem at hand - that is, statistics should serve the needs of the user rather than dictate them (???). This pragmatic focus is driving the growth of data science in industry, and it is reflected in the way data science has been taught at still other schools including Chicago, Georgia Tech, UC Santa Barbara, Princeton, UC Berkeley, at Berlins Hertie School of Governance, and in Columbias School of Journalism. some features of the text There are a number of different approaches to teaching data science. The present text includes several distinguishing features. R In a recent informal survey of introductory data science courses, I saw a pretty even split between those which begin with Python and those which begin with the statistical programming language R. This difference corresponds, very loosely, to the split noted above: Computer science based approaches to data science are frequently grounded in Python, while statistics-based approaches are generally grounded in R. Our course, like those for most of the syllabi and courses linked above, will be based in R. Reproducible science The course will provide an introduction to some of the methods and tools of reproducible science. We will consider the replication crisis in the natural and social sciences, and then consider three distinct approaches which serve as partial solutions to the crisis. The first of these is training in a notebook-based approach to writing analyses, reports and projects (using R markdown documents). The second is using public repositories (such as the Open Science Framework and, to a limited extent, GitHub) to provide snapshots of projects over time. Finally, the third is to consider the place of significance testing in the age of Big Data, and to provide training in the use of descriptive, exploratory techniques of data analysis. Good visualizations Part of Type C data science is communication, and this includes not just writing up results, but also designing data displays that incisively convey the key ideas or features in a flood of data. Well examine and develop data visualizations such as plots, networks and text clouds. More advanced topics may include maps, interactive displays, and animations. All Some of the data Its been claimed that in the last fifteen years, humans have produced more than 60 times as much information as existed in the entire previous history of humankind. (It sounds like hyperbole, but even if its off by an order of magnitude its still amazing). There are plenty of data sources for us to examine, and well consider existing datasets from disciplines ranging from literature to economics to public health, with sizes ranging from a few dozen to millions of data points. We will also clean and create new datasets. All A few of the latest tools One feature of Data Science is that it is changing rapidly. The tools, methods, data sources, and ethical concerns that face us in 2021 are different from those which shaped the field just one or two years ago. In fields that are undergoing rapid change, there is some trade-off between building expertise with existing (older) tools and trying the newer approaches. Partly because I want to equip you with skills which will not be obsolete, partly because some of these new approaches promise more accessibility, elegance, and/or power, and partly because of my own interest in staying current, well be using some of the latest packages and programs. In the last few years, Ive shifted the class from the standard R dialect (as I learned it from the Johns Hopkins-Coursera Data Science Specialization) to the Tidyverse. This year, for the first time, much of our work with the R language will take place online, using RStudio cloud, for this should help overcome some hiccups that arise when students are using different machines, operating systems, etc., and should facilitate collaboration among us as well. Well also be experimenting for the first time with the learnr package, and emphasizing other approaches to learning R, including Swirl, less. In the past, Ive used the Slack platform for messaging, communication and collaboration; this year, Ive somewhat reluctantly moved to the Canvas LMS. And, in the past, Ive recommended using dedicated markdown editors such as Typora. While I still think that these are great for some text-editing and note-taking, well do our work instead with the editor in the latest variant of RStudio on our laptops, as this allows WYSIWIG (what you see is what you get) formatting of documents - such as this one - that are intended as publication-ready texts. Well continue to use spreadsheets such as Excel or Google Sheets as well. the book is for you Its my intention that this text should serve every college student, regardless of concentration or college major. The skills and insights that you will gain in this course will help you in graduate and professional schools, will help you in your careers, and will help you in your goal of making a better world. And it will help you train the next generation of data scientists as well. "],["data-science-for-the-liberal-arts.html", "1 data science for the liberal arts 1.1 type C data science = data science for the liberal arts 1.2 the incompleteness of the data science Venn diagram 1.3 a dimension of depth 1.4 Google and the liberal arts 1.5 data sci and TMI 1.6 discussion: what will you do with data science?", " 1 data science for the liberal arts Hochster, in (???), describes two broad types of data scientists: Type A (Analysis) data scientists, whose skills are like those of an applied statistician, and Type B (Building) data scientists, whose skills lie in problem solving or coding, using the skills of the computer scientist. This view arguably omits a critical component of the field, as data science is driven not just by statistics and computer science, but also by domain expertise: Fig 1.1 - The iconic data science Venn diagram 1.1 type C data science = data science for the liberal arts The iconic Venn diagram model of data science shown above suggests what we will call Type C data science. It begins with domain expertise in your concentration in the arts, humanities, social and/or natural sciences, it both informs and can be informed by new methods and tools of data analysis, and it includes such things as communication (including writing and the design and display of quantitative data), collaboration (making use of the tools of team science), and citizenship (serving the public good, overcoming the digital divide, furthering social justice, increasing public health, diminishing human suffering, and making the world a more beautiful place). Its shaped, too, by an awareness of the fact that the world and workforce are undergoing massive change: This puts the classic liberal arts focus of learning how to learn (as opposed to memorization) at center stage. And Type C data science is shaped, not least, by the creepiness of living increasingly in a measured, observed world. Type C data science does not merely integrate domain expertise with statistics and computing, it places content squarely at the center. We can appreciate the compelling logic and power of statistics as well as the elegance of well-written code, but for our purposes these are means to an end. Programming and statistics are tools in the service of social and scientific problems and cultural concerns. Type C data science aims for work which is not merely cool, efficient, or elegant but responsible and meaningful. 1.2 the incompleteness of the data science Venn diagram Data visualizations are starting points which can provide insights, typically highlighting big truths or effects by obscuring other, presumably smaller ones. The Venn diagram model of data science is no exception: As with other graphs, figures, and maps, it allows us to see by showing only part of the picture. What does it omit? That is, beyond statistics, computing/hacking, and domain expertise, what other skills contribute to the success of the data scientist? The complexity of data science is such that individuals typically have expertise in some but not all facets of the area. Consequently, problem solving requires collaboration. Collaboration, even more than statistical and technical sophistication, is arguably the most distinctive feature of contemporary scholarship in the natural and social sciences as well as in the private sector (???). Communication is central to data science because results are inconsequential unless they are recognized, understood, and built upon; facets of communication include oral presentations, written texts and, too, clear data visualizations. Reproducibility is related to both communication and collaboration. There has been something of a crisis in recent years in the social and natural sciences as many results initially characterized as statistically significant have been found not to replicate. The reasons for this are multiple and presently contentious, but one path towards better science includes the public sharing of methods and data, ideally before experiments are undertaken. Reproducible methods are a key feature of contemporary data science. Pragmatism refers to the relevance of work towards real-world goals. These real-world goals should be informed by ethical concerns including a respect for the privacy and autonomy of our fellow humans. 1.3 a dimension of depth Cutting across these various facets (statistics, computing, domain expertise, collaboration, communication, reproducibility, pragmatism, and ethics), a second dimension can be articulated. No one of us can excel in all of these domains, rather, we might aim towards goals ranging from literacy (can understand) through proficiency (can get by) to fluency (can practice) to leadership (can create new solutions or methods). That is, we can think of a continuum of knowledge, skills, interests, and goals, ranging from that which characterizes the data consumer to the data citizen to the data science contributor. A Type C data science includes this dimension of depth as well. 1.4 Google and the liberal arts Data science is at its core empirical, and all of this rhetoric would be meaningless if not grounded in real world findings. Although it was reported in late 2017 that soft skills rather than STEM training were the most important predictors of success among Google employees, its difficult to know whether these results would generalize to a less select group. Nonetheless, there is a clear need for individuals with well-rounded training in the liberal arts in data science positions and, conversely, learning data science is arguably a key part of a contemporary liberal arts education. 1.5 data sci and TMI One difference between traditional statistics and data science is that the former is typically concerned with making inferences from datasets that are too small, while the latter is concerned with extracting a signal from data that is or are too big (???). The struggle to extract meaning from a sea of information - of finding needles in haystacks, of finding faint signals in a cacophony of overstimulation - is arguably the question of the age. It is a question we deal with as individuals on a moment-by-moment basis. It is a challenge I face as I wade through the many things that I could include in this class and these notes. The primacy of editing or selection lies at the essence of human perception and the creation of art forms ranging from novels to film. And it is a key challenge that the data scientist faces as well. 1.6 discussion: what will you do with data science? Imagine it is ten years from today. You are working in a cool job (yay). How, ideally, would data science inform your professional contributions? More proximally (closer to today) - what are your own goals for progress in data science, in terms of the model described above? "],["getting-started.html", "2 getting started 2.1 are you already a programmer and statistician? 2.2 some best practices for spreadsheets 2.3 setting up your machine: some basic tools 2.4 a modified 15-minute rule 2.5 discussion: who deserves a good grade?", " 2 getting started We begin with a brief self-assessment, asking you to reflect on your own knowledge of data science, including the necessary-but-not-sufficient areas of computer programming and statistics. We then move to a description of some rudimentary tools that we will be using. 2.1 are you already a programmer and statistician? Regarding programming, you may know more than you think you do. Heres a simple program - a set of instructions - for producing a cup of coffee: add water to the kettle and turn it on if its morning, put regular coffee in the French press, otherwise use decaf if the water has boiled, add it to the French press, else keep waiting if the coffee has steeped for four minutes, depress (smash) piston/plunger, else keep waiting pour coffee into cup enjoy As a post-millennial student from a WEIRD culture, or Western, Educated, Industrialized, Rich Democracy (???), youve programmed computers, too, if only to enter a password, open an app, and upload a photo on your cell phone. Statistics is of fundamental importance, not just for understanding abstract trends, but for making decisions about everyday life. Consider the case of Susie, a college senior: Exercise 2_1 Susie is applying to two med schools. At School A, 25% of students are accepted, and at School B, 25% are accepted as well. You are Susie. Are you going to get in to at least one of these programs? What is the probability? Does your estimate depend upon any assumptions? Questions such as these are important for us. If the combined probability is low, it will likely (another probability concept) make sense for Susie to spend time, money, and energy to apply to additional programs. If the probability is higher, it may not. But problems like this are hard - our estimates of probability are frequently poorly calibrated, and combining probability estimates is challenging (see, e.g., (???), and consider taking a course in Behavioral Economics or Thinking and Decision Making to learn more). You may have worked with data in spreadsheets such as Excel or Google Sheets. Exercise 2_2 Open the Google Sheet at http://bit.ly/dslaX2_1. Save a copy and edit it, entering the following in cell B7: =SUM (B2:B6) What is the result? Now copy cell B7 to C7 What happens? Is this the result you expected? Would another approach be more useful? 2.2 some best practices for spreadsheets Spreadsheets are great tools - the first one, Visi-Calc, was the first killer app to usher in the personal computer revolution. But they have limitations as well. Best practices have been proposed for using spreadsheets in data science such as, for example, include only data (and not calculations) in spreadsheets, use what we will recognize as a tidy format in which data are in a simple rectangle (avoid combining cells and using multi-line headers), and save spreadsheets as simple text files, typically in comma-delimited or CSV format (???). Typically, this means that each worksheet will be in a separate file, rather than in workbooks with multiple tabs. There are good reasons for these recommendations: For example, when we sort data in spreadsheets, we risk chaos, for example, only certain columns may be sorted. When we manipulate data in spreadsheets, we typically will not have a record of what was (and wasnt) changed, and this compromises the reproducibility of our work. The bottom line is that spreadsheets should generally be used to store data rather than to analyze it. But dont be a slave to this - if you are making some quick calculations that are inconsequential and/or to be used on just one occasion, working in Excel or Google Sheets is often the way to go. 2.3 setting up your machine: some basic tools Collaboration and communication are integral to data science. In the world beyond universities, the most important messaging and collaboration platform is Slack. Slack is a commercial app, but it has a free tier. Slack is handy for group work of all forms, but most students in full-time programs will be expected to use a Learning Management System such as Canvas so that all of their classes are on the same platform. Documents produced in word processors typically include invisible characters used for formatting, security, etc. These characters, when fed into a computer program, lead to unpredictable results. Markdown is a simple syntax for producing documents that are easily read by both humans and machines. Most of the documents you will produce and work with this term, including the present chapter, are and will be written in some version of Markdown. You can find an introduction to markdown syntax in Chapter 3 of (???). I use Typora (currently free for both Windows and Mac), but there are many alternatives. Install this or another Markdown editor on your laptop and play with it. Google Docs is free and is convenient for collaborative work. One other important feature of Google Docs is that it provides a framework for version control, a critical skill in information management. You can learn more about how to see and revert to prior versions of a project in Google Docs here. Version control can help you avoid the chaos and confusion of having a computer (or several computers) full of files that look like Chams (2012) comic: Fig 2.1: Never call anything final.doc. Version control is an important concept in data science. Collaboratively built programs and platforms, including most of the add-ons (libraries, packages) which make R so powerful, are open-source projects built by many individuals over time. For projects such as these, both the current build and its associated history are typically maintained on GitHub, a website for hosting code. When we contribute to these projects, we will first mirror the web-based GitHub site using a program on our own Macs or Windows PCs called Git, then upload our proposed changes. Keeping remote and local branches of files in sync can be challenging, however, and you will not be expected to use this technology in this class. But if you are curious, or want to learn more, an introduction to using Git and R together may be found here. 2.4 a modified 15-minute rule You will run into problems, if not here, then elsewhere. An important determinant of your success will be the balance you maintain between persistence and help-seeking. The 15-minute rule is one guideline for this balance: It has been cleverly summarized as You must try, and then you must ask. That is, if you get stuck, keep trying for 15 minutes, then reach out to others. I think that this rule is basically sound, particularly if it is applied with cognitive flexibility, social sensitivity, and reciprocity. So when you get stuck, make a note of the problem, then move to another part of your project (thats the cognitive flexibility part): This allows your problem to percolate and still make progress. When you ask others for help, ask in a way that shows an awareness of the demands on their time (social sensitivity): Part of this means that you should explain your problem in as detailed a fashion as possible - in technical terms, a reprex or reproducible example. Finally, you should be willing to provide as well as give help (reciprocity). 2.5 discussion: who deserves a good grade? In an introductory class in data science, students invariably come to class with different backgrounds. Should this be taken into account in assigning grades? That is, would it be possible (and desirable) to assign grades in a class based not just on what students know at the end of the term, but also on how much they have learned? A formal, statistical approach to this could use regression analysis. That is, one could predict final exam scores from pretest scores, and use the residuals - the extent to which students did better or worse than expected - as a contributor to final exam grades. Interestingly, there would be an unusual incentive for students on this pretest to do, seemingly perversely, as poorly as possible. How could this be addressed? Another problem with this approach is that there may be ceiling effects - students who are the strongest coming in to the class cant improve as much as those who have more room to grow. Again, how might this be addressed? Should it? "],["welcome-to-r-world.html", "3 welcome to R world 3.1 Using RStudio cloud 3.2 Using R Studio on your laptop", " 3 welcome to R world In this chapter, well learn how to install and use R in two ways - in the cloud and on your own laptop. For each of these, well use RStudio as a front end (an integrated development environment, or IDE). 3.1 Using RStudio cloud Using the link provided to you by your instructor, set up a new account in RStudio (it is free for you). Then click on the Join Space button. It will bring you to the Welcome page. (Not much to see here as yet). Click on the hamburger menu. You should see, on the left side of the screen, a menu with three sections: Spaces, Learn, and Help. The Spaces menu will likely include three options. The first will be Your Workspace, the second will be the workspace for this class (e.g., IntroDataSciSpring21), and the third will allow you to create a New Space. Click on the class workspace, then on the first assignment. After a few seconds, youll see a screen that looks something like the following: Fig 3.1 - R Studio Cloud. Click on the assignment, and youll see that there are now four windows on the screen (note that these windows are fully customizable, so that the locations and characterizations I am giving here are just a starting point): In the upper left is the source window - the code you are working on.. This is where we will be doing much of our work, typically in R markdown documents like this one. R markdown is, not surprisingly, a flavor of markdown. Other tabs in this quadrant may include displays of your data and other scripts that you may be working on. The console, by default in the lower left quadrant, can be used to execute single lines of code. You may occasionally use the terminal tab here as well. The environment, in the upper right, includes datasets, variables, and functions that are available to us. The Build and Git tabs are useful for producing documents and version control/collaboration, respectively. The history tab is useful to go back and look at the syntax you used in a prior chunk of code. The files in our directory are listed in the lower right; additional tabs include a list of available packages (libraries) and some resources for help. Now read the code in the console, and follow the instructions to complete your first homework assignment. Congratulations, youve just run your first code in R. 3.2 Using R Studio on your laptop Install the latest version of R on your own Windows or Mac laptop. Then install the preview version of R studio, as this has a visual (WYSIWYG) editor that is a significant advance over the existing version. Feeling ambitious? Once this is loaded, (a) set up a directory for this class, then (b) a subdirectory you can call Assignment01. Go back to your Rstudio.cloud / Assignment01, and check the boxes next to Assiignment01.Rmd, movies.Rdata, and project.rproj. Then click on More, then Export and put these in your new subdirectory. Now go back to your desktop environment, and try to run the same code on your home machine. (Note that to be successful, you will need to first execute the command on your laptop). install.packages(tidyverse) Get as far as you can, but remember the 15 minute rule. How far did you get? Be prepared to share your frustrations, discoveries, and accomplishments with your classmates. "],["r-stands-for.html", "4 R stands for  4.1 a few characteristics of R 4.2 finding help 4.3 Wickham and R for Data Science", " 4 R stands for  Historically, R grew out of S which could stand for Statistics. But what does R stand for? R is a system for Reproducible analysis, and reproducibility is essential. R markdown documents, like Jupyter notebooks in the Python world, facilitate reproducible work, as they include comments or explanations, code, links to data, and results. R is for Research. Research is not just an end-product, not just a published paper or book:  these documents are not the research [rather] these documents are the advertising. The research is the full software environment, code, and data that produced the results (???; ???). When we separate the research from its advertisement we are making it difficult for others to verify the findings by reproducing them (???). R is a system for Representing data in cool, insight-facilitating ways, a tool for creating (reproducible) data visualizations which can provide insights and communicate results. R is Really popular, and this matters, because learning R will make you a more attractive candidate for many graduate programs as well as jobs in the private sector. Because R is popular, there are many Resources, including, for example - Online resources include the simple (and less simple) lessons of SwirlR, which offers the possibility of learning R in R, as well as DataCamp, the Data Science Certificate Program at Johns Hopkins, and other MOOCs. Books include (Peng 2014) - which includes not only videos of his lectures in the program at Hopkins, but also a brief list of still more resources - and (Wickham and Grolemund 2016). Youll also learn (more directly) from people, including your classmates, as well as the broader community of people around the world. There are hundreds if not thousands of people, young and old, who are on the road with you. I am as well, just a step or two (hopefully) ahead. R might stand for Relatively high level. Programming languages can be described along a continuum from high to low level, the former (like R) are more accessible to humans, the latter (like assembly language) more accessible to machines. Python, Java, and C++ are all more towards the middle of this continuum. R does not stand for [arggh](https://www.urbandictionary.com/define.php?term=ARGH), although you may proclaim this in frustration (arggh, why cant I get this to work?) or, perhaps, in satisfaction (arggh, matey, that be a clever way of doing this). But R does stand for Rewarding. A language is a way of thinking about the world, and this is true for computer languages as well. Youll be challenged by its complexity, its idiosyncracy, its alien logic. But you will succeed, and you will find that you can do things that you did not believe possible. 4.1 a few characteristics of R R includes the base together with packages. These packages (libraries) are customized add-ons which simplify certain tasks, such as text analysis. There are, at this writing, 15,373 available packages on the CRAN package repository - and though there is not yet an R package for ordering pizza (Peng 2014), there are many for most data tasks, including, for example, over 50 different packages for text analysis. So how do you choose, and where do you begin? We will start with the curated list of packages which jointly comprise the tidyverse (Wickham and Grolemund 2016), which is effectively a dialect of R. R is an object-oriented language - one conceptually organized around objects and data rather than actions and logic. In R, at the atomic level, objects include characters, real numbers, integers, complex numbers, and logical. These atoms are combined into vectors, which generally include objects of the same type (one kind of object, lists, is an exception to this; Peng 2014). Vectors can be further combined into data frames, which are two-dimensional tables or arrays. A tibble is a particular type of data frame which is used in the tidyverse. It is, in some ways, handier to work with than other data frames. Well be working extensively with data frames in general, and tibbles in particular, as we move forward. Objects have attributes. Attributes of R include such things as name, dimensions (for vectors and arrays), class (thats the type of object described in the previous paragraph), length, etc. Real world data sets are messy, and frequently have missing values. In R, missing values may be characterized by NA (not available) or NaN (not a number, implying an undefined or impossible value). RStudio, the environment we will use to write, test, and run R code, is a commercial enterprise whose business model, judged from afar, is an important one in the world of technology. Most of what RStudio offers is free (97% according to Garrett Grolemund in the video below). The commercial product they offer makes sense for a relative few, but it is sufficiently lucrative to fund the enterprise. The free product helps to drive the popularity of Rstudio; this widespread use, in turn, makes it increasingly essential for businesses to use. This mixed free/premium, or freemium, model characterizes Slack as well, but while the ratio of free to paid users of Slack is on the order of 3:1, for R it is, I am guessing, an order of magnitude higher than this. 4.2 finding help One does not simply learn R. Unlike, say, learning to ride a bicycle, fry an egg, or drive a car with a manual transmission, learning R is not a discrete accomplishment that one can be said to have mastered and from which one then moves on. Rather, R is an evolving, open system of applications and tools which is so vast that there is always more that one can achieve, new lessons that one can learn. And, the complexity of R syntax is such that, for almost all of us, we will need help for coding on any non-trivial task. For us, the key ideas in looking for help will include not just the tools on the RStudio IDE, but also (a) using Google searches wisely, and (b) reaching out to your classmates on Slack. Here, as in the real world, there is an etiquette for help-seeking which is based on consideration. Your search for help should begin by making sure that others will encounter the same result, then by stripping the problem down to its essence. Once you have reduced the problem to this minimal, reproducible essence, you will often be able to spot the problem yourself - and, if not, you will make it easier for others to help you. There is an R package (reprex) which will likely facilitate this, but I havent tried it yet. Here is a good introduction. Again, remember the 15 minute rule - we all need somebody to lean on. Finally, to get a sense of some of the ways you can get help in RStudio (and to see how a master uses the R Studio interface), consider this video: video 4.1: Garrett Grolemund of RStudio 4.3 Wickham and R for Data Science The first chapter of the Wickham text (Wickham and Grolemund 2016) provides both a framework for his approach and a brief introduction to the tidyverse which will be the dialect of R we will study in the weeks ahead. Please read it now. References "],["now-draw-the-rest-of-the-owl.html", "5 now draw the rest of the owl 5.1 Review the RStudio cloud primers 5.2 Play with RStudio on your laptop 5.3 Take a DataCamp class 5.4 Swirl (Swirlstats) 5.5 Read Pengs text and/or watch the associated videos 5.6 Code along with a tidy webinar, or attend a virtual conference 5.7 Something else", " 5 now draw the rest of the owl Fig 5.1: Draw the rest of the owl. There are many sources for learning the basics of R. A few of these follow. Please spend at least 90 mins exploring at least two of the following. Be prepared to discuss your progress next class (you will be asked which source(s) you used, what you struggled with, and whether you would recommend it to your classmates. (Note that all of these are free, though you may choose to make a donation to the author if you use the Peng text). Hint: If you find the material too challenging - if you feel like you are drawing the rest of the owl - take a break away from your machine and other screens, clear your head, then try a different approach. 5.1 Review the RStudio cloud primers A series of up-to-date, interactive tutorials may be found for working with RStudio.cloud here. There are two basic tutorials - one on data visualization, the other on programming. 5.2 Play with RStudio on your laptop Last week, we began exploring Mine Çetinkaya-Rundels movies dataset in RStudio.cloud. Were you successful in getting this to run on your laptop as well? 5.2.1 Create a new R Markdown document and knit it to a PDF or a Word doc In literate programming, comments, code, and results are integrated in a clear and reproducible way - they document our work. R markdown is built around this idea. To create an R markdown (Rmd) document in Rstudio (on your laptop) or Rstudio.cloud, begin with (File -&gt; New File -&gt; R Markdown). A window will open up with a file that begins with some YAML (Yet Another Markdown Language). You can edit this as needed: --- title: &quot;Yo movies in R Markdown Document&quot; author: &quot;Frankie McFrank Frank&quot; date: &quot;1/27/2020&quot; output: html_document --- Go ahead and click on the clever knit icon in the bar just above the source window to create a sample document. Youll need to save the file with a new name, then R will finally create an HTML (Hyper Text Markdown Language - see the pattern?) page. Compare the R Markdown document (your code) with the result (the HTML). Now, try to knit the document into different output formats (click on the down arrow next to knit, and see if you can knit to PDF and Word). It may take a while the first time you do this, as you may need to load some additional formatting tools or packages (e.g., TinyTex) to successfully render these files. Can you knit directly to a PDF document on your laptop? A Word document? If you struggle, you are not alone here - an advantage of RStudio.cloud over RStudio on the desktop is that, on your own machine, you may run into system-specific problems associated with the pandoc document converter. If, after spending 15 minutes on this problem, you cannot knit directly to a PDF, stop and think about the problem. 5.2.2 Play with and explore the movies data Iain Carmichael used these data to introduce R for his students several years ago; you may wish to use his code as a starting point. Continue to ask interesting questions about the data - can you see any way in which movies appear to have changed over time? What does spilled mean? Write up your results as an R markdown document. 5.3 Take a DataCamp class Many folks swear by (and others, I presume, at) DataCamp, which kind of gamifies learning software. As a student in this class, you have access to all of their stuff free. You can even do lessons on your phone. Use the link given to you in class to enroll. 5.4 Swirl (Swirlstats) I, like thousands of others, learned R in the process of completing the Johns Hopkins Data Science Specialization offered through Coursera. The sequence can be challenging, but their introduction to R used an accessible, interactive R package called Swirl. You can read about swirl (learn R in R) at https://swirlstats.com/. Using Swirl. After loading R (and opening R studio), you will get to the Swirl lessons with the following steps: Install the Swirl package on your computer (you only need to do this once). Type the following into your console window in R studio (typically left hand side of your screen or lower left) install.packages(swirl) Then load the package into your workspace (youll need to do this at the beginning of every session you use Swirl) library (swirl) Then run it! swirl () Swirl will ask a few questions then give you the option of choosing one of several courses. Youll choose the R Programming option, which leads to 15 separate lessons. At the end of each lesson, youll be asked Would you like to receive credit for completing this course on Coursera.org? Answer no then do another lesson. 5.5 Read Pengs text and/or watch the associated videos Finally, consider the text and videos from the Coursera R class. Most of the material from that class can be found in the Peng text (2014). A slightly updated version of the text can be found at https://bookdown.org/rdpeng/rprogdatascience/, and the videos in the series may be found by clicking on the following: . Video 5.2: Roger Peng introducing R 5.6 Code along with a tidy webinar, or attend a virtual conference Among these resources for R studio beginners, you will find Thomas Mocks introduction to Tidy statistics in R. It includes a link to the code used in the seminar at the bottom of the launch screen. The second chapter of Healys online book about Data visualizations provides an introduction to R and R studio which largely parallels the discussions here and in the Wickham and Grolemund text (Healy 2017; Wickham and Grolemund 2016) The Grolemund video linked in the prior chapter is part of a series of videos on programming in RStudio. Its a little out of date now (from 2015), but still likely to be useful. Most of the offerings at the Rstudio::global conference are aimed at more experienced R users, some are aimed at (relative) noobs, including Learning R with Humorous Side Projects. The 2021 conference, to be held on January 21, 2021, is free; you can enroll and explore here 5.7 Something else The something else category includes Datacarpentry.org, which is aimed at fostering data literacy and provides free lessons for learning and applying R in areas such as Ecology, Genomics and Geospatial data analysis. Of particular interest are the social science lessons, as well as a workshop in data science based on the \"Studying African Farmer-led Irrigation (SAFI)\" dataset. References "],["principles-of-data-visualization.html", "6 principles of data visualization 6.1 some opening thoughts 6.2 some early graphs 6.3 Tukey and EDA 6.4 approaches to graphs 6.5 Tufte: first principles 6.6 the politics of data visualization 6.7 the psychology of data visualization 6.8 further reading and resources", " 6 principles of data visualization 6.1 some opening thoughts Graphs arent just to inform, but to make you reflect. We are concerned not just with the design of graphs, but with our role in society as leaders, as arbiters of facts, as educators. We want to foster data literacy. How do people understand graphs? How can you use graphs to tell people the truth in a way that they understand? How can you convey uncertainty in a graph? When you see a graph, what do you notice, what do you wonder, and what is the story? Is story telling what visualizations should be about? A graph might begin with perception and understanding (the consumer), with knowledge and design values (the producer), but it also reflects the truth of the data. How much is each? 6.2 some early graphs Visual displays of information reach back to prehistoric times; modern graphs date back, arguably, to Playfairs 1786 Political Atlas - in which  spatial dimensions were used to represent nonspatial, quantitative, idiographic, empirical data. Although it now seems natural to represent, for example, rising and falling imports over time as a rising and falling line, it does not seem to have been done before that time and was quite an accomplishment. Notably, in addition to the statistical line chart, Playfair at one fell swoop single-handedly invented most of the remaining forms of the statistical graphic repertoire used today-the bar chart and histogram, the surface chart, and the circle diagram orpie chart\" (Wainer and Thissen 1981). Fig 6.1: Playfairs 1786 analysis of trade deficits The most celebrated early graph is that of Minard: Fig 6.2: Minards display of Napoleons catastrophic assault on Moscow, 1812 The visualization depicts the size, latitude, and longitude of Napoleons army as they moved towards (tan line) then away (black line) from Moscow; temperature during the retreat is plotted as well. Further, vertical lines linking the temperature display to the number of troops indicate the often perilous river crossings which further decimated Napoleons troops). Cheng (2014) decomposes the graph and provides some simpler visualizations; she also provides the following background: \"Czar Alexander of Russia sees that Napoleon was becoming too powerful, so he refuses to participate in this embargo [against the UK]. Angry at Czar Alexanders decision, Napoleon gathers a massive army of over 400,000 to attack Russia in June of 1812. While Russias troops are not as numerous as Frances, Russia has a plan. Russian troops keep retreating as Napoleons troops move forward, burning everything they pass, ensuring that the French forces could not take anything from their environment. Eventually the French army follows the Russian army all the way to Moscow during October, suffering major losses from lack of food. By the time Napoleon gets to Moscow, he knows he has to retreat. As winter settles into Europe and the temperature drops, Napoleons troops suffer even more losses, returning to France from lack of food, disease, and weather conditions.\" Of course, the casualties and retreat of Napoleons army are immortalized not just in this graph, but also in Russian literature (Tolstoys War and Peace) and music (Tchaikovskys 1812 overture, in which five cannon shots mark the battle of Borodino and eleven more mark the arrival of Napoleon in the now-razed city of Moscow). 6.3 Tukey and EDA For (???), the publication of John Tukeys Future of Data Analysis (???) arguably marks the beginning of data science. As the first data scientist, Tukey embraced a descriptive and exploratory approach to data analysis, particularly in his publication of Exploratory Data Analysis (???). In EDA, Tukey presented an idiosyncratic, coherent approach to looking at data, beginning with tallying. The distributions of small counts of single variables, for Tukey, could best be presented in stem and leaf displays. Comparisons between groups can be presented in box plots. To examine relationships between variables and the adequacy of simple models, he argues for thoughtfully transforming data to uncover linear trends, then to examine residuals to find where these trends do not hold. 6.4 approaches to graphs A graph might begin with perception and understanding (the consumer), with knowledge and design values (the producer), but it also reflects the truth of the data. How much is each? In thinking about how to design graphs, we can begin with abstract theory, with principles of design informed by our understanding of perception, or with empirical analyses of understanding and memory. 6.5 Tufte: first principles (???) describes Graphical Excellence. Graphs should, among other things, Induce the viewer to think about the substance, rather than about methodology, graphic design, the technology of graphic productions, or something else. Graphs should Present many numbers in a small space, make large data sets coherent, and encourage the eye to compare different pieces of data. Graphs should serve a reasonably clear purpose: description, exploration, tabulation, or decoration [and] be closely integrated with the statistical and verbal descriptions of a data set. Tufte concludes with the following Principles of Graphical Excellence, which I quote verbatim: Graphical excellence is the well-designed presentation of interesting dataa matter of substance, of statistics, and of design. Graphical excellence consists of complex ideas communicated with clarity, precision and efficiency. Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space. Graphical excellence is nearly always multivariate. And graphical excellence requires telling the truth. 6.6 the politics of data visualization On a cold morning in January, 1986, Space Shuttle Challenger lifted off from Cape Canaveral. Because of the cold weather, engineers at Morton Thiokol, who designed the rocket boosters, considered the possibility that the O-rings which sealed joints on the rockets would be too hard and brittle to prevent the release of potentially explosive fuel. They examined the relation between temperature and o-ring damage on prior flights, using the following visualization: Figure 6.3: What Morton Thiokol engineers saw prior to deciding to launch the Challenger Space Shuttle in January, 1986 What, if anything, can we see here? The graphic was chaotic and poorly designed; if the engineers had wanted to systematically explore the relation between o-ring damage and temperature they could have removed the rockets and arranged them on a single axis by liftoff-temperature rather on several rows by liftoff-date. Heres what they would have seen: Figure 6.4: What the engineers could have seen, perhaps, with a better graph. The forecast for the morning of the launch was far colder than that for prior launches, and within the set of prior launches a clear relationship between temperature and o-ring damage is evident. But the Challenger did launch, exploding shortly after takeoff, killing the seven astronauts on board. With a clearer display of the data, the Challenger launch would likely have been postponed (???). 6.6.1 poor design leads to an uninformed or misinformed world In the flood of information that surrounds us, our peripheral and sometimes our focal attention will be drawn to pictorial summaries of consequential events and effects. But while data visualizations and data journalism has become more responsible, poorly designed graphs - what Tufte has described as chartjunk - are still common. Consider the following graph, which recently appeared in a number of newspapers or feeds in the United States (including the Palm Beach Post, on January 12, 2019). Figure 6.5: Which smartphone manufacturers are doing well? Exercise 6.1 Examine the graph shown above. Look at it for just a moment, as if you might while reading something else on the screen, or as your ten-year old little sister might. What does the graph tell you? Look at it more closely. What does it actually say? How could you improve it to make the content of the graph more informative about its title? Why was the graph designed in this way? Does this matter? Poorly designed graphs dont just confuse rocket scientists. They reach us, mislead us, and make fools of all of us as well. With better graphs, we become a better informed, better educated citizenry. 6.6.2 poor design can be a tool to deceive Figure 6.6: Trump as the first datavis President (Meeks, 2019). The apps on our smartphones and electronic devices are meticulously designed to maintain our attention. They are largely successful, but, for many of us, made it more difficult to read in depth (Wu, The Attention Merchants). With information all around us demanding our attention, visual representations of data have become particularly important. The data visualization is, increasingly, not just a supplement to the story, but the story itself. When this premise is coupled with the idea that the truth is malleable, the product of what our leaders say rather than empirical statements of fact, we approach a dangerous situation, one in which real threats are misrepresented and lives are again endangered. Presenting information in self-promoting ways includes so-called Sharpie-gate, where President Trump simply altered a hurricane prediction map in defense of a misstatement. It also includes more subtle misrepresentations: Does stand-your-ground really make us safer? Figure 6.7: Stand your ground makes us all safer. Or not. 6.7 the psychology of data visualization Speaking of America, consider the following: Figure 6.8: Chernoffs too-clever faces In this figure, from (Wainer and Thissen 1981), (Chernoffs) faces are used to represent multivariate data. The display is loosely based on the (psychological) premise that, from infancy, we are inordinately sensitive to facial features. Consequently, the most powerful, effective, or informative data displays should manipulate facial features. Does this graph succeed? Why or why not? Would an updated model, based on richer, more complex, and more realistic simulations of facial expressions (???) be more successful? 6.7.1 the power of animation Animated data displays bring the dimension of time into data visualization. Here are two brief (&lt; 5 minutes) animated data displays, each powerful in its own way, and each of which gives the viewer an appreciation of an important social phenomenon. The first is from the late Hans Rosling, and summarizes a chunk of the Gapminder data. There is a Gapminder package in R if you would like to explore the data further, and perhaps become the next Rosling: Video 6.9: Rosling and social progress The second is from Kim Rees and her ex-colleagues at Periscopic (Rees is now at CapitalOne). For me, its an important graphic because it tries to overcome what has been called psychic numbing - we are sensitive to the individual case, but lose our perspective when faced with large-scale tragedies, so that effectively the more lives lost the less we care (???). Video 6.10: Rees and stolen years 6.7.2 telling the truth when the truth is unclear We live in an uncertain world, and a major challenge in data visualization is how to convey this uncertainty in a way that people understand readily. Examples of this are familiar. Here in Florida, projected hurricane paths are frequently represented by a cone of uncertainty surrounding a single track. An alternative approach is to use a display which shows a distribution of possible tracks. Figure 6.11: Two approaches to displaying hurricane paths 6.7.3 visualizing uncertainty To further illustrate the idea of uncertainty, data visualizations may be animated. To display hurricane outcomes, for example, individual lines in a spaghetti plot might fade in and out (???). Another use of animation is suggested by (???) who use hypothetical outcome plots rather than simple error bars to illustrate sampling distributions. Empirically, subjects have a better understanding of the meaning of statistical effects when these are represented by dynamic displays. During the 2016 US Presidential Election, the NY Times graphics presented the range of likely possibilities (here, between the 25th and 75th percentiles) as a dynamically changing jittery gauge . Following the election, there was some criticism of these gauges, as readers claimed that they tended to make them anxious. However, it appears that at least some of the anxiety was attributable not to the gauges themselves, but to the electoral outcome itself. The gauges were back in 2018, and will likely be used again in the future. Finally, animation may be used not just to display uncertainty or a possible range, but also data displays which both carry a great deal of information and unfold over time, such as the language of personality development in my own work. 6.8 further reading and resources If youd like to learn more, (???) and his other books are beautiful and thought provoking. (???) examines graphs from a more rigorous psychological and empirical viewpoint. The Data Stories podcasts are often excellent, despite the challenge of an auditory medium for visual display (especially the episode on Hans Rosling). And (???) provides a comprehensive introduction that, in essence, provides the basis for a semester-long course based in the ggplot2 package in R that we will be working with in the next chapter (Wickham and Grolemund 2016). Finally, several years ago Alberto Cairo of the University of Miami offered an online MOOC (massive open online course) on data visualizations. You can find the course materials here. References "],["visualization-in-r-with-ggplot.html", "7 visualization in R with ggplot 7.1 a picture &gt; (words, numbers)? 7.2 Read Hadley ggplots 7.3 exploring more data", " 7 visualization in R with ggplot In the last chapter, we introduced data visualization, citing vision-aries including Edward Tufte and Hans Rosling, inspired works such as Minards Carte Figurative and Periscopics stolen years, as well as a few cautionary tales of misleading and confusing graphs. Here, in playing with and learning the R package ggplot, we begin to move from consumers to creators of data visualizations. As the first visualization in (Wickham and Grolemund 2016) reminds us, data visualization is at the core of exploratory data analysis: Fig 7.1: Data visualization is at the core of data analysis ((Wickham and Grolemund 2016)) In the world of data science, statistical programming is about discovering and communicating truths within your data. This exploratory data analysis is the corner of science, particularly at a time in which confirmatory studies are increasingly found to be unreproducible. Most of your reading will be from Chapter 3 of (Wickham and Grolemund 2016), this is intended only as a supplement. 7.1 a picture &gt; (words, numbers)? The chapter begins with a quote from John Tukey about the importance of graphs. Yet there is a tendency among some statisticians and scientists to discount graphs, to consider graphic representations of data as less valuable than statistical ones. It is true that, because there are many ways to graph data, and because scientists and data journalists are humans with pre-existing beliefs and values, a graphical displays should not be assumed to simply depict a singular reality. But the same can be said about statistical analyses (see Chapter 8). To consider the value of statistical versus graphical displays, consider Anscombes quartet (screenshot below, live at http://bit.ly/anscombe2019): Table 7.1: An adaptation of Anscombes quartet (???) Exercise 7_1 Consider the spreadsheet chunk presented above, which I am characterizing as data collected on a sample of ten primary school children at recess on four consecutive days. Working with your classmates, compute the mean, standard deviation, and correlation between the two measures for one day. Share your results with the class. The four pairs of variables in (???) appear statistically the same, yet the data suggest something else. Later, well try to plot these. Perhaps graphs can reveal truths that statistics can hide. Exercise 7_2 The anscombe data is included as a library in R. Can you find, load, and explore it? 7.2 Read Hadley ggplots In class, we will review and recreate the plots in section 3.2 of (Wickham and Grolemund 2016) and exercises through 3.4. Savor this section, reading slowly, and playing around with the RStudio interface. For example, read about the mpg data in the help panel, pull up the mpg data in a view window, and sort through it by clicking on various columns. Fig. 7.2: A screenshot from RStudio, showing the mpg dataset 7.3 exploring more data Explore the Gapminder data https://cran.r-project.org/web/packages/gapminder/README.html Choose one of the datasets in R, pull out a few variables, and explore these. Try to make a cool graph - one that informs the viewer, and, to paraphrase Tukey, helps us see what we dont expect. Try several different displays. Which fail? Which succeed? Be prepared to share your efforts. Dont be afraid to screw up. Each mistake you wisdom. References "],["examining-local-covid-data-in-r.html", "8 examining local COVID data in R 8.1 tracking the Novel Coronavirus (from Feb 2020) 8.2 plotting confirmed cases (Feb-Mar 2020) 8.3 status (Feb 2021) 8.4 how to create new knowledge", " 8 examining local COVID data in R In Winter 2020, as the COVID-19 pandemic began to take hold, I began chronicling the course of the virus. What follows is an archive, followed by a consideration of some possible new directions. 8.1 tracking the Novel Coronavirus (from Feb 2020) Here, I want to consider a timely (but challenging) dataset. The Novel Coronavirus is an emerging health crisis, particularly in Wuhan (a Chinese city larger than New York City) and the surrounding province of Hubei. It is not yet a threat in the United States - there have, at this writing (02/11/20) been zero cases in Florida. Still, tracking the spread of the virus - the unfolding number of people infected and recovered, as well as the number of deaths - is a fascinating exercise. This is an educational script for students learning R with the Tidyverse. It reads data provided by the Johns Hopkins Center for Systems Science and Engineering (JHU/CSSE). It was modified February 3 because of a new GoogleSheet link and altered variable names, on Feb 5 because of a new URL for the data and additional changes in the variable name for date, and Feb 7 to (a) remove need for OAuth and (b) separate Wuhan from other China. On Feb 9, additional data cleaning was performed and interactive plots were added. On February 11, the code was rewritten to read files from a Github repo rather than Google Sheets. Consequently, this does not use an API or require authorization from Github. library(tidyverse) library(magrittr) library(lubridate) library(htmlwidgets) library(httr) library(purrr) # get list of files filelist &lt;- GET(&quot;https://api.github.com/repos/CSSEGISandData/2019-nCoV/git/trees/master?recursive=1&quot;) %&gt;% content() %&gt;% # there is probably a more efficient way to reduce this # list to a set of filenames flatten() %&gt;% map (&quot;path&quot;) %&gt;% flatten() %&gt;% tibble() %&gt;% rename(filename = 1) %&gt;% filter(str_detect(filename,&quot;.csv&quot;) &amp; str_detect(filename,&quot;daily&quot;)) nsheets &lt;- nrow(filelist) rawGitFiles &lt;- &quot;https://raw.githubusercontent.com/CSSEGISandData/2019-nCoV/master/&quot; 8.1.1 reading the data (Feb 2020) The Novel Coronavirus data consists of a series of csv files in a Github repository. This combines them into a single sheet in R. # variables to retain or create numvars &lt;- c(&quot;Confirmed&quot;, &quot;Deaths&quot;, &quot;Recovered&quot;) varlist &lt;- c(&quot;Province/State&quot;, &quot;Country/Region&quot;, &quot;Last Update&quot;, numvars) # one cool trick to initialize a tibble coronaData &lt;- varlist %&gt;% map_dfr( ~tibble(!!.x := logical() ) ) # add data from files to tibble for (i in 1:nsheets) { j &lt;- read_csv(paste0(rawGitFiles,filelist$filename[i])) # if a variable doesn&#39;t exist in sheet, add it j[setdiff(varlist,names(j))] &lt;- NA # datetime is formatted inconsistently # across files, this must be done before merging j %&lt;&gt;% mutate(`Last Update` = parse_date_time(`Last Update`, c(&#39;mdy hp&#39;,&#39;mdy HM&#39;, &#39;mdy HMS&#39;,&#39;ymd HMS&#39;))) %&gt;% select(varlist) coronaData &lt;- rbind(coronaData, j) } head(coronaData) ## # A tibble: 6 x 6 ## `Province/State` `Country/Region` `Last Update` Confirmed Deaths ## &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Anhui Mainland China 2020-01-21 22:00:00 NA NA ## 2 Beijing Mainland China 2020-01-21 22:00:00 10 NA ## 3 Chongqing Mainland China 2020-01-21 22:00:00 5 NA ## 4 Guangdong Mainland China 2020-01-21 22:00:00 17 NA ## 5 Guangxi Mainland China 2020-01-21 22:00:00 NA NA ## 6 Guizhou Mainland China 2020-01-21 22:00:00 NA NA ## # ... with 1 more variable: Recovered &lt;dbl&gt; str(coronaData) ## tibble [1,238,593 x 6] (S3: tbl_df/tbl/data.frame) ## $ Province/State: chr [1:1238593] &quot;Anhui&quot; &quot;Beijing&quot; &quot;Chongqing&quot; &quot;Guangdong&quot; ... ## $ Country/Region: chr [1:1238593] &quot;Mainland China&quot; &quot;Mainland China&quot; &quot;Mainland China&quot; &quot;Mainland China&quot; ... ## $ Last Update : POSIXct[1:1238593], format: &quot;2020-01-21 22:00:00&quot; &quot;2020-01-21 22:00:00&quot; ... ## $ Confirmed : num [1:1238593] NA 10 5 17 NA NA NA NA 1 NA ... ## $ Deaths : num [1:1238593] NA NA NA NA NA NA NA NA NA NA ... ## $ Recovered : num [1:1238593] NA NA NA NA NA NA NA NA NA NA ... 8.1.2 cleaning (wrangling, munging) the data (Feb 2020) Cleaning the data includes not just finding errors, but adapting it for our own use. Its generally time consuming, as was the case here. The following letters refer to sections of the code below. a - fix a few missing values outside of China for province and country b - the earliest cases, all in China, did not include country c - because province/state is included inconsistently, an unambiguous place variable is created d - reportdate is standardized (above) and renamed e - in some cases, multiple reports are issued for each day. only the last of these is used for each place. f - for dates where no data was supplied, the most recent (previous) data are used g - values of NA for Deaths, Confirmed, and Recovered cases are replaced by zero. h - Prior to Feb 1, 2020 reporting for US included only state, since then, city and state. This drops the (duplicated) province/state-only values beginning Feb 1. coronaData &lt;- coronaData %&gt;% # a mutate (`Province/State` = case_when( (is.na(`Province/State`) &amp; (`Country/Region` == &quot;Australia&quot;)) ~ &quot;New South Wales&quot;, (is.na(`Province/State`) &amp; (`Country/Region` == &quot;Germany&quot;)) ~ &quot;Bavaria&quot;, TRUE ~ `Province/State`)) %&gt;% mutate (`Country/Region` = case_when( `Province/State` == &quot;Hong Kong&quot; ~ &quot;Hong Kong&quot;, `Province/State` == &quot;Taiwan&quot; ~ &quot;Taiwan&quot;, `Province/State` == &quot;Washington&quot; ~ &quot;US&quot;, # b is.na (`Country/Region`) ~ &quot;Mainland China&quot;, TRUE ~ `Country/Region`)) %&gt;% # c mutate(place = ifelse(is.na(`Province/State`), `Country/Region`, paste0(`Province/State`,&quot;, &quot;, `Country/Region`))) %&gt;% mutate(reportDate = date(`Last Update`)) %&gt;% group_by(place,reportDate) %&gt;% # e slice(which.max(`Last Update`)) %&gt;% ungroup() %&gt;% # fill in missing dates for each place for time series # f group_by(place) %&gt;% complete(reportDate = seq.Date(min(reportDate), today(), by=&quot;day&quot;)) %&gt;% fill(c(Confirmed,Deaths,Recovered, `Country/Region`,`Province/State`)) %&gt;% # g ungroup() %&gt;% mutate_if(is.numeric, ~replace_na(., 0)) %&gt;% # h mutate(dropcase = ((!str_detect(`Province/State`,&quot;,&quot;)) &amp; (reportDate &gt; &quot;2020-01-31&quot;) &amp; (`Country/Region` == &quot;Canada&quot; | `Country/Region` == &quot;US&quot;))) %&gt;% # dplyr called explicitly here because plotly has taken over &#39;filter&#39; dplyr::filter (!dropcase) %&gt;% select(-c(`Last Update`,`Province/State`,`Country/Region`,dropcase)) head(coronaData) ## # A tibble: 6 x 5 ## place reportDate Confirmed Deaths Recovered ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 2020-02-24 1 0 0 ## 2 Afghanistan 2020-02-25 1 0 0 ## 3 Afghanistan 2020-02-26 1 0 0 ## 4 Afghanistan 2020-02-27 1 0 0 ## 5 Afghanistan 2020-02-28 1 0 0 ## 6 Afghanistan 2020-02-29 1 0 0 write_csv(coronaData,&quot;coronaData.csv&quot;) 8.1.3 eleven months later: the code still runs! The above code runs without apparent error, and leads to a dataset of 152435 rows by 5 columns. Heres a quick peek (Note that I use the group_by and summarize functions to collapse the file to one line per date): coronaData %&gt;% group_by(reportDate) %&gt;% summarize(Confirmed = sum(Confirmed)) %&gt;% head() ## # A tibble: 6 x 2 ## reportDate Confirmed ## &lt;date&gt; &lt;dbl&gt; ## 1 2020-01-21 332 ## 2 2020-01-22 557 ## 3 2020-01-23 654 ## 4 2020-01-24 941 ## 5 2020-01-25 1771 ## 6 2020-01-26 2795 8.1.4 shall we graph it? (Feb 2021) So far, so good. Lets plot the whole range. What do we see? This will generate a quick graph in ggPlot which shows the global incidence of confirmed cases. coronaData %&gt;% group_by(reportDate) %&gt;% summarize(Confirmed = sum(Confirmed)) %&gt;% ggplot(aes(x=reportDate)) + geom_line(aes(y=Confirmed)) 8.1.5 too bad According to the above graph, there have been no additional COVID cases since mid-March or so. Too bad that the data arent right here - for us and especially the world. Exercise 8_1 How would we find the exact date when the file stopped updating? In class, well consider this question (well use pre-downloaded data to save time and computer resources). Use whatever method you like for this - kludgy or clever - but you should describe your results in explicit language or code so that anyone else would get the same results. 8.1.6 what now? I want to make sure that the rest of my code still works, so Ill look more closely at the valid data. Ill generate a new variable (firstbaddate, which sounds like a corny romcom), and filter by it. When the code was written a year ago, the COVID outbreak was largely contained to the Hubei province (which includes the city of Wuhan). So I tried breaking down the data into three locations, breaking down China into Hubei, other China, and the rest of the world. And I generated summaries for three measures - Confirmed, Deaths, and Recovered cases firstbaddate &lt;- &#39;2020-03-22&#39; # This line is added in 2021 coronaDataSimple &lt;- coronaData %&gt;% filter(reportDate &lt; firstbaddate) %&gt;% # added in 2021 mutate(country = case_when( str_detect(place,&quot;China&quot;) ~ &quot;China&quot;, TRUE ~ &quot;Other countries&quot;)) %&gt;% mutate(location = case_when( place == &quot;Hubei, Mainland China&quot; ~ &quot;Hubei (Wuhan)&quot;, country == &quot;China&quot; ~ &quot;Other China&quot;, # what happens when this line is not commented out? # why is it written this way? # str_detect(place, &quot;ruise&quot;) ~ &quot;Cruise Ship&quot;, TRUE ~ &quot;Elsewhere&quot;)) %&gt;% group_by(location,reportDate) %&gt;% summarize(Confirmed = sum(Confirmed), Deaths = sum(Deaths), Recovered = sum(Recovered)) %&gt;% ungroup() ## `summarise()` has grouped output by &#39;location&#39;. You can override using the `.groups` argument. 8.1.7 an initial plot (Feb 2020) The first plot is simple, including data for only deaths. A caption is added to show the source of the data. Heres what the data looked like last February 11: myCaption &lt;- &quot; Data courtesy JHU/CSSE http://bit.ly/ncvdata&quot; oldData &lt;- coronaDataSimple %&gt;% filter(reportDate &lt; &#39;2020-02-12&#39;) coronaPlot0 &lt;- oldData %&gt;% # filter(reportDate &lt; &#39;2020-02-12&#39;) ggplot(aes(x=reportDate)) + geom_line(aes(y=Deaths, color = location)) + labs(caption = myCaption) coronaPlot0 8.1.8 five weeks later (Mar 2020) About five weeks later our data would stop updating. But the world had already changed: Heres the same graph, through March 21, 2020: coronaPlot0 &lt;- coronaDataSimple %&gt;% ggplot(aes(x=reportDate)) + geom_line(aes(y=Deaths, color = location)) + labs(caption = myCaption) coronaPlot0 Its apparent that the coding for Wuhan versus the rest of China is off for some of the newer data, as one of these increases while the other reaches a plateau. Still, the overall picture is clear. 8.1.9 adding recovered cases (code from Feb, data through Mar 2020) Similar results are obtained if we examine two other populations of interest, recovered and confirmed cases. Here, recovered cases and deaths are included on a single plot, as these are roughly on the same scale. Additional changes to the graph are self-evident. mySubtitle &lt;- paste0( &quot;Recovered cases (solid line) and deaths (dotted) by region through &quot;, firstbaddate) # (month(today())), &quot;/&quot;, # (day(today())),&quot;/&quot;, # (year(today())),&quot;.&quot;) myCaption &lt;- &quot; Data courtesy JHU/CSSE http://bit.ly/ncvdata&quot; coronaPlot1 &lt;- coronaDataSimple %&gt;% ggplot(aes(x=reportDate)) + geom_line(aes(y=Recovered, color = location), linetype = &quot;solid&quot;) + geom_line(aes(y=Deaths, color = location), linetype = &quot;dotted&quot;) + theme(axis.title.y = element_text(angle = 90, vjust = 1,size = 14), legend.position = (c(.2,.8))) + labs(title = &quot;Novel coronavirus&quot;, subtitle = mySubtitle, y = &quot;Cases&quot;, caption = myCaption) coronaPlot1 8.2 plotting confirmed cases (Feb-Mar 2020) Finally, confirmed cases are plotted on a different scale: mySubtitle &lt;- paste0( &quot;Confirmed cases (solid line) through &quot;, firstbaddate) # (month(today())), &quot;/&quot;, # (day(today())),&quot;/&quot;, # (year(today())),&quot;.&quot;) myCaption &lt;- &quot; Data courtesy JHU/CSSE http://bit.ly/ncvdata&quot; coronaPlot1 &lt;- coronaDataSimple %&gt;% ggplot(aes(x=reportDate)) + geom_line(aes(y=Confirmed, color = location), linetype = &quot;solid&quot;) + theme(axis.title.y = element_text(angle = 90, vjust = 1,size = 14), legend.position = (c(.2,.8))) + labs(title = &quot;Novel coronavirus&quot;, subtitle = mySubtitle, y = &quot;Cases&quot;, caption = myCaption) coronaPlot1 8.3 status (Feb 2021) The code above works ok, but the March 2020 data need further cleaning, and no new data have been added in 11 months. In that time, as the pandemic has spread, numerous other resources for tracking COVID have been developed, most of which are far more sophisticated and less cumbersome than the code above. There are new datasets, new R packages, and, perhaps most importantly, new questions that we might consider about variables such as vaccination rates, test-positivity rates, and hospital capacity. 8.3.1 an assignment for Wednesday, please study and be prepared to report on at least one of the following packages for studying COVID data using R: If the last digit of your Z number is 1 or 2, begin with covid19datahub/COVID19: Unified dataset for a better understanding of COVID-19 (github.com) If the last digit of your Z number is 3 or 4, begin with aedobbyn/covid19us: R package for the COVID Tracking Project API providing US COVID-19 data (github.com) If the last digit of your Z number is 5 or 6, begin with RamiKrispin/coronavirus: The coronavirus dataset (github.com) If the last digit of your Z number is 7 or 8, begin with Covid19R/covid19nytimes: Pulls the covid-19 data from the New York Times Public Data Source (github.com) If the last digit of your Z number is 9 or 0, begin with joachim-gassen/tidycovid19: {tidycovid19}: An R Package to Download, Tidy and Visualize Covid-19 Related Data (github.com) Review the GitHub page, especially the README.md section to get a sense of what is available in the data. How current is it (when was the code last modified?). How popular is it (look at the number of watches stars and forks in the upper right hand corner of the webpage)? How buggy is it (look at the issues tab, then look at Open and Closed issues? Is it worth looking further at this library)? If you get this far, consider looking at a second package, Googling R COVID data, and/or installing and playing with a package on your home machine. Well discuss your findings and work together on this in class on Wednesday. 8.4 how to create new knowledge One thing to keep in mind as we move forward is that, despite the fact that hundreds if not thousands of statisticians, data scientists, epidemiologists, policy makers, and journalists have looked at COVID data, there are still unanswered questions. For example, here in Florida, there was recently some controversy as the Governor chose to disperse COVID vaccinations at Publix pharmacies; the controversy arose, in large part, because poorer communities (including those in the Western part of Palm Beach County) appear less likely to have a Publix supermarket. Is it the case that there is a relationship between COVID disease rates and the number of Publix/per capita by city or zip code (Florida data by Zip code is available at https://covid19-usflibrary.hub.arcgis.com/datasets/)??) If so, what would this tell us? In data science, we can often create new knowledge by putting together two data sources which havent been combined before, together with an interesting and potentially important question. "],["on-probability-and-statistics.html", "9 on probability and statistics 9.1 on probability 9.2 the rules of probability 9.3 continuous probability distributions 9.4 the most dangerous equation 9.5 ", " 9 on probability and statistics Last week, we considered (???) and his quartet, and how visualizing data is valuable. This week, we move to a brief discussion of principles of statistics. 9.1 on probability Discrete probability is used to understand the likelihood of categorical events. We can think of initial estimates of probability as subjective or personal. For some events (what is the probability this plane will crash?), an estimate of probability can be drawn from a base rate or relative frequency (e.g., p(this plane will crash) = (number of flights with crashes/ number of flights)). For other events (what is the probability that a US President will resign or be impeached before completing his term of office?), it may be hard to arrive at a suitable base rate. Here, a number of subjective beliefs or principles may be combined to arrive at a subjective or personal probability. In a sense, all probability estimates begin with a personal belief such as this, in part because the choice of the most informative base rate is often not self-evident - in the plane crash example, maybe we should consider a reference group rates such as for this airline etc. (???). The personal origins of probability estimates should become less important as we are exposed to data and revise our estimates in accordance with Bayes theorem. But over the last 45 years, a substantial body of evidence has demonstrates that, under at least some circumstances, we dont make estimates of probability in this way. (This material is discussed in the Thinking and Decision Making/Behavioral Economics class). 9.2 the rules of probability Heres an introduction to the principles of probability. These are presented, with examples and code, in this R markdown document at Harvards datasciencelabs repository: I. For any event A, 0 &lt;= P (A) &lt;= 1 II. Let S be the sample space, or set of all possible outcomes. Then P(S) = 1, and P (not S) = 0. III. If P (A and B) = 0, then P (A or B) = P (A) + P (B). IV. P (A|B) = P (A and B)/ P (B) Principle III applies for mutually exclusive events, such as A = you are here this morning, B = you are at the beach this morning. For mutually exclusive (disjoint, disjunctive) events, the union is the sum of the two events. This is called the addition rule for disjoint events. A different rule applies for events that are mutually independent, such as (A = I toss a coin and it lands on Heads) and (B = it will rain tomorrow). What we mean by independent is that our estimates of the probability of one dont change based on the state of the other - your estimate of the likelihood of rain shouldnt depend on my coin flip. Here, you multiply rather than add: If P (A|B) = P (A), then P (A and B) = P (A) P (B). In words - if the probability of A given B equals the probability of A, then the probability of both A and B equals the probability of A times the probability of B. Ask yourself - are mutually exclusive events independent? Come up with your own examples. This multiplication rule is handy for estimating the probability of an outcome that happens following a chain of independent events, such as the probability that the next eight times I toss a coin it will land on tails every time: P (TTTTTTTT) = P (T) P (T) P (T) P (T) P (T) P (T) P (T) P (T). = .58 = 1/256. Many sets of events are neither disjoint nor independent, so we need more general ways of thinking about pairs of events. For most of us, Venn diagrams are useful to think about combining probabilities. The union or P (A U B) describes the probability that A, B, or both of these will occur. Here, you will use the general addition rule: P (A or B) = P (A) + P (B) - P (A and B) (the probability of A or B is the probability of A plus the probability of B minus the probability of both A and B). For the intersection or P (A  B), we need to consider conditional probabilities. Think of the probability of two events sequentially: First, whats the probability of A? Second, whats the probability of B, given that A has occurred? Multiply these to get the likelihood of A and B: P (A and B) = P (A) P (B|A). Example: The probability of you and your roommate both getting mononucleosis equals the probability of your getting mono times the probability that your roommate gets it, given that you have it also. This is the general multiplication rule. In this abstract example, the order is irrelevant. To estimate the likelihood of A and B, we could as easily take the probability of B, and multiply it by the conditional probability of A given B P (A and B) = P (B) P (A|B). Use the mono example again. What are A and B here? Does it still make sense? When might P (B|A) make more sense than P (A|B)? We are often interested in estimating conditional probabilities, in which case well use the same equation, but solve instead for P (A|B). This leads us back to principle IV: IV. P (A|B) = P (A and B)/ P (B) What is the likelihood of getting in an accident (A), given that one is driving on I-95 (B)? How would you estimate this? If there are fewer accidents on Military Trail, does this mean that you would be safer there? 9.2.1 keeping conditional probabilities straight In general, P (B|A) and P (A|B) are not equivalent. Moores (2000) Basic Practice of Statistics gives the example of P (Police car | Crown Victoria) = .7, and P (Crown Vic | Police car) = .85. Here, one could use a Venn diagram to model this asymmetry. There are several R packages for doing this, including Venn and VennDiagram. Consider exploring these, or at the very least make a rough sketch that can help you answer the following question: &gt; In general, if P (A|B) &lt; P (B|A), what must be true of the relationship of P (A) to P (B)? 9.3 continuous probability distributions We can also use probability with continuous variables such as systolic blood pressure (thats the first one), which has a mean of approximately 120 and a standard deviation of 15. Continuous probability distributions are handy tools for thinking about the meaning of scores, particularly when we express scores in standard deviations from the mean (z scores). More to the point, this way of thinking about probability is widely used in questions of scientific inference, as, for example, in testing hypotheses such that the average systolic blood pressure among a group of people studying at Crux (hence caffeinated) will be significantly greater than that of the population as a whole. This is part of the logic of Null Hypothesis Significance Testing (NHST) - if the result in my Crux sample is sufficiently high, then I say that I have rejected the null hypothesis, and found data which support the hypothesis of interest. 9.4 the most dangerous equation Just as (???) demonstrated that poor data visualizations can be dangerous, leading, for example, to the loss of life in the Challenger disaster, (???) shows that a lack of statistical literacy is also dangerous. He argues that deMoivres equation is the most dangerous equation - this equation (for the standard error) shows that variability decreases with the square root of sample size. Other nominees include the linear regression equation (and, in particular, how coefficients may change or reverse when new variables are added) and regression to the mean. Regarding linear regression, Simpsons paradox shows how the direction of regression coefficients may change when additional variables are added to a model. I would argue that, from the standpoint of psychology, ignorance of regression to the mean was arguably more dangerous than ignorance about the central limit theorem and standard error, in particular because regression effects contribute to an overestimate of the effectiveness of punishment and an under-appreciation of the effectiveness of positive reinforcement as tools for behavior change (???). 9.5 "],["reproducibility-and-the-replication-crisis.html", "10 Reproducibility and the replication crisis 10.1 Answers to the reproducibility crisis 10.2 Answers to the reproducibility crisis III: Pre-registration 10.3 Further readings", " 10 Reproducibility and the replication crisis Probability theory is elegant, and the logic of Null Hypothesis Significance Testing (NHST) is compelling. But philosophers of science have long recognized that this is not really how science works (???). That is, science is not primarily built by the testing and rejecting of null hypotheses. (Think, for example, of how you might describe an experiment for testing the hypothesis that gravity exists, and whether you would ever reject this hypothesis). The problem is a multifaceted one. It arises partly because, in any experiment, there is a large (infinite?) number of conditions which might be invoked to explain-away results of a failure of our work. It arises because we are human, and prone to various biases of decision making and information integration (that is, we are poor Bayesians). It arises, too, because scientific institutions such as journals, funding agencies, and universities are competitive environments which incentivize positive findings. In recent years, the tension between the false ideal of NHST and the real world of science has become increasingly evident. Within psychology, experimental studies have often - even typically - failed to replicate well-known results (Open Science Collaboration 2015). Its not just psychology (???). One of the first important papers to shine light in the area (???) came from medicine; it suggested six contributing factors, which I quote verbatim here: The smaller the studies conducted in a scientific field, the less likely the research findings are to be true. This stems directly from our discussion of the central limit theorem and the instability of results from small samples. The smaller the effect sizes in a scientific field, the less likely the research findings are to be true Well talk about effect size below. The greater the number and the lesser the selection of tested relationships in a scientific field, the less likely the research findings are to be true. (and) The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true. The problem of analytic flexibility leads to p-hacking The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true and The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true. Positive findings rise, and negative ones are ignored. And scientists are human, and subject to incentives. Heres a video which provides some more context for the crisis: . *Video 10.1: On the reproducibility crisis (12 mins) 10.1 Answers to the reproducibility crisis For scientific progress to occur, there needs to be more than a simple rejection of what is wrong with current methods and research programs: Here, as elsewhere, criticism must be constructive to be valuable. There have been a number of solutions proposed to the reproducibility crisis. 10.1.1 Tweak or abandon NHST The first cluster of responses to the reproducibility crisis is concerned with statistics, specifically problems with Null Hypothesis Significance Testing (NHST). These include (a) justifying ones alpha - making it more stringent, for example, for counter-intuitive claims (???), (b) changing the default p value from .05 to .005 (???), and (c) abandoning significance testing altogether (???). (???) goes into some of these issues in more detail and discusses other limitations of significance testing, including the dichotomous, all-or-none silliness of the accept/reject decision. (If you play the NHST game, there is no almost significant, approached significance, highly significant, etc.). (???) argue that the problems are not merely with NHST, but with the whole of data analysis. They maintain that better training in data science - courses like ours, perhaps, are part of the answer. (figure) (???) also argue that threats to reproducible science occur at a number of places in science, not just with the evaluation of hypotheses. munafo2017threats 10.1.2 Keep a log of every step of every analysis in R markdown or Jupyter notebooks A second cluster of responses is concerned with keeping good records. Lets say that you are running a study, say, which looks at the hypothesis of differential variation of females and males in a cognitive measure; your interest is to critically examine the hypothesis discussed by (???) that males show more variability. There have been a lot of studies related to this over the years, so that rather than collect new data you decide that you will work with existing data from several online archives. You find and download spreadsheets from two studies: In the first, gender is coded 1 for male, 2 for female. In the second, gender is coded 1 for female, 2 for male, and 3 for other. There are, in essence, two ways that you can combine the variables into a common format: The first would be to take one of the spreadsheets and do a few find-and-replace commands on the appropriate column of the data. This is quick and easy - but when someone else, or even future you, returns to this data, you will not remember if you have recoded it. The alternative is to keep a record of your work in R markdown. This is more time consuming, and can sometimes be clumsy. But it is virtuous useful and clear - and when you screw up, you will have a full record of what happened. Part of the problem of scientific reproducibility is to keep comprehensive records. This record-keeping and research transparency is at the heart of R markdown documents. 10.2 Answers to the reproducibility crisis III: Pre-registration The third answer to the reproducibility crisis is the most comprehensive; it involves not merely keeping a record of what you have done, but preregistering your work, that is, fully specifying your planned analyses beforehand (???). The author, an economist, outlines his argument in a five-minute video here. For randomized controlled trials, consider socialscienceregistry.org, and for more general use, use the open science framework page. 10.3 Further readings Finally, if you would like to learn more about the reproducibility crisis, there is a collection of papers in Nature here. In this part of the class we will get into the nuts and bolts of R. References "],["literate-programming-with-r-markdown.html", "11 literate programming with R markdown 11.1 scripts are files of code 11.2 projects are directories containing related scripts 11.3 R markdown documents integrate rationale, script, and results 11.4 What to do when you are stuck", " 11 literate programming with R markdown Showing your work, to (future) you as well as others, is a key part of reproducible science. R Markdown documents facilitate this, as they allow you to include comments, code, and results in a single place. But before we consider R markdown, we begin with two more elemental ideas: scripts (R4DS, Chapter 6) and projects (Chapter 8). 11.1 scripts are files of code We begin with R4DS Chapter 6, which shows the R studio interface and encourages you to save your work using scripts, written in the source (editor) window in the upper left quadrant of the default R studio screen. Note the recommendations - for example, include packages (libraries) at the beginning of your code. One more thing - in setting up R studio, consider adjusting the insert spaces for tab setting to something more than 2. This will allow you to more easily see the nested structure of functions, loops, etc. - and will create a modest disincentive against making these nested structures too deep or complex: Fig 11.1: I use 4 spaces here. YMMV. Note, too, the code diagnostics in R. Consider enabling all of these, including the R style diagnostics, to help you keep your code readable: Fig 11.2: Enable code diagnostics 11.1.1 some elements of coding style Good coding is often a combination of several skills ranging from puzzle-solving to communication. I cant claim that these are the elements of coding style (apologies to Strunk &amp; White), but rather that these are merely some of the elements. Good coding is clear and thus commented. You are writing for your future self as well as others, so be explicit about the purpose of each chunk of code. Good coding is concise. When you can write code in 3 lines instead of 30, your code may be more clear and efficient. Take pleasure in writing parsimonious, efficient code. But where efficiency and clarity conflict, choose the latter. Good code should be complete, including all steps from reading the data to producing output. Where appropriate, comment-out (rather than delete) informative errors, again for the future you. Good code may be creative. The coolest solutions are those which pull from and synthesize a number of ideas. Creativity often requires walking away from a problem in order to ultimately arrive at a solution (Wertheimers Productive Thinking). Finally, good code should be considered. Reflect on the impacts of your work - just because you can analyze something doesnt mean that you should. 11.2 projects are directories containing related scripts You will save your work in projects - which isolate your data and scripts into different directories. (See r4ds, Chapter 8). To reinforce the idea that your unit of analysis in R is the project rather than the script, consider associating your Rmd filetype (see next section) with your markdown editor, and only your Rproj filetype with R studio. Soon, it is likely that you will soon be working on R for different things in parallel - for this and another class, for this class and your thesis, or perhaps for two distinct types of analysis within your thesis. When you open up an R project, youll be in the right directory, with the relevant files (and only the relevant files) at your fingertips in the files pane. 11.3 R markdown documents integrate rationale, script, and results R Markdown documents allow you to include comments, scripts, and results in a single place. The basics of R markdown are presented in Chapter 27 of R4DS. I encourage you to use R markdown for nearly everything you do in R. Within R studio, open up a new R markdown document. There are as many as four parts of an R markdown document: A YAML (yet another markdown language) header Text formatted in markdown R code (chunks) surrounded by code fences and, occasionally, inline code There is a handy R Markdown cheat sheet which can give you a sense of what R markdown is about. It describes eight steps, from workflow to publish (and a ninth, learn more). Dont worry about all of the detail here, but do get a sense of how it works. Exercise 11.1: Working in groups, do the exercises in section 27.4.7 of R4DS. Begin with the R markdown file that is included at the beginning of Chapter 27. You can download it here. Study the code, and annotate it so that you have a better sense of how it works. For example, this block loads needed libraries, then takes the _____dataset and ___________ . Play with the graph. Change one or more parameters of it to make it more useful. Again, annotate your changes. 11.4 What to do when you are stuck Google. pay attention to your error messages Ask for help, make your questions clear and reproducible (see R4DS Chapter 1) Take a break, think outside the box and kludge something together if you have to Document your struggle and your cleverness for a future you "],["references.html", "References", " References Healy, Kieran. 2017. Data Visualization for Social Science: A Practical Introduction with R and Ggplot2. https://socviz.co. Open Science Collaboration. 2015. Estimating the Reproducibility of Psychological Science. Science 349 (6251): aac4716aac4716. https://doi.org/10/68c. Peng, Roger D. 2014. R Programming for Data Science. Leanpub. https://leanpub.com/rprogramming. Wainer, Howard, and David Thissen. 1981. Graphical Data Analysis. ETS Research Report Series 1981 (1): 191241. Wickham, Hadley, and Garrett Grolemund. 2016. R for Data Science. "]]
