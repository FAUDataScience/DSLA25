[["index.html", "98-RandomHSGPASAT ", " 98-RandomHSGPASAT Kevin Lanning 2025-01-07 "],["preface.html", "preface the role of the liberal arts in data science some features of the text the book is for you", " preface This work-in-progress will ultimately serve as a textbook for introductory undergraduate courses in data sciences, and is particularly aimed at students in the liberal arts. No prior knowledge of computer programming is presumed, though, ideally, students will have had college algebra (or its equivalent) and an introductory course in statistics, methods, or data analysis. the role of the liberal arts in data science Data science is still a relatively new field of study, and there are multiple approaches to teaching it and to its place in the college curriculum. This book is intended to serve courses such as Introduction to Data Science at the Wilkes Honors College of Florida Atlantic University which, in turn, was initially based on data science classes at the universities of North Carolina, British Columbia, Duke, Maryland, Wisconsin, Stanford, BYU, Harvard, and UC Berkeley. At each of these schools, the Introduction to Data Science is, to my eyes at least, closer to Statistics than to Computer Science. Statistics is itself a broad field, and our approach is aligned with its most applied and pragmatic form. From this perspective, the choice of statistical methods should follow from the data and problem at hand - in other words, statistics should serve the needs of the user rather than dictate them (Loevinger 1957). Pragmatism, in turn, can serve various goals, ranging from maximizing the revenues generated by an online ad to minimizing the carbon footprint of a travel itinerary. Data science for the liberal arts may be seen as a fusion of the pragmatism of data science with social and humanistic concerns; we stand beside programs in Computational Social Science as it has been taught at schools including Chicago, Georgia Tech, UC Santa Barbara, Princeton, UC Berkeley, at Berlin’s Hertie School of Governance, and in Columbia’s School of Journalism. Data science for the liberal arts begins with the person and society rather than with the algorithm and network. In its concern with the liberal arts, it is intended to provide a modest counterbalance to the inherently centripetal, or inequality-accelerating, force of modern information technology.1 some features of the text There are a number of different approaches to teaching data science for the liberal arts. The present text includes several distinguishing features. R In a recent informal survey of introductory data science courses, I saw a pretty even split between those which begin with Python and those which begin with the statistical programming language R. This difference corresponds, very loosely, to the split noted above: Computer science based approaches to data science are frequently grounded in Python, while statistics-based approaches are generally grounded in R. Our course, like those for most of the syllabi and courses linked above, will be based in R. Reproducible science The course will provide an introduction to some of the methods and tools of reproducible science. We will consider the replication crisis in the natural and social sciences, and then consider three distinct approaches which serve as partial solutions to the crisis. The first of these is training in a notebook-based approach to writing analyses, reports and projects (using R markdown documents). The second is using public repositories (such as the Open Science Framework and, to a limited extent, GitHub) to provide snapshots of projects over time. Finally, the third is to consider the place of significance testing in the age of Big Data, and to provide training in the use of descriptive, exploratory techniques of data analysis. Good visualizations As I note in the first chapter, communication is a distinguishing concern of data science for the liberal arts. “Communication” includes not just writing up results, but also designing data displays that incisively convey the key ideas or features in a flood of data. We’ll examine and develop data visualizations such as plots, networks and text clouds. More advanced topics may include maps, interactive displays, and animations. A little data There are plenty of data sources for us to examine, and we’ll consider existing datasets from disciplines ranging from literature to economics to public health, with sizes ranging from a few dozen to millions of data points. We will also clean and create new datasets. A few tools One feature of Data Science is that it is changing rapidly. The tools, methods, data sources, and ethical concerns that face us in 2025 are different from those which shaped the field just one or two years ago. In fields that are undergoing rapid change, there is some trade-off between building expertise with existing (older) tools and trying the newer approaches. Partly because I want to equip you with skills which will not be obsolete, partly because some of these new approaches promise more accessibility, elegance, and/or power, and partly because of my own interest in staying current, we’ll be using some of the latest packages and programs. In the last few years, I’ve shifted the class from the standard R dialect (as I learned it from the Johns Hopkins-Coursera Data Science Specialization) to the Tidyverse, a dialect of R that I find to be relatively clear and concise. A few years ago, I shifted our primary platform from individual laptops to a cloud-based R platform; while this approach has its advantages, I found that these did not outweigh the costs of the approach, so we will go back to the standalone method. We’ll explore different approaches to learning R syntax, including the learnr package, Swirl, and DataCamp. In the past, I’ve recommended using dedicated markdown editors such as Typora and Obsidian. While I still think that these are worth considering for some text-editing and note-taking applications, we’ll do our work instead with the editor in the latest variant of RStudio on our laptops, as this allows WYSIWIG (what you see is what you get) formatting of documents - such as this one - that are intended as “publication-ready” texts. We’ll use, and explore the advantages and disadvantages, of spreadsheets such as Excel or Google Sheets as well. the book is for you It’s my intention that this text should serve every college student, regardless of concentration or college major. The skills and insights that you will gain in this course will help you in graduate and professional schools, will help you in your careers, and will help you in your goal of making a better world. And it will help you train the next generation of data scientists as well. References Loevinger, Jane. 1957. “Objective Tests as Instruments of Psychological Theory.” Psychological Reports 3 (3): 635–94. https://doi.org/b27jpk. Watts, Duncan J. 2004. “The \"New\" Science of Networks.” Annual Review of Sociology 3: 243–70. http://www.jstor.org/stable/29737693. I hope to return to this in a later chapter, but in the meantime consider the discussion of the “Matthew Effect” in sociology and network science (Watts 2004).↩︎ "],["part-i-introduction.html", "PART I: Introduction", " PART I: Introduction "],["what-is-data-science-for-the-liberal-arts.html", "1 what is “data science for the liberal arts?” 1.1 the incompleteness of the data science Venn diagram 1.2 the importance of data science for society 1.3 discussion: what are your objectives in data science?", " 1 what is “data science for the liberal arts?” Hochster, in Hicks and Irizarry (2018), describes two broad types of data scientists: Type A (Analysis) data scientists, whose skills are like those of an applied statistician, and Type B (Building) data scientists, whose skills lie in problem solving or coding, using the skills of the computer scientist. This view arguably omits a critical component of the field, as data science is driven not just by statistics and computer science, but also by “domain expertise:” Fig 1.1 - The iconic data science Venn diagram The iconic Venn diagram model of data science, as shown above, suggests that there are not two but three focal areas in the field, one of which begins not with math or computer science, but with “domain expertise.” Data science for the liberal arts is a ‘Type C’ approach, where ‘C’ refers to a concentration of concern in the arts, humanities, social and/or natural sciences. For the Type C data scientist, coding is in the service of applied problems and concerns. Type C data science does not merely integrate ‘domain expertise’ with statistics and computing, it places content squarely at the center. We can appreciate the compelling logic and power of statistics as well as the elegance of well-written code, but for our purposes these are means to an end. Programming and statistics are tools in the service of social and scientific problems and cultural concerns. Type C data science aims for work which is not merely cool, efficient, or elegant, but responsible and meaningful. At the risk of oversimplifying: Type A data scientists focus on Analysis and questions about ‘how?’ Type B data scientists focus on Building and questions of ‘what?’ Type C data scientists focus on Consideration and questions of ‘why?’, ‘who?’, ‘what for?’, and ‘at what (social) cost?’ 1.1 the incompleteness of the data science Venn diagram Data visualizations are starting points which can provide insights, typically highlighting big truths or effects by obscuring other, presumably smaller ones. The Venn diagram model of data science is no exception: As with other graphs, figures, and maps, it allows us to see by showing only part of the picture. What does it omit? That is, beyond mathematics, computing, and domain expertise, what other skills contribute to the success of the data scientist? 1.1.1 additional domains For the liberal arts data scientist, we can note at least three additional important domains, that is, communication, collaboration, and citizenship. Communication, including writing and the design and display of quantitative data, is central to data science because results are inconsequential unless they are recognized, understood, and built upon. Facets of communication include oral presentations, written texts and good data visualizations. Collaboration is important because problems in data science are sufficiently complex so that any single individual will typically have expertise in some, but not all, facets of the area. Collaboration, even more than statistical and technical sophistication, is arguably the most distinctive feature of contemporary scholarship in the natural and social sciences as well as in the private sector (Isaacson 2014). Citizenship is important because we are humans living in a social world; it includes serving the public good, overcoming the digital divide, furthering social justice, increasing public health, diminishing human suffering, and making the world a more beautiful place. The Type C data scientist is aware of the fact that the world and workforce are undergoing massive change: This puts the classic liberal arts focus of “learning how to learn” (as opposed to memorization) at center stage. Finally, the Type C data scientist is sensitive to the creepiness of living increasingly in a measured, observed world. These real-world goals should be informed by ethical concerns including a respect for the privacy and autonomy of our fellow humans. 1.1.2 an additional dimension Cutting across these various facets (statistics, computing, domain expertise, collaboration, communication, and citizenship), a second dimension can be articulated. No one of us can excel in all of these domains, rather, we might aim towards a hierarchy of goals ranging from literacy (can understand) through proficiency (can get by) to fluency (can practice) to leadership (can create new solutions or methods). That is, we can think of a continuum of knowledge, skills, interests, and goals, ranging from that which characterizes the data consumer to the data citizen to the data science contributor. A Type C data science includes this dimension of ‘depth’ as well. 1.2 the importance of data science for society Communication, collaboration, and citizenship are each associated with the concept of trust. Trust is an important social good because it is associated with both individual well being (Poulin and Haase 2015) and the stability of democratic institutions (Sullivan and Transue 1999). But interpersonal and institutional trust, including trust in science, have declined in recent years (Deane 2024). The decline in trust in science has been exacerbated by the so-called reproducibility (or replication) crisis, in which many scientific results initially characterized as “statistically significant” have been found not to hold up under scrutiny, that is, aren’t reproducible. The reasons for the reproducibility crisis are many and contentious, but there is substantial consensus that one path towards better science involves the public sharing of methods and data. A second path towards better, and more trustworthy, science involves the use of larger datasets: With large datasets, effects are more stable and “statistical significance” is rarely a concern. Other indices, such as measures of accuracy and effect size are typically of primary interest. Data science, with its tools for reproducible analysis and its use of big data sets, can make science more trustworthy and improve the quality of our lives. 1.2.1 training in the liberal arts -&gt; success in tech Data science is at its core empirical, and all of this rhetoric would be meaningless if not grounded in real world findings. Although it was reported in late 2017 that so-called “soft skills” rather than STEM training were the most important predictors of success among Google employees, it’s difficult to know whether these results would generalize to a less select group. Nonetheless, there is a clear need for individuals with well-rounded training in the liberal arts in data science positions and, conversely, learning data science is arguably a key part of a contemporary liberal arts education. 1.2.2 the challenge of TMI The challenges of data science are many, but perhaps the most fundamental is the problem of (literally) TMI. When we compare traditional statistics with modern data science, we realize that the former is typically concerned with making inferences from datasets that are too small, while the latter is concerned with making sense of data that is or are too big (D. Donoho 2017). The challenge of data science is the challenge of “too much information,” or TMI. The challenge of TMI is not new, or restricted to data science. In the 19th Century, Wilhelm Wundt argued that attention was the distinguishing act of the human mind (Blumenthal 1975). That is, in attending to (or focusing on) something, we must overlook everything else, consequently, selection is the essence of human perception (Erdelyi 1974). Selection is important not just in psychology, but in the arts as well, for editing, or choosing what not to write or show, is at the core of the creation of works including novels and film (Ondaatje and Murch 2002). Although the problem of TMI is not new, today it exists at a much greater scale, for there is simply more information around us. Indeed, over the last 20 years, the amount of digital information in the world has increased roughly 200-fold.2 Like perception in psychology and editing in the arts, data science is concerned with extracting meaning from information. Because the amount of information around us has mushroomed and its nature has become more important, our need to extract meaning has become more ubiquitous and more urgent. For these reasons, data science is a foundational discipline in 21st century inquiry. 1.3 discussion: what are your objectives in data science? References Blumenthal, Arthur L. 1975. “A Reappraisal of Wilhelm Wundt.” American Psychologist 30 (11): 1081. https://psycnet.apa.org/record/1976-23157-001. Deane, Claudia. 2024. “Americans’ Deepening Mistrust of Institutions.” https://pew.org/40eFLof. Donoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734. Erdelyi, Matthew H. 1974. “A New Look at the New Look: Perceptual Defense and Vigilance.” Psychological Review 81 (1): 1–25. https://doi.org/cs5c5q. Hicks, Stephanie C., and Rafael A. Irizarry. 2018. “A Guide to Teaching Data Science.” The American Statistician 72 (4): 382–91. https://doi.org/gfr5tf. Isaacson, Walter. 2014. The Innovators: How a Group of Hackers, Geniuses, and Geeks Created the Digital Revolution. Simon; Schuster. Ondaatje, Michael, and Walter Murch. 2002. The Conversations: Walter Murch and the Art of Editing Film. A&amp;C Black. https://books.google.com/books?hl=en&amp;lr=&amp;id=VRS6S1OXdwUC&amp;oi=fnd&amp;pg=PP2&amp;dq=ondaatje+murch&amp;ots=MG0Jx7ktbZ&amp;sig=okJy-Tznz_XVndiYIFh9vUN3Hhc. Poulin, Michael J., and Claudia M. Haase. 2015. “Growing to Trust: Evidence That Trust Increases and Sustains Well-Being Across the Life Span.” Social Psychological and Personality Science 6 (6): 614–21. https://doi.org/10.1177/1948550615574301. Reinsel, David, John Gantz, and John Rydning. 2025. “The Digitization of the World.” Sullivan, J. L., and J. E. Transue. 1999. “THE PSYCHOLOGICAL UNDERPINNINGS OF DEMOCRACY: A Selective Review of Research on Political Tolerance, Interpersonal Trust, and Social Capital.” Annual Review of Psychology 50 (1): 625–50. https://doi.org/cmthvk. According to Wikipedia, the Zettabyte (ZB) Era began in 2012, when the amount of digital information in the world first exceeded 1 ZB (or 1021 bytes). In 2025, it is estimated that the world will house 175 ZBs of digital data (Reinsel, Gantz, and Rydning 2025), hence a 175X increase in in 13 years. My estimate of a 200X increase in 20 years is a conservative extrapolation from these numbers. Incidentally, one ZB = 1,000,000,000,000,000,000,000 bytes, which could be stored on roughly 250 billion DVDs, or 500 million 2 TB hard drives.↩︎ "],["getting-started.html", "2 getting started 2.1 are you already a programmer and statistician? 2.2 spreadsheets - some best practices 2.3 setting up your machine: some basic tools 2.4 a (modified) 15-minute rule 2.5 installing R and RStudio desktop", " 2 getting started We begin with a brief self-assessment, asking you to reflect on your own knowledge of data science, including the necessary-but-not-sufficient areas of computer programming and statistics. We then move to a description of some rudimentary tools that we will be using, then install the R programming environment on our laptops. 2.1 are you already a programmer and statistician? Regarding programming, you may know more than you think you do. Here’s a simple program - a set of instructions - for producing a cup of coffee: add water to the kettle and turn it on if it’s morning, put regular coffee in the French press, otherwise use decaf if the water has boiled, add it to the French press, else keep waiting if the coffee has steeped for four minutes, depress (smash) piston/plunger, else keep waiting pour coffee into cup enjoy As a post-millennial student from a WEIRD culture, or Western, Educated, Industrialized, Rich Democracy (Henrich, Heine, and Norenzayan 2010), you’ve ‘programmed’ computers, too, if only to enter a password, open an app, and upload a photo on your cell phone. Statistics is of fundamental importance, not just for understanding abstract trends, but for making decisions about everyday life. Consider the case of a senior undergraduate who wants to go to med school. How many schools should she apply to? Exercise 2_1 Susie is applying to two med schools. At School A, 25% of students are accepted, and at School B, 25% are accepted as well. You are Susie. Are you going to get in to at least one of these programs? What is the probability? Does your estimate depend upon any assumptions? Questions such as these are important for us. If the combined probability is low, it will likely (another probability concept) make sense for Susie to spend the time, money, energy, and ego-involvement to apply to additional programs. If the probability is higher, it may not. But problems like this are hard - our estimates of probability are frequently poorly calibrated, and combining probability estimates is challenging. See, e.g., Tversky and Kahneman (1974), and consider taking a course in Behavioral Economics or Thinking and Decision Making to learn more. 2.2 spreadsheets - some best practices Spreadsheets are handy tools, particularly for smaller datasets. You may have worked with data in spreadsheets such as Microsoft Excel or Google Sheets. If you haven’t here’s a start: Exercise 2_2 Open the Google Sheet at http://bit.ly/dslaX2_1. Save a copy and edit it, entering the following in cell B7: =SUM (B2:B6) What is the result? Now copy cell B7 to C7 What happens? Is this the result you expected? Spreadsheets are great tools - the first one, VisiCalc, was the first “killer app” to usher in the personal computer revolution. But spreadsheets have limitations as well. Best practices have been proposed for using spreadsheets in data science such as, for example, including only data (and not calculations) in spreadsheets, using what we will recognize as a ‘tidy’ format in which data are in a simple rectangle (that is, avoid combining cells and using multi-line headers), and saving spreadsheets as simple text files, typically in comma-delimited or CSV format (Broman and Woo 2018). Typically, this means that each worksheet will be in a separate file, rather than in workbooks with multiple tabs. There are good reasons for these recommendations: When we manipulate data in spreadsheets, we typically will not have a record of what was (and wasn’t) changed, and this compromises the reproducibility of our work. Similarly, when we sort data in spreadsheets, we risk chaos, for example, if we sort only certain columns, the integrity of spreadsheet-rows will be lost. In general, spreadsheets should generally be used to store data rather than to analyze it. But don’t be a slave to this - if you are making some quick calculations that are inconsequential and/or to be used on just one occasion, working in Excel or Google Sheets is often the way to go. 2.3 setting up your machine: some basic tools Documents produced in word processors typically include invisible characters used for formatting, security, etc. These characters, when fed into a computer program, lead to unpredictable results. Markdown (MD) is a simple syntax for producing documents that are easily read by both humans and machines. Most of the documents you will produce and work with this term, including the present chapter, are and will be written in some version of Markdown. Three markdown editors are worth considering. One of the newest is Obsidian, which is free and includes an extended version of Markdown that allows the drawing of simple network graphs. Typora has an attractive minimalist interface, but is no longer free ($15). Finally, RStudio - the environment within which we will be using R - has a built in visual markdown editor as well. Collaboration and communication are integral to data science. In the world beyond universities, the most important messaging and collaboration platform is Slack. Slack is a commercial app, but it has a free tier. Slack is handy for group work of all forms, but most universities use collaborative tools embedded in Learning Management Systems such as Canvas instead. While Microsoft Word has the advantages of familiarity, ease-of-use offline, and extensive formatting capabilities, Google Docs has several advantages over Word. Google Docs is free, it is convenient for collaborative work (as it allows simultaneous editing), and it provides a solid framework for version control, a critical skill in information management. Version control can help you avoid the chaos and confusion of having a computer (or several computers) full of files that look like Cham’s (2012) comic: Fig 2.1: Never call anything ‘final.doc’. Version control is an important concept in data science. Collaboratively built programs and platforms, including most of the add-ons (libraries, packages) which make R so powerful, are open-source projects built by many individuals over time. For projects such as these, both the current build and its associated history are typically maintained on GitHub, a website for hosting code. When we contribute to these projects, we will first mirror the web-based GitHub site using a program on our own Macs or Windows PCs called Git, then upload our proposed changes. Keeping remote and local branches of files in sync can be challenging, however, and you will not be expected to use this technology in this class. But if you are curious, or want to learn more, an introduction to using Git and R together may be found here. 2.4 a (modified) 15-minute rule While AI tools for coding, including Microsoft Copilot, are helpful for addressing the idiosyncacies of coding syntax, at some point, you will run into problems - if you don’t you aren’t learning enough. An important determinant of your success will be the balance you maintain between persistence and help-seeking. The 15-minute rule is one guideline for this balance: It has been cleverly summarized as “You must try, and then you must ask.” That is, if you get stuck, keep trying for 15 minutes, then reach out to others. I think that this rule is basically sound, particularly if it is applied with cognitive flexibility, social sensitivity, and reciprocity. So when you get stuck, make a note of the problem, then move to another part of your project (that’s the cognitive flexibility part): This allows your problem to percolate and still make progress. When you ask others for help, ask in a way that shows an awareness of the demands on their time (social sensitivity): Part of this means that you should explain your problem in as detailed a fashion as possible - in technical terms, a “reprex” or reproducible example. Finally, you should be willing to provide as well as give help (reciprocity). 2.5 installing R and RStudio desktop Finally, if you have not already done so, install the latest version of R on your own Windows or Mac laptop. Then install RStudio Desktop. (RStudio is our interface, the environment we will use to write, test, and run R code). References Broman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989. Henrich, Joseph, Steven J. Heine, and Ara Norenzayan. 2010. “The Weirdest People in the World?” Behavioral and Brain Sciences 33 (2-3): 61–83. https://doi.org/c9j35b. Tversky, Amos, and Daniel Kahneman. 1974. “Judgment Under Uncertainty: Heuristics and Biases.” Science 185 (4157): 1124–31. https://doi.org/gwh. "],["what-r-stands-for.html", "3 what R stands for … 3.1 some key characteristics of R 3.2 cha-cha-cha-changes 3.3 some technical characteristics 3.4 finding help 3.5 Wickham and R for Data Science 2e", " 3 what R stands for … R was initially developed by Ross Ihaka and Robert Gentleman as a tool to help teach university-level statistics at the University of Auckland. At one level, the name ‘R’ simply stands for the first initial of these two founders (Hornik and Team 2022). But, just as we noted that the ‘C’ in Type C Data Analysis stands for concepts such as concentration, communication, collaboration, the ‘R’ in our programming language means much more: R is a system for reproducible analysis, and reproducibility is essential. When we write R code, we’ll use R markdown documents. An R markdown document can include text (comments or explanations), ‘chunks’ of code, and output including graphs and tables. Having explanations, code, and results in a single document facilitates reproducible work. (Jupyter notebooks in the Python world are similar in this respect). R is for research. Research is not just an end-product, not just a published paper or book: … these documents are not the research [rather] these documents are the “advertising”. The research is the “full software environment, code, and data that produced the results” (Buckheit and Donoho 1995; D. L. Donoho 2010, 385). Published works (including theses as well as books, scholarly papers, and business reports) are summaries; R markdown documents are the raw materials from which these are derived. When we consider only summaries (or the ‘advertising’), we make it difficult for others to verify, or build upon, the findings by reproducing them (Gandrud 2013). R is a system for representing data in cool, insight-facilitating ways, a tool for creating (reproducible) data visualizations which can provide insights and communicate results. The power of R to make clear, honest, and reproducible data visualizations is widely seen as a major strength of the language. R is really popular, and this matters, because learning R will make you a more attractive candidate for many graduate programs as well as jobs in the private sector. Because R is popular, there are many resources, including, for example - Online resources include the simple (and less simple) lessons of SwirlR, which offers the possibility of “learning R in R,” as well as DataCamp, the Data Science Certificate Program at Johns Hopkins, and other MOOCs. Books include (Peng 2014) - which includes not only videos of his lectures in the program at Hopkins, but also a brief list of still more resources - and (Wickham, Çetinkaya-Rundel, and Grolemund 2023) You’ll also learn (more directly) from people, including your classmates, as well as the broader community of people around the world. There are hundreds if not thousands of people, young and old, who are on the road with you. I am as well, just a step or two (hopefully) ahead. R might stand for relatively high level. Programming languages can be described along a continuum from high to low level, the former (like R) are more accessible to humans, the latter (like assembly language) more accessible to machines. Python, Java, and C++ are all more towards the middle of this continuum. R does not stand for ‘arggh,’ although you may proclaim this in frustration (‘arggh, why can’t I get this to work?) or, perhaps, in satisfaction (’arggh, matey, that be a clever way of doing this’).3 But R does stand for rewarding. A language is a way of thinking about the world, and this is true for computer languages as well. You’ll be challenged by its complexity, its idiosyncracy, its alien logic. But you will succeed, and you will find that you can do things that you did not believe possible. 3.1 some key characteristics of R 3.1.1 base R and packages R is a programming language. It can be seen as including two parts, a simple core (Base R) and a large number of additional packages. These packages (libraries) are customized add-ons which simplify certain tasks, such as text analysis. There are, at this writing, 21,861 available packages on the CRAN package repository (as well as additional useful packages that, for one reason or another, do not appear on CRAN. Packages on CRAN are partially indexed by “task view pages.” The task view page for natural language processing or text analysis includes, at this writing, over 60 separate packages. So how do you choose, and where do you begin? For our purposes, we will start with the curated list of packages which jointly comprise the tidyverse (Wickham et al. 2019), which is effectively a dialect of R. To download the tidyverse package from the ‘net, open RStudio, find the ’console’ window on the left side of your screen, and enter the command followed by &lt;enter&gt; or &lt;return&gt; install.packages(“tidyverse”) 3.2 cha-cha-cha-changes R is constantly changing, not just in the proliferation of packages, but also in the organization of the R community. While R is free and open source, RStudio is a commercial product. The company (and website) that develops the RStudio IDE is undergoing a name change (from RStudio to Posit). This is motivated, in part, by the need to make the RStudio platform more welcoming for other languages including Python. Similarly, the R markdown programming language is slowly being replaced with newer, and ultimately more capable, software called Quarto. Quarto is back-compatible with R markdown, but can be used with other languages including Python as well. A description of the differences between R markdown and Quarto may be found here. For our purposes, you can treat Quarto files (.qmd suffix) as R markdown files (.rmd), and vice-versa. One more change: Posit (the company) is developing a new IDE called ‘Positron.’ Positron may ultimately be a more useful environment for data science than the RStudio IDE, but it is in the beta testing stage at this writing. The RStudio environment, and the Rmarkdown documents that are produced within it, will continue to be available, and widely used, for the foreseeable future. 3.3 some technical characteristics R is an object-oriented language - one conceptually organized around objects and data rather than actions and logic. In R, at the most basic or atomic level, “objects” include characters, real numbers, integers, complex numbers, and logicals. These atomic objects may be combined into vectors, which generally include objects of the same type [one kind of object, ‘lists,’ is an exception to this; Peng (2014)]. Vectors can be further combined into data frames, which are two-dimensional tables or arrays. A tibble is a particular type of data frame which is used in the tidyverse. Tibbles are, in some ways, handier to work with than other data frames. We’ll be working extensively with data frames in general, and tibbles in particular, as we move forward. Objects have attributes. Attributes of R include such things as name, dimensions (for vectors and arrays), class (that’s the type of object described in the previous paragraph), length, etc. Real world data sets are messy, and frequently have missing values. In R, missing values may be represented by NA (not available) or NaN (not a number, implying an undefined or impossible value). 3.4 finding help One does not simply ‘learn R.’ Unlike, say, learning to ride a bicycle, fry an egg, or drive a car with a manual transmission, learning R is not a discrete accomplishment that one can be said to have mastered and from which one then moves on. Rather, R is an evolving, open system of applications and tools which is so vast that there is always more that one can achieve, new lessons that one can learn. And, the complexity of R syntax is such that, for almost all of us, we will need help for coding on any non-trivial task. For us, the key ideas in “looking for help” will include not just the tools on the RStudio IDE, but also (a) using Google searches wisely, (b) judicious use of AI assistance, and (c) reaching out to your classmates and instructor. Here, as in the real world, there is an etiquette for help-seeking which is based on consideration. Your search for help should begin by making sure that others will encounter the same result, then by stripping the problem down to its essence. Once you have reduced the problem to this minimal, reproducible essence, you will often be able to spot the problem yourself - and, if not, you will make it easier for others to help you. There is an R package (reprex) which will likely facilitate this, but I haven’t tried it yet. Here is a good introduction. Finally, to get a sense of the power and versatility of R Markdown documents, you might explore this tutorial. Note that, if you want to work interactively with the tutorial, you will need to first sign up for an account on RStudio cloud. Go to posit.cloud, click on “learn more” in the “Free” column, then sign up. When you encounter obstacles, remember the 15-minute rule. 3.5 Wickham and R for Data Science 2e The introduction to the Wickham text (Wickham, Çetinkaya-Rundel, and Grolemund 2023) provides both a framework for his approach and a brief introduction to the tidyverse. Please read it now. References Buckheit, Jonathan B., and David L. Donoho. 1995. “Wavelab and Reproducible Research.” In Wavelets and Statistics, 55–81. Springer. Donoho, David L. 2010. “An Invitation to Reproducible Computational Research.” Biostatistics 11 (3): 385–88. https://doi.org/bxwkns. Gandrud, Christopher. 2013. Reproducible Research with R and R Studio. CRC Press. Hornik, Kurt, and The R Core Team. 2022. “R FAQ: The Comprehensive R Archive Network.” Peng, Roger D. 2014. R Programming for Data Science. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science 2e. \" O’Reilly Media, Inc.\". https://books.google.com/books?hl=en&amp;lr=&amp;id=TiLEEAAAQBAJ&amp;oi=fnd&amp;pg=PT9&amp;dq=r4ds+2e&amp;ots=ZJr-ihzQuM&amp;sig=NKto0ViShGg7yiv76WF2sHejD80. Actually, pirates have little use for R, as pirates love the C (programming language).↩︎ "],["exploring-r-world.html", "4 exploring R world 4.1 go to the movies 4.2 go into the clouds 4.3 open the box 4.4 go to (data)camp 4.5 learn to knit 4.6 older approaches", " 4 exploring R world There are many sources for learning the basics of R. A few of these follow. Please spend at least 180 mins exploring at least two of the following. Be prepared to discuss your progress next class: You will be asked which source(s) you used, what you struggled with, what questions you have, and what you would recommend to your classmates. Hint: If you find the material too challenging, remember the 15 minute rule, take a break away from your machine and other screens, clear your head, then try a different approach. 4.1 go to the movies About seven years ago, Iain Carmichael used data from the Internet Movies Database (IMDB) to introduce R. You can see his introduction here. You can consider his report from the standpoint of style (formatting, organization), coding (how he did this), data (the part of the IMDB data he is looking at), and his results (plots of distributions and relationships). Do you have any questions about the movie data? How might you ask these? A minimal amount of sleuthing - a click on the STOR 390 link at the top of the page, then a quick scroll - reveals that “all of [Carmichael’s] course material is on the github repo” - or repository. Can you find the Rmd document that generated his work? Can you download it on to your machine? If you try to run it, what happens? If you were working on your thesis and came across a problem like this, what would you do next? 4.2 go into the clouds In addition to the desktop version of R (and Rstudio) we will be using in this class, there is a cloud-based environment as well. As mentioned in the last chapter, you can sign up for a starter account at posit.cloud. When you open posit.cloud, you should see a column on the left of the screen that includes four sections - spaces, learn, help, and info. (If you don’t see these, click on the hamburger menu in the upper left corner and it will appear). Browse through the recipes tab, particularly the ones in the left most column, to start to get a sense of how you might solve some common challenges in R. 4.3 open the box Go to datasciencebox/content. Click on the “Hello world” link; this will take you to the beginning of Mine Çetinkaya-Rundel’s introductory lessons on R. These include slides, the source code for the slides (these are written in R markdown), and videos of her lectures, including the one we watched on the first day of class. Midway down this page, you’ll find a link to “the RStudio Cloud workspace for Data Science Course in a Box project.” Open it up and begin to explore the code and data behind her presentation. 4.4 go to (data)camp Many folks swear by (and others, I presume, at) DataCamp, which kind of gamifies learning software. As a student in this class, you have access to all of their stuff… free. You can even do lessons on your phone. Use the link given to you in class to enroll, then explore the introductory-level classes in R at https://www.datacamp.com/category/r 4.5 learn to knit In literate programming, comments, code, and results are integrated in a clear and reproducible way - they document our work. ‘Markdown’ is a simple language for adding formatting to text. ‘R’ is a statistical language. ‘R Markdown’ is a variant of R that you can use to produce or publish complex documents like this one, as well as the Carmichael page described above. To create an R markdown (Rmd) document, open up Rstudio, click on (File -&gt; New File -&gt; R Markdown). A window will open up with a file that begins with a block of YAML (Yet Another Markdown Language). You can edit this as needed: --- title: &quot;Here&#39;s an R Markdown Document&quot; author: &quot;Frankie McFrank Frank&quot; date: &quot;1/12/2025&quot; output: html_document --- Go ahead and click on the clever “knit” icon in the bar just above the source window to create a sample document. You’ll need to save the file with a new name, then R will finally create an HTML (Hyper Text Markdown Language ) page. Compare the R Markdown document (your code) with the result (the HTML). The second chapter of Healy’s online book about Data visualizations provides a more thorough explanation of R Markdown as well as an introduction to R and R studio which largely parallels the discussions here and in the Wickham and Grolemund text (Healy 2017; Wickham, Çetinkaya-Rundel, and Grolemund 2023). We’ll be discussing R Markdown (and its cousin Quarto) in Chapter 6. 4.6 older approaches I, like thousands of others, learned R in the process of completing the Johns Hopkins Data Science Specialization offered through Coursera. The sequence can be challenging, but their introduction to R used an accessible, interactive R package called Swirl. You can read about swirl (“learn R in R”) at https://swirlstats.com/. 4.6.1 using Swirl After loading R (and opening R studio), you will get to the Swirl lessons with the following steps: Install the Swirl package on your computer (you only need to do this once). Type the following into your console window in R studio (typically left hand side of your screen or lower left) install.packages(“swirl”) Then load the package into your workspace (you’ll need to do this at the beginning of every session you use Swirl) library (swirl) Then run it! swirl () Swirl will ask a few questions then give you the option of choosing one of several courses. You’ll choose the R Programming option, which leads to 15 separate lessons. At the end of each lesson, you’ll be asked Would you like to receive credit for completing this course on Coursera.org? Answer no… then do another lesson. 4.6.2 reading/watching Roger Peng’s text and/or videos Finally, you might consider the text and videos from the Coursera R class. Most of the material from that class can be found in Roger Peng’s (2014) text, a slightly updated version of which can be found here. The videos in the series may be found in this playlist. Here’s an introduction: . Video 4.1: Roger Peng introducing R References Healy, Kieran. 2017. “Data Visualization for Social Science: A Practical Introduction with r and Ggplot2.” https://socviz.co. Peng, Roger D. 2014. R Programming for Data Science. Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science 2e. \" O’Reilly Media, Inc.\". https://books.google.com/books?hl=en&amp;lr=&amp;id=TiLEEAAAQBAJ&amp;oi=fnd&amp;pg=PT9&amp;dq=r4ds+2e&amp;ots=ZJr-ihzQuM&amp;sig=NKto0ViShGg7yiv76WF2sHejD80. "],["part-ii-towards-data-literacy.html", "Part II: Towards data literacy", " Part II: Towards data literacy "],["now-draw-the-rest-of-the-owl.html", "5 now draw the rest of the owl 5.1 time for hands-on experience?", " 5 now draw the rest of the owl Fig 5.1: Draw the rest of the owl. In the prior chapter, you explored several different sources for learning how to code in R. Now it’s time to explore other approaches. Take a break from reading, and spend some time coding and consolidating, reviewing tutorials, or playing with data. 5.1 time for hands-on experience? If you want to work actively with a dataset, here are two possibilities that you might consider. Each has been supplied as its own package. 5.1.1 begin by loading the tidyverse The tidyverse allows the use of the ‘pipe’ operator, (“%&gt;%”), which is useful for combining commands. Remember that any package needs to be installed on your machine once before progressing. That is, if you installed the tidyverse previously, you don’t need to do the first line here. If you haven’t installed the tidyverse, you should remove the octothorpe or pound sign (#) before running this next chunk: # install.packages(&quot;tidyverse&quot;) library(tidyverse) 5.1.2 maybe explore the babynames package The babynames dataset is described here. What is in the data? What interesting questions might you ask about the dataset? # install.packages(&quot;babynames&quot;) library(babynames) data(babynames) str(babynames) ## tibble [1,924,665 × 5] (S3: tbl_df/tbl/data.frame) ## $ year: num [1:1924665] 1880 1880 1880 1880 1880 1880 1880 1880 1880 1880 ... ## $ sex : chr [1:1924665] &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; ... ## $ name: chr [1:1924665] &quot;Mary&quot; &quot;Anna&quot; &quot;Emma&quot; &quot;Elizabeth&quot; ... ## $ n : int [1:1924665] 7065 2604 2003 1939 1746 1578 1472 1414 1320 1288 ... ## $ prop: num [1:1924665] 0.0724 0.0267 0.0205 0.0199 0.0179 ... babynames %&gt;% slice_sample(n = 5) ## # A tibble: 5 × 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1913 M Bertie 10 0.0000186 ## 2 2017 M Tillman 31 0.0000158 ## 3 1969 F Tela 5 0.00000284 ## 4 1929 F Carnie 5 0.00000432 ## 5 1944 F Garlene 6 0.00000439 5.1.3 or the (ggplot2)movies package The index page for the movies dataset is here. # install.packages(&quot;ggplot2movies&quot;) library(ggplot2movies) data(movies) str(movies) ## tibble [58,788 × 24] (S3: tbl_df/tbl/data.frame) ## $ title : chr [1:58788] &quot;$&quot; &quot;$1000 a Touchdown&quot; &quot;$21 a Day Once a Month&quot; &quot;$40,000&quot; ... ## $ year : int [1:58788] 1971 1939 1941 1996 1975 2000 2002 2002 1987 1917 ... ## $ length : int [1:58788] 121 71 7 70 71 91 93 25 97 61 ... ## $ budget : int [1:58788] NA NA NA NA NA NA NA NA NA NA ... ## $ rating : num [1:58788] 6.4 6 8.2 8.2 3.4 4.3 5.3 6.7 6.6 6 ... ## $ votes : int [1:58788] 348 20 5 6 17 45 200 24 18 51 ... ## $ r1 : num [1:58788] 4.5 0 0 14.5 24.5 4.5 4.5 4.5 4.5 4.5 ... ## $ r2 : num [1:58788] 4.5 14.5 0 0 4.5 4.5 0 4.5 4.5 0 ... ## $ r3 : num [1:58788] 4.5 4.5 0 0 0 4.5 4.5 4.5 4.5 4.5 ... ## $ r4 : num [1:58788] 4.5 24.5 0 0 14.5 14.5 4.5 4.5 0 4.5 ... ## $ r5 : num [1:58788] 14.5 14.5 0 0 14.5 14.5 24.5 4.5 0 4.5 ... ## $ r6 : num [1:58788] 24.5 14.5 24.5 0 4.5 14.5 24.5 14.5 0 44.5 ... ## $ r7 : num [1:58788] 24.5 14.5 0 0 0 4.5 14.5 14.5 34.5 14.5 ... ## $ r8 : num [1:58788] 14.5 4.5 44.5 0 0 4.5 4.5 14.5 14.5 4.5 ... ## $ r9 : num [1:58788] 4.5 4.5 24.5 34.5 0 14.5 4.5 4.5 4.5 4.5 ... ## $ r10 : num [1:58788] 4.5 14.5 24.5 45.5 24.5 14.5 14.5 14.5 24.5 4.5 ... ## $ mpaa : chr [1:58788] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ Action : int [1:58788] 0 0 0 0 0 0 1 0 0 0 ... ## $ Animation : int [1:58788] 0 0 1 0 0 0 0 0 0 0 ... ## $ Comedy : int [1:58788] 1 1 0 1 0 0 0 0 0 0 ... ## $ Drama : int [1:58788] 1 0 0 0 0 1 1 0 1 0 ... ## $ Documentary: int [1:58788] 0 0 0 0 0 0 0 1 0 0 ... ## $ Romance : int [1:58788] 0 0 0 0 0 0 0 0 0 0 ... ## $ Short : int [1:58788] 0 0 1 0 0 0 0 1 0 0 ... movies %&gt;% slice_sample(n = 5) ## # A tibble: 5 × 24 ## title year length budget rating votes r1 r2 r3 r4 r5 r6 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bamboo P… 1954 79 NA 7 14 0 0 4.5 14.5 4.5 4.5 ## 2 Golden B… 1939 99 NA 6.5 145 4.5 4.5 4.5 4.5 14.5 14.5 ## 3 Communio… 1977 105 NA 7.1 8 0 0 0 14.5 0 24.5 ## 4 Tyven, t… 2002 95 NA 6.9 78 24.5 4.5 4.5 4.5 4.5 4.5 ## 5 Synapse 2001 11 NA 6.6 8 0 0 0 0 34.5 24.5 ## # ℹ 12 more variables: r7 &lt;dbl&gt;, r8 &lt;dbl&gt;, r9 &lt;dbl&gt;, r10 &lt;dbl&gt;, mpaa &lt;chr&gt;, ## # Action &lt;int&gt;, Animation &lt;int&gt;, Comedy &lt;int&gt;, Drama &lt;int&gt;, ## # Documentary &lt;int&gt;, Romance &lt;int&gt;, Short &lt;int&gt; Regardless of whether you have played with one or both of these datasets, worked with the tutorials, or something else, please be prepared to share your experiences with the class at our next meeting. "],["literate-programming.html", "6 literate programming 6.1 projects are directories 6.2 scripts are files of code 6.3 R markdown / Quarto documents combine scripts with comments and (once knit) results 6.4 some elements of coding style 6.5 What to do when you are stuck", " 6 literate programming Showing your work, to (future) you as well as to others, is a key part of reproducible science. R Markdown documents facilitate this, as they allow you to include comments, code, and results in a single place. But before we consider R markdown, we begin with two more elemental ideas: projects and scripts (R4DS, Chapter 6) . 6.1 projects are directories You should save your work in projects. These isolate your data and scripts into discrete directories. There are two reasons I begin with ‘projects:’ The first is that students who are new to coding will often struggle to find their datasets and code on their personal machines; having a project directory makes things easier. The second is that , down the road, it’s likely that you will be working on R for different things in parallel - for this and another class, for this class and your thesis, or perhaps for two distinct types of analysis within your thesis. Fig 6.1: Left panel: Files pane in RStudio for this manuscript. Right panel: Menu showing some other recent projects. When you open up an R project, you’ll be in the right directory, with the relevant files (and only the relevant files) at your fingertips in the files pane. It is yet another way in which the notion of ‘tidiness’ facilitates our work. 6.2 scripts are files of code To do simple exercises in R, you can enter code directly in the Console pane (the default is in the lower left of the RStudio screen), then get an instant response. This (interactive) approach to coding is quick, but it is difficult to recreate. For example, imagine that I were doing an analysis between age and a personality trait that, in one dataset, is referred to as ‘Neuroticism’ (N) and, in a second, the same trait is reverse scored as ‘Emotional Stability’ (ES). If I want to combine measures of N and ES from two different datasets, each of which has scores for the trait on a 1-7 (or 7-1) scale, I could reverse one of these. My code might look like this: 6.2.0.0.1 combine two small datasets, reverse one of them, print first and last few rows: library(tidyverse) file1 &lt;- read_csv(&quot;data/datawithN.csv&quot;) ## Rows: 10 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): gender ## dbl (3): id, age, N ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. glimpse (file1) ## Rows: 10 ## Columns: 4 ## $ id &lt;dbl&gt; 101, 102, 103, 104, 105, 106, 107, 108, 109, 110 ## $ age &lt;dbl&gt; 23, 55, 31, 19, 28, 24, 45, 32, 23, 44 ## $ N &lt;dbl&gt; 6, 4, 5, 3, 2, 7, 1, 1, 1, 1 ## $ gender &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, NA, &quot;M&quot;, &quot;F&quot; file2 &lt;- read_csv(&quot;data/datawithES.csv&quot;) ## Rows: 10 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): gender ## dbl (3): id, age, ES ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. glimpse (file2) ## Rows: 10 ## Columns: 4 ## $ id &lt;dbl&gt; 201, 202, 203, 204, 205, 206, 207, 208, 209, 210 ## $ age &lt;dbl&gt; 19, 41, 27, 27, 48, 21, 20, 26, 42, 37 ## $ ES &lt;dbl&gt; 3, 6, 2, 5, 4, 1, 1, 1, 7, 7 ## $ gender &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;M&quot;, &quot;F&quot; combinedfile &lt;- file1 %&gt;% mutate(ES = 8 - N) %&gt;% # Creates &#39;ES&#39; from &#39;N&#39; for file1 select (-N) %&gt;% bind_rows(file2) combinedfile %&gt;% slice(c(1:3,18:20)) # the &#39;c&#39; is for combine or concatenate ## # A tibble: 6 × 4 ## id age gender ES ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 101 23 M 2 ## 2 102 55 M 4 ## 3 103 31 F 3 ## 4 208 26 f 1 ## 5 209 42 M 7 ## 6 210 37 F 7 You’ll want a record of your code for even simple transformations such as this one. R4DS Chapter 6 shows the R studio interface and encourages you to save your work in scripts. These are written in the source (editor) window in the upper left quadrant of the default R studio screen. 6.3 R markdown / Quarto documents combine scripts with comments and (once knit) results The objectives described in the prior section lead naturally to a consideration of R Markdown documents, which allow you to include comments, scripts, and results in a single place. In R4DS, Wickham [@-wickham2023] describes the use of Quarto rather than R markdown. Regardless of whether you use Quarto (see Chapter 28 of R4DS, or the tutorial here) or R Markdown (see the tutorial here), I encourage you to use one of these powerful, organizing approaches for nearly everything you do in R. There are as many as four parts of an R markdown or Quarto document: A YAML (yet another markdown language) header or metadata Text formatted in markdown R code (chunks) surrounded by code fences and, occasionally, inline code 6.4 some elements of coding style Good coding is often a combination of several skills ranging from puzzle-solving to communication. I can’t claim that these are the elements of coding style (apologies to Strunk &amp; White), but rather that these are merely some of the elements. Good coding is clear and thus commented. You are writing for your future self as well as others, so be explicit about the purpose of each chunk of code. Good coding is concise. When you can write code in 3 lines instead of 30, your code may be more clear and efficient. Take pleasure in writing parsimonious, efficient code. But where efficiency and clarity conflict, choose the latter. Good code should be complete, including all steps from reading the data to producing output. Where appropriate, comment-out (rather than delete) informative errors, again for the future you. Good code may be creative. The coolest solutions are those which pull from and synthesize a number of ideas. Creativity often requires walking away from a problem in order to ultimately arrive at a solution (Wertheimer’s Productive Thinking). Finally, good code should be considered. Reflect on the impacts of your work - just because you can analyze something doesn’t mean that you should. 6.5 What to do when you are stuck Google. pay attention to your error messages Ask for help, make your questions clear and reproducible (see R4DS Chapter 1) Take a break, think outside the box and kludge something together if you have to Document your struggles and your cleverness for a future you "],["principles-of-data-visualization.html", "7 principles of data visualization 7.1 some opening thoughts 7.2 some early graphs 7.3 Tukey and EDA 7.4 approaches to graphs 7.5 Tufte: first principles 7.6 the politics of data visualization 7.7 the psychology of data visualization 7.8 further reading and resources", " 7 principles of data visualization 7.1 some opening thoughts Graphs aren’t just to inform, but to make you reflect. We are concerned not just with the design of graphs, but with our role in society as leaders, as arbiters of facts, as educators. We want to foster data literacy. How do people understand graphs? How can you use graphs to tell people the truth in a way that they understand? How can you convey uncertainty in a graph? When you see a graph, what do you notice, what do you wonder, and what is the story? Is “story telling” what visualizations should be about? A graph might begin with perception and understanding (the consumer), with knowledge and design values (the producer), but it also reflects the truth of the data. How much is each? 7.2 some early graphs Visual displays of information reach back to prehistoric times; modern graphs date back, arguably, to Playfair’s 1786 Political Atlas - in which “… spatial dimensions were used to represent nonspatial, quantitative, idiographic, empirical data. Although it now seems natural to represent, for example, rising and falling imports over time as a rising and falling line, it does not seem to have been done before that time and was quite an accomplishment. Notably, in addition to the statistical line chart, Playfair at one fell swoop single-handedly invented most of the remaining forms of the statistical graphic repertoire used today-the bar chart and histogram, the surface chart, and the circle diagram or”pie chart” (Wainer and Thissen 1981). Fig 6.1: Playfair’s 1786 analysis of trade deficits The most celebrated early graph is that of Minard: Fig 6.2: Minard’s display of Napoleon’s catastrophic assault on Moscow, 1812 The visualization depicts the size, latitude, and longitude of Napoleon’s army as they moved towards (tan line) then away (black line) from Moscow; temperature during the retreat is plotted as well. Further, vertical lines linking the temperature display to the number of troops indicate the often perilous river crossings which further decimated Napoleon’s troops). Cheng (2014) decomposes the graph and provides some simpler visualizations; she also provides the following background: “Czar Alexander of Russia sees that Napoleon was becoming too powerful, so he refuses to participate in this embargo [against the UK]. Angry at Czar Alexander’s decision, Napoleon gathers a massive army of over 400,000 to attack Russia in June of 1812. While Russia’s troops are not as numerous as France’s, Russia has a plan. Russian troops keep retreating as Napoleon’s troops move forward, burning everything they pass, ensuring that the French forces could not take anything from their environment. Eventually the French army follows the Russian army all the way to Moscow during October, suffering major losses from lack of food. By the time Napoleon gets to Moscow, he knows he has to retreat. As winter settles into Europe and the temperature drops, Napoleon’s troops suffer even more losses, returning to France from lack of food, disease, and weather conditions.” Of course, the casualties and retreat of Napoleon’s army are immortalized not just in this graph, but also in Russian literature (Tolstoy’s War and Peace) and music (Tchaikovsky’s 1812 overture, in which five cannon shots mark the battle of Borodino and eleven more mark the arrival of Napoleon in the now-razed city of Moscow). 7.3 Tukey and EDA For (D. Donoho 2017), the publication of John Tukey’s “Future of Data Analysis” (Tukey 1962) arguably marks the beginning of data science. As the first data scientist, Tukey embraced a descriptive and exploratory approach to data analysis, particularly in his publication of Exploratory Data Analysis (Tukey 1977). In EDA, Tukey presented an idiosyncratic, coherent approach to looking at data, beginning with tallying. The distributions of small counts of single variables, for Tukey, could best be presented in stem and leaf displays. Comparisons between groups can be presented in box plots. To examine relationships between variables and the adequacy of simple models, he argues for thoughtfully transforming data to uncover linear trends, then to examine residuals to find where these trends do not hold. 7.4 approaches to graphs A graph might begin with perception and understanding (the consumer), with knowledge and design values (the producer), but it also reflects the truth of the data. How much is each? In thinking about how to design graphs, we can begin with abstract theory, with principles of design informed by our understanding of perception, or with empirical analyses of understanding and memory. 7.5 Tufte: first principles (Tufte 2001) describes Graphical Excellence. Graphs should, among other things, “Induce the viewer to think about the substance, rather than about methodology, graphic design, the technology of graphic productions, or something else.” Graphs should “Present many numbers in a small space, make large data sets coherent, and encourage the eye to compare different pieces of data.” Graphs should “serve a reasonably clear purpose: description, exploration, tabulation, or decoration [and] be closely integrated with the statistical and verbal descriptions of a data set.” Tufte concludes with the following Principles of Graphical Excellence, which I quote verbatim: Graphical excellence is the well-designed presentation of interesting data—a matter of substance, of statistics, and of design. Graphical excellence consists of complex ideas communicated with clarity, precision and efficiency. Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space. Graphical excellence is nearly always multivariate. And graphical excellence requires telling the truth. 7.6 the politics of data visualization On a cold morning in January, 1986, Space Shuttle Challenger lifted off from Cape Canaveral. Because of the cold weather, engineers at Morton Thiokol, who designed the rocket boosters, considered the possibility that the O-rings which sealed joints on the rockets would be too hard and brittle to prevent the release of potentially explosive fuel. They examined the relation between temperature and o-ring damage on prior flights, using the following visualization: Figure 6.3: What Morton Thiokol engineers saw prior to deciding to launch the Challenger Space Shuttle in January, 1986 What, if anything, can we see here? The graphic was chaotic and poorly designed; if the engineers had wanted to systematically explore the relation between o-ring damage and temperature they could have removed the rockets and arranged them on a single axis by liftoff-temperature rather on several rows by liftoff-date. Here’s what they would have seen: Figure 6.4: What the engineers could have seen, perhaps, with a better graph. The forecast for the morning of the launch was far colder than that for prior launches, and within the set of prior launches a clear relationship between temperature and o-ring damage is evident. But the Challenger did launch, exploding shortly after takeoff, killing the seven astronauts on board. With a clearer display of the data, the Challenger launch would likely have been postponed (Tufte 2001). 7.6.1 poor design leads to an uninformed or misinformed world In the flood of information that surrounds us, our peripheral and sometimes our focal attention will be drawn to pictorial summaries of consequential events and effects. But while data visualizations and data journalism has become more responsible, poorly designed graphs - what Tufte has described as “chartjunk” - are still common. Consider the following graph, which recently appeared in a number of newspapers or feeds in the United States (including the Palm Beach Post, on January 12, 2019). Figure 6.5: Which smartphone manufacturers are doing well? Exercise 6.1 Examine the graph shown above. Look at it for just a moment, as if you might while reading something else on the screen, or as your ten-year old little sister might. What does the graph tell you? Look at it more closely. What does it actually say? How could you improve it to make the content of the graph more informative about its title? Why was the graph designed in this way? Does this matter? Poorly designed graphs don’t just confuse rocket scientists. They reach us, mislead us, and make fools of all of us as well. With better graphs, we become a better informed, better educated citizenry. 7.6.2 poor design can be a tool to deceive Figure 6.6: Trump as “the first datavis President” (Meeks, 2019). The apps on our smartphones and electronic devices are meticulously designed to maintain our attention. They are largely successful, but, for many of us, made it more difficult to read in depth (Wu, The Attention Merchants). With information all around us demanding our attention, visual representations of data have become particularly important. The data visualization is, increasingly, not just a supplement to the story, but the story itself. When this premise is coupled with the idea that the truth is malleable, the product of what our leaders say rather than empirical statements of fact, we approach a dangerous situation, one in which real threats are misrepresented and lives are again endangered. Presenting information in self-promoting ways includes so-called “Sharpie-gate,” where President Trump simply altered a hurricane prediction map in defense of a misstatement. It also includes more subtle misrepresentations: Does stand-your-ground really make us safer? Figure 6.7: Stand your ground makes us all safer. Or not. 7.7 the psychology of data visualization Speaking of America, consider the following: Figure 6.8: Chernoff’s too-clever faces In this figure, from (Wainer and Thissen 1981), (Chernoff’s) faces are used to represent multivariate data. The display is loosely based on the (psychological) premise that, from infancy, we are inordinately sensitive to facial features. Consequently, the most powerful, effective, or informative data displays should manipulate facial features. Does this graph succeed? Why or why not? Would an updated model, based on richer, more complex, and more realistic simulations of facial expressions (Thies et al. 2015) be more successful? 7.7.1 the power of animation Animated data displays bring the dimension of time into data visualization. Here are two brief (&lt; 5 minutes) animated data displays, each powerful in its own way, and each of which gives the viewer an appreciation of an important social phenomenon. The first is from the late Hans Rosling, and summarizes a chunk of the Gapminder data. There is a Gapminder package in R if you would like to explore the data further, and perhaps become the next Rosling: Video 6.9: Rosling and social progress The second is from Kim Rees and her ex-colleagues at Periscopic (Rees is now at CapitalOne). For me, it’s an important graphic because it tries to overcome what has been called “psychic numbing” - we are sensitive to the individual case, but lose our perspective when faced with large-scale tragedies, so that effectively the more lives lost… the less we care (Slovic et al. 2013). Video 6.10: Rees and stolen years 7.7.2 telling the truth when the truth is unclear We live in an uncertain world, and a major challenge in data visualization is how to convey this uncertainty in a way that people understand readily. Examples of this are familiar. Here in Florida, projected hurricane paths are frequently represented by a “cone of uncertainty” surrounding a single track. An alternative approach is to use a display which shows a distribution of possible tracks. Figure 6.11: Two approaches to displaying hurricane paths 7.7.3 visualizing uncertainty To further illustrate the idea of uncertainty, data visualizations may be animated. To display hurricane outcomes, for example, individual lines in a spaghetti plot might fade in and out (Cox and Lindell 2013). Another use of animation is suggested by (Hullman, Resnick, and Adar 2015) who use hypothetical outcome plots rather than simple error bars to illustrate sampling distributions. Empirically, subjects have a better understanding of the meaning of statistical effects when these are represented by dynamic displays. During the 2016 US Presidential Election, the NY Times graphics presented the range of likely possibilities (here, between the 25th and 75th percentiles) as a dynamically changing “jittery gauge” . Following the election, there was some criticism of these gauges, as readers claimed that they tended to make them anxious. However, it appears that at least some of the anxiety was attributable not to the gauges themselves, but to the electoral outcome itself. The gauges were back in 2018, and will likely be used again in the future. Finally, animation may be used not just to display uncertainty or a possible range, but also data displays which both carry a great deal of information and unfold over time, such as the language of personality development in my own work. 7.8 further reading and resources If you’d like to learn more, Tufte (2001) and his other books are beautiful and thought provoking. Cleveland (1985) examines graphs from a more rigorous psychological and empirical viewpoint. The Data Stories podcasts are often excellent, despite the challenge of an auditory medium for visual display (especially the episode on Hans Rosling). And Healy(2017) provides a comprehensive introduction that, in essence, provides the basis for a semester-long course based in the ggplot2 package in R that we will be working with in the next chapter (Wickham, Çetinkaya-Rundel, and Grolemund 2023). References Cleveland, William S., and Robert McGill. 1985. “Graphical Perception and Graphical Methods for Analyzing Scientific Data.” Science, New Series 229 (4716): 828–33. http://www.jstor.org/stable/1695272. Cox, Jonathan, and Michael Lindell. 2013. “Visualizing Uncertainty in Predicted Hurricane Tracks.” International Journal for Uncertainty Quantification 3 (2). https://doi.org/gjjsfw. Donoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734. Healy, Kieran. 2017. “Data Visualization for Social Science: A Practical Introduction with r and Ggplot2.” https://socviz.co. Hullman, Jessica, Paul Resnick, and Eytan Adar. 2015. “Hypothetical Outcome Plots Outperform Error Bars and Violin Plots for Inferences about Reliability of Variable Ordering.” Edited by Elena Papaleo. PLOS ONE 10 (11): e0142444. https://doi.org/10.1371/journal.pone.0142444. Slovic, Paul, David Zionts, Andrew K Woods, Ryan Goodman, and Derek Jinks. 2013. “Psychic Numbing and Mass Atrocity.” The Behavioral Foundations of Public Policy, 126–42. https://doi.org/gk4945. Thies, Justus, Michael Zollhöfer, Matthias Nießner, Levi Valgaerts, Marc Stamminger, and Christian Theobalt. 2015. “Real-Time Expression Transfer for Facial Reenactment.” ACM Trans. Graph. 34 (6): 183–81. https://doi.org/f7wqz7. Tufte, Edward R. 2001. The Visual Display of Quantitative Information. 2nd ed. Cheshire, CT: Graphics Press. Tukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67. https://doi.org/d48nqg. ———. 1977. “EDA: Exploratory Data Analysis.” Reading, Mass. Wainer, H, and D Thissen. 1981. “Graphical Data Analysis,” 51. Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science 2e. \" O’Reilly Media, Inc.\". https://books.google.com/books?hl=en&amp;lr=&amp;id=TiLEEAAAQBAJ&amp;oi=fnd&amp;pg=PT9&amp;dq=r4ds+2e&amp;ots=ZJr-ihzQuM&amp;sig=NKto0ViShGg7yiv76WF2sHejD80. "],["exercises-etc..html", "8 exercises etc. 8.1 generating correlated data (GPA and SAT) and putting it in a Google Sheet 8.2 now play with this spreadsheet", " 8 exercises etc. 8.1 generating correlated data (GPA and SAT) and putting it in a Google Sheet It’s simple to generate a single random variable, but we often want to examine data for several variables that might be correlated. The MVRnorm function, in the MASS package, does this. We use this to generate two variables with mean 0 and standard deviation 1, a given correlation, and a given sample size (number of rows). We then fiddle with these variables, setting the means, standard deviations, ranges, and number of significant digits, to make them look like GPA and SAT scores. And we use the randomnames package to make up fake names to go along with the fake scores. Finally, we upload the data to a Google Sheet. 8.2 now play with this spreadsheet what are the means for GPA and SAT? What are the standard deviations? what is the difference between ‘sort range’ and ‘sort sheet’ in Google Sheets? "],["references.html", "References", " References Blumenthal, Arthur L. 1975. “A Reappraisal of Wilhelm Wundt.” American Psychologist 30 (11): 1081. https://psycnet.apa.org/record/1976-23157-001. Broman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989. Buckheit, Jonathan B., and David L. Donoho. 1995. “Wavelab and Reproducible Research.” In Wavelets and Statistics, 55–81. Springer. Cleveland, William S., and Robert McGill. 1985. “Graphical Perception and Graphical Methods for Analyzing Scientific Data.” Science, New Series 229 (4716): 828–33. http://www.jstor.org/stable/1695272. Cox, Jonathan, and Michael Lindell. 2013. “Visualizing Uncertainty in Predicted Hurricane Tracks.” International Journal for Uncertainty Quantification 3 (2). https://doi.org/gjjsfw. Deane, Claudia. 2024. “Americans’ Deepening Mistrust of Institutions.” https://pew.org/40eFLof. Donoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734. Donoho, David L. 2010. “An Invitation to Reproducible Computational Research.” Biostatistics 11 (3): 385–88. https://doi.org/bxwkns. Erdelyi, Matthew H. 1974. “A New Look at the New Look: Perceptual Defense and Vigilance.” Psychological Review 81 (1): 1–25. https://doi.org/cs5c5q. Gandrud, Christopher. 2013. Reproducible Research with R and R Studio. CRC Press. Healy, Kieran. 2017. “Data Visualization for Social Science: A Practical Introduction with r and Ggplot2.” https://socviz.co. Henrich, Joseph, Steven J. Heine, and Ara Norenzayan. 2010. “The Weirdest People in the World?” Behavioral and Brain Sciences 33 (2-3): 61–83. https://doi.org/c9j35b. Hicks, Stephanie C., and Rafael A. Irizarry. 2018. “A Guide to Teaching Data Science.” The American Statistician 72 (4): 382–91. https://doi.org/gfr5tf. Hornik, Kurt, and The R Core Team. 2022. “R FAQ: The Comprehensive R Archive Network.” Hullman, Jessica, Paul Resnick, and Eytan Adar. 2015. “Hypothetical Outcome Plots Outperform Error Bars and Violin Plots for Inferences about Reliability of Variable Ordering.” Edited by Elena Papaleo. PLOS ONE 10 (11): e0142444. https://doi.org/10.1371/journal.pone.0142444. Isaacson, Walter. 2014. The Innovators: How a Group of Hackers, Geniuses, and Geeks Created the Digital Revolution. Simon; Schuster. Loevinger, Jane. 1957. “Objective Tests as Instruments of Psychological Theory.” Psychological Reports 3 (3): 635–94. https://doi.org/b27jpk. Ondaatje, Michael, and Walter Murch. 2002. The Conversations: Walter Murch and the Art of Editing Film. A&amp;C Black. https://books.google.com/books?hl=en&amp;lr=&amp;id=VRS6S1OXdwUC&amp;oi=fnd&amp;pg=PP2&amp;dq=ondaatje+murch&amp;ots=MG0Jx7ktbZ&amp;sig=okJy-Tznz_XVndiYIFh9vUN3Hhc. Peng, Roger D. 2014. R Programming for Data Science. Poulin, Michael J., and Claudia M. Haase. 2015. “Growing to Trust: Evidence That Trust Increases and Sustains Well-Being Across the Life Span.” Social Psychological and Personality Science 6 (6): 614–21. https://doi.org/10.1177/1948550615574301. Reinsel, David, John Gantz, and John Rydning. 2025. “The Digitization of the World.” Slovic, Paul, David Zionts, Andrew K Woods, Ryan Goodman, and Derek Jinks. 2013. “Psychic Numbing and Mass Atrocity.” The Behavioral Foundations of Public Policy, 126–42. https://doi.org/gk4945. Sullivan, J. L., and J. E. Transue. 1999. “THE PSYCHOLOGICAL UNDERPINNINGS OF DEMOCRACY: A Selective Review of Research on Political Tolerance, Interpersonal Trust, and Social Capital.” Annual Review of Psychology 50 (1): 625–50. https://doi.org/cmthvk. Thies, Justus, Michael Zollhöfer, Matthias Nießner, Levi Valgaerts, Marc Stamminger, and Christian Theobalt. 2015. “Real-Time Expression Transfer for Facial Reenactment.” ACM Trans. Graph. 34 (6): 183–81. https://doi.org/f7wqz7. Tufte, Edward R. 2001. The Visual Display of Quantitative Information. 2nd ed. Cheshire, CT: Graphics Press. Tukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67. https://doi.org/d48nqg. ———. 1977. “EDA: Exploratory Data Analysis.” Reading, Mass. Tversky, Amos, and Daniel Kahneman. 1974. “Judgment Under Uncertainty: Heuristics and Biases.” Science 185 (4157): 1124–31. https://doi.org/gwh. Wainer, H, and D Thissen. 1981. “Graphical Data Analysis,” 51. Watts, Duncan J. 2004. “The \"New\" Science of Networks.” Annual Review of Sociology 3: 243–70. http://www.jstor.org/stable/29737693. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science 2e. \" O’Reilly Media, Inc.\". https://books.google.com/books?hl=en&amp;lr=&amp;id=TiLEEAAAQBAJ&amp;oi=fnd&amp;pg=PT9&amp;dq=r4ds+2e&amp;ots=ZJr-ihzQuM&amp;sig=NKto0ViShGg7yiv76WF2sHejD80. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
