[["index.html", "Data science for the liberal arts ", " Data science for the liberal arts Kevin Lanning 2025-03-19 "],["preface.html", "preface the role of the liberal arts in data science some features of the text the book is for you", " preface This work-in-progress will ultimately serve as a textbook for introductory undergraduate courses in data sciences, and is particularly aimed at students in the liberal arts. No prior knowledge of computer programming is presumed, though, ideally, students will have had college algebra (or its equivalent) and an introductory course in statistics, methods, or data analysis. the role of the liberal arts in data science Data science is still a relatively new field of study, and there are multiple approaches to teaching it and to its place in the college curriculum. This book is intended to serve courses such as Introduction to Data Science at the Wilkes Honors College of Florida Atlantic University which, in turn, was initially based on data science classes at the universities of North Carolina, British Columbia, Duke, Maryland, Wisconsin, Stanford, BYU, Harvard, and UC Berkeley. At each of these schools, the Introduction to Data Science is, to my eyes at least, closer to Statistics than to Computer Science. Statistics is itself a broad field, and our approach is aligned with its most applied and pragmatic form. From this perspective, the choice of statistical methods should follow from the data and problem at hand - in other words, statistics should serve the needs of the user rather than dictate them (Loevinger 1957). Pragmatism, in turn, can serve various goals, ranging from maximizing the revenues generated by an online ad to minimizing the carbon footprint of a travel itinerary. Data science for the liberal arts may be seen as a fusion of the pragmatism of data science with social and humanistic concerns; we stand beside programs in Computational Social Science as it has been taught at schools including Chicago, Georgia Tech, UC Santa Barbara, Princeton, UC Berkeley, at Berlin’s Hertie School of Governance, and in Columbia’s School of Journalism. Data science for the liberal arts begins with the person and society rather than with the algorithm and network. In its concern with the liberal arts, it is intended to provide a modest counterbalance to the inherently centripetal, or inequality-accelerating, force of modern information technology.1 some features of the text There are a number of different approaches to teaching data science for the liberal arts. The present text includes several distinguishing features. R In a recent informal survey of introductory data science courses, I saw a pretty even split between those which begin with Python and those which begin with the statistical programming language R. This difference corresponds, very loosely, to the split noted above: Computer science based approaches to data science are frequently grounded in Python, while statistics-based approaches are generally grounded in R. Our course, like those for most of the syllabi and courses linked above, will be based in R. Reproducible science The course will provide an introduction to some of the methods and tools of reproducible science. We will consider the replication crisis in the natural and social sciences, and then consider three distinct approaches which serve as partial solutions to the crisis. The first of these is training in a notebook-based approach to writing analyses, reports and projects (using R markdown documents). The second is using public repositories (such as the Open Science Framework and, to a limited extent, GitHub) to provide snapshots of projects over time. Finally, the third is to consider the place of significance testing in the age of Big Data, and to provide training in the use of descriptive, exploratory techniques of data analysis. Good visualizations As I note in the first chapter, communication is a distinguishing concern of data science for the liberal arts. “Communication” includes not just writing up results, but also designing data displays that incisively convey the key ideas or features in a flood of data. We’ll examine and develop data visualizations such as plots, networks and text clouds. More advanced topics may include maps, interactive displays, and animations. A little data There are plenty of data sources for us to examine, and we’ll consider existing datasets from disciplines ranging from literature to economics to public health, with sizes ranging from a few dozen to millions of data points. We will also clean and create new datasets. A few tools One feature of Data Science is that it is changing rapidly. The tools, methods, data sources, and ethical concerns that face us in 2025 are different from those which shaped the field just one or two years ago. In fields that are undergoing rapid change, there is some trade-off between building expertise with existing (older) tools and trying the newer approaches. Partly because I want to equip you with skills which will not be obsolete, partly because some of these new approaches promise more accessibility, elegance, and/or power, and partly because of my own interest in staying current, we’ll be using some of the latest packages and programs. In the last few years, I’ve shifted the class from the standard R dialect (as I learned it from the Johns Hopkins-Coursera Data Science Specialization) to the Tidyverse, a dialect of R that I find to be relatively clear and concise. A few years ago, I shifted our primary platform from individual laptops to a cloud-based R platform; while this approach has its advantages, I found that these did not outweigh the costs of the approach, so we will go back to the standalone method. We’ll explore different approaches to learning R syntax, including the learnr package, Swirl, and DataCamp. In the past, I’ve recommended using dedicated markdown editors such as Typora and Obsidian. While I still think that these are worth considering for some text-editing and note-taking applications, we’ll do our work instead with the editor in the latest variant of RStudio on our laptops, as this allows WYSIWIG (what you see is what you get) formatting of documents - such as this one - that are intended as “publication-ready” texts. We’ll use, and explore the advantages and disadvantages, of spreadsheets such as Excel or Google Sheets as well. the book is for you It’s my intention that this text should serve every college student, regardless of concentration or college major. The skills and insights that you will gain in this course will help you in graduate and professional schools, will help you in your careers, and will help you in your goal of making a better world. And it will help you train the next generation of data scientists as well. References Loevinger, Jane. 1957. “Objective Tests as Instruments of Psychological Theory.” Psychological Reports 3 (3): 635–94. https://doi.org/10/b27jpk. Watts, Duncan J. 2004. “The \"New\" Science of Networks.” Annual Review of Sociology 3: 243–70. https://www.jstor.org/stable/29737693. I hope to return to this in a later chapter, but in the meantime consider the discussion of the “Matthew Effect” in sociology and network science (Watts 2004).↩︎ "],["part-i-introduction.html", "PART I: Introduction", " PART I: Introduction "],["what-is-data-science-for-the-liberal-arts.html", "1 what is “data science for the liberal arts?” 1.1 the incompleteness of the data science Venn diagram 1.2 the importance of data science for society 1.3 discussion: what are your objectives in data science?", " 1 what is “data science for the liberal arts?” Hochster, in Hicks and Irizarry (2018), describes two broad types of data scientists: Type A (Analysis) data scientists, whose skills are like those of an applied statistician, and Type B (Building) data scientists, whose skills lie in problem solving or coding, using the skills of the computer scientist. This view arguably omits a critical component of the field, as data science is driven not just by statistics and computer science, but also by “domain expertise:” Fig 1.1 - The iconic data science Venn diagram The iconic Venn diagram model of data science, as shown above, suggests that there are not two but three focal areas in the field, one of which begins not with math or computer science, but with “domain expertise.” Data science for the liberal arts is a ‘Type C’ approach, where ‘C’ refers to a concentration of concern in the arts, humanities, social and/or natural sciences. For the Type C data scientist, coding is in the service of applied problems and concerns. Type C data science does not merely integrate ‘domain expertise’ with statistics and computing, it places content squarely at the center. We can appreciate the compelling logic and power of statistics as well as the elegance of well-written code, but for our purposes these are means to an end. Programming and statistics are tools in the service of social and scientific problems and cultural concerns. Type C data science aims for work which is not merely cool, efficient, or elegant, but responsible and meaningful. At the risk of oversimplifying: Type A data scientists focus on Analysis and questions about ‘how?’ Type B data scientists focus on Building and questions of ‘what?’ Type C data scientists focus on Consideration and questions of ‘why?’, ‘who?’, ‘what for?’, and ‘at what (social) cost?’ 1.1 the incompleteness of the data science Venn diagram Data visualizations are starting points which can provide insights, typically highlighting big truths or effects by obscuring other, presumably smaller ones. The Venn diagram model of data science is no exception: As with other graphs, figures, and maps, it allows us to see by showing only part of the picture. What does it omit? That is, beyond mathematics, computing, and domain expertise, what other skills contribute to the success of the data scientist? 1.1.1 additional domains For the liberal arts data scientist, we can note at least three additional important domains, that is, communication, collaboration, and citizenship. Communication, including writing and the design and display of quantitative data, is central to data science because results are inconsequential unless they are recognized, understood, and built upon. Facets of communication include oral presentations, written texts and good data visualizations. Collaboration is important because problems in data science are sufficiently complex so that any single individual will typically have expertise in some, but not all, facets of the area. Collaboration, even more than statistical and technical sophistication, is arguably the most distinctive feature of contemporary scholarship in the natural and social sciences as well as in the private sector (Isaacson 2014). Citizenship is important because we are humans living in a social world; it includes serving the public good, overcoming the digital divide, furthering social justice, increasing public health, diminishing human suffering, and making the world a more beautiful place. The Type C data scientist is aware of the fact that the world and workforce are undergoing massive change: This puts the classic liberal arts focus of “learning how to learn” (as opposed to memorization) at center stage. Finally, the Type C data scientist is sensitive to the creepiness of living increasingly in a measured, observed world. These real-world goals should be informed by ethical concerns including a respect for the privacy and autonomy of our fellow humans. 1.1.2 an additional dimension Cutting across these various facets (statistics, computing, domain expertise, collaboration, communication, and citizenship), a second dimension can be articulated. No one of us can excel in all of these domains, rather, we might aim towards a hierarchy of goals ranging from literacy (can understand) through proficiency (can get by) to fluency (can practice) to leadership (can create new solutions or methods). That is, we can think of a continuum of knowledge, skills, interests, and goals, ranging from that which characterizes the data consumer to the data citizen to the data science contributor. A Type C data science includes this dimension of ‘depth’ as well. 1.2 the importance of data science for society Communication, collaboration, and citizenship are each associated with the concept of trust. Trust is an important social good because it is associated with both individual well being (Poulin and Haase 2015) and the stability of democratic institutions (Sullivan and Transue 1999). But interpersonal and institutional trust, including trust in science, have declined in recent years (Deane 2024). The decline in trust in science has been exacerbated by the so-called reproducibility (or replication) crisis, in which many scientific results initially characterized as “statistically significant” have been found not to hold up under scrutiny, that is, aren’t reproducible. The reasons for the reproducibility crisis are many and contentious, but there is substantial consensus that one path towards better science involves the public sharing of methods and data. A second path towards better, and more trustworthy, science involves the use of larger datasets: With large datasets, effects are more stable and “statistical significance” is rarely a concern. Other indices, such as measures of accuracy and effect size are typically of primary interest. Data science, with its tools for reproducible analysis and its use of big data sets, can make science more trustworthy and improve the quality of our lives. 1.2.1 training in the liberal arts -&gt; success in tech Data science is at its core empirical, and all of this rhetoric would be meaningless if not grounded in real world findings. Although it was reported in late 2017 that so-called “soft skills” rather than STEM training were the most important predictors of success among Google employees, it’s difficult to know whether these results would generalize to a less select group. Nonetheless, there is a clear need for individuals with well-rounded training in the liberal arts in data science positions and, conversely, learning data science is arguably a key part of a contemporary liberal arts education. 1.2.2 the challenge of TMI The challenges of data science are many, but perhaps the most fundamental is the problem of (literally) TMI. When we compare traditional statistics with modern data science, we realize that the former is typically concerned with making inferences from datasets that are too small, while the latter is concerned with making sense of data that is or are too big (D. Donoho 2017). The challenge of data science is the challenge of “too much information,” or TMI. The challenge of TMI is not new, or restricted to data science. In the 19th Century, Wilhelm Wundt argued that attention was the distinguishing act of the human mind (Blumenthal 1975). That is, in attending to (or focusing on) something, we must overlook everything else, consequently, selection is the essence of human perception (Erdelyi 1974). Selection is important not just in psychology, but in the arts as well, for editing, or choosing what not to write or show, is at the core of the creation of works including novels and film (Ondaatje and Murch 2002). Although the problem of TMI is not new, today it exists at a much greater scale, for there is simply more information around us. Indeed, over the last 20 years, the amount of digital information in the world has increased roughly 200-fold.2 Like perception in psychology and editing in the arts, data science is concerned with extracting meaning from information. Because the amount of information around us has mushroomed and its nature has become more important, our need to extract meaning has become more ubiquitous and more urgent. For these reasons, data science is a foundational discipline in 21st century inquiry. 1.3 discussion: what are your objectives in data science? References Blumenthal, Arthur L. 1975. “A Reappraisal of Wilhelm Wundt.” American Psychologist 30 (11): 1081. Deane, Claudia. 2024. “Americans’ Deepening Mistrust of Institutions.” Donoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734. Erdelyi, Matthew H. 1974. “A New Look at the New Look: Perceptual Defense and Vigilance.” Psychological Review 81 (1): 1–25. https://doi.org/10/cs5c5q. Hicks, Stephanie C., and Rafael A. Irizarry. 2018. “A Guide to Teaching Data Science.” The American Statistician 72 (4): 382–91. https://doi.org/10/gfr5tf. Isaacson, Walter. 2014. The Innovators: How a Group of Hackers, Geniuses, and Geeks Created the Digital Revolution. Simon and Schuster. Ondaatje, Michael, and Walter Murch. 2002. The Conversations: Walter Murch and the Art of Editing Film. A&amp;C Black. Poulin, Michael J., and Claudia M. Haase. 2015. “Growing to Trust: Evidence That Trust Increases and Sustains Well-Being Across the Life Span.” Social Psychological and Personality Science 6 (6): 614–21. https://doi.org/10.1177/1948550615574301. Reinsel, David, John Gantz, and John Rydning. 2025. “The Digitization of the World.” Sullivan, J. L., and J. E. Transue. 1999. “THE PSYCHOLOGICAL UNDERPINNINGS OF DEMOCRACY: A Selective Review of Research on Political Tolerance, Interpersonal Trust, and Social Capital.” Annual Review of Psychology 50 (1): 625–50. https://doi.org/10/cmthvk. According to Wikipedia, the Zettabyte (ZB) Era began in 2012, when the amount of digital information in the world first exceeded 1 ZB (or 1021 bytes). In 2025, it is estimated that the world will house 175 ZBs of digital data (Reinsel, Gantz, and Rydning 2025), hence a 175X increase in in 13 years. My estimate of a 200X increase in 20 years is a conservative extrapolation from these numbers. Incidentally, one ZB = 1,000,000,000,000,000,000,000 bytes, which could be stored on roughly 250 billion DVDs, or 500 million 2 TB hard drives.↩︎ "],["getting-started.html", "2 getting started 2.1 are you already a programmer and statistician? 2.2 spreadsheets - some best practices 2.3 setting up your machine: some basic tools 2.4 a (modified) 15-minute rule 2.5 installing R and RStudio desktop", " 2 getting started We begin with a brief self-assessment, asking you to reflect on your own knowledge of data science, including the necessary-but-not-sufficient areas of computer programming and statistics. We then move to a description of some rudimentary tools that we will be using, then install the R programming environment on our laptops. 2.1 are you already a programmer and statistician? Regarding programming, you may know more than you think you do. Here’s a simple program - a set of instructions - for producing a cup of coffee: add water to the kettle and turn it on if it’s morning, put regular coffee in the French press, otherwise use decaf if the water has boiled, add it to the French press, else keep waiting if the coffee has steeped for four minutes, depress (smash) piston/plunger, else keep waiting pour coffee into cup enjoy As a post-millennial student from a WEIRD culture, or Western, Educated, Industrialized, Rich Democracy (Henrich, Heine, and Norenzayan 2010), you’ve ‘programmed’ computers, too, if only to enter a password, open an app, and upload a photo on your cell phone. Statistics is of fundamental importance, not just for understanding abstract trends, but for making decisions about everyday life. Consider the case of a senior undergraduate who wants to go to med school. How many schools should she apply to? Exercise 2_1 Susie is applying to two med schools. At School A, 25% of students are accepted, and at School B, 25% are accepted as well. You are Susie. Are you going to get in to at least one of these programs? What is the probability? Does your estimate depend upon any assumptions? Questions such as these are important for us. If the combined probability is low, it will likely (another probability concept) make sense for Susie to spend the time, money, energy, and ego-involvement to apply to additional programs. If the probability is higher, it may not. But problems like this are hard - our estimates of probability are frequently poorly calibrated, and combining probability estimates is challenging. See, e.g., Tversky and Kahneman (1974), and consider taking a course in Behavioral Economics or Thinking and Decision Making to learn more. 2.2 spreadsheets - some best practices Spreadsheets are handy tools, particularly for smaller datasets. You may have worked with data in spreadsheets such as Microsoft Excel or Google Sheets. If you haven’t here’s a start: Exercise 2_2 Open the Google Sheet at http://bit.ly/dslaX2_1. Save a copy and edit it, entering the following in cell B7: =SUM (B2:B6) What is the result? Now copy cell B7 to C7 What happens? Is this the result you expected? Spreadsheets are great tools - the first one, VisiCalc, was the first “killer app” to usher in the personal computer revolution. But spreadsheets have limitations as well. Best practices have been proposed for using spreadsheets in data science such as, for example, including only data (and not calculations) in spreadsheets, using what we will recognize as a ‘tidy’ format in which data are in a simple rectangle (that is, avoid combining cells and using multi-line headers), and saving spreadsheets as simple text files, typically in comma-delimited or CSV format (Broman and Woo 2018). Typically, this means that each worksheet will be in a separate file, rather than in workbooks with multiple tabs. There are good reasons for these recommendations: When we manipulate data in spreadsheets, we typically will not have a record of what was (and wasn’t) changed, and this compromises the reproducibility of our work. Similarly, when we sort data in spreadsheets, we risk chaos, for example, if we sort only certain columns, the integrity of spreadsheet-rows will be lost. In general, spreadsheets should generally be used to store data rather than to analyze it. But don’t be a slave to this - if you are making some quick calculations that are inconsequential and/or to be used on just one occasion, working in Excel or Google Sheets is often the way to go. 2.3 setting up your machine: some basic tools Documents produced in word processors typically include invisible characters used for formatting, security, etc. These characters, when fed into a computer program, lead to unpredictable results. Markdown (MD) is a simple syntax for producing documents that are easily read by both humans and machines. Most of the documents you will produce and work with this term, including the present chapter, are and will be written in some version of Markdown. Three markdown editors are worth considering. One of the newest is Obsidian, which is free and includes an extended version of Markdown that allows the drawing of simple network graphs. Typora has an attractive minimalist interface, but is no longer free ($15). Finally, RStudio - the environment within which we will be using R - has a built in visual markdown editor as well. Collaboration and communication are integral to data science. In the world beyond universities, the most important messaging and collaboration platform is Slack. Slack is a commercial app, but it has a free tier. Slack is handy for group work of all forms, but most universities use collaborative tools embedded in Learning Management Systems such as Canvas instead. While Microsoft Word has the advantages of familiarity, ease-of-use offline, and extensive formatting capabilities, Google Docs has several advantages over Word. Google Docs is free, it is convenient for collaborative work (as it allows simultaneous editing), and it provides a solid framework for version control, a critical skill in information management. Version control can help you avoid the chaos and confusion of having a computer (or several computers) full of files that look like Cham’s (2012) comic: Fig 2.1: Never call anything ‘final.doc’. Version control is an important concept in data science. Collaboratively built programs and platforms, including most of the add-ons (libraries, packages) which make R so powerful, are open-source projects built by many individuals over time. For projects such as these, both the current build and its associated history are typically maintained on GitHub, a website for hosting code. When we contribute to these projects, we will first mirror the web-based GitHub site using a program on our own Macs or Windows PCs called Git, then upload our proposed changes. Keeping remote and local branches of files in sync can be challenging, however, and you will not be expected to use this technology in this class. But if you are curious, or want to learn more, an introduction to using Git and R together may be found here. 2.4 a (modified) 15-minute rule While AI tools for coding, including Microsoft Copilot, are helpful for addressing the idiosyncacies of coding syntax, at some point, you will run into problems - if you don’t you aren’t learning enough. An important determinant of your success will be the balance you maintain between persistence and help-seeking. The 15-minute rule is one guideline for this balance: It has been cleverly summarized as “You must try, and then you must ask.” That is, if you get stuck, keep trying for 15 minutes, then reach out to others. I think that this rule is basically sound, particularly if it is applied with cognitive flexibility, social sensitivity, and reciprocity. So when you get stuck, make a note of the problem, then move to another part of your project (that’s the cognitive flexibility part): This allows your problem to percolate and still make progress. When you ask others for help, ask in a way that shows an awareness of the demands on their time (social sensitivity): Part of this means that you should explain your problem in as detailed a fashion as possible - in technical terms, a “reprex” or reproducible example. Finally, you should be willing to provide as well as give help (reciprocity). 2.5 installing R and RStudio desktop Finally, if you have not already done so, install the latest version of R on your own Windows or Mac laptop. Then install RStudio Desktop. (RStudio is our interface, the environment we will use to write, test, and run R code). References Broman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989. Henrich, Joseph, Steven J. Heine, and Ara Norenzayan. 2010. “The Weirdest People in the World?” Behavioral and Brain Sciences 33 (2-3): 61–83. https://doi.org/10/c9j35b. Tversky, Amos, and Daniel Kahneman. 1974. “Judgment Under Uncertainty: Heuristics and Biases.” Science 185 (4157): 1124–31. https://doi.org/10/gwh. "],["what-r-stands-for.html", "3 what R stands for … 3.1 some key characteristics of R 3.2 cha-cha-cha-changes 3.3 some technical characteristics 3.4 finding help 3.5 Wickham and R for Data Science 2e", " 3 what R stands for … R was initially developed by Ross Ihaka and Robert Gentleman as a tool to help teach university-level statistics at the University of Auckland. At one level, the name ‘R’ simply stands for the first initial of these two founders (Hornik and Team 2022). But, just as we noted that the ‘C’ in Type C Data Analysis stands for concepts such as concentration, communication, collaboration, the ‘R’ in our programming language means much more: R is a system for reproducible analysis, and reproducibility is essential. When we write R code, we’ll use R markdown documents. An R markdown document can include text (comments or explanations), ‘chunks’ of code, and output including graphs and tables. Having explanations, code, and results in a single document facilitates reproducible work. (Jupyter notebooks in the Python world are similar in this respect). R is for research. Research is not just an end-product, not just a published paper or book: … these documents are not the research [rather] these documents are the “advertising”. The research is the “full software environment, code, and data that produced the results” (Buckheit and Donoho 1995; D. L. Donoho 2010, 385). Published works (including theses as well as books, scholarly papers, and business reports) are summaries; R markdown documents are the raw materials from which these are derived. When we consider only summaries (or the ‘advertising’), we make it difficult for others to verify, or build upon, the findings by reproducing them (Gandrud 2013). R is a system for representing data in cool, insight-facilitating ways, a tool for creating (reproducible) data visualizations which can provide insights and communicate results. The power of R to make clear, honest, and reproducible data visualizations is widely seen as a major strength of the language. R is really popular, and this matters, because learning R will make you a more attractive candidate for many graduate programs as well as jobs in the private sector. Because R is popular, there are many resources, including, for example - Online resources include the simple (and less simple) lessons of SwirlR, which offers the possibility of “learning R in R,” as well as DataCamp, the Data Science Certificate Program at Johns Hopkins, and other MOOCs. Books include (R. D. Peng 2014) - which includes not only videos of his lectures in the program at Hopkins, but also a brief list of still more resources - and (Wickham, Çetinkaya-Rundel, and Grolemund 2023) You’ll also learn (more directly) from people, including your classmates, as well as the broader community of people around the world. There are hundreds if not thousands of people, young and old, who are on the road with you. I am as well, just a step or two (hopefully) ahead. R might stand for relatively high level. Programming languages can be described along a continuum from high to low level, the former (like R) are more accessible to humans, the latter (like assembly language) more accessible to machines. Python, Java, and C++ are all more towards the middle of this continuum. R does not stand for ‘arggh,’ although you may proclaim this in frustration (‘arggh, why can’t I get this to work?) or, perhaps, in satisfaction (’arggh, matey, that be a clever way of doing this’).3 But R does stand for rewarding. A language is a way of thinking about the world, and this is true for computer languages as well. You’ll be challenged by its complexity, its idiosyncracy, its alien logic. But you will succeed, and you will find that you can do things that you did not believe possible. 3.1 some key characteristics of R 3.1.1 base R and packages R is a programming language. It can be seen as including two parts, a simple core (Base R) and a large number of additional packages. These packages (libraries) are customized add-ons which simplify certain tasks, such as text analysis. There are, at this writing, 21,861 available packages on the CRAN package repository (as well as additional useful packages that, for one reason or another, do not appear on CRAN. Packages on CRAN are partially indexed by “task view pages.” The task view page for natural language processing or text analysis includes, at this writing, over 60 separate packages. So how do you choose, and where do you begin? For our purposes, we will start with the curated list of packages which jointly comprise the tidyverse (Wickham et al. 2019), which is effectively a dialect of R. To download the tidyverse package from the ‘net, open RStudio, find the ’console’ window on the left side of your screen, and enter the command followed by &lt;enter&gt; or &lt;return&gt; install.packages(“tidyverse”) 3.2 cha-cha-cha-changes R is constantly changing, not just in the proliferation of packages, but also in the organization of the R community. While R is free and open source, RStudio is a commercial product. The company (and website) that develops the RStudio IDE is undergoing a name change (from RStudio to Posit). This is motivated, in part, by the need to make the RStudio platform more welcoming for other languages including Python. Similarly, the R markdown programming language is slowly being replaced with newer, and ultimately more capable, software called Quarto. Quarto is back-compatible with R markdown, but can be used with other languages including Python as well. A description of the differences between R markdown and Quarto may be found here. For our purposes, you can treat Quarto files (.qmd suffix) as R markdown files (.rmd), and vice-versa. One more change: Posit (the company) is developing a new IDE called ‘Positron.’ Positron may ultimately be a more useful environment for data science than the RStudio IDE, but it is in the beta testing stage at this writing. The RStudio environment, and the Rmarkdown documents that are produced within it, will continue to be available, and widely used, for the foreseeable future. 3.3 some technical characteristics R is an object-oriented language - one conceptually organized around objects and data rather than actions and logic. In R, at the most basic or atomic level, “objects” include characters, real numbers, integers, complex numbers, and logicals. These atomic objects may be combined into vectors, which generally include objects of the same type [one kind of object, ‘lists,’ is an exception to this; R. D. Peng (2014)]. Vectors can be further combined into data frames, which are two-dimensional tables or arrays. A tibble is a particular type of data frame which is used in the tidyverse. Tibbles are, in some ways, handier to work with than other data frames. We’ll be working extensively with data frames in general, and tibbles in particular, as we move forward. Objects have attributes. Attributes of R include such things as name, dimensions (for vectors and arrays), class (that’s the type of object described in the previous paragraph), length, etc. Real world data sets are messy, and frequently have missing values. In R, missing values may be represented by NA (not available) or NaN (not a number, implying an undefined or impossible value). 3.4 finding help One does not simply ‘learn R.’ Unlike, say, learning to ride a bicycle, fry an egg, or drive a car with a manual transmission, learning R is not a discrete accomplishment that one can be said to have mastered and from which one then moves on. Rather, R is an evolving, open system of applications and tools which is so vast that there is always more that one can achieve, new lessons that one can learn. And, the complexity of R syntax is such that, for almost all of us, we will need help for coding on any non-trivial task. For us, the key ideas in “looking for help” will include not just the tools on the RStudio IDE, but also (a) using Google searches wisely, (b) judicious use of AI assistance, and (c) reaching out to your classmates and instructor. Here, as in the real world, there is an etiquette for help-seeking which is based on consideration. Your search for help should begin by making sure that others will encounter the same result, then by stripping the problem down to its essence. Once you have reduced the problem to this minimal, reproducible essence, you will often be able to spot the problem yourself - and, if not, you will make it easier for others to help you. There is an R package (reprex) which will likely facilitate this, but I haven’t tried it yet. Here is a good introduction. Finally, to get a sense of the power and versatility of R Markdown documents, you might explore this tutorial. Note that, if you want to work interactively with the tutorial, you will need to first sign up for an account on RStudio cloud. Go to posit.cloud, click on “learn more” in the “Free” column, then sign up. When you encounter obstacles, remember the 15-minute rule. 3.5 Wickham and R for Data Science 2e The introduction to the Wickham text (Wickham, Çetinkaya-Rundel, and Grolemund 2023) provides both a framework for his approach and a brief introduction to the tidyverse. Please read it now. References Buckheit, Jonathan B., and David L. Donoho. 1995. “Wavelab and Reproducible Research.” In Wavelets and Statistics, 55–81. Springer. Donoho, David L. 2010. “An Invitation to Reproducible Computational Research.” Biostatistics 11 (3): 385–88. https://doi.org/10/bxwkns. Gandrud, Christopher. 2013. Reproducible Research with R and R Studio. CRC Press. Hornik, Kurt, and The R Core Team. 2022. “R FAQ: The Comprehensive R Archive Network.” Peng, Roger D. 2014. R Programming for Data Science. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science 2e. \" O’Reilly Media, Inc.\". Actually, pirates have little use for R, as pirates love the C (programming language).↩︎ "],["exploring-r-world.html", "4 exploring R world 4.1 go to the movies 4.2 go into the clouds 4.3 open the box 4.4 go to (data)camp 4.5 learn to knit 4.6 older approaches", " 4 exploring R world There are many sources for learning the basics of R. A few of these follow. Please spend at least 180 mins exploring at least two of the following. Be prepared to discuss your progress next class: You will be asked which source(s) you used, what you struggled with, what questions you have, and what you would recommend to your classmates. Hint: If you find the material too challenging, remember the 15 minute rule, take a break away from your machine and other screens, clear your head, then try a different approach. 4.1 go to the movies About seven years ago, Iain Carmichael used data from the Internet Movies Database (IMDB) to introduce R. You can see his introduction here. You can consider his report from the standpoint of style (formatting, organization), coding (how he did this), data (the part of the IMDB data he is looking at), and his results (plots of distributions and relationships). Do you have any questions about the movie data? How might you ask these? A minimal amount of sleuthing - a click on the STOR 390 link at the top of the page, then a quick scroll - reveals that “all of [Carmichael’s] course material is on the github repo” - or repository. Can you find the Rmd document that generated his work? Can you download it on to your machine? If you try to run it, what happens? If you were working on your thesis and came across a problem like this, what would you do next? 4.2 go into the clouds In addition to the desktop version of R (and Rstudio) we will be using in this class, there is a cloud-based environment as well. As mentioned in the last chapter, you can sign up for a starter account at posit.cloud. When you open posit.cloud, you should see a column on the left of the screen that includes four sections - spaces, learn, help, and info. (If you don’t see these, click on the hamburger menu in the upper left corner and it will appear). Browse through the recipes tab, particularly the ones in the left most column, to start to get a sense of how you might solve some common challenges in R. 4.3 open the box Go to datasciencebox/content. Click on the “Hello world” link; this will take you to the beginning of Mine Çetinkaya-Rundel’s introductory lessons on R. These include slides, the source code for the slides (these are written in R markdown), and videos of her lectures, including the one we watched on the first day of class. Midway down this page, you’ll find a link to “the RStudio Cloud workspace for Data Science Course in a Box project.” Open it up and begin to explore the code and data behind her presentation. 4.4 go to (data)camp Many folks swear by (and others, I presume, at) DataCamp, which kind of gamifies learning software. As a student in this class, you have access to all of their stuff… free. You can even do lessons on your phone. Use the link given to you in class to enroll, then explore the introductory-level classes in R at https://www.datacamp.com/category/r 4.5 learn to knit In literate programming, comments, code, and results are integrated in a clear and reproducible way - they document our work. ‘Markdown’ is a simple language for adding formatting to text. ‘R’ is a statistical language. ‘R Markdown’ is a variant of R that you can use to produce or publish complex documents like this one, as well as the Carmichael page described above. To create an R markdown (Rmd) document, open up Rstudio, click on (File -&gt; New File -&gt; R Markdown). A window will open up with a file that begins with a block of YAML (Yet Another Markdown Language). You can edit this as needed: --- title: &quot;Here&#39;s an R Markdown Document&quot; author: &quot;Frankie McFrank Frank&quot; date: &quot;1/12/2025&quot; output: html_document --- Go ahead and click on the clever “knit” icon in the bar just above the source window to create a sample document. You’ll need to save the file with a new name, then R will finally create an HTML (Hyper Text Markdown Language ) page. Compare the R Markdown document (your code) with the result (the HTML). The second chapter of Healy’s online book about Data visualizations provides a more thorough explanation of R Markdown as well as an introduction to R and R studio which largely parallels the discussions here and in the Wickham and Grolemund text (Healy 2017; Wickham, Çetinkaya-Rundel, and Grolemund 2023). We’ll be discussing R Markdown (and its cousin Quarto) in Chapter 6. 4.6 older approaches I, like thousands of others, learned R in the process of completing the Johns Hopkins Data Science Specialization offered through Coursera. The sequence can be challenging, but their introduction to R used an accessible, interactive R package called Swirl. You can read about swirl (“learn R in R”) at https://swirlstats.com/. 4.6.1 using Swirl After loading R (and opening R studio), you will get to the Swirl lessons with the following steps: Install the Swirl package on your computer (you only need to do this once). Type the following into your console window in R studio (typically left hand side of your screen or lower left) install.packages Then load the package into your workspace (you’ll need to do this at the beginning of every session you use Swirl) library (swirl) Then run it! swirl () Swirl will ask a few questions then give you the option of choosing one of several courses. You’ll choose the R Programming option, which leads to 15 separate lessons. At the end of each lesson, you’ll be asked Would you like to receive credit for completing this course on Coursera.org? Answer no… then do another lesson. 4.6.2 reading/watching Roger Peng’s text and/or videos Finally, you might consider the text and videos from the Coursera R class. Most of the material from that class can be found in Roger Peng’s (2014) text, a slightly updated version of which can be found here. The videos in the series may be found in this playlist. Here’s an introduction: . Video 4.1: Roger Peng introducing R References Healy, Kieran. 2017. “Data Visualization for Social Science: A Practical Introduction with r and Ggplot2.” Peng, Roger D. 2014. R Programming for Data Science. Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science 2e. \" O’Reilly Media, Inc.\". "],["part-ii-towards-data-literacy.html", "Part II: Towards data literacy", " Part II: Towards data literacy "],["now-draw-the-rest-of-the-owl.html", "5 now draw the rest of the owl 5.1 time for hands-on experience 5.2 assignment", " 5 now draw the rest of the owl Fig 5.1: Draw the rest of the owl. In the prior chapter, you explored several different sources for learning how to code in R. Now it’s time to explore other approaches. Take a break from reading, and spend some time coding and consolidating, reviewing tutorials, or playing with data. 5.1 time for hands-on experience If you want to work actively with a dataset, here are two possibilities. (You are not limited to just these, so if you want to look at something else that’s fine too). Each of these datasets has been supplied as its own package. 5.1.1 consider loading the tidyverse The tidyverse allows the use of the ‘pipe’ operator, (“%&gt;%”), which is useful for combining commands. Now there is a native pipe in Base R (“|&gt;”), which does the same thing. But we will be using the tidyverse for a number of reasons, so go ahead and install it if you haven’t already, then load it. Remember that any package needs to be installed on your machine once before progressing. That is, if you installed the tidyverse previously, you don’t need to do the first line here. If you haven’t installed the tidyverse, you should remove the octothorpe or pound sign (#) on the second line before running this next chunk: # install.packages(&quot;tidyverse&quot;) library(tidyverse) 5.1.2 now explore the babynames package The babynames dataset is described here. What is in the data? What interesting questions might you ask about the dataset? # install.packages(&quot;babynames&quot;) library(babynames) data(babynames) str(babynames) ## tibble [1,924,665 × 5] (S3: tbl_df/tbl/data.frame) ## $ year: num [1:1924665] 1880 1880 1880 1880 1880 1880 1880 1880 1880 1880 ... ## $ sex : chr [1:1924665] &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; ... ## $ name: chr [1:1924665] &quot;Mary&quot; &quot;Anna&quot; &quot;Emma&quot; &quot;Elizabeth&quot; ... ## $ n : int [1:1924665] 7065 2604 2003 1939 1746 1578 1472 1414 1320 1288 ... ## $ prop: num [1:1924665] 0.0724 0.0267 0.0205 0.0199 0.0179 ... babynames %&gt;% slice_sample(n = 5) ## # A tibble: 5 × 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2012 M Braheem 6 0.00000296 ## 2 1917 F Hermione 12 0.0000107 ## 3 1938 F Merriam 5 0.00000438 ## 4 1946 F Dorella 7 0.00000434 ## 5 2013 M Karson 977 0.000484 5.1.3 or the (ggplot2)movies package The index page for the movies dataset is here. # install.packages(&quot;ggplot2movies&quot;) library(ggplot2movies) data(movies) str(movies) ## tibble [58,788 × 24] (S3: tbl_df/tbl/data.frame) ## $ title : chr [1:58788] &quot;$&quot; &quot;$1000 a Touchdown&quot; &quot;$21 a Day Once a Month&quot; &quot;$40,000&quot; ... ## $ year : int [1:58788] 1971 1939 1941 1996 1975 2000 2002 2002 1987 1917 ... ## $ length : int [1:58788] 121 71 7 70 71 91 93 25 97 61 ... ## $ budget : int [1:58788] NA NA NA NA NA NA NA NA NA NA ... ## $ rating : num [1:58788] 6.4 6 8.2 8.2 3.4 4.3 5.3 6.7 6.6 6 ... ## $ votes : int [1:58788] 348 20 5 6 17 45 200 24 18 51 ... ## $ r1 : num [1:58788] 4.5 0 0 14.5 24.5 4.5 4.5 4.5 4.5 4.5 ... ## $ r2 : num [1:58788] 4.5 14.5 0 0 4.5 4.5 0 4.5 4.5 0 ... ## $ r3 : num [1:58788] 4.5 4.5 0 0 0 4.5 4.5 4.5 4.5 4.5 ... ## $ r4 : num [1:58788] 4.5 24.5 0 0 14.5 14.5 4.5 4.5 0 4.5 ... ## $ r5 : num [1:58788] 14.5 14.5 0 0 14.5 14.5 24.5 4.5 0 4.5 ... ## $ r6 : num [1:58788] 24.5 14.5 24.5 0 4.5 14.5 24.5 14.5 0 44.5 ... ## $ r7 : num [1:58788] 24.5 14.5 0 0 0 4.5 14.5 14.5 34.5 14.5 ... ## $ r8 : num [1:58788] 14.5 4.5 44.5 0 0 4.5 4.5 14.5 14.5 4.5 ... ## $ r9 : num [1:58788] 4.5 4.5 24.5 34.5 0 14.5 4.5 4.5 4.5 4.5 ... ## $ r10 : num [1:58788] 4.5 14.5 24.5 45.5 24.5 14.5 14.5 14.5 24.5 4.5 ... ## $ mpaa : chr [1:58788] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ Action : int [1:58788] 0 0 0 0 0 0 1 0 0 0 ... ## $ Animation : int [1:58788] 0 0 1 0 0 0 0 0 0 0 ... ## $ Comedy : int [1:58788] 1 1 0 1 0 0 0 0 0 0 ... ## $ Drama : int [1:58788] 1 0 0 0 0 1 1 0 1 0 ... ## $ Documentary: int [1:58788] 0 0 0 0 0 0 0 1 0 0 ... ## $ Romance : int [1:58788] 0 0 0 0 0 0 0 0 0 0 ... ## $ Short : int [1:58788] 0 0 1 0 0 0 0 1 0 0 ... movies %&gt;% slice_sample(n = 5) ## # A tibble: 5 × 24 ## title year length budget rating votes r1 r2 r3 r4 r5 r6 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Moonrake… 1958 82 NA 5.9 53 0 4.5 4.5 4.5 14.5 34.5 ## 2 Wells Fa… 1937 97 1.5e6 7 41 4.5 0 0 4.5 0 24.5 ## 3 Forest W… 1996 93 NA 3.7 162 24.5 14.5 4.5 4.5 4.5 4.5 ## 4 Rendez-v… 1955 75 NA 7.1 7 0 0 0 0 14.5 14.5 ## 5 Alibi 1955 103 NA 6 10 0 0 0 0 14.5 64.5 ## # ℹ 12 more variables: r7 &lt;dbl&gt;, r8 &lt;dbl&gt;, r9 &lt;dbl&gt;, r10 &lt;dbl&gt;, mpaa &lt;chr&gt;, ## # Action &lt;int&gt;, Animation &lt;int&gt;, Comedy &lt;int&gt;, Drama &lt;int&gt;, ## # Documentary &lt;int&gt;, Romance &lt;int&gt;, Short &lt;int&gt; Regardless of whether you have played with one or both of these datasets, worked with the tutorials, or something else, please be prepared to share your experiences with the class at our next meeting. 5.2 assignment In our next meeting, go as far as you can with the following: Open the dataset. Describe the data in a paragraph based on one or more R functions (such as str, glimpse, and slice). What are the variables? What are the observations? What are the data types? What are the ranges of the variables? Are there missing values? After looking at the data, describe one or more questions of interest that you would like to ask about the data. (I do mean “of interest” - something that has meaning, that people would actually like to know). Write each question in a separate paragraph. Use headings to structure your document. Describe, in words, how you would do look at your question. Be as specific as possible, but don’t worry about R syntax (e.g., I would pull out such-and-such variables, or such-and-such observations, and I would compare them with x, or I would like this with ‘y’). Explain what you might find, and why (again) that would be interesting. Now draw the rest of the owl - translate your words into code, and run the analysis. If appropriate, describe what a graph or visualization of the data might look like. go for it if you can. Save your work as an R markdown document, and knit it to an html file. "],["literate-programming.html", "6 literate programming 6.1 projects are directories 6.2 scripts are files of code 6.3 R markdown / Quarto documents combine scripts with comments and (once knit) results 6.4 some elements of coding style 6.5 What to do when you are stuck", " 6 literate programming Showing your work, to (future) you as well as to others, is a key part of reproducible science. R Markdown documents facilitate this, as they allow you to include comments, code, and results in a single place. But before we consider R markdown, we begin with two more elemental ideas: projects and scripts (R4DS, Chapter 6) . 6.1 projects are directories You should save your work in projects. These isolate your data and scripts into discrete directories. There are two reasons I begin with ‘projects:’ The first is that students who are new to coding will often struggle to find their datasets and code on their personal machines; having a project directory makes things easier. The second is that , down the road, it’s likely that you will be working on R for different things in parallel - for this and another class, for this class and your thesis, or perhaps for two distinct types of analysis within your thesis. Fig 6.1: Left panel: Files pane in RStudio for this manuscript. Right panel: Menu showing some other recent projects. When you open up an R project, you’ll be in the right directory, with the relevant files (and only the relevant files) at your fingertips in the files pane. It is yet another way in which the notion of ‘tidiness’ facilitates our work. 6.2 scripts are files of code To do simple exercises in R, you can enter code directly in the Console pane (the default is in the lower left of the RStudio screen), then get an instant response. This (interactive) approach to coding is quick, but it is difficult to recreate. For example, imagine that I were doing an analysis between age and a personality trait that, in one dataset, is referred to as ‘Neuroticism’ (N) and, in a second, the same trait is reverse scored as ‘Emotional Stability’ (ES). If I want to combine measures of N and ES from two different datasets, each of which has scores for the trait on a 1-7 (or 7-1) scale, I could reverse one of these. My code might look like this: 6.2.0.0.1 combine two small datasets, reverse one of them, print first and last few rows: library(tidyverse) file1 &lt;- read_csv(&quot;data/datawithN.csv&quot;) ## Rows: 10 Columns: 4 ## ── Column specification ──────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): gender ## dbl (3): id, age, N ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. glimpse (file1) ## Rows: 10 ## Columns: 4 ## $ id &lt;dbl&gt; 101, 102, 103, 104, 105, 106, 107, 108, 109, 110 ## $ age &lt;dbl&gt; 23, 55, 31, 19, 28, 24, 45, 32, 23, 44 ## $ N &lt;dbl&gt; 6, 4, 5, 3, 2, 7, 1, 1, 1, 1 ## $ gender &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, NA, &quot;M&quot;, &quot;F&quot; file2 &lt;- read_csv(&quot;data/datawithES.csv&quot;) ## Rows: 10 Columns: 4 ## ── Column specification ──────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): gender ## dbl (3): id, age, ES ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. glimpse (file2) ## Rows: 10 ## Columns: 4 ## $ id &lt;dbl&gt; 201, 202, 203, 204, 205, 206, 207, 208, 209, 210 ## $ age &lt;dbl&gt; 19, 41, 27, 27, 48, 21, 20, 26, 42, 37 ## $ ES &lt;dbl&gt; 3, 6, 2, 5, 4, 1, 1, 1, 7, 7 ## $ gender &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;M&quot;, &quot;F&quot; combinedfile &lt;- file1 %&gt;% mutate(ES = 8 - N) %&gt;% # Creates &#39;ES&#39; from &#39;N&#39; for file1 select (-N) %&gt;% bind_rows(file2) combinedfile %&gt;% slice(c(1:3,18:20)) # the &#39;c&#39; is for combine or concatenate ## # A tibble: 6 × 4 ## id age gender ES ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 101 23 M 2 ## 2 102 55 M 4 ## 3 103 31 F 3 ## 4 208 26 f 1 ## 5 209 42 M 7 ## 6 210 37 F 7 You’ll want a record of your code for even simple transformations such as this one. R4DS Chapter 6 shows the R studio interface and encourages you to save your work in scripts. These are written in the source (editor) window in the upper left quadrant of the default R studio screen. 6.3 R markdown / Quarto documents combine scripts with comments and (once knit) results The objectives described in the prior section lead naturally to a consideration of R Markdown documents, which allow you to include comments, scripts, and results in a single place. In R4DS, Wickham [@-wickham2023] describes the use of Quarto rather than R markdown. Regardless of whether you use Quarto (see Chapter 28 of R4DS, or the tutorial here) or R Markdown (see the tutorial here), I encourage you to use one of these powerful, organizing approaches for nearly everything you do in R. There are as many as four parts of an R markdown or Quarto document: A YAML (yet another markdown language) header or metadata Text formatted in markdown R code (chunks) surrounded by code fences and, occasionally, inline code 6.4 some elements of coding style Good coding is often a combination of several skills ranging from puzzle-solving to communication. I can’t claim that these are the elements of coding style (apologies to Strunk &amp; White), but rather that these are merely some of the elements. Good coding is clear and thus commented. You are writing for your future self as well as others, so be explicit about the purpose of each chunk of code. Good coding is concise. When you can write code in 3 lines instead of 30, your code may be more clear and efficient. Take pleasure in writing parsimonious, efficient code. But where efficiency and clarity conflict, choose the latter. Good code should be complete, including all steps from reading the data to producing output. Where appropriate, comment-out (rather than delete) informative errors, again for the future you. Good code may be creative. The coolest solutions are those which pull from and synthesize a number of ideas. Creativity often requires walking away from a problem in order to ultimately arrive at a solution (Wertheimer’s Productive Thinking). Finally, good code should be considered. Reflect on the impacts of your work - just because you can analyze something doesn’t mean that you should. 6.5 What to do when you are stuck Google. pay attention to your error messages Ask for help, make your questions clear and reproducible (see R4DS Chapter 1) Take a break, think outside the box and kludge something together if you have to Document your struggles and your cleverness for a future you "],["principles-of-data-visualization.html", "7 principles of data visualization 7.1 some opening thoughts 7.2 some early graphs 7.3 Tukey and EDA 7.4 approaches to graphs 7.5 Tufte: first principles 7.6 the politics of data visualization 7.7 the psychology of data visualization 7.8 exercises 7.9 further reading and resources 7.10 addendum", " 7 principles of data visualization 7.1 some opening thoughts Graphs aren’t just to inform, but to make you reflect. We are concerned not just with the design of graphs, but with our role in society as leaders, as arbiters of facts, as educators. We want to foster data literacy. Graphs and other data visualizations are arguably the most important tool we have in scientific communication. How do people understand graphs? How can you use graphs to tell people the truth in a way that they understand? How can you convey uncertainty in a graph? When you see a graph, what do you notice, what do you wonder, and what is the story? Is “story-telling” what visualizations should be about? 7.2 some early graphs Visual displays of information reach back to prehistoric times; modern graphs date back, arguably, to Playfair’s 1786 Political Atlas - in which “… spatial dimensions were used to represent nonspatial, quantitative, idiographic, empirical data. Although it now seems natural to represent, for example, rising and falling imports over time as a rising and falling line, it does not seem to have been done before that time and was quite an accomplishment. Notably, in addition to the statistical line chart, Playfair at one fell swoop single-handedly invented most of the remaining forms of the statistical graphic repertoire used today-the bar chart and histogram, the surface chart, and the circle diagram or”pie chart” (H. Wainer and Thissen 1981). Fig 7.1: Playfair’s 1786 analysis of trade deficits The most celebrated early graph is that of Minard: Fig 7.2: Minard’s display of Napoleon’s catastrophic assault on Moscow, 1812 The visualization depicts the size, latitude, and longitude of Napoleon’s army as they moved towards (tan line) then away (black line) from Moscow; temperature during the retreat is plotted as well. Further, vertical lines linking the temperature display to the number of troops indicate the often perilous river crossings which further decimated Napoleon’s troops). Cheng (2014) decomposes the graph and provides some simpler visualizations; she also provides the following background: “Czar Alexander of Russia sees that Napoleon was becoming too powerful, so he refuses to participate in this embargo [against the UK]. Angry at Czar Alexander’s decision, Napoleon gathers a massive army of over 400,000 to attack Russia in June of 1812. While Russia’s troops are not as numerous as France’s, Russia has a plan. Russian troops keep retreating as Napoleon’s troops move forward, burning everything they pass, ensuring that the French forces could not take anything from their environment. Eventually the French army follows the Russian army all the way to Moscow during October, suffering major losses from lack of food. By the time Napoleon gets to Moscow, he knows he has to retreat. As winter settles into Europe and the temperature drops, Napoleon’s troops suffer even more losses, returning to France from lack of food, disease, and weather conditions.” Of course, the casualties and retreat of Napoleon’s army are immortalized not just in this graph, but also in Russian literature (Tolstoy’s War and Peace) and music (Tchaikovsky’s 1812 overture, in which five cannon shots mark the battle of Borodino and eleven more mark the arrival of Napoleon in the now-razed city of Moscow). 7.3 Tukey and EDA For Donoho (2017), the publication of John Tukey’s “Future of Data Analysis” (1962) arguably marks the beginning of data science. As the first data scientist, Tukey embraced a descriptive and exploratory approach to data analysis, particularly in his publication of Exploratory Data Analysis (EDA, Tukey 1977). In EDA, Tukey presented an idiosyncratic, coherent approach to looking at data, beginning with tallying. The distributions of small counts of single variables, for Tukey, could best be presented in stem and leaf displays. Comparisons between groups can be presented in box plots. To examine relationships between variables and the adequacy of simple models, he argues for thoughtfully transforming data to uncover linear trends, then to examine residuals to find where these trends do not hold. 7.4 approaches to graphs A graph might begin with perception and understanding (the consumer), with knowledge and design values (the producer), but it also reflects the truth of the data. How much is each? In thinking about how to design graphs, we can begin with abstract theory, with principles of design informed by our understanding of perception, or with empirical analyses of understanding and memory. 7.5 Tufte: first principles Tufte (2001) describes Graphical Excellence: Graphs should, among other things, “Induce the viewer to think about the substance, rather than about methodology, graphic design, the technology of graphic productions, or something else.” Graphs should “Present many numbers in a small space, make large data sets coherent, and encourage the eye to compare different pieces of data.” Graphs should “serve a reasonably clear purpose: description, exploration, tabulation, or decoration [and] be closely integrated with the statistical and verbal descriptions of a data set.” Tufte concludes with the following Principles of Graphical Excellence, which I quote verbatim: Graphical excellence is the well-designed presentation of interesting data—a matter of substance, of statistics, and of design. Graphical excellence consists of complex ideas communicated with clarity, precision and efficiency. Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space. Graphical excellence is nearly always multivariate. And graphical excellence requires telling the truth. 7.6 the politics of data visualization On a cold morning in January, 1986, Space Shuttle Challenger lifted off from Cape Canaveral. Because of the cold weather, engineers at Morton Thiokol, who designed the rocket boosters, considered the possibility that the O-rings which sealed joints on the rockets would be too hard and brittle to prevent the release of potentially explosive fuel. They examined the relation between temperature and o-ring damage on prior flights, using the following visualization: Figure 7.3: What Morton Thiokol engineers saw prior to deciding to launch the Challenger Space Shuttle in January, 1986 What, if anything, can we see here? The graphic was chaotic and poorly designed; if the engineers had wanted to systematically explore the relation between o-ring damage and temperature they could have removed the rockets and arranged them on a single axis by liftoff-temperature rather on several rows by liftoff-date. Here’s what they would have seen: Figure 7.4: What the engineers could have seen, perhaps, with a better graph. The forecast for the morning of the launch was far colder than that for prior launches, and within the set of prior launches a clear relationship between temperature and o-ring damage is evident. But the Challenger did launch, exploding shortly after takeoff, killing the seven astronauts on board. With a clearer display of the data, the Challenger launch would likely have been postponed (Tufte 2001). 7.6.1 poor design leads to an uninformed or misinformed world In the flood of information that surrounds us, our peripheral and sometimes our focal attention will be drawn to pictorial summaries of consequential events and effects. But while data visualizations and data journalism has become more responsible, poorly designed graphs - what Tufte has described as “chartjunk” - are still common. Poorly designed graphs don’t just confuse rocket scientists. They reach us, mislead us, and make fools of all of us as well. With better graphs, we become a better informed, better educated citizenry. 7.6.2 poor design can be a tool to deceive Figure 7.7: Trump as “the first datavis President” (Meeks, 2019). The apps on our smartphones and electronic devices are meticulously designed to maintain our attention. They are largely successful, but, for many of us, made it more difficult to read in depth (Wu, The Attention Merchants). With information all around us demanding our attention, visual representations of data have become particularly important. The data visualization is, increasingly, not just a supplement to the story, but the story itself. When this premise is coupled with the idea that the truth is malleable, the product of what our leaders say rather than empirical statements of fact, we approach a dangerous situation, one in which real threats are misrepresented and lives are again endangered. Presenting information in self-promoting ways includes so-called “Sharpie-gate,” where President Trump simply altered a hurricane prediction map in defense of a misstatement. 7.7 the psychology of data visualization Speaking of America, consider the following: Figure 7.8: Chernoff’s too-clever faces In this figure, from Wainer (1981), Chernoff’s faces are used to represent multivariate data. The display is loosely based on the (psychological) premise that, from infancy, we are inordinately sensitive to facial features. Consequently, the most powerful, effective, or informative data displays should manipulate facial features. Does this graph succeed? Why or why not? Would an updated model, based on richer, more complex, and more realistic simulations of facial expressions (Thies et al. 2015) be more successful? 7.7.1 the power of animation Animated data displays bring the dimension of time into data visualization. Here are two brief (&lt; 5 minutes) animated data displays, each powerful in its own way, and each of which gives the viewer an appreciation of an important social phenomenon. The first is from the late Hans Rosling, and summarizes a chunk of the Gapminder data. There is a Gapminder package in R if you would like to explore the data further, and perhaps become the next Rosling: Video 7.9: Rosling and social progress The second is from Kim Rees and her ex-colleagues at Periscopic (Rees is now at CapitalOne). For me, it’s an important graphic because it tries to overcome what has been called “psychic numbing” - we are sensitive to the individual case, but lose our perspective when faced with large-scale tragedies, so that effectively the more lives lost… the less we care (Slovic et al. 2013). Video 7.10: Rees and stolen years 7.7.2 telling the truth when the truth is unclear We live in an uncertain world, and a major challenge in data visualization is how to convey this uncertainty in a way that people understand readily. Examples of this are familiar. Here in Florida, projected hurricane paths are frequently represented by a “cone of uncertainty” surrounding a single track. An alternative approach is to use a display which shows a distribution of possible tracks. Figure 7.11: Two approaches to displaying hurricane paths 7.7.3 visualizing uncertainty To further illustrate the idea of uncertainty, data visualizations may be animated. To display hurricane outcomes, for example, individual lines in a spaghetti plot might fade in and out (Cox and Lindell 2013). Another use of animation is suggested by (Hullman, Resnick, and Adar 2015) who use hypothetical outcome plots rather than simple error bars to illustrate sampling distributions. Empirically, subjects have a better understanding of the meaning of statistical effects when these are represented by dynamic displays. During the 2016 US Presidential Election, the NY Times graphics presented the range of likely possibilities (here, between the 25th and 75th percentiles) as a dynamically changing “jittery gauge” . Following the election, there was some criticism of these gauges, as readers claimed that they tended to make them anxious. However, it appears that at least some of the anxiety was attributable not to the gauges themselves, but to the electoral outcome itself. The gauges were back in 2018, and will likely be used again in the future. Finally, animation may be used not just to display uncertainty or a possible range, but also data displays which both carry a great deal of information and unfold over time, such as the language of personality development in my own work. 7.8 exercises Please consider each of the following data visualizations. Smartphone sales Figure 7.12: Which smartphone manufacturers are doing well? Look at the smartphone sales visualization for just a moment, as if you might while reading something else on another screen, or as your ten-year old little sister might look at it, or most of your classmates in high school, or the average person you might see at a rest stop on the Florida Turnpike. What does the graph tell them? Now look at the diagram more closely. What does it actually say? How could you improve it to make the content of the graph more informative about its title? Why was the graph designed in this way? Does this matter? Stand your ground Figure 7.13: Does “stand your ground” make us safer? Again - Glance at the visualization, as you might glance at dozens of images on &gt; your phone or computer screen. What does the graph tell you? Again, look at it more closely. What does it actually say? Would you change the figure? If so, how? Would your changed visualization be “better”? If yes, why? Big businesses Figure 7.14: Voronoi’s “Top 50 Most Profitable Companies” Finally, What does this visualization tell you at first glance? Do you think this is a good visualization? Would Tufte agree? Why or why not? Are Tufte’s principles of data visualization sound? It’s been argued that we live in an attention economy, in which ‘eyes’ (e.g., in a screen) are more valuable than wealth. How does this relate to the design of data visualizations? 7.9 further reading and resources If you’d like to learn more, Tufte (2001) and his other books are beautiful and thought provoking. Cleveland (1985) examines graphs from a more rigorous psychological and empirical viewpoint. The Data Stories podcasts are often excellent, despite the challenge of an auditory medium for visual display (especially the episode on Hans Rosling). And Healy(2017) provides a comprehensive introduction that, in essence, provides the basis for a semester-long course based in the ggplot2 package in R that we will be working with in the next chapter (Wickham, Çetinkaya-Rundel, and Grolemund 2023). 7.10 addendum 7.10.1 some notes on Healy References Cleveland, William S., and Robert McGill. 1985. “Graphical Perception and Graphical Methods for Analyzing Scientific Data.” Science, New Series 229 (4716): 828–33. https://www.jstor.org/stable/1695272. Cox, Jonathan, and Michael Lindell. 2013. “Visualizing Uncertainty in Predicted Hurricane Tracks.” International Journal for Uncertainty Quantification 3 (2). https://doi.org/10/gjjsfw. Donoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734. Healy, Kieran. 2017. “Data Visualization for Social Science: A Practical Introduction with r and Ggplot2.” Hullman, Jessica, Paul Resnick, and Eytan Adar. 2015. “Hypothetical Outcome Plots Outperform Error Bars and Violin Plots for Inferences about Reliability of Variable Ordering.” Edited by Elena Papaleo. PLOS ONE 10 (11): e0142444. https://doi.org/10.1371/journal.pone.0142444. Slovic, Paul, David Zionts, Andrew K Woods, Ryan Goodman, and Derek Jinks. 2013. “Psychic Numbing and Mass Atrocity.” The Behavioral Foundations of Public Policy, 126–42. https://doi.org/10/gk4945. Thies, Justus, Michael Zollhöfer, Matthias Nießner, Levi Valgaerts, Marc Stamminger, and Christian Theobalt. 2015. “Real-Time Expression Transfer for Facial Reenactment.” ACM Trans. Graph. 34 (6): 183–81. https://doi.org/10/f7wqz7. Tufte, Edward R. 2001. The Visual Display of Quantitative Information. 2nd ed. Cheshire, CT: Graphics Press. Tukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67. https://doi.org/10/d48nqg. ———. 1977. “EDA: Exploratory Data Analysis.” Reading, Mass. Wainer, H, and D Thissen. 1981. “Graphical Data Analysis,” 51. Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science 2e. \" O’Reilly Media, Inc.\". "],["visualization-in-r-with-ggplot.html", "8 visualization in R with ggplot 8.1 a picture &gt; (words, numbers)? 8.2 Read Wickham’s opening chapter 8.3 explore", " 8 visualization in R with ggplot In the last chapter, we introduced data visualization, citing “vision-aries” including Edward Tufte and Hans Rosling, inspired works such as Minard’s Carte Figurative and Periscopic’s stolen years, as well as a few cautionary tales of misleading and confusing graphs. Here, in playing with and learning the R package ggplot, we begin to move from consumers to creators of data visualizations. As the first visualization in (Wickham, Çetinkaya-Rundel, and Grolemund 2023) reminds us, data visualization is at the core of exploratory data analysis: Fig 8.1: Data visualization is at the core of data analysis ((Wickham, Çetinkaya-Rundel, and Grolemund 2023)) In the world of data science, statistical programming is about discovering and communicating truths within your data. This exploratory data analysis is the corner of science, particularly at a time in which confirmatory studies are increasingly found to be unreproducible. Most of your reading will be from Chapter 1 of (Wickham, Çetinkaya-Rundel, and Grolemund 2023), this is intended only as a supplement. 8.1 a picture &gt; (words, numbers)? The chapter begins with a quote from John Tukey about the importance of graphs. Yet there is a tendency among some statisticians and scientists to discount graphs, to consider graphic representations of data as less valuable than statistical ones. It is true that, because there are many ways to graph data, and because scientists and data journalists are humans with pre-existing beliefs and values, a graphical displays should not be assumed to simply depict a singular reality. But the same can be said about statistical analyses (see Chapter 9). To consider the value of statistical versus graphical displays, consider ‘Anscombe’s quartet’ (screenshot below, live at http://bit.ly/anscombe2019): Table 8.1: An adaptation of Anscombe’s “quartet” (Anscombe 1973) Exercise 8_1 Consider the spreadsheet chunk presented above, which I am characterizing as data collected on a sample of ten primary school children at recess on four consecutive days. Working with your classmates, compute the mean, standard deviation, and correlation between the two measures for one day. Share your results with the class. The four pairs of variables in (Anscombe 1973) appear statistically “the same,” yet the data suggest something else. Additional examples of the problem of relying on simple statistics, in particular correlation coefficients, are considered in the first chapter of Healy (2017). Perhaps graphs can reveal truths that statistics can hide. Exercise 8_2 The Anscombe data is included in base Ras a library in R. Can you find, load, and explore it? 8.2 Read Wickham’s opening chapter In class, we will review and recreate the plots in section 1.2 of (Wickham, Çetinkaya-Rundel, and Grolemund 2023) and exercises in 1.2.5 and 1.4.3 and 1.5.5 Savor this section: Read slowly, and play around with the RStudio interface. For example, read about the mpg data in the ‘help’ panel, pull up the mpg data in a view window, and sort through it by clicking on various columns. Fig. 8.2: A screenshot from RStudio, showing the mpg dataset 8.3 explore Try to make a cool graph - one that informs the viewer, and, to paraphrase Tukey, helps us see what we don’t expect. Try several different displays. Which fail? Which succeed? Be prepared to share your efforts. Remember the 15 minute rule, and don’t be afraid to screw up. Each mistake you wisdom. 8.3.0.1 some sources The Datacamp ggplot course https://app.datacamp.com/learn/courses/introduction-to-data-visualization-with-ggplot2 The Gapminder data https://cran.r-project.org/web/packages/gapminder/readme/README.html A graph in The Economist. Here, Andrew Couch, host of the “Tidy Tuesday” podcast, walks through the recreation of a fairly complex plot from The Economist. Follow along, beginning with the links to GitHub https://youtu.be/gcDQ_KbXQ3o?si=wFadvTi886hQm6H- For another example of an Economist-style visualization, there is also this analysis of Global Terrorism Data from Rpubs: https://rpubs.com/tangerine/economist-plot. (This appears to be a student assignment). As with the ’movies” data described in an earlier project, the link to the data is no longer valid. To access it, you can establish an account on Kaggle (which links to older data) and/or the global terrorism database at the University of Maryland. References Anscombe, FJ. 1973. “Graphs in Statistical Analysis.” American Statistician 27 (1): 17–21. Healy, Kieran. 2017. “Data Visualization for Social Science: A Practical Introduction with r and Ggplot2.” Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science 2e. \" O’Reilly Media, Inc.\". "],["on-probability-and-statistics.html", "9 on probability and statistics 9.1 on probability 9.2 the rules of probability 9.3 continuous probability distributions 9.4 dangerous equations 9.5 notes for next revision", " 9 on probability and statistics We previously considered Anscombe(1973) and his quartet, and how visualizing data is valuable. This week, we move to a brief discussion of principles of statistics. 9.1 on probability Discrete probability is used to understand the likelihood of categorical events. We can think of initial estimates of probability as subjective or personal. For some events (what is the probability this plane will crash?), an estimate of probability can be drawn from a base rate or relative frequency (e.g., p(this plane will crash) = (number of flights with crashes/ number of flights). For other events (e.g., what is the probability that a US President will resign or be impeached before completing their term of office?), it may be hard to arrive at a suitable base rate. Here, a number of subjective beliefs or principles may be combined to arrive at a subjective or personal probability. In a sense, all probability estimates begin with a personal belief such as this, in part because the choice of the most informative base rate is often not self-evident - in the plane crash example, maybe we should consider a reference group rates such as ‘for this airline,’ or ‘for this type of jet’ etc. The claim that there is a simple distinction between ‘relative frequencies’ and ‘personal probabilities’ turns out to be an oversimplification. Relative frequencies are not statistical givens, because in many of not most situations there is no single base rate that is clearly superior to others. For the plane crash example, we might consider crash rates among all planes, all jets, and all carriers, or particular planes (Boeing 737 Max jets), particular carriers (United), or the intersection of some or all of these as well as other variables (United 737 Max airliners flying out of LaGuiardia at night). There is no single answer to the plane crash estimate, in other words. Similarly, a baseball manager, in considering whether a pinch hitter might be brought in to bat at a crucial spot in a game, might consider an omnibus batting average (effectively a relative frequency of hits/opportunities), batting average at night, against this pitcher, etc. In general, there is not a correct answer to this “problem of the reference class” in part because a more precise reference group (737 Max planes, batting against a particular pitcher) is inherently based on a smaller sample of data, and is therefore less stable, than a broader, but coarser reference group upon which a probability estimate might also be based (Lanning 1987). The personal origins of probability estimates should become less important as we are exposed to data and revise our estimates in accordance with Bayes theorem. But over the last 50 years, a substantial body of evidence has demonstrates that, under at least some circumstances, we don’t make estimates of probability in this way. 9.2 the rules of probability Here’s an introduction to the principles of probability. These are presented, with examples and code, in this R markdown document at Harvard’s datasciencelabs repository: I. For any event A, 0 &lt;= P (A) &lt;= 1 II. Let S be the sample space, or set of all possible outcomes. Then P(S) = 1, and P (not S) = 0. III. If P (A and B) = 0, then P (A or B) = P (A) + P (B). IV. P (A|B) = P (A and B)/ P (B) Principle III applies for mutually exclusive events, such as A = you are in class this morning, B = you are at the beach this morning. For mutually exclusive (disjoint, disjunctive) events, the union is the sum of the two events. This is called the addition rule for disjoint events. A different rule applies for events that are mutually independent, such as (A = I toss a coin and it lands on ‘Heads’) and (B = it will rain tomorrow). What we mean by independent is that our estimates of the probability of one don’t change based on the state of the other - your estimate of the likelihood of rain shouldn’t depend on my coin flip. Here, you multiply rather than add: If P (A|B) = P (A), then P (A and B) = P (A) P (B). In words - if the probability of A given B equals the probability of A, then the probability of both A and B equals the probability of A times the probability of B. This multiplication rule is handy for estimating the probability of an outcome that happens following a chain of independent events, such as the probability that the next eight times I toss a coin it will land on “tails” every time: P (TTTTTTTT) = P (T) P (T) P (T) P (T) P (T) P (T) P (T) P (T). = .58 = 1/256. Many sets of events are neither disjoint nor independent, so we need more general ways of thinking about pairs of events. For most of us, Venn diagrams are useful to think about combining probabilities. The union or P (A U B) describes the probability that A, B, or both of these will occur. Here, you will use the general addition rule: P (A or B) = P (A) + P (B) - P (A and B) (the probability of A or B is the probability of A plus the probability of B minus the probability of both A and B). For the intersection or P (A ∩ B), we need to consider conditional probabilities. Think of the probability of two events sequentially: First, what’s the probability of A? Second, what’s the probability of B, given that A has occurred? Multiply these to get the likelihood of A and B: P (A and B) = P (A) P (B|A). Example: The probability of you and your roommate both getting mononucleosis equals the probability of your getting mono times the probability that your roommate gets it, given that you have it also. This is the general multiplication rule. In this abstract example, the order is irrelevant. To estimate the likelihood of A and B, we could as easily take the probability of B, and multiply it by the conditional probability of A given B P (A and B) = P (B) P (A|B). Use the mono example again. What are A and B here? Does it still make sense? When might P (B|A) make more sense than P (A|B)? We are often interested in estimating conditional probabilities, in which case we’ll use the same equation, but solve instead for P (A|B). This leads us back to principle IV: IV. P (A|B) = P (A and B)/ P (B) 9.2.1 keeping conditional probabilities straight In general, P (B|A) and P (A|B) are not equivalent. Here, one could use a Venn diagram to model this asymmetry. There are several R packages for doing this, including Venn and VennDiagram. 9.2.2 exercises Exercise 9.1. In 2024, the Florida Highway Patrol won a national competition for “best looking cruiser.” The winning car was a Dodge Charger. Not all FHP cruisers are Dodge Chargers, but some are. Assume that there are 8 million registered cars in Florida, that all cars (including all FHP cruisers) are registered, and that 80,000 of these are Dodge Chargers. On the basis of the above information, if you see a Dodge Charger on the road, can you compute the probability that it is an FHP cruiser (i.e., p(FHP cruiser | Dodge Charger)? If you can compute this, what is the probability? If you cannot compute this, what is the minimum additional information would you need to compute this probability (p(FHP cruiser | Dodge Charger)? Provide a reasonable estimate of this additional value, then compute (p(FHP cruiser | Dodge Charger). Working with your own numbers, what is p(Dodge Charger | FHP cruiser)? How confident are you in these results? Are there any additional assumptions that you might make that would make you more confident about your results? Exercise 9.2. Sketch out a Venn Diagram that accurately reflects the relationships you described in exercise 9.1. Use R to generate your Venn Diagram. Look at your figure. In general, if P (A|B) &lt; P (B|A), what must be true of the relationship of P (A) to P (B)? 9.3 continuous probability distributions We can also use probability with continuous variables such as systolic blood pressure (that’s the first one), which has a mean of approximately 120 and a standard deviation of 15. Continuous probability distributions are handy tools for thinking about the meaning of scores, particularly when we express scores in standard deviations from the mean (z scores). More to the point, this way of thinking about probability is widely used in questions of scientific inference, as, for example, in testing hypotheses such that “the average systolic blood pressure among a group of people studying at a coffee shop (hence caffeinated) will be significantly greater than that of the population as a whole.” This is part of the logic of Null Hypothesis Significance Testing (NHST) - if the result in my coffee shop sample is sufficiently high, then I say that I have rejected the null hypothesis, and found data which support the hypothesis of interest. 9.4 dangerous equations Just as Tufte (2001) demonstrated that poor data visualizations can be dangerous, leading, for example, to the loss of life in the Challenger disaster, Wainer (2007) shows that a lack of statistical literacy is also “dangerous.” Wainer cites three specific examples of important, yet widely misunderstood, statistical laws. The first of these is deMoivre’s equation, which shows that variability decreases with the square root of sample size. Because the variability of a sample decreases with the size of that sample, small samples tend to have extreme scores. For example, the counties with the highest and lowest rates of kidney cancer (or most other unexplained health measures) will be sparsely populated, typically rural places. For Wainer, a second form of statistical illiteracy is the failure to understand the complex interdependencies that arise in multiple regresson analysis, in particular, how coefficients may change or even reverse in sign when new variables are added as predictors. Wainer’s third example of statistical illiteracy is the failure to appreciate regression to the mean. I consider this to be the most dangerous form of statistical illiteracy, in part because regression effects contribute to an overestimate of the effectiveness of punishment and an under-appreciation of the effectiveness of positive reinforcement as tools for behavior change (Hastie and Dawes 2010). 9.5 notes for next revision This chapter should include a mnore explicit treatment of Bayes theorem as well as a demonstration of empirically generated probability distributions. References Anscombe, FJ. 1973. “Graphs in Statistical Analysis.” American Statistician 27 (1): 17–21. Hastie, Reid, and Robyn M Dawes. 2010. Rational Choice in an Uncertain World: The Psychology of Judgment and Decision Making. Sage. Lanning, Kevin. 1987. “Some Reasons for Distinguishing Between ‘Non-normative Response’ and ‘Irrational Decision’.” The Journal of Psychology 121 (2): 109–17. https://doi.org/10/fv4hh5. Tufte, Edward R. 2001. The Visual Display of Quantitative Information. 2nd ed. Cheshire, CT: Graphics Press. Wainer, Howard. 2007. “The Most Dangerous Equation.” American Scientist 95 (3): 249. https://doi.org/10.1511/2007.65.249. "],["reproducibility-and-the-replication-crisis.html", "10 reproducibility and the replication crisis 10.1 answers to the reproducibility crisis 10.2 further readings 10.3 notes for next revision", " 10 reproducibility and the replication crisis Probability theory is elegant, and the logic of Null Hypothesis Significance Testing (NHST) is compelling. But philosophers of science have long recognized that this is not really how science works (Lakatos 1969). That is, science is not primarily built by the testing and rejecting of null hypotheses. (Think, for example, of how you might describe an experiment for testing the hypothesis that gravity exists, and whether you would ever reject this hypothesis). The problem is a multifaceted one. It arises partly because, in any experiment, there is a large (infinite?) number of conditions which might be invoked to explain-away results of a failure of our work. It arises because we are human, and prone to various biases of decision making and information integration (that is, we are poor Bayesians). It arises, too, because scientific institutions such as journals, funding agencies, and universities are competitive environments which incentivize positive findings. In recent years, the tension between the false ideal of NHST and the real world of science has become increasingly evident. Within psychology, experimental studies have often - even typically - failed to replicate well-known results (Open Science Collaboration 2015). It’s not just psychology (Baker 2016). One of the first important papers to shine light in the area (Ioannidis 2005) came from medicine; it suggested six contributing factors, which I quote verbatim here: The smaller the studies conducted in a scientific field, the less likely the research findings are to be true. This stems directly from our discussion of the central limit theorem and the instability of results from small samples. The smaller the effect sizes in a scientific field, the less likely the research findings are to be true We’ll talk about effect size below. The greater the number and the lesser the selection of tested relationships in a scientific field, the less likely the research findings are to be true. (and) The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true. The “problem” of analytic flexibility leads to ‘p-hacking’ The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true and The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true. Positive findings rise, and negative ones are ignored. And scientists are human, and subject to incentives. Here’s a video which provides some more context for the crisis: . *Video 10.1: On the reproducibility crisis (12 mins) 10.1 answers to the reproducibility crisis For scientific progress to occur, there needs to be more than a simple rejection of what is wrong with current methods and research programs: Here, as elsewhere, criticism must be constructive to be valuable. There have been a number of solutions proposed to the reproducibility crisis. 10.1.1 partial answer 1: Tweak or abandon NHST The first cluster of responses to the reproducibility crisis is concerned with statistics, specifically problems with Null Hypothesis Significance Testing (NHST). These include (a) justifying one’s alpha - making it more stringent, for example, for counter-intuitive claims (Grange et al. 2018), (b) changing the default p value from .05 to .005 (Benjamin et al. 2018), and (c) abandoning significance testing altogether (McShane et al. 2017). (Szucs and Ioannidis 2017) goes into some of these issues in more detail and discusses other limitations of significance testing, including the dichotomous, all-or-none silliness of the accept/reject decision. (If you play the NHST game, there is no ‘almost’ significant, ‘approached significance,’ ‘highly significant’, etc.). (Leek and Peng 2015) argue that the problems are not merely with NHST, but with the whole of data analysis. They maintain that better training in data science - courses like ours, perhaps, are part of the answer. (figure) (Munafò et al. 2017) also argue that threats to reproducible science occur at a number of places in science, not just with the evaluation of hypotheses. munafo2017 10.1.2 partial answer 2: Keep a log of every step of every analysis in R markdown or Jupyter notebooks A second cluster of responses is concerned with keeping good records. Let’s say that you are running a study, say, which looks at the hypothesis of differential variation of females and males in a cognitive measure; your interest is to critically examine the hypothesis discussed by (Howard Wainer 2007) that males show more variability. There have been a lot of studies related to this over the years, so that rather than collect new data you decide that you will work with existing data from several online archives. You find and download spreadsheets from two studies: In the first, gender is coded ‘1’ for male, ‘2’ for female. In the second, gender is coded ‘1’ for female, ‘2’ for male, and ‘3’ for other. There are, in essence, two ways that you can combine the variables into a common format: The first would be to take one of the spreadsheets and do a few find-and-replace commands on the appropriate column of the data. This is quick and easy - but when someone else, or even future you, returns to this data, you will not remember if you have recoded it. The alternative is to keep a record of your work in R markdown. This is more time consuming, and can sometimes be clumsy. But it is virtuous useful and clear - and when you screw up, you will have a full record of what happened. Part of the problem of scientific reproducibility is to keep comprehensive records. This record-keeping and research transparency is at the heart of R markdown documents. 10.1.3 partial answer 3: Pre-registration of your research The third answer to the reproducibility crisis is the most comprehensive; it involves not merely keeping a record of what you have done, but preregistering your work, that is, fully specifying your planned analyses beforehand (Miguel et al. 2014). The author, an economist, outlines his argument in a five-minute video here. For randomized controlled trials, consider socialscienceregistry.org, and for more general use, use the open science framework page. 10.2 further readings Finally, if you would like to learn more about the reproducibility crisis, there is a recent collection of papers in Nature here. 10.3 notes for next revision This chapter could include more recent scholarship on the reproducibility crisis and links to sites including OSF. It could also introduce an empirical approach to reproducibility (e.g., k-fold sampling). References Baker, Monya. 2016. “Is There a Reproducibility Crisis?” Nature 533: 26. Benjamin, Daniel J., James O. Berger, Magnus Johannesson, Brian A. Nosek, E.-J. Wagenmakers, Richard Berk, Kenneth A. Bollen, et al. 2018. “Redefine Statistical Significance.” Nature Human Behaviour 2 (1): 6–10. https://doi.org/10/cff2. Grange, JA, D Lakens, F Adolfi, C Albers, F Anvari, M Apps, S Argamon, et al. 2018. “Justify Your Alpha.” Nature Human Behavior. Ioannidis, John PA. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124. https://doi.org/10/chhf6b. Lakatos, Imre. 1969. “Falsification and the Methodology of Scientific Research Programmes.” Criticism and the Growth of Knowledge. Cambridge University Press: Cambridge. Leek, Jeffrey T, and Roger D Peng. 2015. “Statistics: P Values Are Just the Tip of the Iceberg.” Nature 520 (7549): 612. https://doi.org/10/gfb8jm. McShane, Blakeley B., David Gal, Andrew Gelman, Christian Robert, and Jennifer L. Tackett. 2017. “Abandon Statistical Significance.” arXiv:1709.07588 [Stat], September. https://arxiv.org/abs/1709.07588. Miguel, Edward, Colin Camerer, Katherine Casey, Joshua Cohen, Kevin M Esterling, Alan Gerber, Rachel Glennerster, et al. 2014. “Promoting Transparency in Social Science Research.” Science 343 (6166): 30–31. https://doi.org/10/gdrcpz. Munafò, Marcus R., Brian A. Nosek, Dorothy V. M. Bishop, Katherine S. Button, Christopher D. Chambers, Nathalie Percie du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J. Ware, and John P. A. Ioannidis. 2017. “A Manifesto for Reproducible Science.” Nature Human Behaviour 1 (1): 0021. https://doi.org/10.1038/s41562-016-0021. Open Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716–16. https://doi.org/10/68c. Szucs, Denes, and John Ioannidis. 2017. “When Null Hypothesis Significance Testing Is Unsuitable for Research: A Reassessment.” Frontiers in Human Neuroscience 11: 390. https://doi.org/10/gc6vws. Wainer, Howard. 2007. “The Most Dangerous Equation.” American Scientist 95 (3): 249. https://doi.org/10.1511/2007.65.249. "],["wrangling-and-tidying.html", "11 wrangling and tidying 11.1 the structure of the tidyverse 11.2 where should we eat? 11.3 more on tidy coding", " 11 wrangling and tidying wild horses are beautiful, wild data … not so much The term ‘wrangling’ has been used to describe the process by which unruly, messy, and complex datasets are organized and restructured so that they can be summarized, interpreted, and understood. Wrangling includes finding errors, which requires looking at your data closely enough to identify problems, and transforming the data as needed to allow data analyses to take place. Tidying is also concerned with data preparation, but is focused on the narrower task of arranging data into simple data frames (tibbles) in which each variable is in its own column, each observation is in its own row, and each cell includes one and only one value. In practice, the processes of data wrangling and data tidying are often overlapping. In this chapter, we will explore (a little) wrangling and (some) tidying using simulated data. First, we briefly consider some characteristics of the tidyverse 11.1 the structure of the tidyverse The R programming language was initially intended to serve engineers, and the tidyverse can be seen as a collection of packages which are aimed at a broader audience of statisticians and data scientists, and to help them become more productive (R. Peng 2018). Tidyverse packages have a common syntax that makes it easier to generate and understand code and create reproducible results. Tidyverse packages may be described as having three layers. The core of the tidyverse includes nine packages. You will see the list of these packages when you load the tidyverse: library(tidyverse) Each package includes a number of functions. Many tidyverse packages are described on handy cheatsheets such as this one for dplyr: The periphery of the tidyverse consists of additional packages which are less commonly invoked than those in the tidyverse core. Unlike the core packages, all of these peripheral or auxiliary packages must be loaded into memory by an explicit statement, e.g., library(googlesheets4). You can see the list of all (core and peripheral) packages with the tidyverse_packages command: &gt; # install.packages(“tidyverse”) &gt; tidyverse_packages () For me, the most useful packages in this peripheral or auxiliary layer have been googledrive, googlesheets4, and readxl (for working with Google apps and Excel files). I’ve also used rvest, xml2, and jsonlite (for web scraping and parsing more complex data structures). In addition to the core and periphery of the tidyverse, there is a more loosely-defined third layer of what may be considered to be tidyverse-friendly packages. These packages, which are not installed with the core and peripheral tidyverse, include a wide range of tools that are handy for using tidy syntax in particular applications, such as janitor (for cleaning data), tidytext (for text analysis), and tidygraph (for network analysis). When you use these, you’ll need to first make sure that they are installed on your machine. (R studio will remind you to do this). Then you can load them into memory. At each of these three levels, the tidyverse is constantly evolving. For example, at this writing, one of the peripheral or auxiliary packages (httr) has been superseded by a newer package (httr2); despite this, httr (and not httr2) remains in the auxiliary layer. 11.2 where should we eat? Tidyverse functions from at least two of the core tidyverse packages, dplyr and tidyr, help wrangle, clean, and shape our data into a tidy form that can ‘spark joy’ (Kondo 2016). To learn more about tidy data and how we use dplyr and tidyr in everyday coding, we will construct and explore a simulated dataset, consisting of a 100 restaurants, each of which is described by just a few variables. To do this, we’ll first name the restaurants. We begin with the data from the babynames package, then will use filter function (from dplyr) to choose only names since 1981, then we group and summarize the data by name, then arrange (sort) the names by popularity, then slice (take only) the first 100 observations, then finally select only the name column. Each of these functions is part of the dplyr package: library(babynames) # make restaurant names restodata &lt;- babynames |&gt; filter(year &gt; 1980) |&gt; group_by(name) |&gt; summarise(n=sum(n)) |&gt; arrange(desc(n)) |&gt; slice(1:100) |&gt; select(name) head(restodata,3) ## # A tibble: 3 × 1 ## name ## &lt;chr&gt; ## 1 Michael ## 2 Christopher ## 3 Matthew In the prior chunk of text we wrangled - or, if you prefer, massaged - the babynames data in a few ways to get us a simple list of 100 popular names. But this is only a start. Let’s change the set of ‘person names’ into ‘restaurant names.’ To do this, we will use mutate (another dplyr function) then combine each name with the phrase ’s_place.’ To combine strings, we use the str_c function. This is from the stringr package, which is another package in the core of the tidyverse: restodata &lt;- restodata |&gt; mutate (resto_name = str_c(name,&quot;s_place&quot;)) head(restodata,3) ## # A tibble: 3 × 2 ## name resto_name ## &lt;chr&gt; &lt;chr&gt; ## 1 Michael Michaels_place ## 2 Christopher Christophers_place ## 3 Matthew Matthews_place Now, assume that the restaurants has been graded by the Department of Health on an A to F scale. We’ll assign these health ratings randomly, using the following three steps. First, we seed the (pseudo) random number generator; this allows the result to be reproducible. If you don’t do this, you will almost certainly get different ratings each time you run the code. Then, we declare health as a random integer for each restaurant on 1 to 5 scale. We can do this in several ways. Here, I use the sample function to directly generate my 100 integers. (I originally used runif, which generates random numbers, then rounded this score to the nearest integer, but this is more complicated, and leads to fewer observations with extreme values. It’s commented out in my code.). Then, we recode these as letter grades using the case_when function. This function is akin to an ‘if-else’ statement. Note that the case_when syntax is tricky. If we think of the double equals as ‘is equal to’, the tilde as ‘call it’, and the comma as ‘else,’ the first part of the case when statement becomes the following: when health is equal to 1, call it F, else … In addition to health, assume that we have cost data as well. We’ll use essentially the same code to come up with our cost variable. Then we drop our original name variable. # add health ratings set.seed(33458) restodata &lt;- restodata |&gt; # mutate(health = round(runif(n=n(),1,5))) |&gt; mutate (health = sample(1:5, 100, replace=T)) |&gt; mutate(health = case_when(health == 1 ~ &#39;F&#39;, health == 2 ~ &#39;D&#39;, health == 3 ~ &#39;C&#39;, health == 4 ~ &#39;B&#39;, health == 5 ~ &#39;A&#39;)) |&gt; mutate(cost = sample(1:3, 100, replace=T)) |&gt; mutate(cost = case_when(cost == 1 ~ &#39;$&#39;, cost == 2 ~ &#39;$$&#39;, cost == 3 ~ &#39;$$$&#39;)) |&gt; select(-name) head(restodata,3) ## # A tibble: 3 × 3 ## resto_name health cost ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Michaels_place B $$$ ## 2 Christophers_place A $$ ## 3 Matthews_place C $$$ Each restaurant now has a restaurant name, a health rating, and a classification as cheap ($), moderate ($$), or expensive ($$$). Now we add an additional variable - popularity. This is again randomly generated on a 1 to 5 scale. But restaurants that are popular at lunch may not be popular at breakfast or dinner (they may cater to office workers, for example), and some restaurants may be closed for one or more of these meals - say, 30% at breakfast, 25% at lunch and 15% at dinner. Here, there are four separate steps: Create ‘breakfastdata’ as a copy of the fakerestodata Generate a popularity score as a random variable. Generate a second random variable called ‘meal’, which is initially set to a random integer between 1 and 100. Then, use the case_when syntax to recode this: if mean is below a cutoff (30 for Breakfast), set the value to NA (missing), otherwise call it ‘Breakfast.’ Then do the same for ‘lunchdata’ and for ‘dinnerdata.’ We’ll use different cutoffs here as we expect that more restaurants will be open for dinner, and fewer for breakfast. Then bind the lunchdata and the dinnerdata to the breakfastdata. Finally, remove the lines where the meal data is missing. We are calling this dataset ‘tallrestodata’ for reasons that will become apparent. # note that the same code runs three times # we should simplify this by creating a function breakfastdata &lt;- restodata |&gt; mutate (popularity = sample(1:5, 100, replace=T)) |&gt; mutate (meal = sample(1:100, 100, replace=F)) |&gt; mutate (meal = case_when (meal &lt; 31 ~ NA, TRUE ~ &quot;Breakfast&quot; )) lunchdata &lt;- restodata |&gt; mutate (popularity = sample(1:5, 100, replace=T)) |&gt; mutate (meal = sample(1:100, 100, replace=F)) |&gt; mutate (meal = case_when (meal &lt; 26 ~ NA, TRUE ~ &quot;Lunch&quot; )) dinnerdata &lt;- restodata |&gt; mutate (popularity = sample(1:5, 100, replace=T)) |&gt; mutate (meal = sample(1:100, 100, replace=F)) |&gt; mutate (meal = case_when (meal &lt; 16 ~ NA, TRUE ~ &quot;Dinner&quot; )) tallrestodata &lt;- lunchdata |&gt; bind_rows(dinnerdata, breakfastdata) |&gt; drop_na(meal) head(tallrestodata,3) ## # A tibble: 3 × 5 ## resto_name health cost popularity meal ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Michaels_place B $$$ 4 Lunch ## 2 Christophers_place A $$ 5 Lunch ## 3 Matthews_place C $$$ 5 Lunch 11.2.1 tall and wide formats Tallrestodata is tidy - each column is a variable, each row is an observation, and each cell includes a unique value. But it may not be exactly what we need. I might, for example, want to go to a popular restaurant when no one else is there - perhaps to go to breakfast at a restaurant that is popular only at lunch and dinner. One approach to this is to create a new variable in the data for the (average popularity at lunch and dinner) minus (popularity at breakfast). High scores on this would give us restaurants that might be really good or interesting, but not crowded, for a breakfast meal. But popularity at breakfast, lunch, and dinner are in different rows of the restodata; so this is a little tricky. The easiest way to do this is to use the pivot_wider function from the tidyr package. Again, the syntax is challenging - you may find the cheatsheet to be useful. This will reshape the data from tall and narrow to a format that is typically wider and shorter. From this wider data, we use mutate twice t ultimately create a new variable called secretBreakfastPlace. widerestodata &lt;- tallrestodata |&gt; pivot_wider(names_from = meal, names_prefix = &quot;popularity_&quot;, values_from = c(popularity)) |&gt; # we could create the secretBreakfast variable # in a single step, but this is probably more clear mutate (lunchDinnerPop = (popularity_Lunch + popularity_Dinner)/2) |&gt; mutate (secretBreakfastPlace = lunchDinnerPop - popularity_Breakfast) head(widerestodata) ## # A tibble: 6 × 8 ## resto_name health cost popularity_Lunch popularity_Dinner ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Michaels_place B $$$ 4 2 ## 2 Christophers_place A $$ 5 3 ## 3 Matthews_place C $$$ 5 4 ## 4 Joshuas_place A $$ 1 2 ## 5 Daniels_place C $ 4 3 ## 6 Davids_place F $ 1 4 ## # ℹ 3 more variables: popularity_Breakfast &lt;int&gt;, lunchDinnerPop &lt;dbl&gt;, ## # secretBreakfastPlace &lt;dbl&gt; Although the widerestodata has what we want, it is not tidy: The ‘popularity’ variable is now in three columns rather than one, and there are many cells with missing values. So we reshape the data back to its tall and tidy form, but which now includes the variable describing the two new mutated variables. Then we sort it by our desired variable, secretBreakfastPlace tallrestodata &lt;- widerestodata |&gt; pivot_longer(names_to = &#39;meal&#39;, names_prefix = &quot;popularity_&quot;, cols = c(&#39;popularity_Breakfast&#39;, &#39;popularity_Lunch&#39;, &#39;popularity_Dinner&#39;), values_to = (&#39;popularity&#39;), values_drop_na = TRUE) |&gt; arrange(desc(secretBreakfastPlace)) head(tallrestodata) ## # A tibble: 6 × 7 ## resto_name health cost lunchDinnerPop secretBreakfastPlace meal popularity ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 Jamess_place B $ 4.5 3.5 Brea… 1 ## 2 Jamess_place B $ 4.5 3.5 Lunch 5 ## 3 Jamess_place B $ 4.5 3.5 Dinn… 4 ## 4 Matthews_pl… C $$$ 4.5 2.5 Brea… 2 ## 5 Matthews_pl… C $$$ 4.5 2.5 Lunch 5 ## 6 Matthews_pl… C $$$ 4.5 2.5 Dinn… 4 11.2.2 exercises Study and understand the code. Ask and see if you can answer questions about each chunk of code. For example, there are 100 observations in each of the breakfast, lunch, and dinner datasets. How many are there in tall and wide restodata sets? Why? Expand on the code: Where you would really want to eat? Look at the data and think about it. Make it more realistic if you can. How would you actually decide? Come up with a decision rule that you might use for choosing a restaurant. This rule might include filters and/or simple algebraic expressions (such as ‘+’ or ‘-’). Express your decision rule using code, then select your best restaurant. You may also want to create new variables such as distance, ambiance, or type of cuisine. 11.3 more on tidy coding There are many sources for help. The help panel in R Studio is a start, but you may have better look on Google. Here are some suggestions: work with tidy data. Make each row an observation, and each column a variable. Complex data sets, such as samples of text, become much more manageable when reduced to simple arrays. think in tidy; talk the talk. For example, recognize that %&gt;% or |&gt; (the pipe) means then. Statements with pipes begin with data, may include queries (extract, combine, arrange), and finish with a command. search for tidyverse solutions. When you have a problem in your code, for example, “how do I compute the mean for different groups of a variable in R?,” do a Google search for R mean groups tidyverse, not just R mean groups. This will get you in the habit of working with tidy solutions where they can be found. look for new answers. Because R and the tidyverse are constantly evolving, consider looking at recent pages first. In your Google search bar, click on Tools -&gt; Past year). adhere to good coding style. Well-written code is reasonably parsimonious, readable, and easily debugged. There are a few style manuals for R, including one from Hadley, and this [Rchaeological Commentary] (https://cran.r-project.org/web/packages/rockchalk/vignettes/Rstyle.pdf). write functions. If you repeat a section of code, rewrite it as a function. (See the example above). annotate your work. Assume that you will come back to it at a later date, while working on a different project, and use portions of your current code. Your R markdown documents should be a log. When you run in to a significant, challenging problem, don’t delete your mistakes, but ## comment them out - as I have done in a few places above. library(gapminder) b &lt;- gapminder %&gt;% # when should you comment out an error # instead of deleting it? for me, I&#39;ll # comment out errors that took me a long time # to solve, and/or that I&#39;ll learn from. # Probably not here, in other words... # filter(lifeExp) &gt; 70 bad parens filter(lifeExp &gt; 70) Finally, maintain perspective. Your need to solve problems (how to analyze x, etc.) should not take a back seat to your desire to write the best code. There is almost always a better way to do things. Strive reasonably to accomplish this, but be prepared to kludge. knitr::opts_chunk$set(echo = TRUE) library(readxl) library(janitor) library(tidyverse) library(corrr) library(kableExtra) References Kondo, Marie. 2016. Spark Joy: An Illustrated Master Class on the Art of Organizing and Tidying up. Ten Speed Press. Peng, Roger. 2018. “Teaching r to New Users - from Tapply to the Tidyverse.” "],["finding-exploring-cleaning-and-combining-data.html", "12 finding, exploring, cleaning, and combining data 12.1 florida educational data 12.2 combining datasets 12.3 recap / on joining files", " 12 finding, exploring, cleaning, and combining data Data science, oddly enough, begins not with R… but with data. There is no shortage of datasets available to analyze, and each can give rise to a host of interesting analyses and insights. What do you want to study? Let’s begin by looking at schools 12.1 florida educational data Florida, like many states, makes data on school quality publicly available. Schools are assessed, in part, on student performance (scores on comprehensive tests in fields such as English Language Arts). Schools are also assessed on measures such as whether this performance has increased across years, the percent of students who graduate in four years, and the percentage of students who pass Advanced Placement and related exams. You can learn more about these measures at https://www.fldoe.org/core/fileparse.php/18534/urlt/SchoolGradesOverview24.pdf). The data itself are available in an Excel spreadsheet. Here’s a screenshot of the first few columns and rows of the file. It’s apparent that the file is a little messy. As we saw in a prior chapter, we would like the first row of the dataset to include the variable (column) names: here, there are three rows of header prior to this. Further, many of the variable names include spaces, minus signs, and the like. We will first download the data from the web (at this writing, you can find it at https://www.fldoe.org/file/18534/SchoolGrades24.xlsx). We will store it on our disk in a subdirectory of our project folder called ‘data.’ Slash: Windows or Mac? 12.1.1 a digression: Slash, Windows and the world. Once we download the data, we need to tell R where to find it. If we are working with an Rproject, we might keep everything - code, data, and output - in the same directory. But we often need or want to store data in a separate place, in which case you will need to specify a file path, which will include one or more slashes (and not the Guns N’ Roses guitarist kind). In most of the computing world, including Macs, filepaths are delineated by forward slashes (“/”). On a Windows machine, they instead include backwards slashes (“\\”). To further complicate matters, the backwards slash has a special significance as an ‘escape’ character - this means, as we will see briefly below as well as in the chapter on text analysis, that it tells the system to interpret the following character literally (for example, a comma is read as a comma) rather than symbolically (where a comma might be read as a separator between two objects). In any operating system, we can locate files using relative paths (starting in your project directory) or absolute ones (starting in your computer’s root directory; see below). Relative paths generally work better, as you can use the code on multiple machines. But if you can’t find your datafile, try the absolute path as a kludge. 12.1.2 getting data from our machine into R Ones we have specified the datapath, we can use the read_excel command, which is in the readxl library, which is part of the peripheral tidyverse (so you do not need to first install it on your computer). We’ll tell R to skip the first three lines of text. We’ll continue the pipe with a simple command from the janitor package - which you will need to load on to your machine. That command gets rid of spaces in variable names and replaces them with camelCase or, the default, snake_case. (To see why you want to do this try omitting this line from your code). # relative path with backward slashes replaced by forward ones datadir &lt;- &quot;data/&quot; # this is a relative path # absolute path with backward slashes made literal with escapes # datadir &lt;- &quot;C:\\\\Users\\\\me\\\\OneDrive\\\\GitRepos\\\\DSLA25\\\\data\\\\&quot; FloridaSchools &lt;- read_excel( # here, we could also just do # &quot;data/SchoolGrades24.xlsx&quot;), paste0(datadir, &quot;SchoolGrades24.xlsx&quot;), skip = 3) |&gt; clean_names() 12.1.3 which ones are “high schools”? There are a few ways that we could reduce this to just High Schools - one is to include only schools which report a graduation rate that is not a missing value; the other is to include just schools that are explicitly named “high school.” (We could also use both of these, or something else). Here, we will use the latter - filtering on schools which have “HIGH SCHOOL” in the school_name. We then reduce the columns to a handful of measures of interest. FloridaHighSchools &lt;- FloridaSchools |&gt; # drop_na(&#39;graduation_rate_2022_23&#39;) |&gt; filter(str_detect(school_name, &quot;HIGH SCHOOL&quot;)) |&gt; select (district_number, district_name, school_number, school_name, english_language_arts_achievement, mathematics_achievement, science_achievement, social_studies_achievement, graduation_rate_2022_23, grade_2024, percent_of_economically_disadvantaged_students) head(FloridaHighSchools) ## # A tibble: 6 × 11 ## district_number district_name school_number school_name english_language_art…¹ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 01 ALACHUA 0151 GAINESVILL… 54 ## 2 01 ALACHUA 0201 HAWTHORNE … 32 ## 3 01 ALACHUA 0261 NEWBERRY H… 53 ## 4 01 ALACHUA 0271 SANTA FE H… 56 ## 5 01 ALACHUA 0411 PROFESSION… 64 ## 6 01 ALACHUA 0421 EASTSIDE H… 47 ## # ℹ abbreviated name: ¹​english_language_arts_achievement ## # ℹ 6 more variables: mathematics_achievement &lt;dbl&gt;, science_achievement &lt;dbl&gt;, ## # social_studies_achievement &lt;dbl&gt;, graduation_rate_2022_23 &lt;dbl&gt;, ## # grade_2024 &lt;chr&gt;, percent_of_economically_disadvantaged_students &lt;dbl&gt; 12.1.4 can we compute district (county) means from these data? We can reduce the set of high schools to one line per district, with scores the simple means of all schools in the district. FloridaHighSchoolsbyDistict &lt;- FloridaHighSchools |&gt; # select(-grade, -school_name) |&gt; group_by(district_name) |&gt; summarise_if(is.numeric, mean, na.rm=TRUE) These average scores should be viewed with skepticism, because it treats small and large schools as equal. Consider graduation rates: If a district has just two schools, one with just 10 students (and graduates all of them), and a larger school with 990 students (but graduates only half - 495 - of them), we would get an estimated graduation rate of 75% ((1.0 + .5)/2). But actually, the district-wide graduation rate would be 50.5%. School enrollment data is needed to accurately estimate district effects from individual schools. 12.1.5 estimating school enrollments At this writing (March 2025), I can’t find a dataset on Florida HS enrollments that is free, recent, easy to pull down, and reasonably comprehensive. For our purposes, we can work with an estimate of this: There are measures of school size in football league data (https://fhsaa.com/news/2023/12/21/football-classifications-available-for-2024-25-2025-26.aspx). The names for schools there are formatted differently from our other dataset, so this will take a little work. We will first read in the data, then estimate enrollments based on the class of the school (Rural, 1R, 1A, 2A, 3A, 4A, 5A, 6A, 7A). Can you describe what is happening in each line of code in this section? Note the use of the escape character in this chunk EnrollmentsFromFHSAA &lt;- read_excel( paste0(datadir, &quot;Football_2024_26.xlsx&quot;), skip = 1) |&gt; clean_names() head(EnrollmentsFromFHSAA) ## # A tibble: 6 × 4 ## school_name class region district ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 All Saints&#39; (Winter Haven) Independent Independent Independent ## 2 Alonso (Tampa) 7A 2 7 ## 3 American (Hialeah) 4A 4 16 ## 4 American Heritage (Delray Beach) 2A 3 12 ## 5 American Heritage (Plantation) 4A 4 15 ## 6 Anclote (Holiday) 3A 3 9 EnrollmentsFromFHSAA &lt;- EnrollmentsFromFHSAA |&gt; separate(col = &quot;school_name&quot;, into = c(&quot;school_name&quot;, &quot;school_place&quot;), sep = &quot;\\\\(&quot;, extra = &quot;merge&quot;) |&gt; mutate (est_enrollment = case_when( class == &quot;Rural&quot; ~ mean(111,558), class == &quot;1R&quot; ~ mean(111,558), class == &quot;1A&quot; ~ mean(61,643), class == &quot;2A&quot; ~ mean(644,1166), class == &quot;3A&quot; ~ mean(1167,1542), class == &quot;4A&quot; ~ mean(1543,1822), class == &quot;5A&quot; ~ mean(1823,2135), class == &quot;6A&quot; ~ mean(2136,2512), class == &quot;7A&quot; ~ mean(2512,4627), TRUE ~ NA)) |&gt; mutate(school_name = toupper(school_name)) |&gt; mutate(school_place = str_remove(school_place,&quot;\\\\)&quot;)) ## Warning: Expected 2 pieces. Missing pieces filled with ## `NA` in 192 rows [8, 18, 21, 22, 24, 26, 32, ## 33, 44, 46, 47, 49, 53, 57, 59, 60, 62, 67, 73, ## 81, ...]. When we run this chunk, we get a warning. What is it about? Is it ok? 12.2 combining datasets The enrollment data are now in the EnrollmentsFromFHSAA dataset. We first edit the school names from the FloridaHighSchools file to see if we can get them to match. Then we try to merge (left join) these with the FloridaHighSchools data, using the school_name variable as the key. FloridaHighSchools &lt;- FloridaHighSchools |&gt; mutate(school_name = str_replace(school_name, &quot;JUNIOR/SENIOR HIGH SCHOOL&quot;,&quot;&quot;)) |&gt; mutate(school_name = str_replace(school_name, &quot;MIDDLE/HIGH SCHOOL&quot;,&quot;&quot;)) |&gt; mutate(school_name = str_replace(school_name, &quot;SENIOR HIGH SCHOOL&quot;,&quot;&quot;)) |&gt; mutate(school_name = str_replace(school_name, &quot;HIGH SCHOOL&quot;,&quot;&quot;)) FloridaHighSchools2 &lt;- FloridaHighSchools |&gt; left_join(EnrollmentsFromFHSAA, by = &quot;school_name&quot;) ## Warning in left_join(FloridaHighSchools, EnrollmentsFromFHSAA, by = &quot;school_name&quot;): Detected an unexpected many-to-many ## relationship between `x` and `y`. ## ℹ Row 38 of `x` matches multiple rows in `y`. ## ℹ Row 187 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, ## set `relationship = &quot;many-to-many&quot;` to ## silence this warning. But when we do this, we get another warning message. Why is it there? 12.2.1 challenges in joining datasets In each of the two datasets, there are duplicate names - for example, there are two “Atlantic” High Schools in the both the Florida High School (FHS) and the Florida High School Athletic Association (FHSAA) datasets. We can see this by looking at the output of the following commands: SchoolsWithSameNamesInFHS &lt;- FloridaHighSchools |&gt; group_by(school_name) |&gt; filter(n() &gt; 1) |&gt; # select (school_name, district_name) |&gt; ungroup() |&gt; arrange(school_name) SchoolsWithSameNamesInEnrollments &lt;- EnrollmentsFromFHSAA |&gt; group_by(school_name) |&gt; filter(n() &gt; 1) |&gt; # select (school_name, school_place) |&gt; ungroup() |&gt; arrange(school_name) There are 12 schools (6 pairs) in the FHS data, and 26 schools (13 pairs) in the FHSAA data. Ten of these schools are in common across the two sets, the remainder are unique to one or the other. 12.2.2 some approaches to fixing the data In dealing with large datasets, it is fairly common that code will work correctly for a large majority of the cases, and that creativity is needed to efficiently fix the remainder. Here, there are at least four (non-mutually-exclusive) approaches that might work. In decreasing order of comprehensiveness: The first approach would be to dig deeper, and to find another source for the school enrollment data. The data we have are imperfect in at least three ways: Enrollment data are missing for many schools (i.e., “independent” schools). Many schools on this list do not show up in the FHS list The enrollment data we have from these schools are estimates, not actual counts The second approach would be to find a geographic dataset that includes towns and counties, then to use this as a key to join our two high school files. If we were working with a larger dataset, this would be worthwhile to try. The third approach would be to manually edit the datasets so that we could include the schools with multiple names. The fourth approach would be to run an initial analysis on the data we have, putting aside the duplicate schools. In the event that the results warrant closer analysis, we could then move to a more comprehensive solution. This is the place to begin: 12.2.2.1 the simplest approach In order to merge the data, we first need to make sure that there are no spaces or tabs in the “school_name” variable that will join the datasets. SchoolsWithUniqueNamesInFHS &lt;- FloridaHighSchools |&gt; group_by(school_name) |&gt; mutate(school_name = str_trim(school_name)) |&gt; filter(n() == 1) |&gt; ungroup() SchoolsWithUniqueNamesInEnrollments &lt;- EnrollmentsFromFHSAA |&gt; group_by(school_name) |&gt; mutate(school_name = str_trim(school_name)) |&gt; filter(n() == 1) |&gt; ungroup() FloridaHighSchools2 &lt;- SchoolsWithUniqueNamesInFHS |&gt; left_join(SchoolsWithUniqueNamesInEnrollments, by = &quot;school_name&quot;) We compute the estimated graduation rate for districts (adjGradRate) by weighing schools by their estimated enrollments as follows GradCountsRates &lt;- FloridaHighSchools2 |&gt; mutate(est_Ngrads = round(est_enrollment * .01 * graduation_rate_2022_23),0) |&gt; select(district_name, est_enrollment, est_Ngrads) |&gt; group_by(district_name) |&gt; summarise_if(is.numeric,sum, na.rm = TRUE) |&gt; mutate(adjGradRate = est_Ngrads / est_enrollment) 12.2.2.2 adding in the schools with duplicate names If we wanted to examine the high school enrollment and graduation data more closely, we can manually rename the duplicate school names. Here’s one approach: Begin by cleaning up the white space in the school name field (str_trim). Then, in the FHS data, rename schools by concatenating (str_c) school name and district name. Then manually edit the relevant cases in the FHSAA data (using mutate + case_when), and finally concatenating school and district name here as well. FixNamesInFHS &lt;- SchoolsWithSameNamesInFHS |&gt; mutate(school_name = str_trim(school_name)) |&gt; mutate(school_name = str_c(school_name, &quot;_&quot;,district_name)) FixNamesInFHSAA &lt;- SchoolsWithSameNamesInEnrollments |&gt; mutate(school_name = str_trim(school_name)) |&gt; mutate(district_name = case_when ( school_place == &quot;Delray Beach&quot; ~ &quot;PALM BEACH&quot;, school_place == &quot;Port Orange&quot; ~ &quot;VOLUSIA&quot;, school_place == &quot;Fort Myers&quot; ~ &quot;LEE&quot;, school_place == &quot;Kissimmee&quot; ~ &quot;OSCEOLA&quot;, school_place == &quot;Oakland Park&quot; ~ &quot;BROWARD&quot;, school_place == &quot;St. Petersburg&quot; ~ &quot;PINELLAS&quot;, school_place == &quot;Riverview&quot; ~ &quot;HILLSBOROUGH&quot;, school_place == &quot;Sarasota&quot; ~ &quot;SARASOTA&quot;, school_place == &quot;Sanford&quot; ~ &quot;PINELLAS&quot;, school_place == &quot;Seminole&quot; ~ &quot;SEMINOLE&quot;, TRUE ~ &quot;&quot;) ) |&gt; mutate(school_name = str_c(school_name, &quot;_&quot;,district_name)) |&gt; select(-district_name) then we add these data back in with the FHS and FHSAA data and rerun the analysis FHSdata &lt;- SchoolsWithUniqueNamesInFHS |&gt; bind_rows(FixNamesInFHS) FHSAAdata &lt;- SchoolsWithUniqueNamesInEnrollments |&gt; bind_rows (FixNamesInFHSAA) FloridaHighSchools3 &lt;- FHSdata |&gt; left_join(FHSAAdata, by = &quot;school_name&quot;) GradCountsRates &lt;- FloridaHighSchools3 |&gt; mutate(est_Ngrads = round(est_enrollment * .01 * graduation_rate_2022_23),0) |&gt; select(district_name, est_enrollment, est_Ngrads) |&gt; group_by(district_name) |&gt; summarise_if(is.numeric,sum, na.rm = TRUE) |&gt; mutate(adjGradRate = est_Ngrads / est_enrollment) 12.2.3 estimating the relationship between economic disadvantage and graduation rates We now have two approaches to estimating graduation rates at the level of school districts - the first is based on the simple average by schools, the second is based on the estimated enrollment data. How do these relate to each other and to other measures in the data such as economic disadvantage? Here, we join the two district-level files, rename our two graduation measures, and compute the correlations among the measures. We use the correlate function (in the corrr package), which makes the correlation matrix into a tibble, round the entries to two decimal places, and make into a table, which we format using the kable function from the kableExtra package. FloridaHighSchoolsbyDistict |&gt; left_join(GradCountsRates) |&gt; select(graduation_rate_2022_23, adjGradRate, percent_disadvantaged_students = percent_of_economically_disadvantaged_students) |&gt; rename(grad_rate_raw = graduation_rate_2022_23, grad_rate_weighted = adjGradRate) |&gt; correlate() |&gt; mutate_if(is.numeric, round, 2) |&gt; kable(table.attr = &quot;style=&#39;width:30%;&#39;&quot;) |&gt; column_spec(1:4, width = &quot;14em&quot;) |&gt; kable_styling(font_size = 12) ## Joining with `by = join_by(district_name)` ## Correlation computed with • Method: &#39;pearson&#39; • ## Missing treated using: &#39;pairwise.complete.obs&#39; term grad_rate_raw grad_rate_weighted percent_disadvantaged_students grad_rate_raw NA 0.88 -0.51 grad_rate_weighted 0.88 NA -0.38 percent_disadvantaged_students -0.51 -0.38 NA The two measures of graduation rates are similar but not identical (r = .88), and each is negatively associated with economic disadvantage (rs of -.38 and -.51). These correlations are high. 12.3 recap / on joining files In this chapter, we’ve examined Florida High School data and found an (expected) negative relationship between economic disadvantage and graduation rates. We also saw a picture of the guitarist from the hair band Guns ‘n’ Roses. We used a number of functions, such as select, filter, mutate, group_by, summarise, and case_when, that we also considered in the last chapter. We also used some new libraries, including readxl, corrr, and kableExtra, and their functions read_excel, correlate, and kable. The core of this chapter is to introduce some of the challenges of combining files, including both (a) the frequent need to wrangle datasets so that they can be correctly put together and (b) the use of the left_join function. The left_join function is one of many different ways of joining data. In our left_join, we kept all of the rows in the first dataset (x, FHSdata), and linked them to y (FHSAAdata) if and only if they have a match on the key variable (school_name). In a right join, we keep all of y, and rows in x if and only if they have a match in y. So we could also have written our code as FloridaHighSchools3 &lt;- FHSAAdata |&gt; right_join(FHSdata, by = “school_name”) There are also inner_joins (which can be thought of as the intersection of the two datasets, including only rows in each dataset which match) , and full_joins (unions, which keep all rows in both datasets). And anti_joins include only the rows in x which do not have a match in y. We will return to anti-joins in our discussion of text analysis. In combining datasets, joins are but one approach. We can also simply bind files together, without regard for a common key. Here, (and in the last chapter) we used bind_rows to add new observations to an existing dataset - this is typically used when two datasets have the same set of variables. Finally, bind_cols can be used to add new variables when two datasets have the same observations and in the same order. knitr::opts_chunk$set(echo = TRUE) library(tidyverse) "],["applied-data-science.html", "13 applied data science 13.1 public health and covid 13.2 other datasets in and beyond R", " 13 applied data science From the standpoint of applied data science, data analysis should have meaning, and should lead to consequential social or practical consequences. Here, we consider a few examples of applications of data science. 13.1 public health and covid In Winter 2020, as the COVID-19 pandemic began to take hold, I began chronicling the course of the virus. In a prior version of this book, I wrote the following: tracking the Novel Coronavirus (from Feb 2020) Here, I want to consider a timely (but challenging) dataset. The Novel Coronavirus is an emerging health crisis, particularly in Wuhan (a Chinese city larger than New York City) and the surrounding province of Hubei. It is not yet a threat in the United States - there have, at this writing (02/11/20) been zero cases in Florida. Still, tracking the spread of the virus - the unfolding number of people infected and recovered, as well as the number of deaths - is a fascinating exercise. I then went on to provide code for accessing data provided by the Johns Hopkins Center for Systems Science and Engineering (JHU/CSSE). The data was initially provided on a GitHub site, then moved to a dashboard here. You can learn more about the efforts of this team here. 13.1.1 COVID data in 2025 Today, there are a number of R packages intended to help analyze COVID data. The COVID19 package provides records of the outbreak on a global scale. Here’s some sample code: # install.packages(&quot;COVID19&quot;) library(COVID19) allCovid &lt;- covid19() ## We have invested a lot of time and effort in creating COVID-19 Data ## Hub, please cite the following when using it: ## ## Guidotti, E., Ardia, D., (2020), &quot;COVID-19 Data Hub&quot;, Journal of Open ## Source Software 5(51):2376, doi: 10.21105/joss.02376 ## ## The implementation details and the latest version of the data are ## described in: ## ## Guidotti, E., (2022), &quot;A worldwide epidemiological database for ## COVID-19 at fine-grained spatial resolution&quot;, Sci Data 9(1):112, doi: ## 10.1038/s41597-022-01245-1 ## To print citations in BibTeX format use: ## &gt; print(citation(&#39;COVID19&#39;), bibtex=TRUE) ## ## To hide this message use &#39;verbose = FALSE&#39;. CovidbyDate &lt;- allCovid |&gt; select(date,confirmed, deaths, recovered, people_vaccinated, place = administrative_area_level_1) |&gt; group_by(date, place) |&gt; summarise_if(is.numeric, sum, na.rm=TRUE) CovidbyDate |&gt; filter (date &lt; &quot;2023-01-01&quot;) |&gt; filter(place == &quot;United States&quot;) |&gt; summarize(confirmed = sum(confirmed)) %&gt;% ggplot(aes(x=date)) + geom_line(aes(y=confirmed)) A second package can be used to examine the impact of COVID more indirectly, but possibly more accurately, by looking at excess mortality data. 13.1.2 a brief digression on causality Human deaths, like most events, are multiply determined. In the case of COVID, many of those who contracted the disease suffered from other vulnerabilities including diabetes, obesity, pre-existing heart disease, and “old age.” They may have contracted pneumonia as a proximal cause in a pathway that might have included, for example, chronic cigarette smoking -&gt; emphysema -&gt; chronic obstructive pulmonary disease (COPD) -&gt; COVID -&gt; death. In these cases, like most cases, isolating an individual “cause” can be difficult if not arbitrary, Determinations as to the cause of death may be difficult to make, particularly in an environment in which political or economic considerations may be non-trivial. 13.1.3 the excess mortality package The R excess mortality package (excessmort) can be used to calculate the expected number of deaths in a region and time period. Among other things, it can be used to estimate the effects of the pandemic on mortality in individual states based on historical data rather than particular diagnoses. You can learn more about the package at https://cran.r-project.org/web/packages/excessmort/vignettes/excessmort.html. # install.packages(&quot;excessmort&quot;) library(excessmort) exclude_dates &lt;- c(seq(make_date(2017, 12, 16), make_date(2018, 1, 16), by = &quot;day&quot;), seq(make_date(2020, 1, 1), max(cdc_state_counts$date), by = &quot;day&quot;)) counts &lt;- cdc_state_counts %&gt;% filter(state == &quot;Florida&quot;) %&gt;% compute_expected(exclude = exclude_dates) ## Warning in compute_expected(., exclude = exclude_dates): Including a trend in ## the model is not recommended with less than five years of data. Consider ## setting include.trend = FALSE. ## No frequency provided, determined to be 52 measurements per year. ## Overall death rate is 10.5. expected_plot(counts, title = &quot;Expected (blue) and actual (grey) Weekly Mortality Counts in Florida&quot;) counts &lt;- cdc_state_counts %&gt;% filter(state == &quot;California&quot;) %&gt;% compute_expected(exclude = exclude_dates) ## Warning in compute_expected(., exclude = exclude_dates): Including a trend in ## the model is not recommended with less than five years of data. Consider ## setting include.trend = FALSE. ## No frequency provided, determined to be 52 measurements per year. ## Overall death rate is 7.33. expected_plot(counts, title = &quot;Expected (blue) and actual (grey) Weekly Mortality Counts in California&quot;) 13.2 other datasets in and beyond R The datasets library in R includes about 90 datasets of this writing. Many of these (e.g., iris, cars) are ubiquitous in R training; they are typically small and easy to work with. The Fivethirtyeight and fivethirtyeightdata packages include another 150 or so datasets on politics and popular culture. A set of 2500+ datasets which are in R packages may be found at https://vincentarelbundock.github.io/Rdatasets/datasets.html (you can find this in a sortable spreadsheet here). These range in scope from the small (“Death By Horse Kicks”, 5 rows and 2 columns) to the large “US Military Demographics”, 1.4 million rows and 6 columns. Please consider explorinfg this site. The openintro package includes several hundred datasets; they are described here Note also that the R fivethirtyeight library provides access to a number of clever, clean, and largely manageable datasets, each of which underlies the empirical analyses and reports of Nate Silver and his team (You can learn more at https://data.fivethirtyeight.com/). Kaggleis a noun (a community, a website, a challenge), and a verb (to kaggle is to participate in a data challenge) which describes a crowdsourced competition to improve on a problem in prediction. Perhaps the first and best known example of this was the Netflix prize (Jackson 2017), which, in 2006, promised one million dollars to the first team to improve the algorithm by which that company recommended movies to its customer base. The competition took several years, and inspired substantial improvements in machine learning as well as in crowdsourced science. At this writing, Kaggle hosts many active competitions - including some with prizes of one million dollars or more.” (Good luck!) Kaggle also hosts hundreds of thousands of datasets. A good place to start is to filter the datasets stored in comma separated value format (.csv) and a usability rating of 8 or more. Within psychology and behavioral science, the Open Science Framework (OSF) provides a system for hosting and sharing code and data from research articles. One OSF page is a compilation of many datasets from prominent papers in psychology and psychiatry: this now forwards to a spreadsheet which, though it does include data from a number of large and important studies, it appears insufficiently curated, with many dead links. Outside of psychology, repositories of data from many disciplines may be found at Re3data https://www.re3data.org/. There are many datasets about music - songs, artists, lyrics, etc. - at millionsongdataset. Note that many of these are quite large, but more accessible datasets are available, including here. Or just Google datasets. 13.2.1 make/extract/combine your own data Despite the petabytes (exabytes? zettabytes? yottabytes?) of data in the datasets described above, it’s possible that the dataset that you want to examine does not yet exist. But you may be able to create it, for example, by scraping data from the Web. Typically, you would use an Application Programming Interface (API) to pull data down from platforms such as Twitter or Reddit. For these and other major social media and news platforms, there are R packages which will walk you through the process of getting the data from webpage to tidy dataset. (Be aware, though, that the methods for data access on these platforms frequently changes, so that code that worked a year ago might not work today). Another source of data is … your own life. If you wear a pedometer or sleep tracker, are a calorie counter or keep other logs as a member of the quantified self movement, consider how such data might relate to aspects of the physical environment (such as temperature, or the time between sunrise and sunset) and/or the broader social and cultural context (a measure, perhaps of the sentiment, or mood, of news articles from papers like the NY Times). Finally, you might want to combine multiple datasets, such as county-level home pricing data from Zillow (https://www.zillow.com/research/data/), county-level elections data from, for example, here: https://github.com/tonmcg/US_County_Level_Election_Results_08-16, and the boundaries of Woodard’s 11 American Nations (see Lanning). In joining different datasets, or data from different sources, we can go beyond a pedagogical exercise (learning about learning) and contribute new and meaningful knowledge. 13.2.2 keep it manageable Proceed with caution - many of these datasets are likely to be quite large (for example, analyses of images) and/or in formats that for now are too challenging (JSON). I encourage you to stick with data that are available in a .csv format and that don’t have more than, say, a million data points (e.g., 50,000 observations * 20 variables). References Jackson, Dan. 2017. “The Netflix Prize: How a 1 Million Contest Changed Binge-Watching Forever.” Thrillist. Com. "],["strings-factors-dates-and-times.html", "14 strings, factors, dates, and times 14.1 strings 14.2 factors 14.3 dates 14.4 times", " 14 strings, factors, dates, and times This chapter discusses some of the types of data other than numeric and logical, in particular strings, factors, and dates/times. For the time being, please consider this as a supplement to R4DS, 2e, Chapter 14. 14.1 strings Strings are sets of characters which may include “123” as well as “why *DID* the chicken cross the road?” Samples of text, from names to novels, are the most interesting type of string. Among the tools that are used in examining texts are searches (do these tweets include language associated with hate speech?), validity checks (does the string correspond to a valid zip code?), and reformatting (to lower case so that BOB, Bob, and bob are all coded as identical). These ideas are simple, but quickly become challenging when, for example, the strings in which we are interested include characters that R usually interprets as code - such as commas, quotes, and slashes. See the section on string basics (14.2) for how to “escape” these characters, for example, how to treat a hashtag (#) as just a character as opposed to the beginning of a comment. These rules are codified as regular expressions (regex, sometimes regexp). Regex are not unique to R, but are shared with other languages as well. In R, particularly in the tidyverse package, regex are typically implicit, represented within commands that are part of the stringr package and that typically begin with str_. For example, str_detect returns a set of logical values: donuts &lt;- c(&quot;glazed&quot;, &quot;cakes&quot;, &quot;Pink sprinkled&quot;, &quot;cream filled&quot;, &quot;day-old frosted&quot;, &quot;chocolates&quot;) donuts %&gt;% str_detect(&quot; &quot;) ## [1] FALSE FALSE TRUE TRUE TRUE FALSE Most of the str_ functions are straightforward, but remember that str_sub provides a subset, not a substitution; to change a string, use str_replace. As Hadley points out in R4DS 2e, the autocomplete function in R_studio is very handy for helping you explore the different functions - in your console, type str_ … then scroll through the possibilities. donuts %&gt;% str_sub(1,5) ## [1] &quot;glaze&quot; &quot;cakes&quot; &quot;Pink &quot; &quot;cream&quot; &quot;day-o&quot; &quot;choco&quot; donuts %&gt;% str_replace(&quot; &quot;,&quot;_&quot;) ## [1] &quot;glazed&quot; &quot;cakes&quot; &quot;Pink_sprinkled&quot; &quot;cream_filled&quot; ## [5] &quot;day-old_frosted&quot; &quot;chocolates&quot; As you work with texts, simple problems sometimes require sophisticated codes. The regex that are used to solve these problems quickly become dense and challenging. One tool that can help you is the str_view command, which returns highlighted text showing corresponding passages. For example: slashMovieTitles &lt;- c(&quot;Face/off&quot;, &quot;8 1/2&quot;, &quot;F/X&quot;, &quot;Frost/Nixon&quot;, &quot;Victor/Victoria&quot;) slashMovieTitles %&gt;% str_view (&quot;/&quot;) ## [1] │ Face&lt;/&gt;off ## [2] │ 8 1&lt;/&gt;2 ## [3] │ F&lt;/&gt;X ## [4] │ Frost&lt;/&gt;Nixon ## [5] │ Victor&lt;/&gt;Victoria Regex statements are dense statements that allow us to work efficiently, for example, with special characters (like backslashes), repeated characters (zzz), and sets of characters [AEIOU], and characters at the beginning (^) or end ($) of strings. See R4DS 2e Chapter 15 for more. A particularly useful function in stringr is str_split, which can be used to quickly break a text into discrete words. Note that using a space and the explicit “word” boundary give different results. str_split(donuts, &quot; &quot;, simplify = TRUE) ## [,1] [,2] ## [1,] &quot;glazed&quot; &quot;&quot; ## [2,] &quot;cakes&quot; &quot;&quot; ## [3,] &quot;Pink&quot; &quot;sprinkled&quot; ## [4,] &quot;cream&quot; &quot;filled&quot; ## [5,] &quot;day-old&quot; &quot;frosted&quot; ## [6,] &quot;chocolates&quot; &quot;&quot; str_split(donuts, boundary (&quot;word&quot;), simplify = TRUE) ## [,1] [,2] [,3] ## [1,] &quot;glazed&quot; &quot;&quot; &quot;&quot; ## [2,] &quot;cakes&quot; &quot;&quot; &quot;&quot; ## [3,] &quot;Pink&quot; &quot;sprinkled&quot; &quot;&quot; ## [4,] &quot;cream&quot; &quot;filled&quot; &quot;&quot; ## [5,] &quot;day&quot; &quot;old&quot; &quot;frosted&quot; ## [6,] &quot;chocolates&quot; &quot;&quot; &quot;&quot; The output of str_split is generally a list (more on that soon), but here the lists are simplified into tibbles. In the tidyverse, str_split is typically one of the first steps in preparing text. The tidytext package (https://www.tidytextmining.com/), which is discussed at length in the computational social science course, builds on this foundation and is a powerful set of tools for all sorts of problems in formal text analysis. 14.2 factors Conditions (experimental vs control), categories (male or female), types (scorpio, “hates astrology”) and other nominal measures are categorical variables or factors. In the tidyverse, the r package for dealing with this type of measure is forcats, one of the core parts of the tidyverse. Here’s an example of a categorical variable. Why is it set up like this, and what does it do? # Example of a factor eyes &lt;- factor(x = c(&quot;blue&quot;, &quot;green&quot;, &quot;green&quot;, &quot;zombieRed&quot;), levels = c(&quot;blue&quot;, &quot;brown&quot;, &quot;green&quot;)) eyes ## [1] blue green green &lt;NA&gt; ## Levels: blue brown green In base R, string variables (“donut”, “anti-Brexit”, and “yellow”) are generally treated as factors by default. In the tidyverse, string variables are treated as strings until they are explicitly declared as factors. The syntax for working with factors-as-categories is given in Chapter 16 of R4DS 2e. I will not duplicate that here, but I will point out that factors are represented internally in R as numbers, and converting (coercing) factors to other data types can occasionally lead to nasty surprises. Sections 16.4 and 16.5 describe how factors can be cleanly reordered and modified. 14.2.1 types of babies In the babynames data, baby’s gender is a categorical variable, which is treated (because tidyverse) as a character or string. Here, we make it into a factor. We create two other factors as well. # adding third level for non-binary babies sexlevels &lt;- c(&quot;M&quot;, &quot;F&quot;, &quot;O&quot;) babynames2 &lt;- babynames %&gt;% mutate(sex = factor(sex, levels = sexlevels)) %&gt;% mutate(beginVowel = case_when( substr(name,1,1) %in% c(&quot;A&quot;,&quot;E&quot;,&quot;I&quot;,&quot;O&quot;,&quot;U&quot;) ~ &quot;Vowel&quot;, TRUE ~ &quot;Consonant&quot;)) %&gt;% mutate(beginVowel = factor(beginVowel)) %&gt;% mutate (century = case_when( year &lt; 1900 ~ &quot;19th&quot;, year &lt; 2000 ~ &quot;20th&quot;, year &gt; 1999 ~ &quot;21st&quot;)) %&gt;% mutate(century = factor(century)) Use the syntax above to create types of names for different generations (boomers, gen x, Millenials, gen z). Use https://www.kasasa.com/articles/generations/gen-x-gen-y-gen-z to determine your groupings. Say something interesting about the data - names, genders, etc. Plot this. 14.2.2 types of grown-ups If you would instead like to examine survey data, the forcats package includes a set of categorical variables. Using the discussion in Chapter 15 of R4DS as your guide, examine the relationship between two or more of these categorical variables. Again, plot these gss_cat ## # A tibble: 21,483 × 9 ## year marital age race rincome partyid relig denom tvhours ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 2000 Never married 26 White $8000 to 9999 Ind,near … Prot… Sout… 12 ## 2 2000 Divorced 48 White $8000 to 9999 Not str r… Prot… Bapt… NA ## 3 2000 Widowed 67 White Not applicable Independe… Prot… No d… 2 ## 4 2000 Never married 39 White Not applicable Ind,near … Orth… Not … 4 ## 5 2000 Divorced 25 White Not applicable Not str d… None Not … 1 ## 6 2000 Married 25 White $20000 - 24999 Strong de… Prot… Sout… NA ## 7 2000 Never married 36 White $25000 or more Not str r… Chri… Not … 3 ## 8 2000 Divorced 44 White $7000 to 7999 Ind,near … Prot… Luth… NA ## 9 2000 Married 44 White $25000 or more Not str d… Prot… Other 0 ## 10 2000 Married 47 White $25000 or more Strong re… Prot… Sout… 3 ## # ℹ 21,473 more rows 14.3 dates The challenges of combining time-demarcated data (Chapter 17 of R4DS 2e) are significant. For dates, a variety of different formats (3-April, October 23, 1943, 10/12/92) must be made sense of. Sometimes we are concerned with durations (how many days, etc.); on other occasions, we are concerned with characteristics of particular dates (as in figuring out the day of the week on which you were born). And don’t forget about leap years. In R, the lubridate package (a non-core part of the tidyverse, i.e., one that you must load separately) helps to handle dates and times smoothly. It anticipates many of the problems we might encounter in extracting date and time information from strings. Lubridate generally works well to simplify files with dates and times, and can be used to help in data munging. For example, in my analyses of the Corona data, dates and times were reported in four different ways. The code below decodes these transparently and combines them into a common date/time format . 2/3/20 6 PM 2/3/20 18:00 2/3/20 18:00:00 2020-02-03 18:00:00 # not run #coronaData2 &lt;- coronaData %&gt;% mutate # (`FixedDate = # parse_date_time(`Last Update`, # c(&#39;mdy hp&#39;,&#39;mdy HM&#39;, # &#39;mdy HMS&#39;,&#39;ymd HMS&#39;))) 14.4 times Working with temporal data is often challenging. The existence of, for example, 12 versus 24 hour clocks, time zones, and daylight savings, can make a simple question about duration quite challenging. Imagine that Fred was born in Singapore at the exact moment of Y2K. He now lives in NYC. How many hours has he been alive as of right now? How would you solve this? # find timezones for Singapore and NYC # a = get datetime for Y2K in Singapore in UTC # b = get datetime for now in NYC in UTC # compute difference and express in sensible metric "],["lists.html", "15 lists", " 15 lists Up until now, we have we have thought about ‘data structures’ as matrices (rectangles, two-dimensional arrays), in which columns typically correspond to variables and rows to observations, and in which each variable has a particular type, such as numeric or character (or, in the last chapter, special types such as factors and dates/times). In base R, data matrices are typically represented as data frames (type = df). In the Tidyverse, we have been using a special type of data frame, the tibble (type = df and tbl_df). The diamonds dataset, for example, is a tibble: babynames2 &lt;- babynames %&gt;% mutate(generation = case_when( (1944 &lt;= year) &amp; (year &lt;= 1964) ~ &quot;boomer&quot;, (1965 &lt;= year) &amp; (year &lt;= 1979) ~ &quot;genX&quot;, (1980 &lt;= year) &amp; (year &lt;= 1994) ~ &quot;millenial&quot;, (1995 &lt;= year) &amp; (year &lt;= 2019) ~ &quot;genZ&quot;)) %&gt;% mutate(generation = factor(generation)) str(babynames2$generation) ## Factor w/ 4 levels &quot;boomer&quot;,&quot;genX&quot;,..: NA NA NA NA NA NA NA NA NA NA ... babynames3 &lt;- babynames2 %&gt;% # count(generation) %&gt;% na.omit(generation) Within the diamonds tibble, we can examine the types of each variable. This uses the sapply function to SIMPLY APPLY a function (class) to the columns of a data frame or tibble. Here, each column (such as $carat) is a vector, and each vector is homogeneous (of one particular type): diamonds %&gt;% sapply(class) ## $carat ## [1] &quot;numeric&quot; ## ## $cut ## [1] &quot;ordered&quot; &quot;factor&quot; ## ## $color ## [1] &quot;ordered&quot; &quot;factor&quot; ## ## $clarity ## [1] &quot;ordered&quot; &quot;factor&quot; ## ## $depth ## [1] &quot;numeric&quot; ## ## $table ## [1] &quot;numeric&quot; ## ## $price ## [1] &quot;integer&quot; ## ## $x ## [1] &quot;numeric&quot; ## ## $y ## [1] &quot;numeric&quot; ## ## $z ## [1] &quot;numeric&quot; Beyond these atomic vectors, data can take more complex forms, such as hierarchical or tree-like structures such as the following. Wilkes Honors College Courses ├───Area: Psychology ├───Name: Personality and Social Development └───Term: Spring 2025 └───Instructors: Lanning └───Students: └───Al └───Year: Freshman └───Concentration: Psychology └───Barb └───etc. ├───Name: Political Psychology └───Term: Fall 2020 └───Instructors: Lanning ├───etc. Nested data sets such as these are common across the Internet. They describe the structure of the webpage you are looking at (which you can see, depending upon your browser, by clicking on something like ‘developer tools’). Data formats for representing nested structures include XML (Extensible Markup Language) and JSON (Java Script Object Notation). Many datasets of interest, such as this set of ratings of 10,000 books on Goodreads are structured as XML as well. In R, XML and JSON files will (typically after some massaging), be represented as lists. Lists are recursive, that is, they may include other lists. In addition to external data sources, the results of many procedures within R may also be represented as lists. Consider the following code. What does it do? What is in ‘mod’? Why is it stored like this? mod &lt;- lm(price ~ carat, data = diamonds) In R studio, you can inspect the structure of the list by clicking on it in the global environment window, by using the View tab, or with the command str(mod). You can extract rows of your list by including them in single brackets (which will return another list), or double brackets (which will return a vector or data frame). Compare the structure of the following data sets: b1 &lt;- mod[&#39;coefficients&#39;] b2 &lt;- mod[[&#39;coefficients&#39;]] c1 &lt;- mod[&#39;model&#39;] c2 &lt;- mod[[&#39;model&#39;]] Lists are, in a sense, containers. A single bracket gives us the wrapper as well as what is inside; the double bracket extracts only the inner element. Lists can be challenging, but they are necessary in a world where data is complexly structured. The R package purrr, a core part of the tidyverse, includes functions which simplify working with lists; to learn more, there is a tutorial here, and an overview of the package (with a cheatsheet) here. Lists are also discussed in Chapter 23 of R4DS 2e. "],["loops-functions-and-beyond.html", "16 loops, functions, and beyond 16.1 loops 16.2 from loop to apply to purrr::map 16.3 some examples of functions 16.4 how many bottles of what?", " 16 loops, functions, and beyond In one of the most important contemporary theoretical models of intelligence, (Sternberg 1999) has argued that the ability to automatize, that is, to work efficiently on repeated or habitual tasks, is a key component of intelligent behavior. Solving habitual problems efficiently - whether it is making a cup of coffee or finding the shortest path to complete a shopping list in a supermarket or a series of errands across town - allows us to focus our limited resources on other challenging tasks that, in turn, may determine whether we survive, or at least prosper. In programming, loops and functions are essential tools for making repetitive tasks simple. Simplifying your code is one of the more intellectually satisfying aspects of working in R or in any programming language. In R, loops are supplemented by additional tools for simplifying and avoiding repetition in code, including the ‘apply’ family in Base R and the map function in the tidyverse. Functions (and, beyond this, custom libraries) can further streamline your work. 16.1 loops Consider the task of printing out a series of numbers. Here’s a simple example of how this could be done in a loop in Base R. It prints numbers between 1995 and 1998 (inclusive). for (i in 1995:1998) { # i is an index code print(i) # print the ith value in the sequence } # go to the next one until the range is complete ## [1] 1995 ## [1] 1996 ## [1] 1997 ## [1] 1998 Let’s expand on this a little, connecting back to the babynames data. This will print counts of the number of babies for each year in GenerationZ, which includes years from 1995-2015. genZ &lt;- (1995:2015) # this reduces the (big) babynames to a simple file of years and counts babyCounts &lt;- babynames %&gt;% group_by(year) %&gt;% filter(year %in% genZ) %&gt;% summarize(nbabies = sum(n)) # and this uses a for loop to print each row in turn # for (i in (1:nrow(babyCounts))) { for (i in seq_along(nrow(babyCounts))) { # filename[i,j] == ith row and jth column print(paste((babyCounts[i,1]), babyCounts[i,2])) } ## [1] &quot;1995 3661351&quot; The syntax of loops, including where to put parentheses in index statements, can be tricky. Expect to refer to Google and StackExchange often in order to get your code running. Another good source is Chapter 26 of R4DS (2e). There, Wickham goes in to additional detail, including a description of seq_along(df) as a tool for creating an index corresponding to 1:ncol(df). Here’s an example with the diamonds dataset: diamonds2 &lt;- diamonds %&gt;% select_if(., is.numeric) # for (i in (1:ncol(diamonds2))) { for (i in seq_along(diamonds2)) { print(paste(names(diamonds2[i]), round(mean(diamonds2[[i]]),2))) } ## [1] &quot;carat 0.8&quot; ## [1] &quot;depth 61.75&quot; ## [1] &quot;table 57.46&quot; ## [1] &quot;price 3932.8&quot; ## [1] &quot;x 5.73&quot; ## [1] &quot;y 5.73&quot; ## [1] &quot;z 3.54&quot; Chapter 26 of R4DS also considers some extensions to related problems such as loops of indefinite length, which can often be addressed using the ‘while’ command. 16.2 from loop to apply to purrr::map Understanding for loops is fundamental in programming, but in R they should often be sidestepped. If the order of iteration isn’t important (if, for example, it doesn’t matter which of the diamonds variables we take the mean of first), then using one of the measures from the apply family can generally be used to make your code simpler and more efficient. The logic is that one takes a dataframe (df or tibble), then applies a function to its rows or columns: # apply = apply the function (mean) to the columns(2) of the df diamonds2 %&gt;% apply(2,mean) ## carat depth table price x y ## 0.7979397 61.7494049 57.4571839 3932.7997219 5.7311572 5.7345260 ## z ## 3.5387338 # sapply = simply apply - guesses that you are looking for col. means diamonds2 %&gt;% sapply(mean) ## carat depth table price x y ## 0.7979397 61.7494049 57.4571839 3932.7997219 5.7311572 5.7345260 ## z ## 3.5387338 The many variants of the apply family, including lapply [list apply] and tapply [table apply] as well as sapply, each have their own uses and can be quite efficient but, again, can be syntactically challenging. In the evolving tidyverse, the map family of commands is supplementing if not supplanting apply; these commands (part of the purrr package in the core tidyverse) may prove to be more convenient and clear. For example, the map_df function will apply a function and return a dataframe (tibble), which can be handy for further analysis. diamonds2 %&gt;% map_df(mean) ## # A tibble: 1 × 7 ## carat depth table price x y z ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.798 61.7 57.5 3933. 5.73 5.73 3.54 16.3 some examples of functions If you repeat a series of lines of code several times in your program, it is often best to wrap this into a function. The first example is from a preregistered study I recently started of language and politics. For the preregistration, I ran analyses using simulated data, both to increase the likelihood that the code will run without error on real data and to help anticipate the analyses which are to be run on ‘real’ data. I began by getting a real body of text from the net and scrambling it, then constructing fake ‘Republican’ and ‘Democratic’ texts from this. Here, I illustrate this by constructing 50 sample documents, each consisting of between 5 and 20 words. The project is given in four steps: 16.3.1 preliminaries Here is the preliminary stuff, where I pull the data off the net and initialize the variables sampledata &lt;- read_csv(&quot;data/sentiment-words-DFE-785960.csv&quot;)#url( # &quot;https://www.crowdflower.com/wp-content/uploads/2016/03/sentiment-words-DFE-785960.csv&quot;)) #&quot;https://raw.githubusercontent.com/totalgood/hope/master/data/corpora/sentiment-words-DFE-785960.csv&quot;)) # pulls off four words sampledata &lt;- sampledata[22:25] %&gt;% na.omit() ndocs &lt;- 50 minDocLength &lt;- 5 maxDocLength &lt;- 20 doc &lt;- vector(mode = &quot;character&quot;, length = ndocs) 16.3.2 the function Here’s the simple function which pulls a random word out of the matrix of sampledata. set.seed(33458) # a random seed is used to allow reproducible results getword &lt;- function() { rowid &lt;- sample(1:nrow(sampledata), 1) colid &lt;- sample(1:ncol(sampledata), 1) sampledata[rowid,colid] } 16.3.3 applying the function The function is applied, first, to extract one word, then, in successive loops, to build up one phrase and then many. # combine words into docs # establish length of first phrase docLength &lt;- sample(minDocLength:maxDocLength,1) # initialize with one word sampleCorpus &lt;- getword() # loop to build up first phrase for (i in 1:docLength) { addWord &lt;- getword() sampleCorpus &lt;- paste(sampleCorpus, addWord) } #add additional simulated documents for (j in 2:ndocs) { docLength &lt;- sample(minDocLength:maxDocLength,1) newdoc &lt;- getword() for (i in 1:docLength - 1) { addWord &lt;- getword() newdoc &lt;- paste(newdoc, addWord) } sampleCorpus &lt;- rbind(newdoc,sampleCorpus) } Finally, the results are combined with a vector of alternating labels of ‘Dem’ and ‘Rep’: row.names(sampleCorpus) &lt;- NULL evenOdd &lt;- rep(c(&quot;Dem&quot;,&quot;Rep&quot;),length.out = nrow(sampleCorpus)) workingCorpus &lt;- as_tibble(cbind(evenOdd,sampleCorpus)) head(workingCorpus,5) ## # A tibble: 5 × 2 ## evenOdd V2 ## &lt;chr&gt; &lt;chr&gt; ## 1 Dem wild at heart nobody loves me lost art miss the sun need a doctor kin… ## 2 Rep great player awfully nice back hurts not paying pretty rubbish found … ## 3 Dem yahooo make no promises horrible daughter ew crazy evening fit ## 4 Rep can&#39;t sleep lazy afternoon sucks #fedup great shame positive break fa… ## 5 Dem problems nice funniest hurt not necessarily sad nose eat no problem d… 16.4 how many bottles of what? To put the fun back into function, here’s a solution to the “99 bottles of beer” function described in r4DS 21.2.1. Study the code. Ask or answer a question about it in class or on Slack. beerSong &lt;- function(liquid = &quot;beer&quot;, count = 99, surface = &quot;wall&quot;) { songtext &lt;- &quot;&quot; for (i in (count:1)) { thisLine = (paste0(i, &quot; bottles of &quot;, liquid, &quot; on the &quot;, surface, &quot;, you take one down and pass it around,\\n&quot;)) songtext = c(songtext, thisLine) } songtext = c(songtext, (paste0(&quot;no more bottles of &quot;, liquid,&quot; on the &quot;, surface,&quot;...&quot;))) cat(songtext) # cat prints without line numbers } #beerSong() And here are solutions proposed by some of your classmates over the last few years. How do they differ from each other, and from the solution given above? beersng &lt;- function(n) { if (n == 1) { cat(&quot;\\n&quot;,n,&quot; bottle of beer on the wall, &quot;,n, &quot; bottles of beer.\\nTake one down and pass it around,&quot;, &quot; no more bottles of beer on the wall.\\n&quot;, sep = &quot;&quot;) cat(&quot;\\nNo more bottles of beer on the wall, no more bottles of beer.\\n&quot;, &quot;Go to the store and buy some more, 99 bottles of beer on the wall.\\n&quot;, sep=&quot;&quot;) } else { cat(&quot;\\n&quot;,n,&quot; bottles of beer on the wall, &quot;,n, &quot; bottles of beer.\\nTake one down and pass it around, &quot;, n-1, &quot; more bottles of beer on the wall.\\n&quot;, sep=&quot;&quot;) return(beersng(n-1)) } } #beersng(99) moreBeer &lt;- function () { for (i in 0:100){ starting_number &lt;- 100 if (starting_number - i == 0) { print(&quot;No more bottles of beer on the wall, no more bottles of beer, Go to the store and buy some more, 99 bottles of beer on the wall.&quot;) break } print(paste(starting_number - i, &quot;bottles of beer on the wall&quot;, &quot;Take one down and pass it around,&quot;, starting_number - i - 1, &quot;bottles of beer on the wall.&quot;)) } } # moreBeer() song &lt;- function(bottlesofbeer){ for(i in bottlesofbeer:1){ cat(bottlesofbeer,&quot; bottles of beer on the wall \\n&quot;, bottlesofbeer,&quot; bottles of beer \\nTake one down, pass it around \\n&quot;, bottlesofbeer-1, &quot; bottles of beer on the wall \\n&quot;,&quot; \\n&quot;) bottlesofbeer = bottlesofbeer - 1 } } #song(99) Which code is ‘best’? Good code is clear, but it is also efficient. We probably shouldn’t expect much in the way of differences between these functions in terms of speed (as each includes 99 iterations of a simple print command), but here’s a simple way to compare. Note that I’ve used the “sink” command to write my output to files rather than consoles: start_time &lt;- Sys.time() sink (file = &quot;f1.txt&quot;) beerSong(&quot;beer&quot;,99) sink() end_time &lt;- Sys.time() beerSongtime &lt;- end_time - start_time start_time &lt;- Sys.time() sink (file = &quot;f2.txt&quot;) beersng(99) sink() end_time &lt;- Sys.time() beersngtime &lt;- end_time - start_time start_time &lt;- Sys.time() sink (file = &quot;f3.txt&quot;) song(99) sink() end_time &lt;- Sys.time() songtime &lt;- end_time - start_time start_time &lt;- Sys.time() sink (file = &quot;f4.txt&quot;) moreBeer() sink() end_time &lt;- Sys.time() moreBeertime &lt;- end_time - start_time Here is the first line of output from each function, together with table showing the elapsed times: readLines(&quot;f1.txt&quot;,1) ## [1] &quot; 99 bottles of beer on the wall, you take one down and pass it around,&quot; readLines(&quot;f2.txt&quot;,2) ## [1] &quot;&quot; ## [2] &quot;99 bottles of beer on the wall, 99 bottles of beer.&quot; readLines(&quot;f3.txt&quot;,4) ## [1] &quot;99 bottles of beer on the wall &quot; &quot; 99 bottles of beer &quot; ## [3] &quot;Take one down, &quot; &quot; pass it around &quot; readLines(&quot;f4.txt&quot;,1) ## [1] &quot;[1] \\&quot;100 bottles of beer on the wall Take one down and pass it around, 99 bottles of beer on the wall.\\&quot;&quot; tibble(functionName = c(&quot;beerSongtime&quot;, &quot;beersngtime&quot;, &quot;songtime&quot;, &quot;moreBeertime&quot;), time = c(beerSongtime,beersngtime, songtime, moreBeertime)) %&gt;% kable(digits = 2) functionName time beerSongtime 0.01 secs beersngtime 0.01 secs songtime 0.01 secs moreBeertime 0.02 secs References Sternberg, Robert J. 1999. “The Theory of Successful Intelligence.” Review of General Psychology 3 (4): 292–316. https://doi.org/10/cqrkxh. "],["from-correlation-to-multiple-regression.html", "17 from correlation to multiple regression 17.1 bivariate analysis: Galton’s height data 17.2 multivariate data", " 17 from correlation to multiple regression In the previous chapters, we have learned how to summarize and visualize data. We have seen that we can summarize data using descriptive statistics and visualize data using plots. We can distinguish between analyses of just one variable (the univariate case), two variables (bivariate), and multivariate (many variables). 17.1 bivariate analysis: Galton’s height data (Note that this section is excerpted directly from From https://github.com/datasciencelabs). Francis Galton, a polymath and cousin of Charles Darwin, is one of the fathers of modern statistics. Galton liked to count - his motto is said to have been “whenever you can, count”. He collected data on the heights of families in England, and he found that there was a strong correlation between the heights of fathers and their sons. We have access to Galton’s family height data through the HistData package. We will create a dataset with the heights of fathers and the first son of each family. Here are the key univariate statistics for the two variables of father and son height, each taken alone: data(&quot;GaltonFamilies&quot;) galton_heights &lt;- GaltonFamilies %&gt;% filter(childNum == 1 &amp; gender == &quot;male&quot;) %&gt;% select(father, childHeight) %&gt;% rename(son = childHeight) galton_heights %&gt;% summarise(mean(father), sd(father), mean(son), sd(son)) ## mean(father) sd(father) mean(son) sd(son) ## 1 69.09888 2.546555 70.45475 2.557061 This univariate description fails to capture the key characteristic of the data, namely, the idea that there is a relationship between the two variables. To summarize this relationship, we can compute the correlation between the two variables. galton_heights %&gt;% summarize(cor(father, son)) %&gt;% round(3) ## cor(father, son) ## 1 0.501 In these data, the correlation (r) is about .50. (This means that for every standard deviation increase in the father’s height, we expect the son’s height to increase by about half a standard deviation). Incidentally, if we want to save correlations as a matrix, we can use the correlate() function from the corrr package. This function computes the correlation matrix for all pairs of variables in a data frame, which can be easily saved and formatted as a table. The fashion() function can be used to easily clean up the output. (And the parentheses around the whole statement allows us to print out the result to the console / RMarkdown document, as well aas saving rmatrix in our environment). (rmatrix &lt;- galton_heights %&gt;% select(father, son) %&gt;% correlate() %&gt;% fashion(decimals = 3)) ## term father son ## 1 father .501 ## 2 son .501 17.1.1 correlations based on small samples are unstable: A Monte Carlo demonstration Correlations based on small samples can bounce around quite a bit. Consider what happens when, for example, we sample just 25 cases from Galton’s data, and compute the correlation within this sample. Note that I begin by setting a seed for the random sequence. I repeat this 1000 times, then plot the distribution of these sample rs: set.seed(33458) # why do I do this? nTrials &lt;- 1000 nPerTrial &lt;- 25 replications &lt;- replicate(nTrials, { sample_n(galton_heights, nPerTrial, replace = TRUE) %&gt;% # we sample with replacement here summarize(r=cor(father, son)) %&gt;% .$r }) replications %&gt;% as_tibble() %&gt;% ggplot(aes(replications)) + geom_histogram(binwidth = 0.05, col = &quot;blue&quot;, fill = &quot;cyan&quot;) These sample correlations range from -0.002 to 0.882. Their average, however, at 0.503 is almost exactly that of the overall population. Often in data science, we will estimate population parameters in this way - by repeated sampling, and by studying the extent to which results are consistent across samples. More on that later. 17.1.2 from correlation to regression In bivariate analysis, there is often an asymmetry between the two variables - one is often considered the predictor (or independent variable, typically x) and the other the response (or dependent variable, y). In these data, we are likely to consider the father’s height as the predictor and the son’s height as the response. As noted above, one way of thinking about a correlation between variables like heights of fathers (x) and sons (y), is that for every one standard deviation increase in x (father’s height), we expect the son’s height to increase by about \\(r\\) times the standard deviation of y (the son’s height). We can compute all of these things manually and plot the points with a regression line. (We use the pull function to extract the values from the statistics from tibbles into single values). mu_x &lt;- galton_heights |&gt; summarise(mean(father)) |&gt; pull() mu_y &lt;- galton_heights |&gt; summarise(mean(son)) |&gt; pull() s_x &lt;- galton_heights |&gt; summarise(sd(father)) |&gt; pull() s_y &lt;- galton_heights |&gt; summarise(sd(son)) |&gt; pull() r &lt;- galton_heights %&gt;% summarize(cor(father, son)) |&gt; pull () m &lt;- r * s_y / s_x b &lt;- mu_y - m*mu_x galton_heights %&gt;% ggplot(aes(father, son)) + geom_point(alpha = 0.5) + geom_abline(intercept = b, slope = m, col = &quot;blue&quot;) Finally, if we first standardize the variables, then the regression line has intercept 0 and slope equal to the correlation \\(\\rho\\). Here, the slope, regression line, and correlation are all equal (I’ve made the plot square to better indicate this). galton_heights %&gt;% ggplot(aes(scale(father), scale(son))) + geom_point(alpha = 0.5) + geom_abline(intercept = 0, slope = r, col = &quot;blue&quot;) 17.2 multivariate data For the Galton data, we examined the relationship between two variables - one a predictor (father’s height) and the other a response (son’s height). In this section (drawn from Peng, Caffo, and Leek’s treatment from Coursera - the Johns Hopkins Data Science Program), we will extend our analysis to consider multiple predictors of a single response or outcome variable. You may need to install the packages “UsingR”, “GGally” and/or”Hmisc”. We begin with a second dataset. You can learn about it by typing ?swiss in the console. data(swiss) str(swiss) ## &#39;data.frame&#39;: 47 obs. of 6 variables: ## $ Fertility : num 80.2 83.1 92.5 85.8 76.9 76.1 83.8 92.4 82.4 82.9 ... ## $ Agriculture : num 17 45.1 39.7 36.5 43.5 35.3 70.2 67.8 53.3 45.2 ... ## $ Examination : int 15 6 5 12 17 9 16 14 12 16 ... ## $ Education : int 12 9 5 7 15 7 7 8 7 13 ... ## $ Catholic : num 9.96 84.84 93.4 33.77 5.16 ... ## $ Infant.Mortality: num 22.2 22.2 20.2 20.3 20.6 26.6 23.6 24.9 21 24.4 ... Here’s a scatterplot matrix of the Swiss data. Look at the first column of plots (or first row of the correlations). What is the relationship between fertility and each of the other variables? # ds_theme_set() set.seed(0) ggpairs (swiss, lower = list( continuous = &quot;smooth&quot;), axisLabels =&quot;none&quot;, switch = &#39;both&#39;) Here, we predict fertility from all of the remaining variables together in a single regression analysis, using the lm (linear model) command. Note that the result of this analysis is a list. We can pull out the key features of the data using the summary() command. How do you interpret this? swissReg &lt;- lm(Fertility ~ ., data=swiss) summary(swissReg) ## ## Call: ## lm(formula = Fertility ~ ., data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.2743 -5.2617 0.5032 4.1198 15.3213 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66.91518 10.70604 6.250 1.91e-07 *** ## Agriculture -0.17211 0.07030 -2.448 0.01873 * ## Examination -0.25801 0.25388 -1.016 0.31546 ## Education -0.87094 0.18303 -4.758 2.43e-05 *** ## Catholic 0.10412 0.03526 2.953 0.00519 ** ## Infant.Mortality 1.07705 0.38172 2.822 0.00734 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.165 on 41 degrees of freedom ## Multiple R-squared: 0.7067, Adjusted R-squared: 0.671 ## F-statistic: 19.76 on 5 and 41 DF, p-value: 5.594e-10 Regression is a powerful tool for understanding the relationship between a response variable and one or more predictor variables. We can use it where our variables are not normally distributed, as in the case of dichotomous variables (yes/no, true/false), as well as counts, which often are skewed. Here’s a second dataset, the marital affairs data, which is also included in the Ecdat package. We’ll apply ggpairs here, but for clarity will show only half of the data at a time. The dependent variable of interest (nbaffairs) will be included in both plots: Fair1 &lt;- Fair %&gt;% select(sex:child, nbaffairs) ggpairs(Fair1, # if you wanted to jam all 9 vars onto one page you could do this # upper = list(continuous = wrap(ggally_cor, size = 10)), lower = list(continuous = &#39;smooth&#39;), axisLabels = &quot;none&quot;, switch = &#39;both&#39;) Fair2 &lt;- Fair %&gt;% select(religious:nbaffairs) ggpairs(Fair2, lower = list(continuous = &#39;smooth&#39;), axisLabels = &quot;none&quot;, switch = &#39;both&#39;) affairReg &lt;- lm(nbaffairs ~ ., data=Fair) summary(affairReg) ## ## Call: ## lm(formula = nbaffairs ~ ., data = Fair) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.0503 -1.7226 -0.7947 0.2101 12.7036 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.87201 1.13750 5.162 3.34e-07 *** ## sexmale 0.05409 0.30049 0.180 0.8572 ## age -0.05098 0.02262 -2.254 0.0246 * ## ym 0.16947 0.04122 4.111 4.50e-05 *** ## childyes -0.14262 0.35020 -0.407 0.6840 ## religious -0.47761 0.11173 -4.275 2.23e-05 *** ## education -0.01375 0.06414 -0.214 0.8303 ## occupation 0.10492 0.08888 1.180 0.2383 ## rate -0.71188 0.12001 -5.932 5.09e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.095 on 592 degrees of freedom ## Multiple R-squared: 0.1317, Adjusted R-squared: 0.12 ## F-statistic: 11.23 on 8 and 592 DF, p-value: 7.472e-15 "],["references.html", "References", " References Anscombe, FJ. 1973. “Graphs in Statistical Analysis.” American Statistician 27 (1): 17–21. Baker, Monya. 2016. “Is There a Reproducibility Crisis?” Nature 533: 26. Benjamin, Daniel J., James O. Berger, Magnus Johannesson, Brian A. Nosek, E.-J. Wagenmakers, Richard Berk, Kenneth A. Bollen, et al. 2018. “Redefine Statistical Significance.” Nature Human Behaviour 2 (1): 6–10. https://doi.org/10/cff2. Blumenthal, Arthur L. 1975. “A Reappraisal of Wilhelm Wundt.” American Psychologist 30 (11): 1081. Broman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989. Buckheit, Jonathan B., and David L. Donoho. 1995. “Wavelab and Reproducible Research.” In Wavelets and Statistics, 55–81. Springer. Cleveland, William S., and Robert McGill. 1985. “Graphical Perception and Graphical Methods for Analyzing Scientific Data.” Science, New Series 229 (4716): 828–33. https://www.jstor.org/stable/1695272. Cox, Jonathan, and Michael Lindell. 2013. “Visualizing Uncertainty in Predicted Hurricane Tracks.” International Journal for Uncertainty Quantification 3 (2). https://doi.org/10/gjjsfw. Deane, Claudia. 2024. “Americans’ Deepening Mistrust of Institutions.” Donoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734. Donoho, David L. 2010. “An Invitation to Reproducible Computational Research.” Biostatistics 11 (3): 385–88. https://doi.org/10/bxwkns. Erdelyi, Matthew H. 1974. “A New Look at the New Look: Perceptual Defense and Vigilance.” Psychological Review 81 (1): 1–25. https://doi.org/10/cs5c5q. Gandrud, Christopher. 2013. Reproducible Research with R and R Studio. CRC Press. Grange, JA, D Lakens, F Adolfi, C Albers, F Anvari, M Apps, S Argamon, et al. 2018. “Justify Your Alpha.” Nature Human Behavior. Hastie, Reid, and Robyn M Dawes. 2010. Rational Choice in an Uncertain World: The Psychology of Judgment and Decision Making. Sage. Healy, Kieran. 2017. “Data Visualization for Social Science: A Practical Introduction with r and Ggplot2.” Henrich, Joseph, Steven J. Heine, and Ara Norenzayan. 2010. “The Weirdest People in the World?” Behavioral and Brain Sciences 33 (2-3): 61–83. https://doi.org/10/c9j35b. Hicks, Stephanie C., and Rafael A. Irizarry. 2018. “A Guide to Teaching Data Science.” The American Statistician 72 (4): 382–91. https://doi.org/10/gfr5tf. Hornik, Kurt, and The R Core Team. 2022. “R FAQ: The Comprehensive R Archive Network.” Hullman, Jessica, Paul Resnick, and Eytan Adar. 2015. “Hypothetical Outcome Plots Outperform Error Bars and Violin Plots for Inferences about Reliability of Variable Ordering.” Edited by Elena Papaleo. PLOS ONE 10 (11): e0142444. https://doi.org/10.1371/journal.pone.0142444. Ioannidis, John PA. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124. https://doi.org/10/chhf6b. Isaacson, Walter. 2014. The Innovators: How a Group of Hackers, Geniuses, and Geeks Created the Digital Revolution. Simon and Schuster. Jackson, Dan. 2017. “The Netflix Prize: How a 1 Million Contest Changed Binge-Watching Forever.” Thrillist. Com. Kondo, Marie. 2016. Spark Joy: An Illustrated Master Class on the Art of Organizing and Tidying up. Ten Speed Press. Lakatos, Imre. 1969. “Falsification and the Methodology of Scientific Research Programmes.” Criticism and the Growth of Knowledge. Cambridge University Press: Cambridge. Lanning, Kevin. 1987. “Some Reasons for Distinguishing Between ‘Non-normative Response’ and ‘Irrational Decision’.” The Journal of Psychology 121 (2): 109–17. https://doi.org/10/fv4hh5. Leek, Jeffrey T, and Roger D Peng. 2015. “Statistics: P Values Are Just the Tip of the Iceberg.” Nature 520 (7549): 612. https://doi.org/10/gfb8jm. Loevinger, Jane. 1957. “Objective Tests as Instruments of Psychological Theory.” Psychological Reports 3 (3): 635–94. https://doi.org/10/b27jpk. McShane, Blakeley B., David Gal, Andrew Gelman, Christian Robert, and Jennifer L. Tackett. 2017. “Abandon Statistical Significance.” arXiv:1709.07588 [Stat], September. https://arxiv.org/abs/1709.07588. Miguel, Edward, Colin Camerer, Katherine Casey, Joshua Cohen, Kevin M Esterling, Alan Gerber, Rachel Glennerster, et al. 2014. “Promoting Transparency in Social Science Research.” Science 343 (6166): 30–31. https://doi.org/10/gdrcpz. Munafò, Marcus R., Brian A. Nosek, Dorothy V. M. Bishop, Katherine S. Button, Christopher D. Chambers, Nathalie Percie du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J. Ware, and John P. A. Ioannidis. 2017. “A Manifesto for Reproducible Science.” Nature Human Behaviour 1 (1): 0021. https://doi.org/10.1038/s41562-016-0021. Ondaatje, Michael, and Walter Murch. 2002. The Conversations: Walter Murch and the Art of Editing Film. A&amp;C Black. Open Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716–16. https://doi.org/10/68c. Peng, Roger. 2018. “Teaching r to New Users - from Tapply to the Tidyverse.” Peng, Roger D. 2014. R Programming for Data Science. Poulin, Michael J., and Claudia M. Haase. 2015. “Growing to Trust: Evidence That Trust Increases and Sustains Well-Being Across the Life Span.” Social Psychological and Personality Science 6 (6): 614–21. https://doi.org/10.1177/1948550615574301. Reinsel, David, John Gantz, and John Rydning. 2025. “The Digitization of the World.” Slovic, Paul, David Zionts, Andrew K Woods, Ryan Goodman, and Derek Jinks. 2013. “Psychic Numbing and Mass Atrocity.” The Behavioral Foundations of Public Policy, 126–42. https://doi.org/10/gk4945. Sternberg, Robert J. 1999. “The Theory of Successful Intelligence.” Review of General Psychology 3 (4): 292–316. https://doi.org/10/cqrkxh. Sullivan, J. L., and J. E. Transue. 1999. “THE PSYCHOLOGICAL UNDERPINNINGS OF DEMOCRACY: A Selective Review of Research on Political Tolerance, Interpersonal Trust, and Social Capital.” Annual Review of Psychology 50 (1): 625–50. https://doi.org/10/cmthvk. Szucs, Denes, and John Ioannidis. 2017. “When Null Hypothesis Significance Testing Is Unsuitable for Research: A Reassessment.” Frontiers in Human Neuroscience 11: 390. https://doi.org/10/gc6vws. Thies, Justus, Michael Zollhöfer, Matthias Nießner, Levi Valgaerts, Marc Stamminger, and Christian Theobalt. 2015. “Real-Time Expression Transfer for Facial Reenactment.” ACM Trans. Graph. 34 (6): 183–81. https://doi.org/10/f7wqz7. Tufte, Edward R. 2001. The Visual Display of Quantitative Information. 2nd ed. Cheshire, CT: Graphics Press. Tukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67. https://doi.org/10/d48nqg. ———. 1977. “EDA: Exploratory Data Analysis.” Reading, Mass. Tversky, Amos, and Daniel Kahneman. 1974. “Judgment Under Uncertainty: Heuristics and Biases.” Science 185 (4157): 1124–31. https://doi.org/10/gwh. Wainer, Howard. 2007. “The Most Dangerous Equation.” American Scientist 95 (3): 249. https://doi.org/10.1511/2007.65.249. Wainer, H, and D Thissen. 1981. “Graphical Data Analysis,” 51. Watts, Duncan J. 2004. “The \"New\" Science of Networks.” Annual Review of Sociology 3: 243–70. https://www.jstor.org/stable/29737693. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science 2e. \" O’Reilly Media, Inc.\". "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
