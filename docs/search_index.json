[
["index.html", "Data science for the liberal arts", " Data science for the liberal arts Kevin Lanning 2019-01-27 "],
["an-invitation.html", "an invitation what will be in the class? should you enroll?", " an invitation This work-in-progress includes my notes for Introduction to Data Science at the Wilkes Honors College of Florida Atlantic University. Data science is still a new field of study, and there are multiple approaches to teaching it and to its place in the college curriculum. This course is like those at the universities of North Carolina, British Columbia, Duke, Maryland, Wisconsin, Stanford, BYU, Harvard, Pennsylvania, and UC Berkeley (and will likely draw from all of these) in that it is closer to Statistics than to Computer Science. But if our approach is closer to statistics than to programming, it is particularly close to statistics in its most applied and pragmatic form. The choice of statistical methods should follow from the data and problem at hand - or, as Loevinger (1957) once put it, statistics should be the handmaiden of real-world concerns rather than technology. This pragmatic focus is not unique. Courses with similar goals (which we may again draw from) include those at Chicago, Georgia Tech, UC Santa Barbara, Princeton, UC Berkeley, at Berlin’s Hertie School of Governance, and in Columbia’s School of Journalism. what will be in the class? R In my 2017 survey of introductory data science courses, I saw a pretty even split between those which begin with Python and those which begin with the statistical programming language R. This difference corresponds, loosely, to the split noted above: Computer science based approaches to data science are frequently grounded in Python, while stats based approaches are generally grounded in R. Our course, like those for most of the syllabi and courses linked above, will be based in R. Reproducible science The course will provide an introduction to some of the methods and tools of reproducible science. We will consider the replication crisis in the natural and social sciences, and then consider three distinct approaches which serve as partial solutions to the crisis. The first of these is training in a notebook-based approach to writing analyses, reports and projects (using R markdown). The second is using public repositories (such as the Open Science Framework and GitHub) to provide snapshots of projects over time. Finally, the third is to consider the place of significance testing in the age of Big Data, and to provide training in the use of descriptive, exploratory techniques of data analysis. Good visualizations Part of Type C data science is communication, and this includes not just writing up results, but also designing data displays that incisively convey the key ideas or features in a flood of data. We’ll examine and develop data visualizations such as plots, networks and text clouds. More advanced topics may include maps, interactive displays, and animations. All Some of the data It’s been argued that in the last dozen years, humans have produced more than 60 times as much information as existed in the entire previous history of humankind. (It sounds like hyperbole, but even if it’s off by an order of magnitude it’s still amazing). There are plenty of data sources for us to examine, and we’ll consider existing datasets from disciplines ranging from literature to economics to public health, with sizes ranging from a few dozen to millions of data points. We will also clean and create new datasets. All Some of the tools In addition to R, we’ll use a range of other tools: We’ll communicate on the Slack platform. We’ll write using markdown editors such as Typora. We’ll certainly use spreadsheets such as Excel or Google Sheets. We may use additional tools for visualizing data such as Gephi and Tableau. In any event, there will be computing throughout the course. You will be expected to bring a laptop every day. (Please let Dr. Lanning know ASAP if you don’t have access to this). The place of data science in the WHC (and FAU) curriculum At this writing, there is enthusiasm across units of FAU and its affiliated institutes, including Max Planck and FAU’s Colleges of Science and Engineering as well as the WHC, for integrating data science into our curriculum. Within the WHC, a data science minor and a multi-track concentration are under development. Until these proposals have been formally approved, students interested in concentrating in Data Science are encouraged to pursue an individual concentration (see Dr. Lanning for details). In addition, there are several integrated ‘4 + 1’ pathways which will lead to a master’s degree in the College of Engineering. These programs are also in progress; again, see Dr. Lanning for additional details. should you enroll? It’s my intention that this course should serve every Wilkes Honors College student, regardless of concentration. The WHC was built and funded by the state of Florida to train tomorrow’s leaders. That’s you. The skills and insights that you will gain in this course will help you in graduate and professional schools, will help you in your careers, and will help you in your goal of making a better world. And it will help you train the next generation of data scientists, too. How do I sign up? If you are interested in taking the class, complete the Google form here. Note that enrollment in the class and lab will be limited to 30. References "],
["data-science-for-the-liberal-arts.html", "1 data science for the liberal arts 1.1 type C data science = data science for the liberal arts 1.2 the incompleteness of the data science Venn diagram 1.3 a dimension of depth 1.4 Google and the liberal arts 1.5 data sci and TMI 1.6 discussion: what will you do with data science?", " 1 data science for the liberal arts Hochster, in Hicks and Irizarry (2017), describes two broad types of data scientists: Type A (Analysis) data scientists, whose skills are like those of an applied statistician, and Type B (Building) data scientists, whose skills lie in problem solving or coding, using the skills of the computer scientist. This view arguably omits a critical component of the field, as data science is driven not just by statistics and computer science, but also by “domain expertise:” Fig 1.1 - The iconic data science Venn diagram 1.1 type C data science = data science for the liberal arts The iconic Venn diagram model of data science shown above suggests what we will call “Type C data science.” It begins with “domain expertise” in your concentration in the arts, humanities, social and/or natural sciences, it both informs and can be informed by new methods and tools of data analysis, and it includes such things as communication (including writing and the design and display of quantitative data), collaboration (making use of the tools of team science), and citizenship (serving the public good, overcoming the digital divide, furthering social justice, increasing public health, diminishing human suffering, and making the world a more beautiful place). It’s shaped, too, by an awareness of the fact that the world and workforce are undergoing massive change: This puts the classic liberal arts focus of “learning how to learn” (as opposed to memorization) at center stage. And Type C data science is shaped, not least, by the creepiness of living increasingly in a measured, observed world. Type C data science does not merely integrate ‘domain expertise’ with statistics and computing, it places content squarely at the center. We can appreciate the compelling logic and power of statistics as well as the elegance of well-written code, but for the purposes of this book, these are means to an end. Programming and statistics are tools in the service of social and scientific problems and cultural concerns. Type C data science aims for work which is not merely cool, efficient, or elegant but responsible and meaningful. 1.2 the incompleteness of the data science Venn diagram Data visualizations are starting points which can provide insights, typically highlighting big truths or effects by obscuring other, presumably smaller ones. The Venn diagram model of data science is no exception: As with other graphs, figures, and maps, it allows us to see by showing only part of the picture. What does it omit? That is, beyond statistics, computing/hacking, and domain expertise, what other skills contribute to the success of the data scientist? The complexity of data science is such that individuals typically have expertise in some but not all facets of the area. Consequently, problem solving requires collaboration. Collaboration, even more than statistical and technical sophistication, is arguably the most distinctive feature of contemporary scholarship in the natural and social sciences as well as in the private sector (Isaacson 2014). Communication is central to data science because results are inconsequential unless they are recognized, understood, and built upon; facets of communication include oral presentations, written texts and, too, clear data visualizations. Reproducibility is related to both communication and collaboration. There has been something of a crisis in recent years in the social and natural sciences as many results initially characterized as “statistically significant” have been found not to replicate. The reasons for this are multiple and presently contentious, but one path towards better science includes the public sharing of methods and data, ideally before experiments are undertaken. Reproducible methods are a key feature of contemporary data science. Pragmatism refers to the relevance of work towards real-world goals. Ideally, these pragmatic concerns take into account ethical concerns as well. 1.3 a dimension of depth Cutting across these eight facets (statistics, computing, domain expertise, collaboration, communication, reproducibility, pragmatism, and ethics), a second dimension can be articulated. No one of us can excel in all eight domains, rather, we might aim towards goals ranging from literacy (can understand) through proficiency (can get by) to fluency (can practice) to leadership (can create new solutions or methods). That is, we can think of a continuum of knowledge, skills, interests, and goals, ranging from that which characterizes the data consumer to the data citizen to the data science contributor. A Type C data science includes this dimension of ‘depth’ as well. 1.4 Google and the liberal arts Data science is at its core empirical, and all of this rhetoric would be meaningless if not grounded in real world findings. Although it was recently reported that soft skills rather than STEM training were the most important predictors of success among Google employees, it’s difficult to know whether these results would generalize to a less select group. Nonetheless, there is a clear need for individuals with well-rounded training in the liberal arts in data science positions and, conversely, learning data science is arguably a key part of a contemporary liberal arts education. 1.5 data sci and TMI One difference between traditional statistics and data science is that the former is typically concerned with making inferences from datasets that are too small, while the latter is concerned with extracting a signal from data that is or are too big (Donoho 2017). The struggle to extract meaning from a sea of information - of finding needles in haystacks, of finding faint signals in a cacophony of overstimulation - is arguably the question of the age. It is a question we deal with as individuals on a moment-by-moment basis. It is a challenge I face as I wade through the many things that I could include in this class and these notes. The primacy of editing or selection lies at the essence of human perception and the creation of art forms ranging from novels to film. And it is a key challenge that the data scientist faces as well. 1.6 discussion: what will you do with data science? Imagine it is ten years from today. You are working in a cool job (yay). How, ideally, would ‘data science’ inform your professional contributions? More proximally (closer to today) - what are your own goals for progress in data science, in terms of the model described above? References "],
["getting-started.html", "2 getting started 2.1 are you already a programmer and statistician? 2.2 setting up your machine: some basic tools 2.3 discussion: who deserves a good grade?", " 2 getting started We begin with a brief self-assessment, asking you to reflect on your own knowledge of data science, including the necessary-but-not-sufficient areas of computer programming and statistics. We then move to a description of some rudimentary tools that we will be using. 2.1 are you already a programmer and statistician? Regarding programming, you may know more than you think you do. Here’s a simple program - a set of instructions - for producing a cup of coffee: add water to the kettle and turn it on if it’s morning, put regular coffee in the French press, otherwise use decaf if the water has boiled, add it to the French press, else keep waiting if the coffee has steeped for four minutes, depress (smash) piston/plunger, else keep waiting pour coffee into cup enjoy As a post-millennial student from a WEIRD culture (a Western, Educated, Industrialized, Rich Democracy, Henrich, Heine, and Norenzayan (2010), you’ve ‘programmed’ computers, too, if only to enter a password, open an app, and upload a photo on your cell phone. Statistics is of fundamental importance, not just for understanding abstract trends, but for making decisions about everyday life. Consider the case of Susie, a college senior: Exercise 2_1 Susie is applying to two med schools. At School A, 25% of students are accepted, and at School B, 25% are accepted as well. You are Susie. Are you going to get in to at least one of these programs? What is the probability? Does your estimate depend upon any assumptions? Questions like these are important for us. If the combined probability is low, it likely (another probability concept) will make sense for Susie to spend time, money, and energy to apply to additional programs. If the probability is higher, it may not. But problems like this are hard - our estimates of probability are frequently poorly calibrated, and combining probability estimates is challenging (see, e.g., Tversky and Kahneman (1974), and consider taking a course in Behavioral Economics or Thinking and Decision Making to learn more). You may have worked with data in spreadsheets such as Excel or Google Sheets. Exercise 2_2 Open the Google Sheet at http://bit.ly/dslaX2_1. Save a copy and edit it, entering the following in cell B7: =SUM (B2:B6) What is the result? Now copy cell B7 to C7 What happens? Is this the result you expected? Would another approach be more useful?** In data science, spreadsheets are used largely to store data rather than to analyze it. Some best practices for using spreadsheets in data science are given in Broman and Woo (2017). 2.2 setting up your machine: some basic tools Collaboration and communication are integral to data science. In the world beyond universities, the most important messaging and collaboration platform is Slack. Slack is a commercial app, but we will use the free tier. We’ll use Slack for group work, class announcements, and help-seeking and help-providing. Slack includes a simple markdown editor (for ‘posts’). You can find an introduction to markdown syntax in Chapter 3 of Freeman and Ross (2017). I use Typora (currently free for both Windows and Mac), but there are many alternatives. Install a Markdown editor on your laptop and play with it. Install R (https://cran.rstudio.com/) then R studio (https://www.rstudio.com/products/rstudio/#Desktop) on your own Windows or Mac laptop. If you get stuck, reach out to others on Slack; if you don’t get stuck, help your classmates. We’ll use R studio as a front end (an ‘integrated development environment’, or IDE) for R, and will write most of our code in R markdown which is, not surprisingly, a ‘flavor’ of markdown. We’ll go into R in increasing depth beginning in the next chapter; if you want to get a head start, consider Carmichael (2017) Getting started and the first chapter of Wickham and Grolemund (2016). (Those documents, like this one, are all written in R markdown). Eager to start coding in R? Go to Chapter 4 (draw the rest of the owl), and begin the exercises in swirl (swirlstats). Finally, Google Docs is free and is convenient for collaborative work. One other important feature of Google Docs is that it provides a framework for version control, a critical skill in information management. You can learn more about how to see and revert to prior versions of a project in Google Docs here. Version control can help you avoid the chaos and confusion of having a computer (or several computers) full of files that look like Cham’s (2012) comic: Fig 2.1: Never call anything ‘final.doc’. We’ll be talking about the challenge of version control throughout this text - and I am hoping that my own habits in file management can improve as we move forward together. 2.3 discussion: who deserves a good grade? In an introductory class in data science, students invariably come to class with different backgrounds. Should this be taken into account in assigning grades? That is, would it be possible (and desirable) to assign grades in a class based not just on what students know at the end of the term, but also on how much they have learned? A formal, statistical approach to this could use regression analysis. That is, one could predict final exam scores from pretest scores, and use the residuals - the extent to which students did better or worse than expected - as a contributor to final exam grades. Interestingly, there would be an unusual incentive for students on this ‘pretest’ to do, seemingly perversely, as poorly as possible. How could this be addressed? Another problem with this approach is that there may be ‘ceiling effects’ - students who are the strongest coming in to the class can’t improve as much as those who have more room to grow. Again, how might this be addressed? References "],
["an-introduction-to-r.html", "3 an introduction to R 3.1 some other things that R stands for 3.2 a few characteristics of R 3.3 finding help 3.4 Wickham and R for Data Science 3.5 discussion: is open-source software secure?", " 3 an introduction to R R is a system for Reproducible research, and reproducibility is essential (Gandrud 2016). Research is often presented in very selective containers: slideshows, journal articles, books, or maybe even websites… these documents are not the research [rather] these documents are the “advertising”. The research is the “full software environment, code, and data that produced the results” [Buckheit and Donoho, 1995, Donoho, 2010, 385]. When we separate the research from its advertisement we are making it difficult for others to verify the findings by reproducing them. R markdown documents (like Jupyter notebooks in the Python world) facilitate reproducible research, as they include comments or explanations, code, links to data, and results. 3.1 some other things that R stands for Historically, R grew out of S which could stand for Statistics. But what does R stand for? R is a system for Representing data in cool, insight-facilitating ways. R is Really popular, and really growing. Learning R will make you a more attractive candidate for many graduate programs as well as jobs in the private sector. R might stand for Relatively high level. Programming languages can be described along a continuum from high to low level, the former (like R) are more accessible to humans, the latter (like assembly language) more accessible to machines. Python, Java, and C++ are all more towards the middle of this continuum. R stands, in part, for Resources. Because R is popular, there are many resources, including, for example - Online resources include the simple (and less simple) lessons of SwirlR, which offers the possibility of “learning R in R,” as well as DataCamp, the Data Science Certificate Program at Johns Hopkins, and other MOOCs. Books include Peng (2015) - which includes not only videos of his lectures in the program at Hopkins, but also a brief list of still more resources - and Wickham and Grolemund (2016). You’ll also learn (more directly) from people, including your classmates, as well as the broader community of people around the world. There are hundreds if not thousands of people, young and old, who are on the road with you. I am as well, just a step or two (hopefully) ahead. R does not stand for ‘argh,’ although you may proclaim this in frustration (‘arggh, why can’t I get this to work?) or, perhaps, in satisfaction (’arggh, matey, that be a clever way of doing this’). But R does stand for Rewarding. A language is a way of thinking about the world, and this is true for computer languages as well. You’ll be challenged by its complexity, its idiosyncracy, its alien logic. But you will succeed, and you will find that you can do things that you did not believe possible. 3.2 a few characteristics of R R includes the base plus thousands of packages. These packages are customized add-ons which simplify certain tasks, such as text analysis. But there are, at this writing, over 50 different packages for text analysis - so where do you begin? One recent answer, and where we will start, is the curated list of packages which jointly comprise the tidyverse (Wickham and Grolemund 2016). A few years ago, Peng (2015) speculated that “it would be straightforward to build an R package for ordering pizza.” Does one exist now? R is an object-oriented language - one conceptually organized around objects and data rather than actions and logic. In R, at the atomic level, objects include characters, real numbers, integers, complex numbers, and logical. These atoms are combined into vectors, which generally include objects of the same type (one kind of object, “lists,” is an exception to this; Peng 2015). Vectors can be further combined into data frames, which are two-dimensional tables or arrays. A tibble is a particular type of data frame which is, in some ways, handier to work with than other data frames. We’ll be working extensively with data frames in general, and tibbles in particular, as we move forward. Objects have attributes. Attributes of R include such things as name, dimensions (for vectors and arrays), class (that’s the type of object described in the previous paragraph), length, etc. Real world data sets are messy, and frequently have missing values. In R, missing values may be characterized by NA (not available) or NaN (not a number, implying an undefined or impossible value). R Studio, the environment we will use to write, test, and run R code, is a commercial enterprise whose business model, judged from afar, is an important one in the world of technology. Most of what R Studio offers is free (97% according to Garrett Grolemund in the video below). The commercial product they offer makes sense for a relative few, but it is sufficiently lucrative to fund the enterprise. The free product helps to drive the popularity of R studio; this widespread use, in turn, makes it increasingly essential for businesses to use. This mixed free/premium, or ‘freemium,’ model characterizes Slack as well, but while the ratio of free to paid users of Slack is on the order of 3:1, for R it is, I am guessing, an order of magnitude higher than this. 3.3 finding help one does not simply ‘learn R.’ Unlike, say, learning to ride a bicycle, fry an egg, or drive a car with a manual transmission, learning R is not a discrete accomplishment that one can be said to have mastered and from which one then moves on. Rather, R is an evolving, open system of applications and tools which is so vast that there is always more that one can achieve, new lessons that one can learn. And, the complexity of R syntax is such that, for almost all of us, we will need help for coding on any non-trivial task. For us, the key ideas in “looking for help” will include not just the tools on the R Studio IDE, but also (a) using google searches wisely, and (b) reaching out to your classmates on Slack. Here, as in the real world, there is an etiquette for help-seeking which is based on consideration. Your search for help should begin by making sure that others will encounter the same result, then by stripping the problem down to its essence. Once you have reduced the problem to this minimal, reproducible essence, you will often be able to spot the problem yourself - and, if not, you will make it easier for others to help you. There is an R package (reprex) which will likely facilitate this, but I haven’t tried it yet. Here is a good introduction. Finally, to get a sense of some of the ways you can get help in R studio (and to see how a master uses the R Studio interface), consider this video: Video 3.1: Garret Grolemund of R Studio 3.4 Wickham and R for Data Science The first chapter of the Wickham text (Wickham and Grolemund 2016) provides a framework for his approach and a brief introduction to the tidyverse which will be the dialect of R we will study in the weeks ahead. Please read it now. 3.5 discussion: is open-source software secure? Perhaps the most important feature of R is that it is open-source software. This is important not just because it saves you money, but because contributing to the world of R is an act of digital democracy. In using and contributing to the world of R we open up knowledge to others who may lack our privileges. R, like Android or Wikipedia, is a tool for all of us, maintained and continually improved upon by the crowd. But is open-source software safe? More generally, in a data-dependent world, who should be the guardians of the code that connects us? Securing the Internet of Vehicles. To consider just one example, the computer systems in modern cars typically run millions of lines of code. As cars become increasingly autonomous, this complexity will only increase. (Incidentally, the Society of Automotive Engineers, or SAE, describes 6 levels of ‘auto autonomy.’ At this writing, the most sophisticated systems available to consumers, such as Tesla Autopilot, are at level 2. What lies ahead are cars which are self-driving on carefully selected, geo-fenced roads, and ultimately cars “which can operate on any road… a human driver could negotiate”). Our roads and highways will become an Internet of Vehicles (IOV), which will include not just connections between cars and an intelligent cloud ‘above us’ but also direct links between a distributed system of intelligent cars, stoplights, and road sensors in a fog ‘around us’ (Bonomi et al. 2012). Fog computing and the IOV will reduce travel times and increase both fuel efficiency and automotive safety. Obviously, there are cybersecurity concerns. While the prospects for a chaotic, choreographed hack of hundreds of vehicles on the streets of Manhattan, such as that in the 2017 movie “The Fate of the Furious”, are remote at best (or worst), there have been examples of “white-hat hackers” who have successfully infiltrated (and thereby helped secure) car information systems. As the IOV develops, there will be vulnerabilities to privacy as well as safety, and the security of the system will be paramount. Different car manufacturers are taking different approaches to developing secure information systems, with many using a closed or proprietary approach. But the scope of the problem is so large that there is a movement towards pooling resources and encouraging collaboration among industry partners, academics, and citizen scientists in the development of an open-source autonomous driving platform, such as Apollo. Perhaps counterintuitively, there may be significant security advantages to using source code that is open to all (Clarke, Dorwin, and Nash 2009; FitzGerald, Levin, and Parziale 2016). References "],
["now-draw-the-rest-of-the-owl.html", "4 now draw the rest of the owl 4.1 Carmichael 4.2 DataCamp 4.3 Swirl (Swirlstats) 4.4 Peng text and videos 4.5 Something else 4.6 Exercise", " 4 now draw the rest of the owl Fig 4.1: Draw the rest of the owl. There are many sources for learning the basics of R. A few of these follow. Please spend at least 90 mins exploring at least two of the following. Be prepared to discuss your progress next class (you will be asked which source(s) you used, what you struggled with, and whether you would recommend it to your classmates. (Note that all of these are free, though you may choose to make a donation to the author if you use the Peng text). Hint: If you find the material too challenging - if you feel like you are drawing the rest of the owl - take a break away from your machine and other screens, clear your head, then try a different approach. 4.1 Carmichael Iain Carmichael prepared the following for his Intro to Data Science course at UNC-Chapel Hill. I think it is a great place to start: https://idc9.github.io/stor390/notes/getting_started/getting_started.html 4.2 DataCamp Many folks swear by (and others, I presume, at) DataCamp, which kind of gamifies learning software. As a student in this class, you have access to all of their stuff… free. You can even do lessons on your phone. 4.3 Swirl (Swirlstats) I, like thousands of others, learned R in the process of completing the Johns Hopkins Data Science Specialization offered through Coursera. The sequence can be challenging, but their introduction to R used an accessible, interactive R package called Swirl. You can read about swirl (“learn R in R”) at https://swirlstats.com/. Using Swirl. After loading R (and opening R studio), you will get to the Swirl lessons with the following steps: Install the Swirl package on your computer (you only need to do this once). Type the following into your console window in R studio (typically left hand side of your screen or lower left) install.packages(“swirl”) Then load the package into your workspace (you’ll need to do this at the beginning of every session you use Swirl) library (swirl) Then run it! swirl () Swirl will ask a few questions then give you the option of choosing one of several courses. You’ll choose the R Programming option, which leads to 15 separate lessons. At the end of each lesson, you’ll be asked Would you like to receive credit for completing this course on Coursera.org? Answer no… then do another lesson. 4.4 Peng text and videos Finally, consider the text and videos from the Coursera R class. Most of the material from that class can be found in Peng (2015). A slightly updated version of the text can be found at https://bookdown.org/rdpeng/rprogdatascience/, and the videos in the series may be found by clicking on the following: . Video 4.2: Roger Peng introducing R 4.5 Something else The something else category includes Datacarpentry.org, which is aimed at fostering data literacy and provides free lessons in areas such as Genomics and Geospatial data analysis. Of particular interest is the social science lessons, which include a basic introduction to R and data science based on the &quot;Studying African Farmer-led Irrigation (SAFI)&quot; dataset. 4.6 Exercise Review Carmichael’s Getting started with R. Open R studio, and create a new R script called myMovies. Using his code as a reference, do each of the following Work in your source window. On the first line, enter the command to install the tidyverse. (If you already have done this, you can comment out the command …) # install.packages (&quot;tidyverse&quot;) Hit ctrl+enter to run this line. Then, comment it out if you haven’t already done so (why)? Load the tidyverse into your workspace. Load the movies/IMDB dataset. Start exploring the data Apply the str (structure), head, and summary commands. When are each of these useful? Double-click the movies dataset in your environment tab in R studio. Click on a few columns to sort the data. In the data, what does ‘spilled’ mean? How did you find out? How many rows and columns are in the data We can think about the movies dataset as a matrix with rows and columns, and subset it using the following. # data.frame[rownumber,colnumber] # data.frame[&quot;rowname&quot;, &quot;colname&quot;] # data.frame[rowname, c(&quot;colname, colname&quot;)] movies[&quot;title&quot;] movies[title] movies[title,] Ask a question about the data, and enter it as a comment in your code, e.g., # how long was the movie 42-up? Try to find the answer, ideally using reproducible code, and be prepared to share it with the class. References "],
["principles-of-data-visualization.html", "5 Principles of data visualization 5.1 Some opening thoughts 5.2 Some early graphs 5.3 Tukey and EDA 5.4 Approaches to graphs 5.5 Tufte: First principles 5.6 Telling the truth, when the truth is unclear 5.7 a supplement: Code for Asymmetrical Euler/Venn diagrams 5.8 further reading and resources", " 5 Principles of data visualization 5.1 Some opening thoughts Graphs aren’t just to inform, but to make you reflect. We are concerned not just with the design of graphs, but with our role in society as leaders, as arbiters of facts, as educators. We want to foster data literacy. How do people understand graphs? How can you use graphs to tell people the truth in a way that they understand? How can you convey uncertainty in a graph? When you see a graph, what do you notice, what do you wonder, and what is the story? Is “story telling” what visualizations should be about? A graph might begin with perception and understanding (the consumer), with knowledge and design values (the producer), but it also reflects the truth of the data. How much is each? 5.2 Some early graphs Visual displays of information reach back to prehistoric times; modern graphs date back, arguably, to Playfair’s 1786 Political Atlas - in which “… spatial dimensions were used to represent nonspatial, quantitative, idiographic, empirical data. Although it now seems natural to represent, for example, rising and falling imports over time as a rising and falling line, it does not seem to have been done before that time and was quite an accomplishment. Notably, in addition to the statistical line chart, Playfair at one fell swoop single-handedly invented most of the remaining forms of the statistical graphic repertoire used today-the bar chart and histogram, the surface chart, and the circle diagram or”pie chart.&quot; (Wainer and Thissen 1981) source The most celebrated early graph is that of Minard: source The visualization depicts the size, latitude, and longitude of Napoleon’s army as they moved towards (tan line) then away (black line) from Moscow; temperature during the retreat is plotted as well. Further, vertical lines linking the temperature display to the number of troops indicate the often perilous river crossings which further decimated Napoleon’s troops). Cheng (2014) decomposes the graph and provides some simpler visualizations; she also provides the following background: &quot;Czar Alexander of Russia sees that Napoleon was becoming too powerful, so he refuses to participate in this embargo [against the UK]. Angry at Czar Alexander’s decision, Napoleon gathers a massive army of over 400,000 to attack Russia in June of 1812. While Russia’s troops are not as numerous as France’s, Russia has a plan. Russian troops keep retreating as Napoleon’s troops move forward, burning everything they pass, ensuring that the French forces could not take anything from their environment. Eventually the French army follows the Russian army all the way to Moscow during October, suffering major losses from lack of food. By the time Napoleon gets to Moscow, he knows he has to retreat. As winter settles into Europe and the temperature drops, Napoleon’s troops suffer even more losses, returning to France from lack of food, disease, and weather conditions.&quot; Of course, the casualties and retreat of Napoleon’s army are immortalized not just in this graph, but also in Russian literature (Tolstoy’s War and Peace) and music (Tchaikovsky’s 1812 overture, in which five cannon shots mark the battle of Borodino and eleven more mark the arrival of Napoleon in the now-razed city of Moscow). 5.3 Tukey and EDA For Donoho (2017), the publication of John Tukey’s “Future of Data Analysis” (Tukey 1962) arguably marks the beginning of data science. As the first data scientist, Tukey embraced a descriptive and exploratory approach to data analysis, particularly in his publication of Exploratory Data Analysis (Tukey 1977). In EDA, Tukey presented an idiosyncratic, coherent approach to looking at data, beginning with tallying. The distributions of small counts of single variables, for Tukey, could best be presented in stem and leaf displays. Comparisons between groups can be presented in box plots. To examine relationships between variables and the adequacy of simple models, he argues for thoughtfully transforming data to uncover linear trends, then to examine residuals to find where these trends do not hold. 5.4 Approaches to graphs A graph might begin with perception and understanding (the consumer), with knowledge and design values (the producer), but it also reflects the truth of the data. How much is each? In thinking about how to design graphs, we can begin with abstract theory, with principles of design informed by our understanding of perception, or with empirical analyses of understanding and memory. 5.5 Tufte: First principles Tufte (2001) describes Graphical Excellence. Graphs should, among other things, “Induce the viewer to think about the substance, rather than about methodology, graphic design, the technology of graphic productions, or something else.” Graphs should “Present many numbers in a small space, make large data sets coherent, and encourage the eye to compare different pieces of data.” Graphs should “serve a reasonably clear purpose: description, exploration, tabulation, or decoration [and] be closely integrated with the statistical and verbal descriptions of a data set.” Tufte concludes with the following Principles of Graphical Excellence, which I quote verbatim: Graphical excellence is the well-designed presentation of interesting data—a matter of substance, of statistics, and of design. Graphical excellence consists of complex ideas communicated with clarity, precision and efficiency. Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space. Graphical excellence is nearly always multivariate. And graphical excellence requires telling the truth. 5.5.1 The cost of poor design I: Space Shuttle Challenger In a cold morning in January, 1986, Space Shuttle Challenger lifted off from Cape Canaveral. Because of the cold weather, engineers at Morton Thiokol, who designed the rocket boosters, considered the possibility that the O-rings which sealed joints on the rockets would be too hard and brittle to prevent the release of potentially explosive fuel. They examined the relation between temperature and o-ring damage on prior flights, using the following visualization: Figure 5.3: What Motron Thiokol engineers saw prior to deciding to launch the Challenger Space Shuttle in January, 1986 What, if anything, can we see here? The graphic was chaotic and poorly designed; if the engineers had wanted to systematically explore the relation between o-ring damage and temperature they could have removed the rockets and arranged them on a single axis by liftoff-temperature rather on several rows by liftoff-date. Here’s what they would have seen: Figure 5.4: What the engineers could have seen, perhaps, with a better graph. The forecast for the morning of the launch was far colder than that for prior launches, and within the set of prior launches a clear relationship between temperature and o-ring damage is evident. But the Challenger did launch, exploding shortly after takeoff, killing the seven astronauts on board. With a clearer display of the data, the Challenger launch would likely have been postponed (Tufte 2001 ). 5.5.2 The cost of poor design II: An uninformed or misinformed world. In the flood of information that surrounds us, our peripheral and sometimes our focal attention will be drawn to pictorial summaries of consequential events and effects. But while data visualizations and data journalism has become more responsible, poorly designed graphs - what Tufte has described as “chartjunk” - are still common. Consider the following graph, which recently appeared in a number of newspapers or feeds in the United States (including the Palm Beach Post, on January 12, 2019). badgraph Exercise 5_1 Examine the graph shown above. Look at it for just a moment, as if you might while reading something else on the screen, or as your ten-year old little sister might. What does the graph tell you? Look at it more closely. What does it actually say? How could you improve it to make the content of the graph more informative about its title? Why was the graph designed in this way? Does this matter? Poorly designed graphs don’t just confuse rocket scientists. They reach us, mislead us, and make fools of all of us as well. With better graphs, we become a better informed, better educated citizenry. 5.5.3 Should graphs begin with psychological theory? Speaking of America, consider the following. Figure 5.5: Chernoff’s too-clever faces In this figure, from Wainer and Thissen (1981), (Chernoff’s) faces are used to represent multivariate data. The display is loosely based on the (psychological) premise that, from infancy, we are inordinately sensitive to facial features. Consequently, the most powerful, effective, or informative data displays should manipulate facial features. Does this graph succeed? Why or why not? Would an updated model, based on richer, more complex, and more realistic simulations of facial expressions (Thies et al. 2015) be more successful? 5.5.4 The power of animation This section concludes with two brief (&lt; 5 minutes) animated data displays, each powerful in its own way, and each giving the viewer an appreciation of an important social phenomenon. The first is from the late Hans Rosling, and summarizes a chunk of the Gapminder data. There is a Gapminder package in R if you would like to explore the data further, and perhaps become the next Rosling: Video 5.6: Rosling and social progress The second is from Kim Rees and her ex-colleagues at Periscopic (Rees is now at CapitalOne). For me, it’s an important graphic because it shows an approach to overcoming what has been called “psychic numbing” - we are sensitive to the individual case, but lose our perspective when faced with large-scale tragedies, so that effectively the more lives lost… the less we care (Slovic et al. 2013). Video 5.7: Rees and stolen years 5.6 Telling the truth, when the truth is unclear We live in an uncertain world, and a major challenge in data visualization is how to convey this uncertainty in a way that people understand readily. Examples of this are familiar. Here in Florida, projected hurricane paths are frequently represented by a “cone of uncertainty” surrounding a single track. An alternative approach is to use a display which shows a distribution of possible tracks. Figure 5.8: Two approaches to displaying hurricane paths 5.6.1 Animated approaches To further illustrate the idea of uncertainty, data visualizations may be animated. To display hurricane outcomes, for example, individual lines in a spaghetti plot might fade in and out (see Cox and Lindell 2013). Another use of animation is suggested by (Hullman, Resnick, and Adar 2015) who use hypothetical outcome plots rather than simple error bars to illustrate sampling distributions. Empirically, subjects have a better understanding of the meaning of statistical effects when these are represented by dynamic displays. During the 2016 US Presidential Election, the NY Times graphics presented the range of likely possibilities (here, between the 25th and 75th percentiles) as a dynamically changing “jittery gauge” . Following the election, there was some criticism of these gauges, as readers claimed that they tended to make them anxious. However, it appears that at least some of the anxiety was attributable not to the gauges themselves, but to the electoral outcome itself. The gauges were back in 2018, and will likely be used again in the future. Finally, animation may be used not just to display uncertainty or a possible range, but also data displays which both carry a great deal of information and unfold over time, such as the language of personality development in my own work. 5.7 a supplement: Code for Asymmetrical Euler/Venn diagrams What follows is part of the code I used in a recent talk about asymmetries in set sizes (Lanning 2018). 5.7.1 Setup In the first block, I load libraries, generate a palette. Palette for Venn is a little weird as must be (a) visible for colorblind, (b) intersections of colors should be distinguishable, and (c) not ugly. So I fiddled here. library(dplyr) library(tidytext) library(tidyverse) library(eulerr) library(grid) library(gridExtra) library(RColorBrewer) cbPalette &lt;- brewer.pal(9,&quot;PuBuGn&quot;) threeColors &lt;- cbPalette[c(9,7,5)] fourColors &lt;- cbPalette[c(9,7,6,4)] eulerr_options(pointsize = 16, fills = list(fill = threeColors), edges = list (lty = 1, col = &quot;blue&quot;), labels = list(fontfamily = &#39;sans&#39;, font = 1, col = &quot;yellow&quot;), quantities = list (fontfamily = &#39;sans&#39;, font = 3, col = &quot;yellow&quot;)) savePlot &lt;- function(myPlot,filename) { png(filename) print(myPlot) dev.off() } 5.7.2 CPI- CQ This examines the reciprocal predictability of two personality measures, the California Psychological Inventory and the California Q-set. CPIArea &lt;- 1 CQArea &lt;- 154/107 # Ratio of Rsq from article fit3 &lt;- euler(c (&quot;A&quot;=CPIArea, &quot;B&quot;=CQArea, &quot;A&amp;B&quot;=.107), shape = &quot;ellipse&quot;) plot(fit3, labels = c(&quot;CPI&quot;, &quot;CQ-Set&quot;)) 5.7.3 Scholarly communities Here, I generate proportional Venn for scholarly communities. First for personality v social. Counts describe the number of community members in (Lanning 2017). # order of sets is shifted to put # earliest level at 12 o&#39;clock personality &lt;- 12 self_reg &lt;- 7 attitudes &lt;- 13 fit4 &lt;- euler(c (&quot;A&quot;=personality, &quot;B&quot;=self_reg, &quot;C&quot;=attitudes, &quot;A&amp;B&quot; = 1, &quot;A&amp;C&quot; = 0, &quot;B&amp;C&quot; = 1, &quot;A&amp;B&amp;C&quot; = 0), shape = &quot;ellipse&quot;) eulerr_options(pointsize = 12, fills = list(fill = threeColors), edges = list (lty = 1, col = &quot;blue&quot;), labels = list(fontfamily = &#39;sans&#39;, font = 1, col = &quot;yellow&quot;), quantities = list (fontfamily = &#39;sans&#39;, font = 3, col = &quot;yellow&quot;)) plot(fit4, labels = c(&quot;Personality&quot;, &quot;Self-Regulation&quot;, &quot;Attitudes&quot;)) 5.8 further reading and resources If you’d like to learn more, Tufte (2001) and his other books are beautiful and thought provoking. Cleveland and McGill (1985) examines graphs from a more rigorous psychological and empirical viewpoint. The Data Stories podcasts are often excellent, despite the challenge of an auditory medium for visual display (especially the episode on Hans Rosling). And Healy (2017) provides a comprehensive introduction that, in essence, provides the basis for a semester-long course based in the ggplot2 package in R that we will be working with in the next chapter (Wickham and Grolemund 2016). References "],
["visualization-in-r-with-ggplot.html", "6 visualization in R with ggplot 6.1 picture &gt; (words, numbers)? 6.2 your ggplots 6.3 facets - displaying the Anscombe data", " 6 visualization in R with ggplot In the last chapter, we introduced data visualization, citing vision-aries including Edward Tufte and Hans Rosling, inspired works such as Minard’s Carte Figurative and Periscopic’s stolen years, as well as a few cautionary tales of misleading and confusing graphs. Here, in playing with and learning the R package ggplot, we begin to move from consumers to creators of data visualizations. As the first visualization in Wickham and Grolemund (2016) reminds us, data visualization is at the core of exploratory data analysis: In the world of data science, statistical programming is about discovering and communicating truths within your data. This exploratory data analysis is the corner of science, particularly at a time in which confirmatory studies are increasingly found to be unreproducible. Most of your reading will be from Chapter 3 of Wickham and Grolemund (2016), this is intended only as a supplement. 6.1 picture &gt; (words, numbers)? The chapter begins with a quote from John Tukey about the importance of graphs. Yet there is a tendency among some statisticians and scientists to consider graphic representations of data as less valuable than statistical ones. Perhaps, if a picture is worth a thousand words, a graph can likewise tell us more than good solid numbers. Consider ‘Anscombe’s quartet’: spreadsheet61 Exercise 6_1 Consider the spreadsheet chunk presented above, which I am (fictionally) characterizing as data collected on a sample of ten primary school children at recess on four consecutive days. Working with your classmates, compute the mean, standard deviation, and correlation between the two measures for one day. Share your results with the class. The four pairs of variables in Anscombe (1973b) appear statistically “the same,” yet the data suggest something else. Later, we’ll try to plot these. Perhaps graphs can reveal truths that statistics can hide. 6.2 your ggplots In class, we will review and recreate the plots in section 3.2 of Wickham and Grolemund (2016) and exercises in 3.2.4. On your laptop, within Rstudio, consider reading about the mpg data in the ‘help’ panel, and pulling up the mpg data in a view window. Click in various places (the stars) in each window to see what happens: Fig. 6.1: Some data If you haven’t already done so, you should be able to successfully recreate the plots in sections 3.2 through 3.4. 6.3 facets - displaying the Anscombe data When we get to section 5 (facets), it may occur to you that this would be a nice way to display the Anscombe data. Fortunately, they are already, like many other datasets, stored in R. library (tidyverse) # to get a list of preloaded datasets, uncomment this line # data() data(anscombe) str(anscombe) ## &#39;data.frame&#39;: 11 obs. of 8 variables: ## $ x1: num 10 8 13 9 11 14 6 4 12 7 ... ## $ x2: num 10 8 13 9 11 14 6 4 12 7 ... ## $ x3: num 10 8 13 9 11 14 6 4 12 7 ... ## $ x4: num 8 8 8 8 8 8 8 19 8 8 ... ## $ y1: num 8.04 6.95 7.58 8.81 8.33 ... ## $ y2: num 9.14 8.14 8.74 8.77 9.26 8.1 6.13 3.1 9.13 7.26 ... ## $ y3: num 7.46 6.77 12.74 7.11 7.81 ... ## $ y4: num 6.58 5.76 7.71 8.84 8.47 7.04 5.25 12.5 5.56 7.91 ... head(anscombe) ## x1 x2 x3 x4 y1 y2 y3 y4 ## 1 10 10 10 8 8.04 9.14 7.46 6.58 ## 2 8 8 8 8 6.95 8.14 6.77 5.76 ## 3 13 13 13 8 7.58 8.74 12.74 7.71 ## 4 9 9 9 8 8.81 8.77 7.11 8.84 ## 5 11 11 11 8 8.33 9.26 7.81 8.47 ## 6 14 14 14 8 9.96 8.10 8.84 7.04 Unfortunately, the Anscombe data in R have a different structure. Here’s one way to reshape it: anscombe$kid &lt;- c(&quot;Al&quot;, &quot;Barb&quot;, &quot;Cathy&quot;, &quot;Dirk&quot;, &quot;Edwin&quot;, &quot;Flo&quot;, &quot;George&quot;, &quot;Henry&quot;, &quot;Isaiah&quot;, &quot;Jim&quot;, &quot;Ken&quot;) # make a file anscombe2 from anscombe anscombe2 &lt;- anscombe %&gt;% # make a new variable called x from x1:x4 gather(x,levelx,x1,x2,x3,x4, # don&#39;t mess with the other variables -c(y1,y2,y3,y4,kid)) # to peek at it, uncomment the next line. # head(anscombe2) anscombe2 &lt;- anscombe2 %&gt;% gather(y,levely,y1,y2,y3,y4, -c(x,levelx,kid)) %&gt;% # keep only pairs where the x and y vars are the same filter (substr(x,2,2) == substr(y,2,2)) %&gt;% # drop one of these select (-y) %&gt;% mutate(x = recode(x, x1=&quot;Mon&quot;,x2 = &quot;Tue&quot;, x3 = &quot;Wed&quot;, x4 = &quot;Thu&quot;)) head(anscombe2) ## kid x levelx levely ## 1 Al Mon 10 8.04 ## 2 Barb Mon 8 6.95 ## 3 Cathy Mon 13 7.58 ## 4 Dirk Mon 9 8.81 ## 5 Edwin Mon 11 8.33 ## 6 Flo Mon 14 9.96 str(anscombe2) ## &#39;data.frame&#39;: 44 obs. of 4 variables: ## $ kid : chr &quot;Al&quot; &quot;Barb&quot; &quot;Cathy&quot; &quot;Dirk&quot; ... ## $ x : chr &quot;Mon&quot; &quot;Mon&quot; &quot;Mon&quot; &quot;Mon&quot; ... ## $ levelx: num 10 8 13 9 11 14 6 4 12 7 ... ## $ levely: num 8.04 6.95 7.58 8.81 8.33 ... Copy this code into your console, then try applying the code in 3.5 to the Anscombe data. Keep track of your challenges…and save your work. References "],
["probability-and-inference.html", "7 probability and inference 7.1 On probability", " 7 probability and inference Begin with a discussion of Anscombe (1973a) and his quartet, and how visualizing data is valuable. 7.1 On probability Discrete probability is used to understand the likelihood of categorical events. We can think of initial estimates of probability as subjective or personal. For some events (what is the probability this plane will crash?), an estimate of probability can be drawn from a base rate or relative frequency (e.g., p(this plane will crash) = (number of flights with crashes/ number of flights)). For other events (what is the probability that the US President will resign or be impeached before completing his term of office?), it may be hard to arrive at a suitable base rate. Here, a number of subjective beliefs or principles may be combined to arrive at a subjective or personal probability. In a sense, all probability estimates begin with a personal belief such as this, in part because the choice of the most informative base rate is often not self-evident - in the plane crash example, maybe we should consider a reference group rates such as ‘for this airline’ etc. (Lanning 1987). The personal origins of probability estimates should become less important as we are exposed to data and revise our estimates in accordance with Bayes theorem. But over the last 45 years, a substantial body of evidence has demonstrates that, under at least some circumstances, we don’t make estimates of probability in this way. There is a nice r markdown document discussing basic laws of probability at Harvard’s datasciencelabs repository: https://github.com/datasciencelabs/2017/blob/master/lectures/prob/discrete-probability.Rmd. We can also use probability with continuous variables such as systolic blood pressure (that’s the first one), which has a mean of approximately 120 and a standard deviation of 15. Continuous probability distributions are handy tools for thinking about the meaning of scores, particularly when we express scores in standard deviations from the mean (z scores). More to the point, this way of thinking about probability is widely used in questions of scientific inference, as, for example, in testing hypotheses such that “the average systolic blood pressure among a group of people studying at Crux (hence caffeinated) will be significantly greater than that of the population as a whole.” This is the logic of Null Hypothesis Significance Testing (NHST) - if the result in my Crux sample is sufficiently high, then I say that I have rejected the null hypothesis, and found data which support the hypothesis of interest. 7.1.0.0.0.1 Let’s write up a sample study, describing the methods we would use to test this. References "],
["the-most-dangerous-equation.html", "8 the most dangerous equation", " 8 the most dangerous equation (Wainer 2007a) argues that deMoivre’s equation is the most dangerous equation - this equation (for the standard error) shows that variability decreases with the square root of sample size. Other nominees include the linear regression equation (and, in particular, how coefficients may change or reverse when new variables are added) and regression to the mean. Regarding linear regression, we discussed (a little) Simpson’s paradox, that is, that the direction of regression coefficients may change when additional variables are added. I argued that, from the standpoint of psychology, ignorance of regression to the mean was arguably more ‘dangerous’ than ignorance about the central limit theorem and standard error, in particular because regression effects contribute to an overestimate of the effectiveness of punishment and an under-appreciation of the effectiveness of positive reinforcement as tools for behavior change (Hastie and Dawes 2010). References "],
["reproducibility-and-the-replication-crisis.html", "9 Reproducibility and the replication crisis 9.1 Answers to the reproducibility crisis I: On NHST 9.2 Answers to the reproducibility crisis II: Pre-registration", " 9 Reproducibility and the replication crisis Probability theory is elegant, and the logic of NHST is compelling. But philosophers of science have long recognized that this is not how science works (Lakatos 1969). (Consider, for example, a simple test of whether gravity exists). In recent years, the tension between the false ideal of NHST and the real world of science has become increasingly evident. Within psychology, experimental studies have often - even typically - failed to replicate (Collaboration and others 2015). It’s not just psychology (Baker 2016). One of the first important papers to shine light in the area (Ioannidis 2005) came from medicine; it suggested six contributing factors, which I quote verbatim here: The smaller the studies conducted in a scientific field, the less likely the research findings are to be true. This stems directly from our discussion of the central limit theorem and the instability of results from small samples. The smaller the effect sizes in a scientific field, the less likely the research findings are to be true We’ll talk about effect size below. The greater the number and the lesser the selection of tested relationships in a scientific field, the less likely the research findings are to be true. (and) The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true. The “problem” of analytic flexibility leads to ‘p-hacking’ The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true and The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true. Positive findings rise, and negative ones are ignored. And scientists are human, and subject to incentives. Here’s a video which provides some more context for the crisis: https://www.youtube.com/watch?v=42QuXLucH3Q (12 mins) 9.1 Answers to the reproducibility crisis I: On NHST The first cluster of responses addresses problems with Null Hypothesis Significance Testing (NHST). These include (a) justifying one’s alpha - making it more stringent, for example, for counter-intuitive claims (Grange et al. 2018), (b) changing the default p value from .05 to .005 (Benjamin et al. 2017), and (c) abandon significance testing altogether (McShane et al. 2017). Szucs and Ioannidis (2017) goes into some of these issues in more detail and discusses other limitations of significance testing, including the dichotomous, all-or-none silliness of the accept/reject decision. (If you play the NHST game, there is no ‘almost’ significant, ‘approached significance,’ etc.). Leek and Peng (2015) argue that the problems are not merely with NHST, but with the whole of data analysis. They maintain that better training in data science - courses like ours, perhaps, are part of the answer. (figure) 9.1.1 Munafò et al. (2017) also argue that threats to reproducible science occur at a number of places in science, not just with the evaluation of hypotheses. munafo2017threats 9.2 Answers to the reproducibility crisis II: Pre-registration The second type of response includes (d) preregistering your work (Miguel et al. 2014). There’s a video here: https://www.futurelearn.com/courses/open-social-science-research/0/steps/31436 (5 mins). For randomized controlled trials, consider socialscienceregistry.org, and for more general use, use the open science framework page. Incidentally, you can post your theses after they are finished at https://thesiscommons.org. garden of the forking paths There’s a collection of papers here… https://www.nature.com/collections/prbfkwmwvz/ Showing exactly what you have done and how (and why) you got there is at the core of reproducible science. 9.2.1 What would results for our sample study look like? How many of these problems would it face? What should we do about it? References "],
["literate-programming-with-r-markdown.html", "10 Literate programming with R markdown 10.1 A challenge 10.2 r markdown 10.3 univariate data 10.4 Homework due Feb 14 (with a solution). 10.5 What to do when you are stuck", " 10 Literate programming with R markdown A note from chapter 4 (coding basics): use assignment (x &lt; - 3+4) rather than x = 3 + 4 because it is more clear. Showing your work - for you as well as others - is another part of reproducible science. R Markdown documents allow you to include comments, scripts, and results in a single place. We begin R4DS 6 (Workflow: Scripts, p. 110) which shows the R studio interface and encourages you to save your work using scripts. You will save your work in projects - which isolate your work, setup files, etc., into different directories. (See r4ds, chapter 8). To reinforce the idea that your unit of analysis in R is “the project” rather than “the script”, consider associating your Rmd filetype with your markdown editor, and only your Rproj filetype with R studio. The R markdown language is discussed in R4ds, Chapter 27 (p. 505). 10.1 A challenge Working with two of your classmates, write an R markdown document titled “The most dangerous equation?” which (a) in the introduction, discusses Wainer (2007b), (b) then illustrates regression to the mean and (c) deMoivre’s equation, ideally (d) using the examples of ‘punishment’ and ‘sex differences in variability’ discussed in class and the text, respectively. Prepare a presentation using Rpres which summarizes your argument and findings. Present this in class on 2/26. (One starting point: the aforementioned r markdown document discussing basic laws of probability at Harvard’s datasciencelabs repository: https://github.com/datasciencelabs/2017/blob/master/lectures/prob/discrete-probability.Rmd. 10.2 r markdown Goals and functions one goal is making your work clear to others, and to a later you Parts YAML header: Some output formats Text formatted in markdown R code (chunks) surrounded by code fences (occasionally, inline code) See Rmd cheatsheet (on Slack) 10.3 univariate data kable and other approaches to generating output exercise 27.4.7 2 (https://raw.githubusercontent.com/hadley/r4ds/master/rmarkdown/diamond-sizes.Rmd) 10.4 Homework due Feb 14 (with a solution). 10.4.1 “Diamond sizes” (27.4.7 - #2) This is from Hadley’s book, with an initial shot at the code from Reilly. “Download diamond-sizes.Rmd from https://github.com/hadley/r4ds/tree/master/rmarkdown. Add a section that describes the largest 20 diamonds, including a table that displays their most important attributes” We copied the text of this into the present file, saving it as an ‘r markdown’ file. 10.4.2 Downloaded file block 1 What does this block do? This block loads needed libraries, then takes the diamond dataset and creates a smaller one which includes only diamonds &lt;= 2.5 carats. library(ggplot2) library(dplyr) smaller &lt;- diamonds %&gt;% filter(carat &lt;= 2.5) 10.4.3 Downloaded text block This section illustrates inline code. We have data about 53940 diamonds. Only 126 are larger than 2.5 carats. The distribution of the remainder is shown below: This block makes a chart, with bins of .01, showing the frequency of different size diamonds smaller %&gt;% ggplot(aes(carat)) + geom_freqpoly(binwidth = 0.01) 10.4.4 My revised graph Change 1 or more parameters of this graph to make it more useful. Describe your changes here I re-binned the x axis and labeled modal values smaller %&gt;% ggplot(aes(carat)) + geom_freqpoly(binwidth = .125) + scale_x_continuous( breaks = c(0, .375, .75, 1, 1.25, 1.5, 2, 3)) 10.4.5 Syntax for table (kable) In class, Lanning showed how to use kable to generate a nicer looking table, of just the first five rows (observations) and columns (variables or attributes). smallest &lt;- smaller [1:5,1:5] # smallest # this would generate an ugly table - remove the first hashtag if you don&#39;t believe me # str(diamonds) knitr::kable(smallest, caption = &quot;Five rows and columns of the diamonds frame.&quot; ) Table 10.1: Five rows and columns of the diamonds frame. carat cut color clarity depth 0.23 Ideal E SI2 61.5 0.21 Premium E SI1 59.8 0.23 Good E VS1 56.9 0.29 Premium I VS2 62.4 0.31 Good J SI2 63.3 10.4.6 The assignment … The assignment is to ‘Add a section that describes the largest 20 diamonds, including a table that displays their most important attributes’ One way to do this is - (a) sort the file by size [Google ‘sort r data frame by a variable.’], - (b) save this sorted file with just the 20 largest diamonds [see the code block above for an example], and, - (c) choose the most important variables [you’ll have to make a judgement call here, then use the above]; then make a new table. Insert your code here attach(smaller) rankedDiamonds&lt;-smaller[order(-carat),] rankedDiamonds&lt;-rankedDiamonds[1:20,c(1:4,7)] knitr::kable(rankedDiamonds, caption = &quot;20 largest diamonds, select columns&quot;) Table 10.2: 20 largest diamonds, select columns carat cut color clarity price 2.50 Fair G I1 7854 2.50 Premium H I1 8467 2.50 Fair G I1 8711 2.50 Premium H SI2 12071 2.50 Fair H SI2 13278 2.50 Fair H SI2 13278 2.50 Fair G SI2 14194 2.50 Ideal J SI2 14502 2.50 Fair J SI2 14652 2.50 Ideal J SI2 15246 2.50 Premium H SI2 15934 2.50 Ideal J SI2 15990 2.50 Ideal I VS2 16955 2.50 Very Good J SI1 17028 2.50 Fair F SI2 17405 2.50 Good J VS2 18325 2.50 Premium I SI2 18447 2.49 Fair J I1 6289 2.49 Ideal J SI1 16915 2.49 Premium I SI2 18325 10.4.7 If at first you don’t succeed How hard did you try? Document that you tried for at least 30 minutes, where you looked, and what you learned in your struggles 10.4.8 If you get this far Now rewrite your code in as few lines as possible. (I do NOT mean omitting comments. Which of the above steps is necessary to answer the assignment?) Here’s one answer - using ‘arrange’ instead of order… knitr::kable(arrange(smaller,-carat)[1:20,c(1:4,7)]) carat cut color clarity price 2.50 Fair G I1 7854 2.50 Premium H I1 8467 2.50 Fair G I1 8711 2.50 Premium H SI2 12071 2.50 Fair H SI2 13278 2.50 Fair H SI2 13278 2.50 Fair G SI2 14194 2.50 Ideal J SI2 14502 2.50 Fair J SI2 14652 2.50 Ideal J SI2 15246 2.50 Premium H SI2 15934 2.50 Ideal J SI2 15990 2.50 Ideal I VS2 16955 2.50 Very Good J SI1 17028 2.50 Fair F SI2 17405 2.50 Good J VS2 18325 2.50 Premium I SI2 18447 2.49 Fair J I1 6289 2.49 Ideal J SI1 16915 2.49 Premium I SI2 18325 Finally, generate HTML, Word, and PDF versions of this document. Are these different output formats useful for you? explain &lt;- replace this line with an answer 10.5 What to do when you are stuck google. pay attention to your error messages ask for help, make your questions clear and reproducible (see R4DS Chapter 1) take a break, think outside the box and kludge something together if you have to document your struggle and your cleverness for a future you References "],
["some-tidyverse-tricks.html", "11 some tidyverse tricks 11.1 From sections 00 and 01: 11.2 From section 02 - Transform 11.3 Dplyr 11.4 Pipe 11.5 Mutate 11.6 Summariz(s)e 11.7 Group by 11.8 Joins 11.9 From section 05 - Data-Types", " 11 some tidyverse tricks “Excerpts from Data Science in the Tidyverse” - Sections 0 - 2 11.1 From sections 00 and 01: Setup chunk will run any time any chunk is run. Do you get an error? Can you fix it? library(tidyverse) library(gapminder) Simple plot syntax ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) 11.2 From section 02 - Transform Here’s Gapminder data (summarized in the Hans Rosling video). We’ll play with it a little. gapminder ## # A tibble: 1,704 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # ... with 1,694 more rows 11.3 Dplyr 11.3.1 Filter Use filter to show the data for United States. (Here and in the following sections, Remove the octothorpe and replace the three dots with your own code.) # filter(gapminder...) Same for NZ after 2000 # filter(gapminder...) 11.4 Pipe The pipe allows us to chain multiple operations, 11.5 Mutate … while mutate creates new variables. What would you expect this to do? (There’s one way to find out!) gapminder %&gt;% mutate(size = ifelse(pop &lt; 10e06, “small”, “large”)) # gapminder Now use the pipe and mutate to create a new variable of your own that describes something meaningful in the dataset. Describe it here: # gapminder 11.6 Summariz(s)e Reduces a variable to one or more values. Use summarise() to compute the earliest (minimum) year in the dataset # gapminder 11.7 Group by Summarize is really useful with group by, and group by is really useful for categorical variables. Find the median life expectancy by continent in 2007, or another comparison between groups (such as continent or year). # gapminder 11.8 Joins You often need to combine different datasets. One way is to add new rows (observations) for your current set of variables. Another is to add new columns (measures or variables) for your current set of observations. Consider the following… country_codes ## # A tibble: 187 x 3 ## country iso_alpha iso_num ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan AFG 4 ## 2 Albania ALB 8 ## 3 Algeria DZA 12 ## 4 Angola AGO 24 ## 5 Argentina ARG 32 ## 6 Armenia ARM 51 ## 7 Aruba ABW 533 ## 8 Australia AUS 36 ## 9 Austria AUT 40 ## 10 Azerbaijan AZE 31 ## # ... with 177 more rows We can combine it with the gapminder data using a version of join, for example, … gapminder %&gt;% left_join(country_codes) ## Joining, by = &quot;country&quot; ## Warning: Column `country` joining factor and character vector, coercing ## into character vector ## # A tibble: 1,704 x 8 ## country continent year lifeExp pop gdpPercap iso_alpha iso_num ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. AFG 4 ## 2 Afghanistan Asia 1957 30.3 9240934 821. AFG 4 ## 3 Afghanistan Asia 1962 32.0 10267083 853. AFG 4 ## 4 Afghanistan Asia 1967 34.0 11537966 836. AFG 4 ## 5 Afghanistan Asia 1972 36.1 13079460 740. AFG 4 ## 6 Afghanistan Asia 1977 38.4 14880372 786. AFG 4 ## 7 Afghanistan Asia 1982 39.9 12881816 978. AFG 4 ## 8 Afghanistan Asia 1987 40.8 13867957 852. AFG 4 ## 9 Afghanistan Asia 1992 41.7 16317921 649. AFG 4 ## 10 Afghanistan Asia 1997 41.8 22227415 635. AFG 4 ## # ... with 1,694 more rows Note that join is smart enough to match the two files on the variable they share… Excerpts from Data Science in the Tidyverse - Sections 5 Note the fivethirtyeight package. Install, open, and play with it as you would a birthday present. library(tidyverse) library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following object is masked from &#39;package:base&#39;: ## ## date library(fivethirtyeight) 11.9 From section 05 - Data-Types Find and open the General Social Survey data. Find the average hours of tv watched (tvhours) for each category of marital status (marital). # why doesn&#39;t this work? #div &lt;- filter(gss_cat, marital==&quot;Divorced&quot;) #div #x = div[3] #x &lt;- as.numeric(x[[1]]) #mean(x) # one method (by_marital &lt;- group_by(gss_cat, marital)) ## # A tibble: 21,483 x 9 ## # Groups: marital [6] ## year marital age race rincome partyid relig denom tvhours ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 2000 Never ma~ 26 White $8000 to~ Ind,near ~ Protes~ Southe~ 12 ## 2 2000 Divorced 48 White $8000 to~ Not str r~ Protes~ Baptis~ NA ## 3 2000 Widowed 67 White Not appl~ Independe~ Protes~ No den~ 2 ## 4 2000 Never ma~ 39 White Not appl~ Ind,near ~ Orthod~ Not ap~ 4 ## 5 2000 Divorced 25 White Not appl~ Not str d~ None Not ap~ 1 ## 6 2000 Married 25 White $20000 -~ Strong de~ Protes~ Southe~ NA ## 7 2000 Never ma~ 36 White $25000 o~ Not str r~ Christ~ Not ap~ 3 ## 8 2000 Divorced 44 White $7000 to~ Ind,near ~ Protes~ Luther~ NA ## 9 2000 Married 44 White $25000 o~ Not str d~ Protes~ Other 0 ## 10 2000 Married 47 White $25000 o~ Strong re~ Protes~ Southe~ 3 ## # ... with 21,473 more rows summarise(by_marital, avetvhours = mean(tvhours, na.rm = TRUE)) ## # A tibble: 6 x 2 ## marital avetvhours ## &lt;fct&gt; &lt;dbl&gt; ## 1 No answer 2.56 ## 2 Never married 3.11 ## 3 Separated 3.55 ## 4 Divorced 3.09 ## 5 Widowed 3.91 ## 6 Married 2.65 # pipe method gss_cat%&gt;% group_by(marital) %&gt;% summarise(means=mean(tvhours,na.rm=TRUE)) ## # A tibble: 6 x 2 ## marital means ## &lt;fct&gt; &lt;dbl&gt; ## 1 No answer 2.56 ## 2 Never married 3.11 ## 3 Separated 3.55 ## 4 Divorced 3.09 ## 5 Widowed 3.91 ## 6 Married 2.65 # gss_cat # etc How did you handle missing values? "],
["messy-data-cleaning-and-curation.html", "12 Messy data: Cleaning and curation", " 12 Messy data: Cleaning and curation "],
["references.html", "13 references", " 13 references library (tidyverse) data() data(anscombe) str(anscombe) ## &#39;data.frame&#39;: 11 obs. of 8 variables: ## $ x1: num 10 8 13 9 11 14 6 4 12 7 ... ## $ x2: num 10 8 13 9 11 14 6 4 12 7 ... ## $ x3: num 10 8 13 9 11 14 6 4 12 7 ... ## $ x4: num 8 8 8 8 8 8 8 19 8 8 ... ## $ y1: num 8.04 6.95 7.58 8.81 8.33 ... ## $ y2: num 9.14 8.14 8.74 8.77 9.26 8.1 6.13 3.1 9.13 7.26 ... ## $ y3: num 7.46 6.77 12.74 7.11 7.81 ... ## $ y4: num 6.58 5.76 7.71 8.84 8.47 7.04 5.25 12.5 5.56 7.91 ... head(anscombe) ## x1 x2 x3 x4 y1 y2 y3 y4 ## 1 10 10 10 8 8.04 9.14 7.46 6.58 ## 2 8 8 8 8 6.95 8.14 6.77 5.76 ## 3 13 13 13 8 7.58 8.74 12.74 7.71 ## 4 9 9 9 8 8.81 8.77 7.11 8.84 ## 5 11 11 11 8 8.33 9.26 7.81 8.47 ## 6 14 14 14 8 9.96 8.10 8.84 7.04 Reshaping Anscombe for us anscombe$kid &lt;- c(&quot;Al&quot;, &quot;Barb&quot;, &quot;Cathy&quot;, &quot;Dirk&quot;, &quot;Edwin&quot;, &quot;Flo&quot;, &quot;George&quot;, &quot;Henry&quot;, &quot;Isaiah&quot;, &quot;Jim&quot;, &quot;Ken&quot;) # make a file anscombe2 from anscombe anscombe2 &lt;- anscombe %&gt;% # make a new variable called x from x1:x4 gather(x,levelx,x1,x2,x3,x4, # don&#39;t mess with the other variables -c(y1,y2,y3,y4,kid)) # peek at it head(anscombe2) ## y1 y2 y3 y4 kid x levelx ## 1 8.04 9.14 7.46 6.58 Al x1 10 ## 2 6.95 8.14 6.77 5.76 Barb x1 8 ## 3 7.58 8.74 12.74 7.71 Cathy x1 13 ## 4 8.81 8.77 7.11 8.84 Dirk x1 9 ## 5 8.33 9.26 7.81 8.47 Edwin x1 11 ## 6 9.96 8.10 8.84 7.04 Flo x1 14 anscombe2 &lt;- anscombe2 %&gt;% gather(y,levely,y1,y2,y3,y4, -c(x,levelx,kid)) %&gt;% # keep only pairs where the x and y vars are the same filter (substr(x,2,2) == substr(y,2,2)) %&gt;% # drop one of these select (-y) %&gt;% mutate(x = recode(x, x1=&quot;Mon&quot;,x2 = &quot;Tue&quot;, x3 = &quot;Wed&quot;, x4 = &quot;Thu&quot;)) head(anscombe2) ## kid x levelx levely ## 1 Al Mon 10 8.04 ## 2 Barb Mon 8 6.95 ## 3 Cathy Mon 13 7.58 ## 4 Dirk Mon 9 8.81 ## 5 Edwin Mon 11 8.33 ## 6 Flo Mon 14 9.96 str(anscombe2) ## &#39;data.frame&#39;: 44 obs. of 4 variables: ## $ kid : chr &quot;Al&quot; &quot;Barb&quot; &quot;Cathy&quot; &quot;Dirk&quot; ... ## $ x : chr &quot;Mon&quot; &quot;Mon&quot; &quot;Mon&quot; &quot;Mon&quot; ... ## $ levelx: num 10 8 13 9 11 14 6 4 12 7 ... ## $ levely: num 8.04 6.95 7.58 8.81 8.33 ... "]
]
