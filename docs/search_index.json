[
["from-regression-to-prediction-and-classification.html", "18 From regression to prediction and classification 18.1 Revisiting the affairs data 18.2 Avoiding capitalizing on chance 18.3 applying logistic regression analysis to the training data 18.4 From regression to classification 18.5 Applying the model to new data: Assessing out-of-sample risk 18.6 Changing our decision threshold 18.7 More confusion 18.8 ROCs and AUC", " 18 From regression to prediction and classification 18.1 Revisiting the affairs data Let’s go back and do what we should have done earlier: Examine and think about “number of affairs”. What does the distribution look like? data(Fair) Fair %&gt;% count(nbaffairs) ## # A tibble: 6 x 2 ## nbaffairs n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 451 ## 2 1 34 ## 3 2 17 ## 4 3 19 ## 5 7 42 ## 6 12 38 We note that the distibution is skewed - and we realize that perhaps we should think about it differently: The meaning of the difference between 0 and 1 is not the same as that between 1 and 2, or perhaps even 1 and 12. We can transform the data in several ways. We might begin with a root transform, rounded as integer: Fair2 &lt;- Fair %&gt;% mutate(rootAffair = as.integer(sqrt(nbaffairs))) Fair2 %&gt;% count(rootAffair) ## # A tibble: 4 x 2 ## rootAffair n ## &lt;int&gt; &lt;int&gt; ## 1 0 451 ## 2 1 70 ## 3 2 42 ## 4 3 38 Or we could simply distinguish between those that do and don’t have affairs. We are going to do this on our initial (Fair) data here as we will be examining this further: Fair &lt;- Fair %&gt;% mutate(affairYN = ifelse (nbaffairs &gt; 0,1,0)) Fair %&gt;% count(affairYN) ## # A tibble: 2 x 2 ## affairYN n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 451 ## 2 1 150 With this change, the regression problem becomes more clearly a classification problem. How can we best predict which ‘type’ (affair-ers vs. not) a given person falls in to? 18.2 Avoiding capitalizing on chance One lesson from the last class was that correlations (and regression coefficients) drawn from small samples were not stable. In regression analysis, as progressively smaller samples were drawn from the Fair data, the ability to predict the outcome increased. In the limiting case, when the number of predictors (variables) was equal to the number of cases in the sample (rows), prediction becomes perfect. # insert code here to show this The problem of capitalizing on chance is a significant one in prediction, and one should always be skeptical of models which are untested beyond the sample from which they were derived. 18.2.1 Splitting the data into training and test subsamples The most basic solution to this problem is to split the data into two groups, a training sample from which we extract our model, and a test sample on which you will assess it. (Often, the logic of this will be extended to include a third group, a validation sample which would be used to tune or select the results of the training run before the test data are examined). Here, we will consider the simpler approach, splitting the Fair data into training and test samples. The critical feature of this analysis is that we will hold out the test data, and not even look at it until after our model building is complete. # establish a seed for your data-split # so that your results will be reproducible set.seed(33458) # drop the nbaffairs variable as we # are no longer using this Fair &lt;- Fair %&gt;% select (-nbaffairs) # create a set of line numbers # of size corresponding to the # desired training sample n &lt;- nrow(Fair) trainIndex &lt;- sample(1:n, size = round(0.6*n), replace=FALSE) # create training and test samples trainFair &lt;- Fair[trainIndex ,] testFair &lt;- Fair[-trainIndex ,] 18.3 applying logistic regression analysis to the training data Last class, we used the lm command in our regression analysis which predicted the (continuous) outcome nbaffairs. Here, the outcome is now dichotomous and therefore binomially distributed variable, and so the desired regression is a logistic one. We run this analysis using only the training data. model2 &lt;- glm(affairYN ~ ., data = trainFair, family = &quot;binomial&quot;) summary (model2) ## ## Call: ## glm(formula = affairYN ~ ., family = &quot;binomial&quot;, data = trainFair) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5415 -0.7703 -0.5984 0.9298 2.2685 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.71899 1.16225 1.479 0.13914 ## sexmale 0.27875 0.31051 0.898 0.36934 ## age -0.06421 0.02418 -2.655 0.00792 ** ## ym 0.10580 0.04176 2.534 0.01128 * ## childyes 0.38298 0.37743 1.015 0.31025 ## religious -0.29793 0.11492 -2.592 0.00953 ** ## education 0.01071 0.06673 0.160 0.87254 ## occupation 0.07694 0.09288 0.828 0.40749 ## rate -0.42725 0.11950 -3.575 0.00035 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 411.94 on 360 degrees of freedom ## Residual deviance: 375.13 on 352 degrees of freedom ## AIC: 393.13 ## ## Number of Fisher Scoring iterations: 4 18.4 From regression to classification Our “predicted scores” are continuous, corresponding to the probability that a given person will have an affair. Here’s how they are distributed (still on the training data here): predictTrain &lt;- predict(model2, trainFair, type = &quot;response&quot;) summary (predictTrain) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.03821 0.15515 0.21647 0.25762 0.34893 0.73426 If we want to classify people, we will need to create a decision threshold at which we will change our prediction from no to yes. One approach is to create a threshold equal to the actual proportion of people who don’t have affairs in our sample. (threshold &lt;- mean(trainFair$affairYN)) ## [1] 0.2576177 This is equal to both the mean of our predicted scores (above) and the mean of our actual scores, and, because this is a dichotomous variable, the proportion of people in the sample who have affairs. We’ll predict that if a person has a predicted score more than this we’ll predict that s/he will be unfaithful, else we will “PredictOK.” Then we will create a confusion matrix, to compare our correct predictions (PredictOK and affairYN = 1, Predictunfaithful and affairYN = 0) with the remainder. classification &lt;- ifelse(predictTrain &gt; threshold, &quot;Predictunfaithful&quot;, &quot;PredictOK&quot;) (b &lt;- table(classification, trainFair$affairYN)) ## ## classification 0 1 ## PredictOK 179 36 ## Predictunfaithful 89 57 (correct &lt;- b[1,1] + b[2,2]) ## [1] 236 (errors &lt;- b[1,2] + b[2,1]) ## [1] 125 (accuracy &lt;- correct/(correct + errors)) ## [1] 0.6537396 18.5 Applying the model to new data: Assessing out-of-sample risk We shouldn’t trust these results, though, because the model is based on the same data that it is tested upon. Now we will apply the model to the new, independent test data. Typically (but not invariably), the percent of accurate classifications will decline, especially if the model is a complex one with many variables or if the number of observations is low. predictTest &lt;- predict(model2, testFair, type = &quot;response&quot;) classification &lt;- ifelse(predictTest &gt; threshold, &quot;Predictunfaithful&quot;, &quot;PredictOK&quot;) (b &lt;- table(classification, testFair$affairYN)) ## ## classification 0 1 ## PredictOK 126 18 ## Predictunfaithful 57 39 (correct &lt;- b[1,1] + b[2,2]) ## [1] 165 (errors &lt;- b[1,2] + b[2,1]) ## [1] 75 (accuracy &lt;- correct/(correct + errors)) ## [1] 0.6875 We are correct 69 percent of the time. 18.6 Changing our decision threshold In many decision problems, there is an asymmetry in the cost of different types of errors: if you are foraging for mushrooms, for example, an error of the form (you decide its safe and it is poisonous) is more costly than the converse (you decide its poisonous and it is safe). This may be true in the present example as well. Consider someone who is looking for a spouse, but is really averse to the idea of getting hurt by an affair. That person might feel like the cost of marrying an unfaithful person is much greater than the cost of not marrying a faithful one. So we adjust the threshold downwards: threshold &lt;- .05 classification &lt;- ifelse(predictTest &gt; threshold, &quot;Predictunfaithful&quot;, &quot;PredictOK&quot;) (b &lt;- table(classification, testFair$affairYN)) ## ## classification 0 1 ## PredictOK 3 1 ## Predictunfaithful 180 56 (correct &lt;- b[1,1] + b[2,2]) ## [1] 59 (errors &lt;- b[1,2] + b[2,1]) ## [1] 181 (accuracy &lt;- correct/(correct + errors)) ## [1] 0.2458333 The “overall accuracy” - that is, number of correct classifications - drops. But that’s not what we are really interested in, rather, we are interested in minimizing hurt. Here’s another example: Someone who is very lonely might feel the opposite, and be willing to accept greater substantially greater risk. threshold &lt;- .5 classification &lt;- ifelse(predictTest &gt; threshold, &quot;Predictunfaithful&quot;, &quot;PredictOK&quot;) (b &lt;- table(classification, testFair$affairYN)) ## ## classification 0 1 ## PredictOK 174 49 ## Predictunfaithful 9 8 (correct &lt;- b[1,1] + b[2,2]) ## [1] 182 (errors &lt;- b[1,2] + b[2,1]) ## [1] 58 (accuracy &lt;- correct/(correct + errors)) ## [1] 0.7583333 Prediction is higher here - but not much higher than it would be if we raised the threshold even further, and just assumed that everyone can be trusted. Then, our error rate would be 26 percent. When overall predictability is low, it’s often the case that you can maximize correct predictions by simply predicting that everyone will be in the most popular category. Predicting rare events, such as suicides, is particularly difficult. 18.7 More confusion There is a nice shortcut to generating confusion matrices such as those above using the caret package. This function describes outcomes in several ways, as there are many languages for describing outcomes in 2 x 2 tables, including Type I vs. Type II errors, Hits vs. False Alarms/False Positives, and Sensitivity vs. Specificity. In these data, it’s been set up so that - hit rate ~ sensitivity ~ (“no affair” | no affair) - correct rejection ~ specificity ~ (“affair” | affair) threshold &lt;- .5 # syntax for classification in caret is a little different # (the labels for the actual and predicted scores have to be the same) #classification &lt;- ifelse(predictTest &gt; threshold, # &quot;Predictunfaithful&quot;, &quot;PredictOK&quot;) classification &lt;- ifelse(predictTest &gt; threshold, 1,0) # caret package (newest) requires explicit matching of factors classification &lt;- as.factor(classification) testFair$affairYN &lt;- as.factor(testFair$affairYN) confusionMatrix(classification, testFair$affairYN) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 174 49 ## 1 9 8 ## ## Accuracy : 0.7583 ## 95% CI : (0.6991, 0.8111) ## No Information Rate : 0.7625 ## P-Value [Acc &gt; NIR] : 0.5948 ## ## Kappa : 0.1202 ## Mcnemar&#39;s Test P-Value : 3.04e-07 ## ## Sensitivity : 0.9508 ## Specificity : 0.1404 ## Pos Pred Value : 0.7803 ## Neg Pred Value : 0.4706 ## Prevalence : 0.7625 ## Detection Rate : 0.7250 ## Detection Prevalence : 0.9292 ## Balanced Accuracy : 0.5456 ## ## &#39;Positive&#39; Class : 0 ## 18.8 ROCs and AUC Each of these decision thresholds describes the performance of a model at a particular point. We can combine the thresholds and plot them in Receiver Operating Characteristic (ROC) curves. The area under the curve (AUC) is a great measure of model accuracy, in that it summarizes how effective a classifier is across all possible thresholds. # fig.width and fig.height specified to get square plots # colAUC function gets stats etc AUCModel2&lt;-colAUC(predictTest, testFair[[&quot;affairYN&quot;]], plotROC = TRUE) AUCModel2 ## [,1] ## 0 vs. 1 0.7349727 "]
]
