[
["another-approach-to-prediction-k-nearest-neighbor.html", "19 Another approach to prediction: k-nearest neighbor 19.1 From 1 doppelganger to many 19.2 Avoiding capitalization on chance again 19.3 The multinomial case", " 19 Another approach to prediction: k-nearest neighbor Our real-life social predictions are often guided not by induction or the (optimized) combination of a set of predictor variables. Rather, we often reason by analogy - we might think, for example, that I won’t go out with Fred because he reminds me of Larry, and Larry was kind of a jerk. If regression analysis is an approach to prediction based in our set of variables, k-nearest neighbor analysis instead makes predictions based on observations. In the simplest form of this analysis, we find the nearest thing to a “doppelganger” (a look-alike or near double) for a given observation. So, in the affairs data, if a person is most like someone else in the dataset who has had an affair, we predict an affair, else not. Begin by loading the affairs data from last time. Using the same seed (33458) means that the same set of training and test cases will be extracted as in the prior analysis data(Fair) # one change here: Note the bidirectional pipe to simplify code # use only when you are sure that your file update is ok Fair %&lt;&gt;% # &lt;- Fair %&gt;% mutate(affairYN = # another change here: nbaffairs is set up as a factor # to allow confusionmatrix to run as.factor(ifelse(nbaffairs &gt; 0,1,0))) %&gt;% select(-nbaffairs) set.seed(33458) n &lt;- nrow(Fair) trainIndex &lt;- sample(1:n, size = round(0.6*n), replace = FALSE) # create training and test samples trainFair &lt;- Fair[trainIndex ,] testFair &lt;- Fair[-trainIndex ,] To run a k-nearest neighbor analysis, we need three inputs: our predictors in the training data, our predictors in the test data, and our outcome/classes in the training data. unlike the lm and glm commands, knn will not automatically create our dummy variables for us. so we need to do this manually. trainFair %&lt;&gt;% mutate(sexMale = ifelse(sex == &quot;female&quot;, 0, 1)) %&gt;% mutate(childyes = ifelse(child == &quot;no&quot;, 0, 1)) %&gt;% select(-(c(sex,child))) testFair %&lt;&gt;% mutate(sexMale = ifelse(sex == &quot;female&quot;, 0, 1)) %&gt;% mutate(childyes = ifelse(child == &quot;no&quot;, 0, 1))%&gt;% select(-(c(sex,child))) Here, as in the regression analysis in the last chapter, we can generate a confusion matrix to assess the accuracy of prediction: set.seed(33458) knnAffair &lt;- knn(trainFair[,-7], # training data testFair[,-7], # test data trainFair$affairYN, # class 1 # number of neighbors ) b&lt;-confusionMatrix(knnAffair, testFair$affairYN) b ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 142 40 ## 1 41 17 ## ## Accuracy : 0.6625 ## 95% CI : (0.5989, 0.7221) ## No Information Rate : 0.7625 ## P-Value [Acc &gt; NIR] : 0.9998 ## ## Kappa : 0.0738 ## Mcnemar&#39;s Test P-Value : 1.0000 ## ## Sensitivity : 0.7760 ## Specificity : 0.2982 ## Pos Pred Value : 0.7802 ## Neg Pred Value : 0.2931 ## Prevalence : 0.7625 ## Detection Rate : 0.5917 ## Detection Prevalence : 0.7583 ## Balanced Accuracy : 0.5371 ## ## &#39;Positive&#39; Class : 0 ## b[[&quot;overall&quot;]][[&quot;Accuracy&quot;]] ## [1] 0.6625 19.1 From 1 doppelganger to many In the above code, we used a k-nearest neighbor analysis based on a single ‘neighbor’ (k = 1). Can we improve prediction by considering more than 1 neighbor? To test a range of values, we can first set up our knn analysis as a function (compare this code with the code in the prior section). knnFairdata &lt;- function (k) { set.seed(33458) knnAffair &lt;- knn(trainFair[,-7], # training data testFair[,-7], # test data trainFair$affairYN, # class k # number of neighbors ) b&lt;-confusionMatrix(knnAffair, testFair$affairYN) b[[&quot;overall&quot;]][[&quot;Accuracy&quot;]] } We run the function on k = 1 and k = 2 to test it: knnFairdata(1) ## [1] 0.6625 knnFairdata(2) ## [1] 0.6916667 Now we can apply it to as many as 100 values of k, using the purrr::map command: kAccuracy &lt;- (map(1:100,knnFairdata)) %&gt;% # map the knnFunction onto numbers 1-100 unlist() %&gt;% # map generates a list, so we will pull these out into a vector as_tibble() %&gt;% # then a tibble so we can do a quick plot rename(Accuracy=value) %&gt;% mutate (k = seq_along(Accuracy)) ## Warning: Calling `as_tibble()` on a vector is discouraged, because the behavior is likely to change in the future. Use `enframe(name = NULL)` instead. ## This warning is displayed once per session. We can graph this, using the syntax from the beginning of the class: kAccuracy %&gt;% ggplot(aes(k, Accuracy)) +#%&gt;% geom_point() + ggtitle(&quot;Overall accuracy for varying levels of k&quot;) # This pulls out the maximum accuracy, and the value of k for which it occurs: (ka &lt;- which.max(kAccuracy$Accuracy)) ## [1] 27 (kb &lt;- max(kAccuracy$Accuracy)) ## [1] 0.7666667 19.2 Avoiding capitalization on chance again In these data, with this split of training and test (and this initial seed) the maximum predictability occurs at k = 27, with an overall accuracy of 0.7666667. Would this hold if we used a different random split? Remember, here, we have tested not one model, but 100 of them, then chosen the best one. The peak in the curve at 27 may well be due to chance characteristics of the test data. We could address this empirically using one of several techniques. One approach is to have a third independent sample on which to test the accuracy of prediction at k = 27. This would require the separate validation sample that was introduced in the last chapter. If we aren’t concerned with optimizing prediction, but just getting a better sense of the shape of the curve and the optimal value of k, an approach would be to repeatedly split our data (e.g., using different random seeds), compute the above curve for each of these, and then average them. This logic, which is often used in regression analysis, is referred to as a k-fold approach. 19.3 The multinomial case Finally, the k-nearest neighbors approach can be extended to classification problems in which we are predicting not just a dichotomous outcome, but a multinomial one - such as a personality type or college concentration. "]
]
