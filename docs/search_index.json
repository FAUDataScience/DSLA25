[["index.html", "99-MoreExercises ", " 99-MoreExercises Kevin Lanning 2025-08-28 "],["preface.html", "preface the role of the liberal arts in data science some features of the text the book is for you", " preface This work-in-progress will ultimately serve as a textbook for introductory undergraduate courses in data sciences, and is particularly aimed at students in the liberal arts. No prior knowledge of computer programming is presumed, though, ideally, students will have had college algebra (or its equivalent) and an introductory course in statistics, methods, or data analysis. the role of the liberal arts in data science Data science is still a relatively new field of study, and there are multiple approaches to teaching it and to its place in the college curriculum. This book is intended to serve courses such as Introduction to Data Science at the Wilkes Honors College of Florida Atlantic University which, in turn, was initially based on data science classes at the universities of North Carolina, British Columbia, Duke, Maryland, Wisconsin, Stanford, BYU, Harvard, and UC Berkeley. At each of these schools, the Introduction to Data Science is, to my eyes at least, closer to Statistics than to Computer Science. Statistics is itself a broad field, and our approach is aligned with its most applied and pragmatic form. From this perspective, the choice of statistical methods should follow from the data and problem at hand - in other words, statistics should serve the needs of the user rather than dictate them (Loevinger 1957). Pragmatism, in turn, can serve various goals, ranging from maximizing the revenues generated by an online ad to minimizing the carbon footprint of a travel itinerary. Data science for the liberal arts may be seen as a fusion of the pragmatism of data science with social and humanistic concerns; we stand beside programs in Computational Social Science as it has been taught at schools including Chicago, Georgia Tech, UC Santa Barbara, Princeton, UC Berkeley, at Berlin’s Hertie School of Governance, and in Columbia’s School of Journalism. Data science for the liberal arts begins with the person and society rather than with the algorithm and network. In its concern with the liberal arts, it is intended to provide a modest counterbalance to the inherently centripetal, or inequality-accelerating, force of modern information technology.1 some features of the text There are a number of different approaches to teaching data science for the liberal arts. The present text includes several distinguishing features. R About ten years ago, in an informal survey of introductory data science courses, I found a pretty even split between those which began with Python and those which began with the statistical programming language R. This difference corresponds, very loosely, to the split noted above: Computer science based approaches to data science were frequently grounded in Python, while statistics-based approaches were generally grounded in R. Today, the picture is a little different: Python is increasing in popularity. Schools including Berkeley and Harvard have shifted largely (but not entirely) away from R towards Python in training undergraduates. But R is still taught as a first course or in certificate programs at Harvard, Oxford, and Johns Hopkins. Python is used more than R in industry, but R remains popular in academic settings, particularly in areas like epidemiology/public health and psychology. R is generally easier than Python for those who have some statistics but no programming, and Python is easier for those who have some programming but limited statistics. Literacy in R is a valuable skill for those planning on applying to graduate study in disciplines including behavioral sciences (psychology, economics), social sciences (anthropology, political science, public policy), and some health sciences (nursing and public health). Our course will be based in R. Reproducible science The course will provide an introduction to some of the methods and tools of reproducible science. We will consider the replication crisis in the natural and social sciences, and then consider three distinct approaches which serve as partial solutions to the crisis. The first of these is training in a notebook-based approach to writing analyses, reports and projects (using R markdown documents). The second is using public repositories (such as the Open Science Framework and, to a limited extent, GitHub) to provide snapshots of projects over time. Finally, the third is to consider the place of significance testing in the age of Big Data, and to provide training in the use of descriptive, exploratory techniques of data analysis. Good visualizations As I note in the first chapter, communication is a distinguishing concern of data science for the liberal arts. “Communication” includes not just writing up results, but also designing data displays that incisively convey the key ideas or features in a flood of data. We’ll examine and develop data visualizations such as plots, networks and text clouds. More advanced topics may include maps, interactive displays, and animations. A little data There are plenty of data sources for us to examine, and we’ll consider existing datasets from disciplines ranging from literature to economics to public health, with sizes ranging from a few dozen to millions of data points. We will also clean and create new datasets. A few tools One feature of Data Science is that it is changing rapidly. The tools, methods, data sources, and ethical concerns that face us today are different from those which shaped the field just one or two years ago. In fields that are undergoing rapid change, there is some trade-off between building expertise with existing (older) tools and trying the newer approaches. Partly because I want to equip you with skills which will not be obsolete, partly because some of these new approaches promise more accessibility, elegance, and/or power, and partly because of my own interest in staying current, we’ll be using some of the latest packages and programs. In the last few years, I’ve shifted the class from the standard R dialect (as I learned it from the Johns Hopkins-Coursera Data Science Specialization) to the Tidyverse, a dialect of R that I find to be relatively clear and concise. A few years ago, I shifted our primary platform from individual laptops to a cloud-based R platform; while this approach has its advantages, I found that these did not outweigh the costs of the approach, so we will go back to the standalone method. We’ll explore different approaches to learning R syntax, including the learnr package, Swirl, and DataCamp. In the past, I’ve recommended using dedicated markdown editors such as Obsidian. While I still think that these are worth considering for some text-editing and note-taking applications, we’ll do our work instead with the editor in the latest variant of RStudio on our laptops, as this allows WYSIWIG (what you see is what you get) formatting of documents - such as this one - that are intended as “publication-ready” texts. We’ll use, and explore the advantages and disadvantages, of spreadsheets such as Excel or Google Sheets as well. the book is for you It’s my intention that this text should serve every college student, regardless of concentration or college major. The skills and insights that you will gain in this course will help you in graduate and professional schools, will help you in your careers, and will help you in your goal of making a better world. And it will help you train the next generation of data scientists as well. References Loevinger, Jane. 1957. “Objective Tests as Instruments of Psychological Theory.” Psychological Reports 3 (3): 635–94. https://doi.org/10/b27jpk. Watts, Duncan J. 2004. “The \"New\" Science of Networks.” Annual Review of Sociology 3: 243–70. https://www.jstor.org/stable/29737693. I hope to return to this in a later chapter, but in the meantime consider the discussion of the “Matthew Effect” in sociology and network science (Watts 2004).↩︎ "],["part-i-introduction.html", "PART I: Introduction", " PART I: Introduction "],["what-is-data-science-for-the-liberal-arts.html", "1 what is “data science for the liberal arts?” 1.1 the incompleteness of the data science Venn diagram 1.2 the importance of data science for society 1.3 discussion: what are your objectives in data science?", " 1 what is “data science for the liberal arts?” Hochster, in Hicks and Irizarry (2018), describes two broad types of data scientists: Type A (Analysis) data scientists, whose skills are like those of an applied statistician, and Type B (Building) data scientists, whose skills lie in problem solving or coding, using the skills of the computer scientist. This view arguably omits a critical component of the field, as data science is driven not just by statistics and computer science, but also by “domain expertise:” The iconic data science Venn diagram The iconic Venn diagram model of data science, as shown above, suggests that there are not two but three focal areas in the field, one of which begins not with math or computer science, but with “domain expertise.” Data science for the liberal arts is a ‘Type C’ approach, where ‘C’ might refer to a concentration of concern in the arts, humanities, social and/or natural sciences. For the Type C data scientist, coding is in the service of applied problems and concerns. Type C data science does not merely integrate ‘domain expertise’ with statistics and computing, it places content squarely at the center. We can appreciate the compelling logic and power of statistics as well as the elegance of well-written code, but for our purposes these are means to an end. Programming and statistics are tools in the service of social and scientific problems and cultural concerns. Type C data science aims for work which is not merely cool, efficient, or elegant, but responsible and meaningful. At the risk of oversimplifying: Type A data scientists focus on Analysis and questions about ‘how?’ Type B data scientists focus on Building and questions of ‘what?’ Type C data scientists focus on Consideration and questions of ‘why?’, ‘who?’, ‘what for?’, and ‘at what (social) cost?’ 1.1 the incompleteness of the data science Venn diagram Data visualizations are starting points which can provide insights, typically highlighting big truths or effects by obscuring other, presumably smaller ones. The Venn diagram model of data science is no exception: As with other graphs, figures, and maps, it allows us to see by showing only part of the picture. What does it omit? That is, beyond mathematics, computing, and domain expertise, what other skills contribute to the success of the data scientist? 1.1.1 additional domains For the liberal arts data scientist, we can note at least three additional important domains, that is, communication, collaboration, and citizenship. Communication, including writing and the design and display of quantitative data, is central to data science because results are inconsequential unless they are recognized, understood, and built upon. Facets of communication include oral presentations, written texts and good data visualizations. Collaboration is important because problems in data science are sufficiently complex so that any single individual will typically have expertise in some, but not all, facets of the area. Collaboration, even more than statistical and technical sophistication, is arguably the most distinctive feature of contemporary scholarship in the natural and social sciences as well as in the private sector (Isaacson 2014). Citizenship is important because we are humans living in a social world; it includes serving the public good, overcoming the digital divide, furthering social justice, increasing public health, diminishing human suffering, and making the world a more beautiful place. The Type C data scientist is aware of the fact that the world and workforce are undergoing massive change: This puts the classic liberal arts focus of “learning how to learn” (as opposed to memorization) at center stage. Finally, the Type C data scientist is sensitive to the creepiness of living increasingly in a measured, observed world. These real-world goals should be informed by ethical concerns including a respect for the privacy and autonomy of our fellow humans. 1.1.2 an additional dimension Cutting across these various facets (statistics, computing, domain expertise, collaboration, communication, and citizenship), a second dimension can be articulated. No one of us can excel in all of these domains, rather, we might aim towards a hierarchy of goals ranging from literacy (can comprehend) through proficiency (can communicate and contribute) to fluency (can practice) to leadership (can create new solutions or methods). That is, we can think of a continuum of ability, including knowledge, skills, interests, and goals. That continuum ranges from the data consumer to the data citizen to the data science contributor. A Type C data science includes this dimension of ‘depth’ as well. 1.2 the importance of data science for society Communication, collaboration, and citizenship are each associated with the concept of trust. Trust is an important social good because it is associated with both individual well being (Poulin and Haase 2015) and the stability of democratic institutions (Sullivan and Transue 1999). But interpersonal and institutional trust, including trust in science, have declined in recent years (Deane 2024). The decline in trust in science has been exacerbated by the so-called reproducibility (or replication) crisis, in which many scientific results initially characterized as “statistically significant” have been found not to hold up under scrutiny, that is, aren’t reproducible. The reasons for the reproducibility crisis are many and contentious, but there is substantial consensus that one path towards better science involves the public sharing of methods and data. A second path towards better and more trustworthy science involves the use of larger datasets: With large datasets, effects are more stable and “statistical significance” is rarely a concern. Other indices, such as measures of accuracy and effect size are typically of primary interest. Data science, with its tools for reproducible analysis and its use of big data sets, can make science more trustworthy and improve the quality of our lives. 1.2.1 intelligence, artificial intelligence (AI), and careers It is difficult to predict the consequences of advances in AI for career selection. Some areas are likely to be impacted by AI more than others, but because AI is still in its infancy, we can only speculate. But one way to approach this question is to begin by considering what constitutes “intelligence.” According to Robert J. Sternberg (2018), intelligence can be thought of as having three components. One of these is Analytical Intelligence, which includes things like planning, reasoning, acquiring knowledge, and problem solving. A second is Creative Intelligence, which includes both the ability to deal with novel situations effectively, and the ability to “automatize” or efficiently execute familiar tasks. The third is Practical or Contextual Intelligence, which includes the ability to adapt to new environments. AI engines or agents (or simply AIs) are, for now at least, effective at only some of these. They are better than humans at knowledge acquisition, logical reasoning and some kinds of problem solving. All of these are parts of Analytical Intelligence. They are also better than us at one part of Sternberg’s Creative Intelligence: They excel at many types of automatized routines. Beyond this, they struggle. My desktop AI (Microsoft Copilot 365 Version 19.2508.31121.0) tells me that the strengths of AI, understood in the context of Sternberg’s theory, make it ideal for things like “Data Analysis, Financial modeling, Diagnostics, Scheduling and logistics, and Repetitive administrative tasks.” The first and last of these are core parts of data science, but not the whole of it. AI struggles (again, according to Copilot), with “Leadership and strategic decision-making, Counseling, therapy, and social work, Cross-cultural negotiation, [and] Artistic innovation requiring deep emotional resonance.” Careers that are likely to grow include creative professions (design, storytelling), human-centered roles (coaching, diplomacy), and also hybrid roles, that is, “Professionals who use AI tools to enhance their work (e.g., data-informed decision-makers).” All of this should be taken with a grain of salt. Today’s AI capabilities are quite limited compared to what they will be a year from now, and the AI that you and I might use on our desktops is, in all likelihood, a simple version of what is already available to large-scale users. Oliver is an embodied AI described as a “helperbot” in the recent Broadway musical Maybe Happy Ending. We can imagine that a helperbot like Oliver could be built on a model that synthesized millions of interactions between people in asymmetrical roles, such as butlers and their employers, secretaries and bosses, and assistants and supervisors. In the musical, Oliver appears to have been an ideal servant, one who anticipated his owner’s needs consistently, promptly and sensitively. I won’t spoil it for you, but Maybe Happy Ending can be seen as portraying the challenges AI agents face, and how Artificial Intelligence is different from human intelligence. In Sternberg’s (2018) terms, Oliver struggles not just with the question of how to adapt to a new environment, but also how to choose an environment altogether. How will it react when it encounters problems for which it has simply no frame of reference? Darren Criss as Oliver in Maybe Happy Ending 1.2.2 the challenge of TMI The challenges of data science are many, but perhaps the most fundamental is the problem of (literally) TMI. When we compare traditional statistics with modern data science, we realize that the former is typically concerned with making inferences from datasets that are too small, while the latter is concerned with making sense of data that is or are too big (D. Donoho 2017). The basic challenge of working in data science is the challenge of “too much information,” or TMI. The challenge of TMI is not new, or restricted to data science. In the 19th Century, Wilhelm Wundt argued that attention was the distinguishing act of the human mind (Blumenthal 1975). That is, in attending to (or focusing on) something, we must overlook everything else, consequently, selection is the essence of human perception (Erdelyi 1974). Selection is important not just in psychology, but in the arts as well, for editing, or choosing what not to write or show, is at the core of the creation of works including novels and film (Ondaatje and Murch 2002). Although the problem of TMI is not new, today it exists at a much greater scale, for there is simply more information around us. Indeed, over the last 20 years, the amount of digital information in the world has increased roughly 200-fold.2 Like perception in psychology and editing in the arts, data science is concerned with extracting meaning from information. Because the amount of information around us has mushroomed and its nature has become more important, our need to extract meaning has become more ubiquitous and more urgent. For these reasons, data science is a foundational discipline in 21st century inquiry. 1.3 discussion: what are your objectives in data science? References Blumenthal, Arthur L. 1975. “A Reappraisal of Wilhelm Wundt.” American Psychologist 30 (11): 1081. Deane, Claudia. 2024. “Americans’ Deepening Mistrust of Institutions.” Donoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734. Erdelyi, Matthew H. 1974. “A New Look at the New Look: Perceptual Defense and Vigilance.” Psychological Review 81 (1): 1–25. https://doi.org/10/cs5c5q. Hicks, Stephanie C., and Rafael A. Irizarry. 2018. “A Guide to Teaching Data Science.” The American Statistician 72 (4): 382–91. https://doi.org/10/gfr5tf. Isaacson, Walter. 2014. The Innovators: How a Group of Hackers, Geniuses, and Geeks Created the Digital Revolution. Simon and Schuster. Ondaatje, Michael, and Walter Murch. 2002. The Conversations: Walter Murch and the Art of Editing Film. A&amp;C Black. Poulin, Michael J., and Claudia M. Haase. 2015. “Growing to Trust: Evidence That Trust Increases and Sustains Well-Being Across the Life Span.” Social Psychological and Personality Science 6 (6): 614–21. https://doi.org/10.1177/1948550615574301. Reinsel, David, John Gantz, and John Rydning. 2025. “The Digitization of the World.” Sternberg, Robert J. 2018. “Theories of Intelligence.” In, edited by Steven I. Pfeiffer, Elizabeth Shaunessy-Dedrick, and Megan Foley-Nicpon, 145161. American Psychological Association. https://doi.org/10.1037/0000038-010. Sullivan, J. L., and J. E. Transue. 1999. “THE PSYCHOLOGICAL UNDERPINNINGS OF DEMOCRACY: A Selective Review of Research on Political Tolerance, Interpersonal Trust, and Social Capital.” Annual Review of Psychology 50 (1): 625–50. https://doi.org/10/cmthvk. According to Wikipedia, the Zettabyte (ZB) Era began in 2012, when the amount of digital information in the world first exceeded 1 ZB (or 1021 bytes). In 2025, it is estimated that the world will house 175 ZBs of digital data (Reinsel, Gantz, and Rydning 2025), hence a 175X increase in in 13 years. My estimate of a 200X increase in 20 years is a conservative extrapolation from these numbers. Incidentally, one ZB = 1,000,000,000,000,000,000,000 bytes, which could be stored on roughly 250 billion DVDs, or 500 million 2 TB hard drives.↩︎ "],["getting-started.html", "2 getting started 2.1 are you already a programmer and statistician? 2.2 spreadsheets - some best practices 2.3 setting up your laptop: some basic tools 2.4 a (modified) 15-minute rule 2.5 installing R and RStudio desktop", " 2 getting started We begin with a brief self-assessment, asking you to reflect on your own knowledge of data science, including the necessary-but-not-sufficient areas of computer programming and statistics. We then move to a description of some rudimentary tools that we will be using, then install the R programming environment on our laptops. 2.1 are you already a programmer and statistician? Regarding programming, you may know more than you think you do. Here’s a simple program - a set of instructions - for producing a cup of coffee: add water to the kettle and turn it on if it’s morning, put regular coffee in the French press, otherwise use decaf if the water has boiled, add it to the French press, else keep waiting if the coffee has steeped for four minutes, depress (smash) piston/plunger, else keep waiting pour coffee into cup enjoy As a post-millennial student from a WEIRD culture, or Western, Educated, Industrialized, Rich Democracy (Henrich, Heine, and Norenzayan 2010), you’ve ‘programmed’ computers, too, if only to enter a password, open an app, and upload a photo on your cell phone. Statistics is of fundamental importance, not just for understanding abstract trends, but for making decisions about everyday life. Consider the case of a senior undergraduate who wants to go to med school. How many schools should she apply to? Exercise 2_1 Susie is applying to two med schools. At School A, 25% of students are accepted, and at School B, 25% are accepted as well. You are Susie. Are you going to get in to at least one of these programs? What is the probability? Does your estimate depend upon any assumptions? Questions such as these are important for us. If the combined probability is low, it will likely (another probability concept) make sense for Susie to spend the time, money, energy, and ego-involvement to apply to additional programs. If the probability is higher, it may not. But problems like this are hard - our estimates of probability are frequently poorly calibrated, and combining probability estimates is challenging. See, e.g., Tversky and Kahneman (1974), and consider taking a course in Behavioral Economics or Thinking and Decision Making to learn more. 2.2 spreadsheets - some best practices Spreadsheets are handy tools, particularly for smaller datasets. You may have worked with data in spreadsheets such as Microsoft Excel or Google Sheets. If you haven’t here’s a start: Exercise 2_2 Open the Google Sheet at http://bit.ly/dslaX2_1. Save a copy and edit it, entering the following in cell B7: =SUM (B2:B6) What is the result? Now copy cell B7 to C7 What happens? Is this the result you expected? Spreadsheets are great tools - the first one, VisiCalc, was the first “killer app” to usher in the personal computer revolution. But spreadsheets have limitations as well. Best practices have been proposed for using spreadsheets in data science such as, for example, including only data (and not calculations) in spreadsheets, using what we will recognize as a ‘tidy’ format in which data are in a simple rectangle (that is, avoid combining cells and using multi-line headers), and saving spreadsheets as simple text files, typically in comma-delimited or CSV format (Broman and Woo 2018). Typically, this means that each worksheet will be in a separate file, rather than in workbooks with multiple tabs. There are good reasons for these recommendations: When we manipulate data in spreadsheets, we typically will not have a record of what was (and wasn’t) changed, and this compromises the reproducibility of our work. Similarly, when we sort data in spreadsheets, we risk chaos, for example, if we sort only certain columns, the integrity of spreadsheet-rows will be lost. In general, spreadsheets should generally be used to store data rather than to analyze it. But don’t be a slave to this - if you are making some quick calculations that are inconsequential and/or to be used on just one occasion, working in Excel or Google Sheets is often the way to go. 2.3 setting up your laptop: some basic tools Documents produced in word processors typically include invisible characters used for formatting, security, etc. These characters, when fed into a computer program, lead to unpredictable results. Markdown (MD) is a simple syntax for producing documents that are easily read by both humans and machines. Most of the documents you will produce and work with this term, including the present chapter, are and will be written in some version of Markdown. Two markdown editors are worth considering. The first of these is Obsidian, which is not just a markdown editor, but a tool for task management and linking ideas - a “second brain” according to some advocates. The drawback of Obsidian is that it can do so much that it can be overwhelming, especially for the beginner. The other markdown editor is RStudio - the environment within which we will be using R - which has a handy built in visual markdown editor as well. Collaboration and communication are integral to data science. In the world beyond universities, the most important messaging and collaboration platform is Slack. Slack is a commercial app, but it has a free tier. Slack is handy for group work of all forms, but most universities use collaborative tools embedded in Learning Management Systems such as Canvas instead. While Microsoft Word has the advantages of familiarity, ease-of-use offline, and extensive formatting capabilities, Google Docs has several advantages over Word. Google Docs is free, it is convenient for collaborative work (as it allows simultaneous editing), and it provides a solid framework for version control, a critical skill in information management. Version control can help you avoid the chaos and confusion of having a computer (or several computers) full of files that look like Cham’s (2012) comic: Never call anything ‘final.doc’. Version control is an important concept in data science. Collaboratively built programs and platforms, including most of the add-ons (libraries, packages) which make R so powerful, are open-source projects built by many individuals over time. For projects such as these, both the current build and its associated history are typically maintained on GitHub, a website for hosting code. When we contribute to these projects, we will first mirror the web-based GitHub site using a program on our own Macs or Windows PCs called Git, then upload our proposed changes. Keeping remote and local branches of files in sync can be challenging, however, and you will not be expected to use this technology in this class. But if you are curious, or want to learn more, an introduction to using Git and R together may be found here. 2.4 a (modified) 15-minute rule While AI tools for coding, including Microsoft Copilot, are helpful for addressing the idiosyncacies of coding syntax, at some point, you will run into problems - if you don’t you aren’t learning enough. An important determinant of your success will be the balance you maintain between persistence and help-seeking. The 15-minute rule is one guideline for this balance: It has been cleverly summarized as “You must try, and then you must ask.” That is, if you get stuck, keep trying for 15 minutes, then reach out to others. I think that this rule is basically sound, particularly if it is applied with cognitive flexibility, social sensitivity, and reciprocity. So when you get stuck, make a note of the problem, then move to another part of your project (that’s the cognitive flexibility part): This allows your problem to percolate and still make progress. When you ask others for help, ask in a way that shows an awareness of the demands on their time (social sensitivity): Part of this means that you should explain your problem in as detailed a fashion as possible - in technical terms, a “reprex” or reproducible example. Finally, you should be willing to provide as well as give help (reciprocity). 2.5 installing R and RStudio desktop Finally, if you have not already done so, install the latest version of R on your own Windows or Mac laptop. Then install RStudio Desktop. (RStudio is our interface, the environment we will use to write, test, and run R code). References Broman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989. Henrich, Joseph, Steven J. Heine, and Ara Norenzayan. 2010. “The Weirdest People in the World?” Behavioral and Brain Sciences 33 (2-3): 61–83. https://doi.org/10/c9j35b. Tversky, Amos, and Daniel Kahneman. 1974. “Judgment Under Uncertainty: Heuristics and Biases.” Science 185 (4157): 1124–31. https://doi.org/10/gwh. "],["what-r-stands-for.html", "3 what R stands for … 3.1 cha-cha-cha-changes 3.2 some technical characteristics 3.3 finding help", " 3 what R stands for … R was initially developed by Ross Ihaka and Robert Gentleman as a tool to help teach university-level statistics at the University of Auckland. At one level, the name ‘R’ simply stands for the first initial of these two founders (Hornik and Team 2022). But, just as we noted that the ‘C’ in Type C Data Analysis stands for concepts such as concentration, communication, collaboration, the ‘R’ in our programming language means much more: R is a system for reproducible analysis, and reproducibility is essential. When we write R code, we’ll use R markdown documents. An R markdown document can include text (comments or explanations), ‘chunks’ of code, and output including graphs and tables. Having explanations, code, and results in a single document facilitates reproducible work. (Jupyter notebooks in the Python world are similar in this respect). R is for research. Research is not just an end-product, not just a published paper or book: … these documents are not the research [rather] these documents are the “advertising”. The research is the “full software environment, code, and data that produced the results” (Buckheit and Donoho 1995; D. L. Donoho 2010, 385). Published works (including theses as well as books, scholarly papers, and business reports) are summaries; R markdown documents are the raw materials from which these are derived. When we consider only summaries (or the ‘advertising’), we make it difficult for others to verify, or build upon, the findings by reproducing them (Gandrud 2013). R is a system for representing data in cool, insight-facilitating ways, a tool for creating (reproducible) data visualizations which can provide insights and communicate results. The power of R to make clear, honest, and reproducible data visualizations is widely seen as a major strength of the language. R is really popular, and this matters, because learning R will make you a more attractive candidate for many graduate programs as well as jobs in the private sector. Because R is popular, there are many resources, including, for example - Online resources include the simple (and less simple) lessons of SwirlR, which offers the possibility of “learning R in R,” as well as DataCamp, the Data Science Certificate Program at Johns Hopkins, and other MOOCs. Books include (R. D. Peng 2014) - which includes not only videos of his lectures in the program at Hopkins, but also a brief list of still more resources - and (Wickham, Çetinkaya-Rundel, and Grolemund 2023) You’ll also learn (more directly) from people, including your classmates, as well as the broader community of people around the world. There are hundreds if not thousands of people, young and old, who are on the road with you. I am as well, just a step or two (hopefully) ahead. R might stand for relatively high level. Programming languages can be described along a continuum from high to low level, the former (like R) are more accessible to humans, the latter (like assembly language) more accessible to machines. Python, Java, and C++ are all more towards the middle of this continuum. R does not stand for ‘arggh,’ although you may proclaim this in frustration (‘arggh, why can’t I get this to work?) or, perhaps, in satisfaction (’arggh, matey, that be a clever way of doing this’).3 But R does stand for rewarding. A language is a way of thinking about the world, and this is true for computer languages as well. You’ll be challenged by its complexity, its idiosyncracy, its alien logic. But you will succeed, and you will find that you can do things that you did not believe possible. 3.0.1 base R and packages R is a programming language. It can be seen as including two parts, a simple core (Base R) and a large number of additional packages. These packages (libraries) are customized add-ons which simplify certain tasks, such as text analysis. There are, at this writing, 21,861 available packages on the CRAN package repository (as well as additional useful packages that, for one reason or another, do not appear on CRAN. Packages on CRAN are partially indexed by “task view pages.” The task view page for natural language processing or text analysis includes, at this writing, over 60 separate packages. So how do you choose, and where do you begin? For our purposes, we will start with the curated list of packages which jointly comprise the tidyverse (Wickham et al. 2019), which is effectively a dialect of R. You can learn more about the tidyverse and its place in data science in the introduction to the Wickham text(Wickham, Çetinkaya-Rundel, and Grolemund 2023). To download the tidyverse package from the ‘net, open RStudio, find the ’console’ window on the left side of your screen, and enter the command followed by &lt;enter&gt; or &lt;return&gt; install.packages(“tidyverse”) 3.1 cha-cha-cha-changes R is constantly changing, not just in the proliferation of packages, but also in the organization of the R community. While R is free and open source, RStudio is a commercial product. The company (and website) that develops the RStudio IDE is undergoing a name change (from RStudio to Posit). This is motivated, in part, by the need to make the RStudio platform more welcoming for other languages including Python. Similarly, the R markdown programming language is slowly being replaced with newer, and ultimately more capable, software called Quarto. Quarto is back-compatible with R markdown, but can be used with other languages including Python as well. A description of the differences between R markdown and Quarto may be found here. For our purposes, you can treat Quarto files (.qmd suffix) as R markdown files (.rmd), and vice-versa. One more change: Posit (the company) is developing a new IDE called ‘Positron.’ Positron may ultimately be a more useful environment for data science than the RStudio IDE, but it is in the beta testing stage at this writing. The RStudio environment, and the Rmarkdown documents that are produced within it, will continue to be available, and widely used, for the foreseeable future. 3.2 some technical characteristics R is an object-oriented language - one conceptually organized around objects and data rather than actions and logic. In R, at the most basic or atomic level, “objects” include characters, real numbers, integers, complex numbers, and logicals. These atomic objects may be combined into vectors, which generally include objects of the same type [one kind of object, ‘lists,’ is an exception to this; R. D. Peng (2014)]. Vectors can be further combined into data frames, which are two-dimensional tables or arrays. A tibble is a particular type of data frame which is used in the tidyverse. Tibbles are, in some ways, handier to work with than other data frames. We’ll be working extensively with data frames in general, and tibbles in particular, as we move forward. Objects have attributes. Attributes of R include such things as name, dimensions (for vectors and arrays), class (that’s the type of object described in the previous paragraph), length, etc. Real world data sets are messy, and frequently have missing values. In R, missing values may be represented by NA (not available) or NaN (not a number, implying an undefined or impossible value). 3.3 finding help One does not simply ‘learn R.’ Unlike, say, learning to ride a bicycle, fry an egg, or drive a car with a manual transmission, learning R is not a discrete accomplishment that one can be said to have mastered and from which one then moves on. Rather, R is an evolving, open system of applications and tools which is so vast that there is always more that one can achieve, new lessons that one can learn. And, the complexity of R syntax is such that, for almost all of us, we will need help for coding on any non-trivial task. For us, the key ideas in “looking for help” will include not just the tools on the RStudio IDE, but also (a) using Google searches wisely, (b) judicious use of AI assistance, and (c) reaching out to your classmates and instructor. Here, as in the real world, there is an etiquette for help-seeking which is based on consideration. Your search for help should begin by making sure that others will encounter the same result, then by stripping the problem down to its essence. Once you have reduced the problem to this minimal, reproducible essence, you will often be able to spot the problem yourself - and, if not, you will make it easier for others to help you. There is an R package (reprex) which will likely facilitate this, but I haven’t tried it yet. Here is a good introduction. Finally, to get a sense of the power and versatility of R Markdown documents, you might explore this tutorial. Note that, if you want to work interactively with the tutorial, you will need to first sign up for an account on RStudio cloud. Go to posit.cloud, click on “learn more” in the “Free” column, then sign up. When you encounter obstacles, remember the 15-minute rule. References Buckheit, Jonathan B., and David L. Donoho. 1995. “Wavelab and Reproducible Research.” In Wavelets and Statistics, 55–81. Springer. Donoho, David L. 2010. “An Invitation to Reproducible Computational Research.” Biostatistics 11 (3): 385–88. https://doi.org/10/bxwkns. Gandrud, Christopher. 2013. Reproducible Research with R and R Studio. CRC Press. Hornik, Kurt, and The R Core Team. 2022. “R FAQ: The Comprehensive R Archive Network.” Peng, Roger D. 2014. R Programming for Data Science. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science 2e. \" O’Reilly Media, Inc.\". Actually, pirates have little use for R, as pirates love the C (programming language).↩︎ "],["exploring-r-world.html", "4 exploring R world 4.1 go to the movies 4.2 go into the clouds 4.3 open the box 4.4 go to (data)camp 4.5 learn to knit 4.6 read (parts of) another introductory book 4.7 older approaches", " 4 exploring R world There are many sources for learning the basics of R. A few of these follow. Please spend at least 180 mins exploring at least two of the following. Be prepared to discuss your progress next class: You will be asked which source(s) you used, what you struggled with, what questions you have, and what you would recommend to your classmates. Hint: If you find the material too challenging, remember the 15 minute rule, take a break away from your machine and other screens, clear your head, then try a different approach. 4.1 go to the movies About nine years ago, Iain Carmichael used data from the Internet Movies Database (IMDB) to introduce R. You can see his introduction here. You can consider his report from the standpoint of style (formatting, organization), coding (how he did this), data (the part of the IMDB data he is looking at), and his results (plots of distributions and relationships). Do you have any questions about the movie data? How might you ask these? A minimal amount of sleuthing - a click on the STOR 390 link at the top of the page, then a quick scroll - reveals that “all of [Carmichael’s] course material is on the github repo” - or repository. Can you find the Rmd document that generated his work? Can you download it on to your machine? If you try to run it, what happens? If you were working on your thesis and came across a problem like this, what would you do next? 4.2 go into the clouds In addition to the desktop version of R (and Rstudio) we will be using in this class, there is a cloud-based environment as well. As mentioned in the last chapter, you can sign up for a starter account at posit.cloud. When you open posit.cloud, you should see a column on the left of the screen that includes four sections - spaces, learn, help, and info. (If you don’t see these, click on the hamburger menu in the upper left corner and it will appear). Browse through the recipes tab, particularly the ones in the left most column, to start to get a sense of how you might solve some common challenges in R. 4.3 open the box Go to datasciencebox/content. Click on the “Hello world” link; this will take you to the beginning of Mine Çetinkaya-Rundel’s introductory lessons on R. These include slides, the source code for the slides (these are written in R markdown), and videos of her lectures, including the one we watched on the first day of class. Midway down this page, you’ll find a link to “the RStudio Cloud workspace for Data Science Course in a Box project.” Open it up and begin to explore the code and data behind her presentation. 4.4 go to (data)camp Many folks swear by (and others, I presume, at) DataCamp, which kind of gamifies learning software. As a student in this class, you have access to all of their stuff… free. You can even do lessons on your phone. Use the link given to you in class to enroll, then explore the introductory-level classes in R at https://www.datacamp.com/category/r 4.5 learn to knit In literate programming, comments, code, and results are integrated in a clear and reproducible way - they document our work. ‘Markdown’ is a simple language for adding formatting to text. ‘R’ is a statistical language. ‘R Markdown’ is a variant of R that you can use to produce or publish complex documents like this one, as well as the Carmichael page described above. To create an R markdown (Rmd) document, open up Rstudio, click on (File -&gt; New File -&gt; R Markdown). A window will open up with a file that begins with a block of YAML (Yet Another Markdown Language). You can edit this as needed: --- title: &quot;Here&#39;s an R Markdown Document&quot; author: &quot;Frankie McFrank Frank&quot; date: &quot;1/12/2025&quot; output: html_document --- Go ahead and click on the clever “knit” icon in the bar just above the source window to create a sample document. You’ll need to save the file with a new name, then R will finally create an HTML (Hyper Text Markdown Language ) page. Compare the R Markdown document (your code) with the result (the HTML). The second chapter of Healy’s online book about Data visualizations provides a more thorough explanation of R Markdown as well as an introduction to R and R studio which largely parallels the discussions here and in the Wickham et al. text (Healy 2017; Wickham, Çetinkaya-Rundel, and Grolemund 2023). We’ll be discussing R Markdown (and its cousin Quarto) in Chapter 6. 4.6 read (parts of) another introductory book In addition to the Wickham text and the Baumer et al. Modern Data Science with R book, one more warrants study: ModernDive (v2), aka Statistical Inference via Data Science is a new introduction to R and data science authored by Chester Ismay, Albert Kim, and Arturo Valdivia. It’s contents largely parallel those in Wickham and Baumer; if you decide to read all or part of it, please let me know what you think. 4.7 older approaches I, like thousands of others, learned R in the process of completing the Johns Hopkins Data Science Specialization offered through Coursera. The sequence can be challenging, but their introduction to R used an accessible, interactive R package called Swirl. You can read about swirl (“learn R in R”) at https://swirlstats.com/. 4.7.1 using Swirl After loading R (and opening R studio), you will get to the Swirl lessons with the following steps: Install the Swirl package on your computer (you only need to do this once). Type the following into your console window in R studio (typically left hand side of your screen or lower left) install.packages Then load the package into your workspace (you’ll need to do this at the beginning of every session you use Swirl) library (swirl) Then run it! swirl () Swirl will ask a few questions then give you the option of choosing one of several courses. You’ll choose the R Programming option, which leads to 15 separate lessons. At the end of each lesson, you’ll be asked Would you like to receive credit for completing this course on Coursera.org? Answer no… then do another lesson. 4.7.2 reading/watching Roger Peng’s text and/or videos Finally, you might consider the text and videos from the Coursera R class. Most of the material from that class can be found in Roger Peng’s (2014) text, a slightly updated version of which can be found here. The videos in the series may be found in this playlist. Here’s an introduction: . Roger Peng introducing R References Healy, Kieran. 2017. “Data Visualization for Social Science: A Practical Introduction with r and Ggplot2.” Peng, Roger D. 2014. R Programming for Data Science. Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science 2e. \" O’Reilly Media, Inc.\". "],["part-ii-towards-data-literacy.html", "Part II: Towards data literacy", " Part II: Towards data literacy "],["now-draw-the-rest-of-the-owl.html", "5 now draw the rest of the owl 5.1 time for hands-on experience 5.2 assignment", " 5 now draw the rest of the owl Draw the rest of the owl. In the prior chapter, you explored several different sources for learning how to code in R. Now it’s time to explore other approaches. Take a break from reading, and spend some time coding and consolidating, reviewing tutorials, or playing with data. 5.1 time for hands-on experience If you want to work actively with a dataset, here are two possibilities. (You are not limited to just these, so if you want to look at something else that’s fine too). Each of these datasets has been supplied as its own package. 5.1.1 consider loading the tidyverse The tidyverse allows the use of the ‘pipe’ operator, (“%&gt;%”), which is useful for combining commands. Now there is a native pipe in Base R (“|&gt;”), which does the same thing. But we will be using the tidyverse for a number of reasons, so go ahead and install it if you haven’t already, then load it. Remember that any package needs to be installed on your machine once before progressing. That is, if you installed the tidyverse previously, you don’t need to do the first line here. If you haven’t installed the tidyverse, you should remove the octothorpe or pound sign (#) on the second line before running this next chunk: # install.packages(&quot;tidyverse&quot;) library(tidyverse) 5.1.2 now explore the babynames package The babynames dataset is described here. What is in the data? What interesting questions might you ask about the dataset? # install.packages(&quot;babynames&quot;) library(babynames) data(babynames) str(babynames) ## tibble [1,924,665 × 5] (S3: tbl_df/tbl/data.frame) ## $ year: num [1:1924665] 1880 1880 1880 1880 1880 1880 1880 1880 1880 1880 ... ## $ sex : chr [1:1924665] &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; ... ## $ name: chr [1:1924665] &quot;Mary&quot; &quot;Anna&quot; &quot;Emma&quot; &quot;Elizabeth&quot; ... ## $ n : int [1:1924665] 7065 2604 2003 1939 1746 1578 1472 1414 1320 1288 ... ## $ prop: num [1:1924665] 0.0724 0.0267 0.0205 0.0199 0.0179 ... babynames %&gt;% slice_sample(n = 5) ## # A tibble: 5 × 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2009 F Amylia 8 0.00000395 ## 2 2009 M Kendrew 8 0.00000378 ## 3 2002 F Monay 17 0.00000861 ## 4 1973 F Elanda 12 0.00000772 ## 5 1925 M Herrell 5 0.00000434 5.1.3 or the (ggplot2)movies package The index page for the movies dataset is here. # install.packages(&quot;ggplot2movies&quot;) library(ggplot2movies) data(movies) str(movies) ## tibble [58,788 × 24] (S3: tbl_df/tbl/data.frame) ## $ title : chr [1:58788] &quot;$&quot; &quot;$1000 a Touchdown&quot; &quot;$21 a Day Once a Month&quot; &quot;$40,000&quot; ... ## $ year : int [1:58788] 1971 1939 1941 1996 1975 2000 2002 2002 1987 1917 ... ## $ length : int [1:58788] 121 71 7 70 71 91 93 25 97 61 ... ## $ budget : int [1:58788] NA NA NA NA NA NA NA NA NA NA ... ## $ rating : num [1:58788] 6.4 6 8.2 8.2 3.4 4.3 5.3 6.7 6.6 6 ... ## $ votes : int [1:58788] 348 20 5 6 17 45 200 24 18 51 ... ## $ r1 : num [1:58788] 4.5 0 0 14.5 24.5 4.5 4.5 4.5 4.5 4.5 ... ## $ r2 : num [1:58788] 4.5 14.5 0 0 4.5 4.5 0 4.5 4.5 0 ... ## $ r3 : num [1:58788] 4.5 4.5 0 0 0 4.5 4.5 4.5 4.5 4.5 ... ## $ r4 : num [1:58788] 4.5 24.5 0 0 14.5 14.5 4.5 4.5 0 4.5 ... ## $ r5 : num [1:58788] 14.5 14.5 0 0 14.5 14.5 24.5 4.5 0 4.5 ... ## $ r6 : num [1:58788] 24.5 14.5 24.5 0 4.5 14.5 24.5 14.5 0 44.5 ... ## $ r7 : num [1:58788] 24.5 14.5 0 0 0 4.5 14.5 14.5 34.5 14.5 ... ## $ r8 : num [1:58788] 14.5 4.5 44.5 0 0 4.5 4.5 14.5 14.5 4.5 ... ## $ r9 : num [1:58788] 4.5 4.5 24.5 34.5 0 14.5 4.5 4.5 4.5 4.5 ... ## $ r10 : num [1:58788] 4.5 14.5 24.5 45.5 24.5 14.5 14.5 14.5 24.5 4.5 ... ## $ mpaa : chr [1:58788] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ Action : int [1:58788] 0 0 0 0 0 0 1 0 0 0 ... ## $ Animation : int [1:58788] 0 0 1 0 0 0 0 0 0 0 ... ## $ Comedy : int [1:58788] 1 1 0 1 0 0 0 0 0 0 ... ## $ Drama : int [1:58788] 1 0 0 0 0 1 1 0 1 0 ... ## $ Documentary: int [1:58788] 0 0 0 0 0 0 0 1 0 0 ... ## $ Romance : int [1:58788] 0 0 0 0 0 0 0 0 0 0 ... ## $ Short : int [1:58788] 0 0 1 0 0 0 0 1 0 0 ... movies %&gt;% slice_sample(n = 5) ## # A tibble: 5 × 24 ## title year length budget rating votes r1 r2 r3 r4 r5 r6 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Off the … 1983 85 NA 3.4 20 14.5 34.5 24.5 4.5 0 14.5 ## 2 Barierata 1979 148 NA 8.1 13 0 0 0 0 0 0 ## 3 Bollywoo… 2002 89 NA 4.4 74 24.5 4.5 0 14.5 4.5 4.5 ## 4 Bromo an… 1926 24 NA 6.2 21 0 0 4.5 0 14.5 34.5 ## 5 Duel at … 1966 103 NA 6.4 281 4.5 4.5 4.5 4.5 4.5 14.5 ## # ℹ 12 more variables: r7 &lt;dbl&gt;, r8 &lt;dbl&gt;, r9 &lt;dbl&gt;, r10 &lt;dbl&gt;, mpaa &lt;chr&gt;, ## # Action &lt;int&gt;, Animation &lt;int&gt;, Comedy &lt;int&gt;, Drama &lt;int&gt;, ## # Documentary &lt;int&gt;, Romance &lt;int&gt;, Short &lt;int&gt; Regardless of whether you have played with one or both of these datasets, worked with the tutorials, or something else, please be prepared to share your experiences with the class at our next meeting. 5.2 assignment In our next meeting, go as far as you can with the following: Open the dataset. Describe the data in a paragraph based on one or more R functions (such as str, glimpse, and slice). What are the variables? What are the observations? What are the data types? What are the ranges of the variables? Are there missing values? After looking at the data, describe one or more questions of interest that you would like to ask about the data. (I do mean “of interest” - something that has meaning, that people would actually like to know). Write each question in a separate paragraph. Use headings to structure your document. Describe, in words, how you would do look at your question. Be as specific as possible, but don’t worry about R syntax (e.g., I would pull out such-and-such variables, or such-and-such observations, and I would compare them with x, or I would like this with ‘y’). Explain what you might find, and why (again) that would be interesting. Now draw the rest of the owl - translate your words into code, and run the analysis. If appropriate, describe what a graph or visualization of the data might look like. go for it if you can. Save your work as an R markdown document, and knit it to an html file. "],["literate-programming.html", "6 literate programming 6.1 projects are directories 6.2 scripts are files of code 6.3 R markdown / Quarto documents combine scripts with comments and (once knit) results 6.4 some elements of coding style 6.5 What to do when you are stuck", " 6 literate programming Showing your work, to (future) you as well as to others, is a key part of reproducible science. R Markdown documents facilitate this, as they allow you to include comments, code, and results in a single place. But before we consider R markdown, we begin with two more elemental ideas: projects and scripts (R4DS, Chapter 6) . 6.1 projects are directories You should save your work in projects. These isolate your data and scripts into discrete directories. There are two reasons I begin with ‘projects:’ The first is that students who are new to coding will often struggle to find their datasets and code on their personal machines; having a project directory makes things easier. The second is that , down the road, it’s likely that you will be working on R for different things in parallel - for this and another class, for this class and your thesis, or perhaps for two distinct types of analysis within your thesis. Left panel: Files pane in RStudio for this manuscript. Right panel: Menu showing some other recent projects. When you open up an R project, you’ll be in the right directory, with the relevant files (and only the relevant files) at your fingertips in the files pane. It is yet another way in which the notion of ‘tidiness’ facilitates our work. 6.2 scripts are files of code To do simple exercises in R, you can enter code directly in the Console pane (the default is in the lower left of the RStudio screen), then get an instant response. This (interactive) approach to coding is quick, but it is difficult to recreate. For example, imagine that I were doing an analysis between age and a personality trait that, in one dataset, is referred to as ‘Neuroticism’ (N) and, in a second, the same trait is reverse scored as ‘Emotional Stability’ (ES). If I want to combine measures of N and ES from two different datasets, each of which has scores for the trait on a 1-7 (or 7-1) scale, I could reverse one of these. My code might look like this: 6.2.0.0.1 combine two small datasets, reverse one of them, print first and last few rows: library(tidyverse) file1 &lt;- read_csv(&quot;data/datawithN.csv&quot;) ## Rows: 10 Columns: 4 ## ── Column specification ────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): gender ## dbl (3): id, age, N ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. glimpse (file1) ## Rows: 10 ## Columns: 4 ## $ id &lt;dbl&gt; 101, 102, 103, 104, 105, 106, 107, 108, 109, 110 ## $ age &lt;dbl&gt; 23, 55, 31, 19, 28, 24, 45, 32, 23, 44 ## $ N &lt;dbl&gt; 6, 4, 5, 3, 2, 7, 1, 1, 1, 1 ## $ gender &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, NA, &quot;M&quot;, &quot;F&quot; file2 &lt;- read_csv(&quot;data/datawithES.csv&quot;) ## Rows: 10 Columns: 4 ## ── Column specification ────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): gender ## dbl (3): id, age, ES ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. glimpse (file2) ## Rows: 10 ## Columns: 4 ## $ id &lt;dbl&gt; 201, 202, 203, 204, 205, 206, 207, 208, 209, 210 ## $ age &lt;dbl&gt; 19, 41, 27, 27, 48, 21, 20, 26, 42, 37 ## $ ES &lt;dbl&gt; 3, 6, 2, 5, 4, 1, 1, 1, 7, 7 ## $ gender &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;M&quot;, &quot;F&quot; combinedfile &lt;- file1 %&gt;% mutate(ES = 8 - N) %&gt;% # Creates &#39;ES&#39; from &#39;N&#39; for file1 select (-N) %&gt;% bind_rows(file2) combinedfile %&gt;% slice(c(1:3,18:20)) # the &#39;c&#39; is for combine or concatenate ## # A tibble: 6 × 4 ## id age gender ES ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 101 23 M 2 ## 2 102 55 M 4 ## 3 103 31 F 3 ## 4 208 26 f 1 ## 5 209 42 M 7 ## 6 210 37 F 7 You’ll want a record of your code for even simple transformations such as this one. R4DS Chapter 6 shows the R studio interface and encourages you to save your work in scripts. These are written in the source (editor) window in the upper left quadrant of the default R studio screen. 6.3 R markdown / Quarto documents combine scripts with comments and (once knit) results The objectives described in the prior section lead naturally to a consideration of R Markdown documents, which allow you to include comments, scripts, and results in a single place. In R4DS, Wickham [@-wickham2023] describes the use of Quarto rather than R markdown. Regardless of whether you use Quarto (see Chapter 28 of R4DS, or the tutorial here) or R Markdown (see the tutorial here), I encourage you to use one of these powerful, organizing approaches for nearly everything you do in R. There are as many as four parts of an R markdown or Quarto document: A YAML (yet another markdown language) header or metadata Text formatted in markdown R code (chunks) surrounded by code fences and, occasionally, inline code 6.4 some elements of coding style Good coding is often a combination of several skills ranging from puzzle-solving to communication. I can’t claim that these are the elements of coding style (apologies to Strunk &amp; White), but rather that these are merely some of the elements. Good coding is clear and thus commented. You are writing for your future self as well as others, so be explicit about the purpose of each chunk of code. Good coding is concise. When you can write code in 3 lines instead of 30, your code may be more clear and efficient. Take pleasure in writing parsimonious, efficient code. But where efficiency and clarity conflict, choose the latter. Good code should be complete, including all steps from reading the data to producing output. Where appropriate, comment-out (rather than delete) informative errors, again for the future you. Good code may be creative. The coolest solutions are those which pull from and synthesize a number of ideas. Creativity often requires walking away from a problem in order to ultimately arrive at a solution (Wertheimer’s Productive Thinking). Finally, good code should be considered. Reflect on the impacts of your work - just because you can analyze something doesn’t mean that you should. 6.5 What to do when you are stuck Google. pay attention to your error messages Ask for help, make your questions clear and reproducible (see R4DS Chapter 1) Take a break, think outside the box and kludge something together if you have to Document your struggles and your cleverness for a future you "],["principles-of-data-visualization.html", "7 principles of data visualization 7.1 some opening thoughts 7.2 some early graphs 7.3 Tukey and EDA 7.4 approaches to graphs 7.5 Tufte: first principles 7.6 the politics of data visualization 7.7 the psychology of data visualization 7.8 exercises 7.9 further reading and resources 7.10 notes for next revision", " 7 principles of data visualization 7.1 some opening thoughts Graphs aren’t just to inform, but to make you reflect. We are concerned not just with the design of graphs, but with our role in society as leaders, as arbiters of facts, as educators. We want to foster data literacy. Graphs and other data visualizations are arguably the most important tool we have in scientific communication. How do people understand graphs? How can you use graphs to tell people the truth in a way that they understand? How can you convey uncertainty in a graph? When you see a graph, what do you notice, what do you wonder, and what is the story? Is “story-telling” what visualizations should be about? 7.2 some early graphs Visual displays of information reach back to prehistoric times; modern graphs date back, arguably, to Playfair’s 1786 Political Atlas - in which “… spatial dimensions were used to represent nonspatial, quantitative, idiographic, empirical data. Although it now seems natural to represent, for example, rising and falling imports over time as a rising and falling line, it does not seem to have been done before that time and was quite an accomplishment. Notably, in addition to the statistical line chart, Playfair at one fell swoop single-handedly invented most of the remaining forms of the statistical graphic repertoire used today-the bar chart and histogram, the surface chart, and the circle diagram or”pie chart” (H. Wainer and Thissen 1981). Playfair’s 1786 analysis of trade deficits The most celebrated early graph is that of Minard: Minard’s display of Napoleon’s catastrophic assault on Moscow, 1812 The visualization depicts the size, latitude, and longitude of Napoleon’s army as they moved towards (tan line) then away (black line) from Moscow; temperature during the retreat is plotted as well. Further, vertical lines linking the temperature display to the number of troops indicate the often perilous river crossings which further decimated Napoleon’s troops). Cheng (2014) decomposes the graph and provides some simpler visualizations; she also provides the following background: “Czar Alexander of Russia sees that Napoleon was becoming too powerful, so he refuses to participate in this embargo [against the UK]. Angry at Czar Alexander’s decision, Napoleon gathers a massive army of over 400,000 to attack Russia in June of 1812. While Russia’s troops are not as numerous as France’s, Russia has a plan. Russian troops keep retreating as Napoleon’s troops move forward, burning everything they pass, ensuring that the French forces could not take anything from their environment. Eventually the French army follows the Russian army all the way to Moscow during October, suffering major losses from lack of food. By the time Napoleon gets to Moscow, he knows he has to retreat. As winter settles into Europe and the temperature drops, Napoleon’s troops suffer even more losses, returning to France from lack of food, disease, and weather conditions.” Of course, the casualties and retreat of Napoleon’s army are immortalized not just in this graph, but also in Russian literature (Tolstoy’s War and Peace) and music (Tchaikovsky’s 1812 overture, in which five cannon shots mark the battle of Borodino and eleven more mark the arrival of Napoleon in the now-razed city of Moscow). 7.3 Tukey and EDA For Donoho (2017), the publication of John Tukey’s “Future of Data Analysis” (1962) arguably marks the beginning of data science. As the first data scientist, Tukey embraced a descriptive and exploratory approach to data analysis, particularly in his publication of Exploratory Data Analysis (EDA, Tukey 1977). In EDA, Tukey presented an idiosyncratic, coherent approach to looking at data, beginning with tallying. The distributions of small counts of single variables, for Tukey, could best be presented in stem and leaf displays. Comparisons between groups can be presented in box plots. To examine relationships between variables and the adequacy of simple models, he argues for thoughtfully transforming data to uncover linear trends, then to examine residuals to find where these trends do not hold. 7.4 approaches to graphs A graph might begin with perception and understanding (the consumer), with knowledge and design values (the producer), but it also reflects the truth of the data. How much is each? In thinking about how to design graphs, we can begin with abstract theory, with principles of design informed by our understanding of perception, or with empirical analyses of understanding and memory. 7.5 Tufte: first principles Tufte (2001) describes Graphical Excellence: Graphs should, among other things, “Induce the viewer to think about the substance, rather than about methodology, graphic design, the technology of graphic productions, or something else.” Graphs should “Present many numbers in a small space, make large data sets coherent, and encourage the eye to compare different pieces of data.” Graphs should “serve a reasonably clear purpose: description, exploration, tabulation, or decoration [and] be closely integrated with the statistical and verbal descriptions of a data set.” Tufte concludes with the following Principles of Graphical Excellence, which I quote verbatim: Graphical excellence is the well-designed presentation of interesting data—a matter of substance, of statistics, and of design. Graphical excellence consists of complex ideas communicated with clarity, precision and efficiency. Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space. Graphical excellence is nearly always multivariate. And graphical excellence requires telling the truth. 7.6 the politics of data visualization On a cold morning in January, 1986, Space Shuttle Challenger lifted off from Cape Canaveral. Because of the cold weather, engineers at Morton Thiokol, who designed the rocket boosters, considered the possibility that the O-rings which sealed joints on the rockets would be too hard and brittle to prevent the release of potentially explosive fuel. They examined the relation between temperature and o-ring damage on prior flights, using the following visualization: What Morton Thiokol engineers saw prior to deciding to launch the Challenger Space Shuttle in January, 1986 What, if anything, can we see here? The graphic was chaotic and poorly designed; if the engineers had wanted to systematically explore the relation between o-ring damage and temperature they could have removed the rockets and arranged them on a single axis by liftoff-temperature rather on several rows by liftoff-date. Here’s what they would have seen: What the engineers could have seen, perhaps, with a better graph. The forecast for the morning of the launch was far colder than that for prior launches, and within the set of prior launches a clear relationship between temperature and o-ring damage is evident. But the Challenger did launch, exploding shortly after takeoff, killing the seven astronauts on board. With a clearer display of the data, the Challenger launch would likely have been postponed (Tufte 2001). 7.6.1 poor design leads to an uninformed or misinformed world In the flood of information that surrounds us, our peripheral and sometimes our focal attention will be drawn to pictorial summaries of consequential events and effects. But while data visualizations and data journalism has become more responsible, poorly designed graphs - what Tufte has described as “chartjunk” - are still common. Poorly designed graphs don’t just confuse rocket scientists. They reach us, mislead us, and make fools of all of us as well. With better graphs, we become a better informed, better educated citizenry. 7.6.2 poor design can be a tool to deceive Trump as “the first datavis President” (Meeks, 2019). The apps on our smartphones and electronic devices are meticulously designed to maintain our attention. They are largely successful, but, for many of us, made it more difficult to read in depth (Wu, The Attention Merchants). With information all around us demanding our attention, visual representations of data have become particularly important. The data visualization is, increasingly, not just a supplement to the story, but the story itself. When this premise is coupled with the idea that the truth is malleable, the product of what our leaders say rather than empirical statements of fact, we approach a dangerous situation, one in which real threats are misrepresented and lives are again endangered. Presenting information in self-promoting ways includes so-called “Sharpie-gate,” where President Trump simply altered a hurricane prediction map in defense of a misstatement. 7.7 the psychology of data visualization Speaking of America, consider the following: Chernoff’s too-clever faces In this figure, from Wainer (1981), Chernoff’s faces are used to represent multivariate data. The display is loosely based on the (psychological) premise that, from infancy, we are inordinately sensitive to facial features. Consequently, the most powerful, effective, or informative data displays should manipulate facial features. Does this graph succeed? Why or why not? Would an updated model, based on richer, more complex, and more realistic simulations of facial expressions (Thies et al. 2015) be more successful? 7.7.1 the power of animation Animated data displays bring the dimension of time into data visualization. Here are two brief (&lt; 5 minutes) animated data displays, each powerful in its own way, and each of which gives the viewer an appreciation of an important social phenomenon. The first is from the late Hans Rosling, and summarizes a chunk of the Gapminder data. There is a Gapminder package in R if you would like to explore the data further, and perhaps become the next Rosling: Rosling and social progress The second is from Kim Rees and her ex-colleagues at Periscopic (Rees is now at CapitalOne). For me, it’s an important graphic because it tries to overcome what has been called “psychic numbing” - we are sensitive to the individual case, but lose our perspective when faced with large-scale tragedies, so that effectively the more lives lost… the less we care (Slovic et al. 2013). Rees and stolen years 7.7.2 telling the truth when the truth is unclear We live in an uncertain world, and a major challenge in data visualization is how to convey this uncertainty in a way that people understand readily. Examples of this are familiar. Here in Florida, projected hurricane paths are frequently represented by a “cone of uncertainty” surrounding a single track. An alternative approach is to use a display which shows a distribution of possible tracks. Two approaches to displaying hurricane paths 7.7.3 visualizing uncertainty To further illustrate the idea of uncertainty, data visualizations may be animated. To display hurricane outcomes, for example, individual lines in a spaghetti plot might fade in and out (Cox and Lindell 2013). Another use of animation is suggested by (Hullman, Resnick, and Adar 2015) who use hypothetical outcome plots rather than simple error bars to illustrate sampling distributions. Empirically, subjects have a better understanding of the meaning of statistical effects when these are represented by dynamic displays. During the 2016 US Presidential Election, the NY Times graphics presented the range of likely possibilities (here, between the 25th and 75th percentiles) as a dynamically changing “jittery gauge” . Following the election, there was some criticism of these gauges, as readers claimed that they tended to make them anxious. However, it appears that at least some of the anxiety was attributable not to the gauges themselves, but to the electoral outcome itself. The gauges were back in 2018, and will likely be used again in the future. Finally, animation may be used not just to display uncertainty or a possible range, but also data displays which both carry a great deal of information and unfold over time, such as the language of personality development in my own work. 7.8 exercises Please consider each of the following data visualizations. Smartphone sales Which smartphone manufacturers are doing well? Look at the smartphone sales visualization for just a moment, as if you might while reading something else on another screen, or as your ten-year old little sister might look at it, or most of your classmates in high school, or the average person you might see at a rest stop on the Florida Turnpike. What does the graph tell them? Now look at the diagram more closely. What does it actually say? How could you improve it to make the content of the graph more informative about its title? Why was the graph designed in this way? Does this matter? Stand your ground Does “stand your ground” make us safer? Again - Glance at the visualization, as you might glance at dozens of images on &gt; your phone or computer screen. What does the graph tell you? Again, look at it more closely. What does it actually say? Would you change the figure? If so, how? Would your changed visualization be “better”? If yes, why? Big businesses Voronoi’s “Top 50 Most Profitable Companies” Finally, What does this visualization tell you at first glance? Do you think this is a good visualization? Would Tufte agree? Why or why not? Are Tufte’s principles of data visualization sound? It’s been argued that we live in an attention economy, in which ‘eyes’ (e.g., in a screen) are more valuable than wealth. How does this relate to the design of data visualizations? 7.9 further reading and resources If you’d like to learn more, Tufte (2001) and his other books are beautiful and thought provoking. Cleveland (1985) examines graphs from a more rigorous psychological and empirical viewpoint. The Data Stories podcasts are often excellent, despite the challenge of an auditory medium for visual display (especially the episode on Hans Rosling). And Healy(2017) provides a comprehensive introduction that, in essence, provides the basis for a semester-long course based in the ggplot2 package in R that we will be working with in the next chapter (Wickham, Çetinkaya-Rundel, and Grolemund 2023). 7.10 notes for next revision add some notes on Healy References Cleveland, William S., and Robert McGill. 1985. “Graphical Perception and Graphical Methods for Analyzing Scientific Data.” Science, New Series 229 (4716): 828–33. https://www.jstor.org/stable/1695272. Cox, Jonathan, and Michael Lindell. 2013. “Visualizing Uncertainty in Predicted Hurricane Tracks.” International Journal for Uncertainty Quantification 3 (2). https://doi.org/10/gjjsfw. Donoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734. Healy, Kieran. 2017. “Data Visualization for Social Science: A Practical Introduction with r and Ggplot2.” Hullman, Jessica, Paul Resnick, and Eytan Adar. 2015. “Hypothetical Outcome Plots Outperform Error Bars and Violin Plots for Inferences about Reliability of Variable Ordering.” Edited by Elena Papaleo. PLOS ONE 10 (11): e0142444. https://doi.org/10.1371/journal.pone.0142444. Slovic, Paul, David Zionts, Andrew K Woods, Ryan Goodman, and Derek Jinks. 2013. “Psychic Numbing and Mass Atrocity.” The Behavioral Foundations of Public Policy, 126–42. https://doi.org/10/gk4945. Thies, Justus, Michael Zollhöfer, Matthias Nießner, Levi Valgaerts, Marc Stamminger, and Christian Theobalt. 2015. “Real-Time Expression Transfer for Facial Reenactment.” ACM Trans. Graph. 34 (6): 183–81. https://doi.org/10/f7wqz7. Tufte, Edward R. 2001. The Visual Display of Quantitative Information. 2nd ed. Cheshire, CT: Graphics Press. Tukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67. https://doi.org/10/d48nqg. ———. 1977. “EDA: Exploratory Data Analysis.” Reading, Mass. Wainer, H, and D Thissen. 1981. “Graphical Data Analysis,” 51. Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science 2e. \" O’Reilly Media, Inc.\". "],["visualization-in-r-with-ggplot.html", "8 visualization in R with ggplot 8.1 a picture &gt; (words, numbers)? 8.2 Read Wickham’s opening chapter 8.3 explore", " 8 visualization in R with ggplot In the last chapter, we introduced data visualization, citing “vision-aries” including Edward Tufte and Hans Rosling, inspired works such as Minard’s Carte Figurative and Periscopic’s stolen years, as well as a few cautionary tales of misleading and confusing graphs. Here, in playing with and learning the R package ggplot, we begin to move from consumers to creators of data visualizations. As the first visualization in (Wickham, Çetinkaya-Rundel, and Grolemund 2023) reminds us, data visualization is at the core of exploratory data analysis: Data visualization is at the core of data analysis ((Wickham, Çetinkaya-Rundel, and Grolemund 2023)) In the world of data science, statistical programming is about discovering and communicating truths within your data. This exploratory data analysis is the corner of science, particularly at a time in which confirmatory studies are increasingly found to be unreproducible. Most of your reading will be from Chapter 1 of (Wickham, Çetinkaya-Rundel, and Grolemund 2023), this is intended only as a supplement. 8.1 a picture &gt; (words, numbers)? The chapter begins with a quote from John Tukey about the importance of graphs. Yet there is a tendency among some statisticians and scientists to discount graphs, to consider graphic representations of data as less valuable than statistical ones. It is true that, because there are many ways to graph data, and because scientists and data journalists are humans with pre-existing beliefs and values, a graphical displays should not be assumed to simply depict a singular reality. But the same can be said about statistical analyses (see Chapter 9). To consider the value of statistical versus graphical displays, consider ‘Anscombe’s quartet’ (screenshot below, live at http://bit.ly/anscombe2019): An adaptation of Anscombe’s “quartet” (Anscombe 1973) Exercise 8_1 Consider the spreadsheet chunk presented above, which I am characterizing as data collected on a sample of ten primary school children at recess on four consecutive days. Working with your classmates, compute the mean, standard deviation, and correlation between the two measures for one day. Share your results with the class. The four pairs of variables in (Anscombe 1973) appear statistically “the same,” yet the data suggest something else. Additional examples of the problem of relying on simple statistics, in particular correlation coefficients, are considered in the first chapter of Healy (2017). Perhaps graphs can reveal truths that statistics can hide. Exercise 8_2 The Anscombe data is included in base Ras a library in R. Can you find, load, and explore it? 8.2 Read Wickham’s opening chapter In class, we will review and recreate the plots in section 1.2 of (Wickham, Çetinkaya-Rundel, and Grolemund 2023) and exercises in 1.2.5 and 1.4.3 and 1.5.5 Savor this section: Read slowly, and play around with the RStudio interface. For example, read about the mpg data in the ‘help’ panel, pull up the mpg data in a view window, and sort through it by clicking on various columns. A screenshot from RStudio, showing the mpg dataset 8.3 explore Try to make a cool graph - one that informs the viewer, and, to paraphrase Tukey, helps us see what we don’t expect. Try several different displays. Which fail? Which succeed? Be prepared to share your efforts. Remember the 15 minute rule, and don’t be afraid to screw up. Each mistake you wisdom. 8.3.0.1 some sources The Datacamp ggplot course https://app.datacamp.com/learn/courses/introduction-to-data-visualization-with-ggplot2 The Gapminder data https://cran.r-project.org/web/packages/gapminder/readme/README.html A graph in The Economist. Here, Andrew Couch, host of the “Tidy Tuesday” podcast, walks through the recreation of a fairly complex plot from The Economist. Follow along, beginning with the links to GitHub https://youtu.be/gcDQ_KbXQ3o?si=wFadvTi886hQm6H- For another example of an Economist-style visualization, there is also this analysis of Global Terrorism Data from Rpubs: https://rpubs.com/tangerine/economist-plot. (This appears to be a student assignment). As with the ’movies” data described in an earlier project, the link to the data is no longer valid. To access it, you can establish an account on Kaggle (which links to older data) and/or the global terrorism database at the University of Maryland. References Anscombe, FJ. 1973. “Graphs in Statistical Analysis.” American Statistician 27 (1): 17–21. Healy, Kieran. 2017. “Data Visualization for Social Science: A Practical Introduction with r and Ggplot2.” Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science 2e. \" O’Reilly Media, Inc.\". "],["on-probability-and-statistics.html", "9 on probability and statistics 9.1 on probability 9.2 the rules of probability 9.3 continuous probability distributions 9.4 dangerous equations 9.5 notes for next revision", " 9 on probability and statistics We previously considered Anscombe(1973) and his quartet, and how visualizing data is valuable. This week, we move to a brief discussion of principles of statistics. 9.1 on probability Discrete probability is used to understand the likelihood of categorical events. We can think of initial estimates of probability as subjective or personal. For some events (what is the probability this plane will crash?), an estimate of probability can be drawn from a base rate or relative frequency (e.g., p(this plane will crash) = (number of flights with crashes/ number of flights). For other events (e.g., what is the probability that a US President will resign or be impeached before completing their term of office?), it may be hard to arrive at a suitable base rate. Here, a number of subjective beliefs or principles may be combined to arrive at a subjective or personal probability. In a sense, all probability estimates begin with a personal belief such as this, in part because the choice of the most informative base rate is often not self-evident - in the plane crash example, maybe we should consider a reference group rates such as ‘for this airline,’ or ‘for this type of jet’ etc. The claim that there is a simple distinction between ‘relative frequencies’ and ‘personal probabilities’ turns out to be an oversimplification. Relative frequencies are not statistical givens, because in many of not most situations there is no single base rate that is clearly superior to others. For the plane crash example, we might consider crash rates among all planes, all jets, and all carriers, or particular planes (Boeing 737 Max jets), particular carriers (United), or the intersection of some or all of these as well as other variables (United 737 Max airliners flying out of LaGuiardia at night). There is no single answer to the plane crash estimate, in other words. Similarly, a baseball manager, in considering whether a pinch hitter might be brought in to bat at a crucial spot in a game, might consider an omnibus batting average (effectively a relative frequency of hits/opportunities), batting average at night, against this pitcher, etc. In general, there is not a correct answer to this “problem of the reference class” in part because a more precise reference group (737 Max planes, batting against a particular pitcher) is inherently based on a smaller sample of data, and is therefore less stable, than a broader, but coarser reference group upon which a probability estimate might also be based (Lanning 1987). The personal origins of probability estimates should become less important as we are exposed to data and revise our estimates in accordance with Bayes theorem. But over the last 50 years, a substantial body of evidence has demonstrates that, under at least some circumstances, we don’t make estimates of probability in this way. 9.2 the rules of probability Here’s an introduction to the principles of probability. These are presented, with examples and code, in this R markdown document at Harvard’s datasciencelabs repository: I. For any event A, 0 &lt;= P (A) &lt;= 1 II. Let S be the sample space, or set of all possible outcomes. Then P(S) = 1, and P (not S) = 0. III. If P (A and B) = 0, then P (A or B) = P (A) + P (B). IV. P (A|B) = P (A and B)/ P (B) Principle III applies for mutually exclusive events, such as A = you are in class this morning, B = you are at the beach this morning. For mutually exclusive (disjoint, disjunctive) events, the union is the sum of the two events. This is called the addition rule for disjoint events. A different rule applies for events that are mutually independent, such as (A = I toss a coin and it lands on ‘Heads’) and (B = it will rain tomorrow). What we mean by independent is that our estimates of the probability of one don’t change based on the state of the other - your estimate of the likelihood of rain shouldn’t depend on my coin flip. Here, you multiply rather than add: If P (A|B) = P (A), then P (A and B) = P (A) P (B). In words - if the probability of A given B equals the probability of A, then the probability of both A and B equals the probability of A times the probability of B. This multiplication rule is handy for estimating the probability of an outcome that happens following a chain of independent events, such as the probability that the next eight times I toss a coin it will land on “tails” every time: P (TTTTTTTT) = P (T) P (T) P (T) P (T) P (T) P (T) P (T) P (T). = .58 = 1/256. Many sets of events are neither disjoint nor independent, so we need more general ways of thinking about pairs of events. For most of us, Venn diagrams are useful to think about combining probabilities. The union or P (A U B) describes the probability that A, B, or both of these will occur. Here, you will use the general addition rule: P (A or B) = P (A) + P (B) - P (A and B) (the probability of A or B is the probability of A plus the probability of B minus the probability of both A and B). For the intersection or P (A ∩ B), we need to consider conditional probabilities. Think of the probability of two events sequentially: First, what’s the probability of A? Second, what’s the probability of B, given that A has occurred? Multiply these to get the likelihood of A and B: P (A and B) = P (A) P (B|A). Example: The probability of you and your roommate both getting mononucleosis equals the probability of your getting mono times the probability that your roommate gets it, given that you have it also. This is the general multiplication rule. In this abstract example, the order is irrelevant. To estimate the likelihood of A and B, we could as easily take the probability of B, and multiply it by the conditional probability of A given B P (A and B) = P (B) P (A|B). Use the mono example again. What are A and B here? Does it still make sense? When might P (B|A) make more sense than P (A|B)? We are often interested in estimating conditional probabilities, in which case we’ll use the same equation, but solve instead for P (A|B). This leads us back to principle IV: IV. P (A|B) = P (A and B)/ P (B) 9.2.1 keeping conditional probabilities straight In general, P (B|A) and P (A|B) are not equivalent. Here, one could use a Venn diagram to model this asymmetry. There are several R packages for doing this, including Venn and VennDiagram. 9.2.2 exercises Exercise 9.1. In 2024, the Florida Highway Patrol won a national competition for “best looking cruiser.” The winning car was a Dodge Charger. Not all FHP cruisers are Dodge Chargers, but some are. Assume that there are 8 million registered cars in Florida, that all cars (including all FHP cruisers) are registered, and that 80,000 of these are Dodge Chargers. On the basis of the above information, if you see a Dodge Charger on the road, can you compute the probability that it is an FHP cruiser (i.e., p(FHP cruiser | Dodge Charger)? If you can compute this, what is the probability? If you cannot compute this, what is the minimum additional information would you need to compute this probability (p(FHP cruiser | Dodge Charger)? Provide a reasonable estimate of this additional value, then compute (p(FHP cruiser | Dodge Charger). Working with your own numbers, what is p(Dodge Charger | FHP cruiser)? How confident are you in these results? Are there any additional assumptions that you might make that would make you more confident about your results? Exercise 9.2. Sketch out a Venn Diagram that accurately reflects the relationships you described in exercise 9.1. Use R to generate your Venn Diagram. Look at your figure. In general, if P (A|B) &lt; P (B|A), what must be true of the relationship of P (A) to P (B)? 9.3 continuous probability distributions We can also use probability with continuous variables such as systolic blood pressure (that’s the first one), which has a mean of approximately 120 and a standard deviation of 15. Continuous probability distributions are handy tools for thinking about the meaning of scores, particularly when we express scores in standard deviations from the mean (z scores). More to the point, this way of thinking about probability is widely used in questions of scientific inference, as, for example, in testing hypotheses such that “the average systolic blood pressure among a group of people studying at a coffee shop (hence caffeinated) will be significantly greater than that of the population as a whole.” This is part of the logic of Null Hypothesis Significance Testing (NHST) - if the result in my coffee shop sample is sufficiently high, then I say that I have rejected the null hypothesis, and found data which support the hypothesis of interest. 9.4 dangerous equations Just as Tufte (2001) demonstrated that poor data visualizations can be dangerous, leading, for example, to the loss of life in the Challenger disaster, Wainer (2007) shows that a lack of statistical literacy is also “dangerous.” Wainer cites three specific examples of important, yet widely misunderstood, statistical laws. The first of these is deMoivre’s equation, which shows that variability decreases with the square root of sample size. Because the variability of a sample decreases with the size of that sample, small samples tend to have extreme scores. For example, the counties with the highest and lowest rates of kidney cancer (or most other unexplained health measures) will be sparsely populated, typically rural places. For Wainer, a second form of statistical illiteracy is the failure to understand the complex interdependencies that arise in multiple regresson analysis, in particular, how coefficients may change or even reverse in sign when new variables are added as predictors. Wainer’s third example of statistical illiteracy is the failure to appreciate regression to the mean. I consider this to be the most dangerous form of statistical illiteracy, in part because regression effects contribute to an overestimate of the effectiveness of punishment and an under-appreciation of the effectiveness of positive reinforcement as tools for behavior change (Hastie and Dawes 2010). 9.5 notes for next revision This chapter should include a mnore explicit treatment of Bayes theorem as well as a demonstration of empirically generated probability distributions. Show how to generate random variables, random splits… References Anscombe, FJ. 1973. “Graphs in Statistical Analysis.” American Statistician 27 (1): 17–21. Hastie, Reid, and Robyn M Dawes. 2010. Rational Choice in an Uncertain World: The Psychology of Judgment and Decision Making. Sage. Lanning, Kevin. 1987. “Some Reasons for Distinguishing Between ‘Non-normative Response’ and ‘Irrational Decision’.” The Journal of Psychology 121 (2): 109–17. https://doi.org/10/fv4hh5. Tufte, Edward R. 2001. The Visual Display of Quantitative Information. 2nd ed. Cheshire, CT: Graphics Press. Wainer, Howard. 2007. “The Most Dangerous Equation.” American Scientist 95 (3): 249. https://doi.org/10.1511/2007.65.249. "],["reproducibility-and-the-replication-crisis.html", "10 reproducibility and the replication crisis 10.1 answers to the reproducibility crisis 10.2 further readings 10.3 notes for next revision", " 10 reproducibility and the replication crisis Probability theory is elegant, and the logic of Null Hypothesis Significance Testing (NHST) is compelling. But philosophers of science have long recognized that this is not really how science works (Lakatos 1969). That is, science is not primarily built by the testing and rejecting of null hypotheses. (Think, for example, of how you might describe an experiment for testing the hypothesis that gravity exists, and whether you would ever reject this hypothesis). The problem is a multifaceted one. It arises partly because, in any experiment, there is a large (infinite?) number of conditions which might be invoked to explain-away results of a failure of our work. It arises because we are human, and prone to various biases of decision making and information integration (that is, we are poor Bayesians). It arises, too, because scientific institutions such as journals, funding agencies, and universities are competitive environments which incentivize positive findings. In recent years, the tension between the false ideal of NHST and the real world of science has become increasingly evident. Within psychology, experimental studies have often - even typically - failed to replicate well-known results (Open Science Collaboration 2015). It’s not just psychology (Baker 2016). One of the first important papers to shine light in the area (Ioannidis 2005) came from medicine; it suggested six contributing factors, which I quote verbatim here: The smaller the studies conducted in a scientific field, the less likely the research findings are to be true. This stems directly from our discussion of the central limit theorem and the instability of results from small samples. The smaller the effect sizes in a scientific field, the less likely the research findings are to be true We’ll talk about effect size below. The greater the number and the lesser the selection of tested relationships in a scientific field, the less likely the research findings are to be true. (and) The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true. The “problem” of analytic flexibility leads to ‘p-hacking’ The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true and The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true. Positive findings rise, and negative ones are ignored. And scientists are human, and subject to incentives. Here’s a video which provides some more context for the crisis: . *Video 10.1: On the reproducibility crisis (12 mins) 10.1 answers to the reproducibility crisis For scientific progress to occur, there needs to be more than a simple rejection of what is wrong with current methods and research programs: Here, as elsewhere, criticism must be constructive to be valuable. There have been a number of solutions proposed to the reproducibility crisis. 10.1.1 partial answer 1: Tweak or abandon NHST The first cluster of responses to the reproducibility crisis is concerned with statistics, specifically problems with Null Hypothesis Significance Testing (NHST). These include (a) justifying one’s alpha - making it more stringent, for example, for counter-intuitive claims (Grange et al. 2018), (b) changing the default p value from .05 to .005 (Benjamin et al. 2018), and (c) abandoning significance testing altogether (McShane et al. 2017). (Szucs and Ioannidis 2017) goes into some of these issues in more detail and discusses other limitations of significance testing, including the dichotomous, all-or-none silliness of the accept/reject decision. (If you play the NHST game, there is no ‘almost’ significant, ‘approached significance,’ ‘highly significant’, etc.). (Leek and Peng 2015) argue that the problems are not merely with NHST, but with the whole of data analysis. They maintain that better training in data science - courses like ours, perhaps, are part of the answer. (figure) (Munafò et al. 2017) also argue that threats to reproducible science occur at a number of places in science, not just with the evaluation of hypotheses. munafo2017 10.1.2 partial answer 2: Keep a log of every step of every analysis in R markdown or Jupyter notebooks A second cluster of responses is concerned with keeping good records. Let’s say that you are running a study, say, which looks at the hypothesis of differential variation of females and males in a cognitive measure; your interest is to critically examine the hypothesis discussed by (Howard Wainer 2007) that males show more variability. There have been a lot of studies related to this over the years, so that rather than collect new data you decide that you will work with existing data from several online archives. You find and download spreadsheets from two studies: In the first, gender is coded ‘1’ for male, ‘2’ for female. In the second, gender is coded ‘1’ for female, ‘2’ for male, and ‘3’ for other. There are, in essence, two ways that you can combine the variables into a common format: The first would be to take one of the spreadsheets and do a few find-and-replace commands on the appropriate column of the data. This is quick and easy - but when someone else, or even future you, returns to this data, you will not remember if you have recoded it. The alternative is to keep a record of your work in R markdown. This is more time consuming, and can sometimes be clumsy. But it is virtuous useful and clear - and when you screw up, you will have a full record of what happened. Part of the problem of scientific reproducibility is to keep comprehensive records. This record-keeping and research transparency is at the heart of R markdown documents. 10.1.3 partial answer 3: Pre-registration of your research The third answer to the reproducibility crisis is the most comprehensive; it involves not merely keeping a record of what you have done, but preregistering your work, that is, fully specifying your planned analyses beforehand (Miguel et al. 2014). The author, an economist, outlines his argument in a five-minute video here. For randomized controlled trials, consider socialscienceregistry.org, and for more general use, use the open science framework page. 10.2 further readings Finally, if you would like to learn more about the reproducibility crisis, there is a recent collection of papers in Nature here. 10.3 notes for next revision This chapter could include more recent scholarship on the reproducibility crisis and links to sites including OSF. It could also introduce a discussion of bootstrapping, effect-size estimation… n empirical approach to reproducibility (e.g., k-fold sampling). References Baker, Monya. 2016. “Is There a Reproducibility Crisis?” Nature 533: 26. Benjamin, Daniel J., James O. Berger, Magnus Johannesson, Brian A. Nosek, E.-J. Wagenmakers, Richard Berk, Kenneth A. Bollen, et al. 2018. “Redefine Statistical Significance.” Nature Human Behaviour 2 (1): 6–10. https://doi.org/10/cff2. Grange, JA, D Lakens, F Adolfi, C Albers, F Anvari, M Apps, S Argamon, et al. 2018. “Justify Your Alpha.” Nature Human Behavior. Ioannidis, John PA. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124. https://doi.org/10/chhf6b. Lakatos, Imre. 1969. “Falsification and the Methodology of Scientific Research Programmes.” Criticism and the Growth of Knowledge. Cambridge University Press: Cambridge. Leek, Jeffrey T, and Roger D Peng. 2015. “Statistics: P Values Are Just the Tip of the Iceberg.” Nature 520 (7549): 612. https://doi.org/10/gfb8jm. McShane, Blakeley B., David Gal, Andrew Gelman, Christian Robert, and Jennifer L. Tackett. 2017. “Abandon Statistical Significance.” arXiv:1709.07588 [Stat], September. https://arxiv.org/abs/1709.07588. Miguel, Edward, Colin Camerer, Katherine Casey, Joshua Cohen, Kevin M Esterling, Alan Gerber, Rachel Glennerster, et al. 2014. “Promoting Transparency in Social Science Research.” Science 343 (6166): 30–31. https://doi.org/10/gdrcpz. Munafò, Marcus R., Brian A. Nosek, Dorothy V. M. Bishop, Katherine S. Button, Christopher D. Chambers, Nathalie Percie du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J. Ware, and John P. A. Ioannidis. 2017. “A Manifesto for Reproducible Science.” Nature Human Behaviour 1 (1): 0021. https://doi.org/10.1038/s41562-016-0021. Open Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716–16. https://doi.org/10/68c. Szucs, Denes, and John Ioannidis. 2017. “When Null Hypothesis Significance Testing Is Unsuitable for Research: A Reassessment.” Frontiers in Human Neuroscience 11: 390. https://doi.org/10/gc6vws. Wainer, Howard. 2007. “The Most Dangerous Equation.” American Scientist 95 (3): 249. https://doi.org/10.1511/2007.65.249. "],["wrangling-and-tidying.html", "11 wrangling and tidying 11.1 the structure of the tidyverse 11.2 where should we eat? 11.3 more on tidy coding", " 11 wrangling and tidying wild horses are beautiful, wild data … not so much The term ‘wrangling’ has been used to describe the process by which unruly, messy, and complex datasets are organized and restructured so that they can be summarized, interpreted, and understood. Wrangling includes finding errors, which requires looking at your data closely enough to identify problems, and transforming the data as needed to allow data analyses to take place. Tidying is also concerned with data preparation, but is focused on the narrower task of arranging data into simple data frames (tibbles) in which each variable is in its own column, each observation is in its own row, and each cell includes one and only one value. In practice, the processes of data wrangling and data tidying are often overlapping. In this chapter, we will explore (a little) wrangling and (some) tidying using simulated data. First, we briefly consider some characteristics of the tidyverse 11.1 the structure of the tidyverse The R programming language was initially intended to serve engineers, and the tidyverse can be seen as a collection of packages which are aimed at a broader audience of statisticians and data scientists, and to help them become more productive (R. Peng 2018). Tidyverse packages have a common syntax that makes it easier to generate and understand code and create reproducible results. Tidyverse packages may be described as having three layers. The core of the tidyverse includes nine packages. You will see the list of these packages when you load the tidyverse: library(tidyverse) Each package includes a number of functions. Many tidyverse packages are described on handy cheatsheets such as this one for dplyr: The periphery of the tidyverse consists of additional packages which are less commonly invoked than those in the tidyverse core. Unlike the core packages, all of these peripheral or auxiliary packages must be loaded into memory by an explicit statement, e.g., library(googlesheets4). You can see the list of all (core and peripheral) packages with the tidyverse_packages command: &gt; # install.packages(“tidyverse”) &gt; tidyverse_packages () For me, the most useful packages in this peripheral or auxiliary layer have been googledrive, googlesheets4, and readxl (for working with Google apps and Excel files). I’ve also used rvest, xml2, and jsonlite (for web scraping and parsing more complex data structures). In addition to the core and periphery of the tidyverse, there is a more loosely-defined third layer of what may be considered to be tidyverse-friendly packages. These packages, which are not installed with the core and peripheral tidyverse, include a wide range of tools that are handy for using tidy syntax in particular applications, such as janitor (for cleaning data), tidytext (for text analysis), and tidygraph (for network analysis). When you use these, you’ll need to first make sure that they are installed on your machine. (R studio will remind you to do this). Then you can load them into memory. At each of these three levels, the tidyverse is constantly evolving. For example, at this writing, one of the peripheral or auxiliary packages (httr) has been superseded by a newer package (httr2); despite this, httr (and not httr2) remains in the auxiliary layer. 11.2 where should we eat? Tidyverse functions from at least two of the core tidyverse packages, dplyr and tidyr, help wrangle, clean, and shape our data into a tidy form that can ‘spark joy’ (Kondo 2016). To learn more about tidy data and how we use dplyr and tidyr in everyday coding, we will construct and explore a simulated dataset, consisting of a 100 restaurants, each of which is described by just a few variables. To do this, we’ll first name the restaurants. We begin with the data from the babynames package, then will use filter function (from dplyr) to choose only names since 1981, then we group and summarize the data by name, then arrange (sort) the names by popularity, then slice (take only) the first 100 observations, then finally select only the name column. Each of these functions is part of the dplyr package: library(babynames) # make restaurant names restodata &lt;- babynames |&gt; filter(year &gt; 1980) |&gt; group_by(name) |&gt; summarise(n=sum(n)) |&gt; arrange(desc(n)) |&gt; slice(1:100) |&gt; select(name) head(restodata,3) ## # A tibble: 3 × 1 ## name ## &lt;chr&gt; ## 1 Michael ## 2 Christopher ## 3 Matthew In the prior chunk of text we wrangled - or, if you prefer, massaged - the babynames data in a few ways to get us a simple list of 100 popular names. But this is only a start. Let’s change the set of ‘person names’ into ‘restaurant names.’ To do this, we will use mutate (another dplyr function) then combine each name with the phrase ’s_place.’ To combine strings, we use the str_c function. This is from the stringr package, which is another package in the core of the tidyverse: restodata &lt;- restodata |&gt; mutate (resto_name = str_c(name,&quot;s_place&quot;)) head(restodata,3) ## # A tibble: 3 × 2 ## name resto_name ## &lt;chr&gt; &lt;chr&gt; ## 1 Michael Michaels_place ## 2 Christopher Christophers_place ## 3 Matthew Matthews_place Now, assume that the restaurants has been graded by the Department of Health on an A to F scale. We’ll assign these health ratings randomly, using the following three steps. First, we seed the (pseudo) random number generator; this allows the result to be reproducible. If you don’t do this, you will almost certainly get different ratings each time you run the code. Then, we declare health as a random integer for each restaurant on 1 to 5 scale. We can do this in several ways. Here, I use the sample function to directly generate my 100 integers. (I originally used runif, which generates random numbers, then rounded this score to the nearest integer, but this is more complicated, and leads to fewer observations with extreme values. It’s commented out in my code.). Then, we recode these as letter grades using the case_when function. This function is akin to an ‘if-else’ statement. Note that the case_when syntax is tricky. If we think of the double equals as ‘is equal to’, the tilde as ‘call it’, and the comma as ‘else,’ the first part of the case when statement becomes the following: when health is equal to 1, call it F, else … In addition to health, assume that we have cost data as well. We’ll use essentially the same code to come up with our cost variable. Then we drop our original name variable. # add health ratings set.seed(33458) restodata &lt;- restodata |&gt; # mutate(health = round(runif(n=n(),1,5))) |&gt; mutate (health = sample(1:5, 100, replace=T)) |&gt; mutate(health = case_when(health == 1 ~ &#39;F&#39;, health == 2 ~ &#39;D&#39;, health == 3 ~ &#39;C&#39;, health == 4 ~ &#39;B&#39;, health == 5 ~ &#39;A&#39;)) |&gt; mutate(cost = sample(1:3, 100, replace=T)) |&gt; mutate(cost = case_when(cost == 1 ~ &#39;$&#39;, cost == 2 ~ &#39;$$&#39;, cost == 3 ~ &#39;$$$&#39;)) |&gt; select(-name) head(restodata,3) ## # A tibble: 3 × 3 ## resto_name health cost ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Michaels_place B $$$ ## 2 Christophers_place A $$ ## 3 Matthews_place C $$$ Each restaurant now has a restaurant name, a health rating, and a classification as cheap ($), moderate ($$), or expensive ($$$). Now we add an additional variable - popularity. This is again randomly generated on a 1 to 5 scale. But restaurants that are popular at lunch may not be popular at breakfast or dinner (they may cater to office workers, for example), and some restaurants may be closed for one or more of these meals - say, 30% at breakfast, 25% at lunch and 15% at dinner. Here, there are four separate steps: Create ‘breakfastdata’ as a copy of the fakerestodata Generate a popularity score as a random variable. Generate a second random variable called ‘meal’, which is initially set to a random integer between 1 and 100. Then, use the case_when syntax to recode this: if mean is below a cutoff (30 for Breakfast), set the value to NA (missing), otherwise call it ‘Breakfast.’ Then do the same for ‘lunchdata’ and for ‘dinnerdata.’ We’ll use different cutoffs here as we expect that more restaurants will be open for dinner, and fewer for breakfast. Then bind the lunchdata and the dinnerdata to the breakfastdata. Finally, remove the lines where the meal data is missing. We are calling this dataset ‘tallrestodata’ for reasons that will become apparent. # note that the same code runs three times # we should simplify this by creating a function breakfastdata &lt;- restodata |&gt; mutate (popularity = sample(1:5, 100, replace=T)) |&gt; mutate (meal = sample(1:100, 100, replace=F)) |&gt; mutate (meal = case_when (meal &lt; 31 ~ NA, TRUE ~ &quot;Breakfast&quot; )) lunchdata &lt;- restodata |&gt; mutate (popularity = sample(1:5, 100, replace=T)) |&gt; mutate (meal = sample(1:100, 100, replace=F)) |&gt; mutate (meal = case_when (meal &lt; 26 ~ NA, TRUE ~ &quot;Lunch&quot; )) dinnerdata &lt;- restodata |&gt; mutate (popularity = sample(1:5, 100, replace=T)) |&gt; mutate (meal = sample(1:100, 100, replace=F)) |&gt; mutate (meal = case_when (meal &lt; 16 ~ NA, TRUE ~ &quot;Dinner&quot; )) tallrestodata &lt;- lunchdata |&gt; bind_rows(dinnerdata, breakfastdata) |&gt; drop_na(meal) head(tallrestodata,3) ## # A tibble: 3 × 5 ## resto_name health cost popularity meal ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Michaels_place B $$$ 4 Lunch ## 2 Christophers_place A $$ 5 Lunch ## 3 Matthews_place C $$$ 5 Lunch 11.2.1 tall and wide formats Tallrestodata is tidy - each column is a variable, each row is an observation, and each cell includes a unique value. But it may not be exactly what we need. I might, for example, want to go to a popular restaurant when no one else is there - perhaps to go to breakfast at a restaurant that is popular only at lunch and dinner. One approach to this is to create a new variable in the data for the (average popularity at lunch and dinner) minus (popularity at breakfast). High scores on this would give us restaurants that might be really good or interesting, but not crowded, for a breakfast meal. But popularity at breakfast, lunch, and dinner are in different rows of the restodata; so this is a little tricky. The easiest way to do this is to use the pivot_wider function from the tidyr package. Again, the syntax is challenging - you may find the cheatsheet to be useful. This will reshape the data from tall and narrow to a format that is typically wider and shorter. From this wider data, we use mutate twice t ultimately create a new variable called secretBreakfastPlace. widerestodata &lt;- tallrestodata |&gt; pivot_wider(names_from = meal, names_prefix = &quot;popularity_&quot;, values_from = c(popularity)) |&gt; # we could create the secretBreakfast variable # in a single step, but this is probably more clear mutate (lunchDinnerPop = (popularity_Lunch + popularity_Dinner)/2) |&gt; mutate (secretBreakfastPlace = lunchDinnerPop - popularity_Breakfast) head(widerestodata) ## # A tibble: 6 × 8 ## resto_name health cost popularity_Lunch popularity_Dinner ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Michaels_place B $$$ 4 2 ## 2 Christophers_place A $$ 5 3 ## 3 Matthews_place C $$$ 5 4 ## 4 Joshuas_place A $$ 1 2 ## 5 Daniels_place C $ 4 3 ## 6 Davids_place F $ 1 4 ## # ℹ 3 more variables: popularity_Breakfast &lt;int&gt;, lunchDinnerPop &lt;dbl&gt;, ## # secretBreakfastPlace &lt;dbl&gt; Although the widerestodata has what we want, it is not tidy: The ‘popularity’ variable is now in three columns rather than one, and there are many cells with missing values. So we reshape the data back to its tall and tidy form, but which now includes the variable describing the two new mutated variables. Then we sort it by our desired variable, secretBreakfastPlace tallrestodata &lt;- widerestodata |&gt; pivot_longer(names_to = &#39;meal&#39;, names_prefix = &quot;popularity_&quot;, cols = c(&#39;popularity_Breakfast&#39;, &#39;popularity_Lunch&#39;, &#39;popularity_Dinner&#39;), values_to = (&#39;popularity&#39;), values_drop_na = TRUE) |&gt; arrange(desc(secretBreakfastPlace)) head(tallrestodata) ## # A tibble: 6 × 7 ## resto_name health cost lunchDinnerPop secretBreakfastPlace meal popularity ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 Jamess_place B $ 4.5 3.5 Brea… 1 ## 2 Jamess_place B $ 4.5 3.5 Lunch 5 ## 3 Jamess_place B $ 4.5 3.5 Dinn… 4 ## 4 Matthews_pl… C $$$ 4.5 2.5 Brea… 2 ## 5 Matthews_pl… C $$$ 4.5 2.5 Lunch 5 ## 6 Matthews_pl… C $$$ 4.5 2.5 Dinn… 4 11.2.2 exercises Study and understand the code. Ask and see if you can answer questions about each chunk of code. For example, there are 100 observations in each of the breakfast, lunch, and dinner datasets. How many are there in tall and wide restodata sets? Why? Expand on the code: Where you would really want to eat? Look at the data and think about it. Make it more realistic if you can. How would you actually decide? Come up with a decision rule that you might use for choosing a restaurant. This rule might include filters and/or simple algebraic expressions (such as ‘+’ or ‘-’). Express your decision rule using code, then select your best restaurant. You may also want to create new variables such as distance, ambiance, or type of cuisine. 11.3 more on tidy coding There are many sources for help. The help panel in R Studio is a start, but you may have better look on Google. Here are some suggestions: work with tidy data. Make each row an observation, and each column a variable. Complex data sets, such as samples of text, become much more manageable when reduced to simple arrays. think in tidy; talk the talk. For example, recognize that %&gt;% or |&gt; (the pipe) means then. Statements with pipes begin with data, may include queries (extract, combine, arrange), and finish with a command. search for tidyverse solutions. When you have a problem in your code, for example, “how do I compute the mean for different groups of a variable in R?,” do a Google search for R mean groups tidyverse, not just R mean groups. This will get you in the habit of working with tidy solutions where they can be found. look for new answers. Because R and the tidyverse are constantly evolving, consider looking at recent pages first. In your Google search bar, click on Tools -&gt; Past year). adhere to good coding style. Well-written code is reasonably parsimonious, readable, and easily debugged. There are a few style manuals for R, including one from Hadley, and this [Rchaeological Commentary] (https://cran.r-project.org/web/packages/rockchalk/vignettes/Rstyle.pdf). write functions. If you repeat a section of code, rewrite it as a function. (See the example above). annotate your work. Assume that you will come back to it at a later date, while working on a different project, and use portions of your current code. Your R markdown documents should be a log. When you run in to a significant, challenging problem, don’t delete your mistakes, but ## comment them out - as I have done in a few places above. library(gapminder) b &lt;- gapminder %&gt;% # when should you comment out an error # instead of deleting it? for me, I&#39;ll # comment out errors that took me a long time # to solve, and/or that I&#39;ll learn from. # Probably not here, in other words... # filter(lifeExp) &gt; 70 bad parens filter(lifeExp &gt; 70) Finally, maintain perspective. Your need to solve problems (how to analyze x, etc.) should not take a back seat to your desire to write the best code. There is almost always a better way to do things. Strive reasonably to accomplish this, but be prepared to kludge. knitr::opts_chunk$set(echo = TRUE) library(readxl) library(janitor) library(tidyverse) library(corrr) library(kableExtra) References Kondo, Marie. 2016. Spark Joy: An Illustrated Master Class on the Art of Organizing and Tidying up. Ten Speed Press. Peng, Roger. 2018. “Teaching r to New Users - from Tapply to the Tidyverse.” "],["finding-exploring-cleaning-and-combining-data.html", "12 finding, exploring, cleaning, and combining data 12.1 florida educational data 12.2 combining datasets 12.3 recap / on joining files", " 12 finding, exploring, cleaning, and combining data Data science, oddly enough, begins not with R… but with data. There is no shortage of datasets available to analyze, and each can give rise to a host of interesting analyses and insights. What do you want to study? Let’s begin by looking at schools 12.1 florida educational data Florida, like many states, makes data on school quality publicly available. Schools are assessed, in part, on student performance (scores on comprehensive tests in fields such as English Language Arts). Schools are also assessed on measures such as whether this performance has increased across years, the percent of students who graduate in four years, and the percentage of students who pass Advanced Placement and related exams. You can learn more about these measures at https://www.fldoe.org/core/fileparse.php/18534/urlt/SchoolGradesOverview24.pdf). The data itself are available in an Excel spreadsheet. Here’s a screenshot of the first few columns and rows of the file. It’s apparent that the file is a little messy. As we saw in a prior chapter, we would like the first row of the dataset to include the variable (column) names: here, there are three rows of header prior to this. Further, many of the variable names include spaces, minus signs, and the like. We will first download the data from the web (at this writing, you can find it at https://www.fldoe.org/file/18534/SchoolGrades24.xlsx). We will store it on our disk in a subdirectory of our project folder called ‘data.’ Slash: Windows or Mac? 12.1.1 a digression: Slash, Windows and the world. Once we download the data, we need to tell R where to find it. If we are working with an Rproject, we might keep everything - code, data, and output - in the same directory. But we often need or want to store data in a separate place, in which case you will need to specify a file path, which will include one or more slashes (and not the Guns N’ Roses guitarist kind). In most of the computing world, including Macs, filepaths are delineated by forward slashes (“/”). On a Windows machine, they instead include backwards slashes (“\\”). To further complicate matters, the backwards slash has a special significance as an ‘escape’ character - this means, as we will see briefly below as well as in the chapter on text analysis, that it tells the system to interpret the following character literally (for example, a comma is read as a comma) rather than symbolically (where a comma might be read as a separator between two objects). In any operating system, we can locate files using relative paths (starting in your project directory) or absolute ones (starting in your computer’s root directory; see below). Relative paths generally work better, as you can use the code on multiple machines. But if you can’t find your datafile, try the absolute path as a kludge. 12.1.2 getting data from our machine into R Ones we have specified the datapath, we can use the read_excel command, which is in the readxl library, which is part of the peripheral tidyverse (so you do not need to first install it on your computer). We’ll tell R to skip the first three lines of text. We’ll continue the pipe with a simple command from the janitor package - which you will need to load on to your machine. That command gets rid of spaces in variable names and replaces them with camelCase or, the default, snake_case. (To see why you want to do this try omitting this line from your code). # relative path with backward slashes replaced by forward ones datadir &lt;- &quot;data/&quot; # this is a relative path # absolute path with backward slashes made literal with escapes # datadir &lt;- &quot;C:\\\\Users\\\\me\\\\OneDrive\\\\GitRepos\\\\DSLA25\\\\data\\\\&quot; FloridaSchools &lt;- read_excel( # here, we could also just do # &quot;data/SchoolGrades24.xlsx&quot;), paste0(datadir, &quot;SchoolGrades24.xlsx&quot;), skip = 3) |&gt; clean_names() 12.1.3 which ones are “high schools”? There are a few ways that we could reduce this to just High Schools - one is to include only schools which report a graduation rate that is not a missing value; the other is to include just schools that are explicitly named “high school.” (We could also use both of these, or something else). Here, we will use the latter - filtering on schools which have “HIGH SCHOOL” in the school_name. We then reduce the columns to a handful of measures of interest. FloridaHighSchools &lt;- FloridaSchools |&gt; # drop_na(&#39;graduation_rate_2022_23&#39;) |&gt; filter(str_detect(school_name, &quot;HIGH SCHOOL&quot;)) |&gt; select (district_number, district_name, school_number, school_name, english_language_arts_achievement, mathematics_achievement, science_achievement, social_studies_achievement, graduation_rate_2022_23, grade_2024, percent_of_economically_disadvantaged_students) head(FloridaHighSchools) ## # A tibble: 6 × 11 ## district_number district_name school_number school_name english_language_art…¹ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 01 ALACHUA 0151 GAINESVILL… 54 ## 2 01 ALACHUA 0201 HAWTHORNE … 32 ## 3 01 ALACHUA 0261 NEWBERRY H… 53 ## 4 01 ALACHUA 0271 SANTA FE H… 56 ## 5 01 ALACHUA 0411 PROFESSION… 64 ## 6 01 ALACHUA 0421 EASTSIDE H… 47 ## # ℹ abbreviated name: ¹​english_language_arts_achievement ## # ℹ 6 more variables: mathematics_achievement &lt;dbl&gt;, science_achievement &lt;dbl&gt;, ## # social_studies_achievement &lt;dbl&gt;, graduation_rate_2022_23 &lt;dbl&gt;, ## # grade_2024 &lt;chr&gt;, percent_of_economically_disadvantaged_students &lt;dbl&gt; 12.1.4 can we compute district (county) means from these data? We can reduce the set of high schools to one line per district, with scores the simple means of all schools in the district. FloridaHighSchoolsbyDistict &lt;- FloridaHighSchools |&gt; # select(-grade, -school_name) |&gt; group_by(district_name) |&gt; summarise_if(is.numeric, mean, na.rm=TRUE) These average scores should be viewed with skepticism, because it treats small and large schools as equal. Consider graduation rates: If a district has just two schools, one with just 10 students (and graduates all of them), and a larger school with 990 students (but graduates only half - 495 - of them), we would get an estimated graduation rate of 75% ((1.0 + .5)/2). But actually, the district-wide graduation rate would be 50.5%. School enrollment data is needed to accurately estimate district effects from individual schools. 12.1.5 estimating school enrollments At this writing (March 2025), I can’t find a dataset on Florida HS enrollments that is free, recent, easy to pull down, and reasonably comprehensive. For our purposes, we can work with an estimate of this: There are measures of school size in football league data (https://fhsaa.com/news/2023/12/21/football-classifications-available-for-2024-25-2025-26.aspx). The names for schools there are formatted differently from our other dataset, so this will take a little work. We will first read in the data, then estimate enrollments based on the class of the school (Rural, 1R, 1A, 2A, 3A, 4A, 5A, 6A, 7A). Can you describe what is happening in each line of code in this section? Note the use of the escape character in this chunk EnrollmentsFromFHSAA &lt;- read_excel( paste0(datadir, &quot;Football_2024_26.xlsx&quot;), skip = 1) |&gt; clean_names() head(EnrollmentsFromFHSAA) ## # A tibble: 6 × 4 ## school_name class region district ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 All Saints&#39; (Winter Haven) Independent Independent Independent ## 2 Alonso (Tampa) 7A 2 7 ## 3 American (Hialeah) 4A 4 16 ## 4 American Heritage (Delray Beach) 2A 3 12 ## 5 American Heritage (Plantation) 4A 4 15 ## 6 Anclote (Holiday) 3A 3 9 EnrollmentsFromFHSAA &lt;- EnrollmentsFromFHSAA |&gt; separate(col = &quot;school_name&quot;, into = c(&quot;school_name&quot;, &quot;school_place&quot;), sep = &quot;\\\\(&quot;, extra = &quot;merge&quot;) |&gt; mutate (est_enrollment = case_when( class == &quot;Rural&quot; ~ mean(111,558), class == &quot;1R&quot; ~ mean(111,558), class == &quot;1A&quot; ~ mean(61,643), class == &quot;2A&quot; ~ mean(644,1166), class == &quot;3A&quot; ~ mean(1167,1542), class == &quot;4A&quot; ~ mean(1543,1822), class == &quot;5A&quot; ~ mean(1823,2135), class == &quot;6A&quot; ~ mean(2136,2512), class == &quot;7A&quot; ~ mean(2512,4627), TRUE ~ NA)) |&gt; mutate(school_name = toupper(school_name)) |&gt; mutate(school_place = str_remove(school_place,&quot;\\\\)&quot;)) ## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 192 rows [8, ## 18, 21, 22, 24, 26, 32, 33, 44, 46, 47, 49, 53, 57, 59, 60, 62, 67, ## 73, 81, ...]. When we run this chunk, we get a warning. What is it about? Is it ok? 12.2 combining datasets The enrollment data are now in the EnrollmentsFromFHSAA dataset. We first edit the school names from the FloridaHighSchools file to see if we can get them to match. Then we try to merge (left join) these with the FloridaHighSchools data, using the school_name variable as the key. FloridaHighSchools &lt;- FloridaHighSchools |&gt; mutate(school_name = str_replace(school_name, &quot;JUNIOR/SENIOR HIGH SCHOOL&quot;,&quot;&quot;)) |&gt; mutate(school_name = str_replace(school_name, &quot;MIDDLE/HIGH SCHOOL&quot;,&quot;&quot;)) |&gt; mutate(school_name = str_replace(school_name, &quot;SENIOR HIGH SCHOOL&quot;,&quot;&quot;)) |&gt; mutate(school_name = str_replace(school_name, &quot;HIGH SCHOOL&quot;,&quot;&quot;)) FloridaHighSchools2 &lt;- FloridaHighSchools |&gt; left_join(EnrollmentsFromFHSAA, by = &quot;school_name&quot;) ## Warning in left_join(FloridaHighSchools, EnrollmentsFromFHSAA, by = &quot;school_name&quot;): Detected an unexpected many-to-many relationship between `x` and `y`. ## ℹ Row 38 of `x` matches multiple rows in `y`. ## ℹ Row 187 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## &quot;many-to-many&quot;` to silence this warning. But when we do this, we get another warning message. Why is it there? 12.2.1 challenges in joining datasets In each of the two datasets, there are duplicate names - for example, there are two “Atlantic” High Schools in the both the Florida High School (FHS) and the Florida High School Athletic Association (FHSAA) datasets. We can see this by looking at the output of the following commands: SchoolsWithSameNamesInFHS &lt;- FloridaHighSchools |&gt; group_by(school_name) |&gt; filter(n() &gt; 1) |&gt; # select (school_name, district_name) |&gt; ungroup() |&gt; arrange(school_name) SchoolsWithSameNamesInEnrollments &lt;- EnrollmentsFromFHSAA |&gt; group_by(school_name) |&gt; filter(n() &gt; 1) |&gt; # select (school_name, school_place) |&gt; ungroup() |&gt; arrange(school_name) There are 12 schools (6 pairs) in the FHS data, and 26 schools (13 pairs) in the FHSAA data. Ten of these schools are in common across the two sets, the remainder are unique to one or the other. 12.2.2 some approaches to fixing the data In dealing with large datasets, it is fairly common that code will work correctly for a large majority of the cases, and that creativity is needed to efficiently fix the remainder. Here, there are at least four (non-mutually-exclusive) approaches that might work. In decreasing order of comprehensiveness: The first approach would be to dig deeper, and to find another source for the school enrollment data. The data we have are imperfect in at least three ways: Enrollment data are missing for many schools (i.e., “independent” schools). Many schools on this list do not show up in the FHS list The enrollment data we have from these schools are estimates, not actual counts The second approach would be to find a geographic dataset that includes towns and counties, then to use this as a key to join our two high school files. If we were working with a larger dataset, this would be worthwhile to try. The third approach would be to manually edit the datasets so that we could include the schools with multiple names. The fourth approach would be to run an initial analysis on the data we have, putting aside the duplicate schools. In the event that the results warrant closer analysis, we could then move to a more comprehensive solution. This is the place to begin: 12.2.2.1 the simplest approach In order to merge the data, we first need to make sure that there are no spaces or tabs in the “school_name” variable that will join the datasets. SchoolsWithUniqueNamesInFHS &lt;- FloridaHighSchools |&gt; group_by(school_name) |&gt; mutate(school_name = str_trim(school_name)) |&gt; filter(n() == 1) |&gt; ungroup() SchoolsWithUniqueNamesInEnrollments &lt;- EnrollmentsFromFHSAA |&gt; group_by(school_name) |&gt; mutate(school_name = str_trim(school_name)) |&gt; filter(n() == 1) |&gt; ungroup() FloridaHighSchools2 &lt;- SchoolsWithUniqueNamesInFHS |&gt; left_join(SchoolsWithUniqueNamesInEnrollments, by = &quot;school_name&quot;) We compute the estimated graduation rate for districts (adjGradRate) by weighing schools by their estimated enrollments as follows GradCountsRates &lt;- FloridaHighSchools2 |&gt; mutate(est_Ngrads = round(est_enrollment * .01 * graduation_rate_2022_23),0) |&gt; select(district_name, est_enrollment, est_Ngrads) |&gt; group_by(district_name) |&gt; summarise_if(is.numeric,sum, na.rm = TRUE) |&gt; mutate(adjGradRate = est_Ngrads / est_enrollment) 12.2.2.2 adding in the schools with duplicate names If we wanted to examine the high school enrollment and graduation data more closely, we can manually rename the duplicate school names. Here’s one approach: Begin by cleaning up the white space in the school name field (str_trim). Then, in the FHS data, rename schools by concatenating (str_c) school name and district name. Then manually edit the relevant cases in the FHSAA data (using mutate + case_when), and finally concatenating school and district name here as well. FixNamesInFHS &lt;- SchoolsWithSameNamesInFHS |&gt; mutate(school_name = str_trim(school_name)) |&gt; mutate(school_name = str_c(school_name, &quot;_&quot;,district_name)) FixNamesInFHSAA &lt;- SchoolsWithSameNamesInEnrollments |&gt; mutate(school_name = str_trim(school_name)) |&gt; mutate(district_name = case_when ( school_place == &quot;Delray Beach&quot; ~ &quot;PALM BEACH&quot;, school_place == &quot;Port Orange&quot; ~ &quot;VOLUSIA&quot;, school_place == &quot;Fort Myers&quot; ~ &quot;LEE&quot;, school_place == &quot;Kissimmee&quot; ~ &quot;OSCEOLA&quot;, school_place == &quot;Oakland Park&quot; ~ &quot;BROWARD&quot;, school_place == &quot;St. Petersburg&quot; ~ &quot;PINELLAS&quot;, school_place == &quot;Riverview&quot; ~ &quot;HILLSBOROUGH&quot;, school_place == &quot;Sarasota&quot; ~ &quot;SARASOTA&quot;, school_place == &quot;Sanford&quot; ~ &quot;PINELLAS&quot;, school_place == &quot;Seminole&quot; ~ &quot;SEMINOLE&quot;, TRUE ~ &quot;&quot;) ) |&gt; mutate(school_name = str_c(school_name, &quot;_&quot;,district_name)) |&gt; select(-district_name) then we add these data back in with the FHS and FHSAA data and rerun the analysis FHSdata &lt;- SchoolsWithUniqueNamesInFHS |&gt; bind_rows(FixNamesInFHS) FHSAAdata &lt;- SchoolsWithUniqueNamesInEnrollments |&gt; bind_rows (FixNamesInFHSAA) FloridaHighSchools3 &lt;- FHSdata |&gt; left_join(FHSAAdata, by = &quot;school_name&quot;) GradCountsRates &lt;- FloridaHighSchools3 |&gt; mutate(est_Ngrads = round(est_enrollment * .01 * graduation_rate_2022_23),0) |&gt; select(district_name, est_enrollment, est_Ngrads) |&gt; group_by(district_name) |&gt; summarise_if(is.numeric,sum, na.rm = TRUE) |&gt; mutate(adjGradRate = est_Ngrads / est_enrollment) 12.2.3 estimating the relationship between economic disadvantage and graduation rates We now have two approaches to estimating graduation rates at the level of school districts - the first is based on the simple average by schools, the second is based on the estimated enrollment data. How do these relate to each other and to other measures in the data such as economic disadvantage? Here, we join the two district-level files, rename our two graduation measures, and compute the correlations among the measures. We use the correlate function (in the corrr package), which makes the correlation matrix into a tibble, round the entries to two decimal places, and make into a table, which we format using the kable function from the kableExtra package. FloridaHighSchoolsbyDistict |&gt; left_join(GradCountsRates) |&gt; select(graduation_rate_2022_23, adjGradRate, percent_disadvantaged_students = percent_of_economically_disadvantaged_students) |&gt; rename(grad_rate_raw = graduation_rate_2022_23, grad_rate_weighted = adjGradRate) |&gt; correlate() |&gt; mutate_if(is.numeric, round, 2) |&gt; kable(table.attr = &quot;style=&#39;width:30%;&#39;&quot;) |&gt; column_spec(1:4, width = &quot;14em&quot;) |&gt; kable_styling(font_size = 12) ## Joining with `by = join_by(district_name)` ## Correlation computed with • Method: &#39;pearson&#39; • Missing treated ## using: &#39;pairwise.complete.obs&#39; term grad_rate_raw grad_rate_weighted percent_disadvantaged_students grad_rate_raw NA 0.88 -0.51 grad_rate_weighted 0.88 NA -0.38 percent_disadvantaged_students -0.51 -0.38 NA The two measures of graduation rates are similar but not identical (r = .88), and each is negatively associated with economic disadvantage (rs of -.38 and -.51). These correlations are high. 12.3 recap / on joining files In this chapter, we’ve examined Florida High School data and found an (expected) negative relationship between economic disadvantage and graduation rates. We also saw a picture of the guitarist from the hair band Guns ‘n’ Roses. We used a number of functions, such as select, filter, mutate, group_by, summarise, and case_when, that we also considered in the last chapter. We also used some new libraries, including readxl, corrr, and kableExtra, and their functions read_excel, correlate, and kable. The core of this chapter is to introduce some of the challenges of combining files, including both (a) the frequent need to wrangle datasets so that they can be correctly put together and (b) the use of the left_join function. The left_join function is one of many different ways of joining data. In our left_join, we kept all of the rows in the first dataset (x, FHSdata), and linked them to y (FHSAAdata) if and only if they have a match on the key variable (school_name). In a right join, we keep all of y, and rows in x if and only if they have a match in y. So we could also have written our code as FloridaHighSchools3 &lt;- FHSAAdata |&gt; right_join(FHSdata, by = “school_name”) There are also inner_joins (which can be thought of as the intersection of the two datasets, including only rows in each dataset which match) , and full_joins (unions, which keep all rows in both datasets). And anti_joins include only the rows in x which do not have a match in y. We will return to anti-joins in our discussion of text analysis. In combining datasets, joins are but one approach. We can also simply bind files together, without regard for a common key. Here, (and in the last chapter) we used bind_rows to add new observations to an existing dataset - this is typically used when two datasets have the same set of variables. Finally, bind_cols can be used to add new variables when two datasets have the same observations and in the same order. knitr::opts_chunk$set(echo = TRUE) library(tidyverse) "],["applied-data-science.html", "13 applied data science 13.1 public health and covid 13.2 other datasets in and beyond R", " 13 applied data science From the standpoint of applied data science, data analysis should have meaning, and should lead to consequential social or practical consequences. Here, we consider a few examples of applications of data science. 13.1 public health and covid In Winter 2020, as the COVID-19 pandemic began to take hold, I began chronicling the course of the virus. In a prior version of this book, I wrote the following: tracking the Novel Coronavirus (from Feb 2020) Here, I want to consider a timely (but challenging) dataset. The Novel Coronavirus is an emerging health crisis, particularly in Wuhan (a Chinese city larger than New York City) and the surrounding province of Hubei. It is not yet a threat in the United States - there have, at this writing (02/11/20) been zero cases in Florida. Still, tracking the spread of the virus - the unfolding number of people infected and recovered, as well as the number of deaths - is a fascinating exercise. I then went on to provide code for accessing data provided by the Johns Hopkins Center for Systems Science and Engineering (JHU/CSSE). The data was initially provided on a GitHub site, then moved to a dashboard here. You can learn more about the efforts of this team here. 13.1.1 COVID data in 2025 Today, there are a number of R packages intended to help analyze COVID data. The COVID19 package provides records of the outbreak on a global scale. Here’s some sample code: # install.packages(&quot;COVID19&quot;) library(COVID19) allCovid &lt;- covid19() ## We have invested a lot of time and effort in creating COVID-19 Data ## Hub, please cite the following when using it: ## ## Guidotti, E., Ardia, D., (2020), &quot;COVID-19 Data Hub&quot;, Journal of Open ## Source Software 5(51):2376, doi: 10.21105/joss.02376 ## ## The implementation details and the latest version of the data are ## described in: ## ## Guidotti, E., (2022), &quot;A worldwide epidemiological database for ## COVID-19 at fine-grained spatial resolution&quot;, Sci Data 9(1):112, doi: ## 10.1038/s41597-022-01245-1 ## To print citations in BibTeX format use: ## &gt; print(citation(&#39;COVID19&#39;), bibtex=TRUE) ## ## To hide this message use &#39;verbose = FALSE&#39;. CovidbyDate &lt;- allCovid |&gt; select(date,confirmed, deaths, recovered, people_vaccinated, place = administrative_area_level_1) |&gt; group_by(date, place) |&gt; summarise_if(is.numeric, sum, na.rm=TRUE) CovidbyDate |&gt; filter (date &lt; &quot;2023-01-01&quot;) |&gt; filter(place == &quot;United States&quot;) |&gt; summarize(confirmed = sum(confirmed)) %&gt;% ggplot(aes(x=date)) + geom_line(aes(y=confirmed)) A second package can be used to examine the impact of COVID more indirectly, but possibly more accurately, by looking at excess mortality data. 13.1.2 a brief digression on causality Human deaths, like most events, are multiply determined. In the case of COVID, many of those who contracted the disease suffered from other vulnerabilities including diabetes, obesity, pre-existing heart disease, and “old age.” They may have contracted pneumonia as a proximal cause in a pathway that might have included, for example, chronic cigarette smoking -&gt; emphysema -&gt; chronic obstructive pulmonary disease (COPD) -&gt; COVID -&gt; death. In these cases, like most cases, isolating an individual “cause” can be difficult if not arbitrary, Determinations as to the cause of death may be difficult to make, particularly in an environment in which political or economic considerations may be non-trivial. 13.1.3 the excess mortality package The R excess mortality package (excessmort) can be used to calculate the expected number of deaths in a region and time period. Among other things, it can be used to estimate the effects of the pandemic on mortality in individual states based on historical data rather than particular diagnoses. You can learn more about the package at https://cran.r-project.org/web/packages/excessmort/vignettes/excessmort.html. # install.packages(&quot;excessmort&quot;) library(excessmort) exclude_dates &lt;- c(seq(make_date(2017, 12, 16), make_date(2018, 1, 16), by = &quot;day&quot;), seq(make_date(2020, 1, 1), max(cdc_state_counts$date), by = &quot;day&quot;)) counts &lt;- cdc_state_counts %&gt;% filter(state == &quot;Florida&quot;) %&gt;% compute_expected(exclude = exclude_dates) ## Warning in compute_expected(., exclude = exclude_dates): Including a trend in ## the model is not recommended with less than five years of data. Consider ## setting include.trend = FALSE. ## No frequency provided, determined to be 52 measurements per year. ## Overall death rate is 10.5. expected_plot(counts, title = &quot;Expected (blue) and actual (grey) Weekly Mortality Counts in Florida&quot;) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## ℹ The deprecated feature was likely used in the excessmort package. ## Please report the issue to the authors. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning ## was generated. counts &lt;- cdc_state_counts %&gt;% filter(state == &quot;California&quot;) %&gt;% compute_expected(exclude = exclude_dates) ## Warning in compute_expected(., exclude = exclude_dates): Including a trend in ## the model is not recommended with less than five years of data. Consider ## setting include.trend = FALSE. ## No frequency provided, determined to be 52 measurements per year. ## Overall death rate is 7.33. expected_plot(counts, title = &quot;Expected (blue) and actual (grey) Weekly Mortality Counts in California&quot;) 13.2 other datasets in and beyond R The datasets library in R includes about 90 datasets of this writing. Many of these (e.g., iris, cars) are ubiquitous in R training; they are typically small and easy to work with. The Fivethirtyeight and fivethirtyeightdata packages include another 150 or so datasets on politics and popular culture. A set of 2500+ datasets which are in R packages may be found at https://vincentarelbundock.github.io/Rdatasets/datasets.html (you can find this in a sortable spreadsheet here). These range in scope from the small (“Death By Horse Kicks”, 5 rows and 2 columns) to the large “US Military Demographics”, 1.4 million rows and 6 columns. Please consider explorinfg this site. The openintro package includes several hundred datasets; they are described here Note also that the R fivethirtyeight library provides access to a number of clever, clean, and largely manageable datasets, each of which underlies the empirical analyses and reports of Nate Silver and his team (You can learn more at https://data.fivethirtyeight.com/). Kaggleis a noun (a community, a website, a challenge), and a verb (to kaggle is to participate in a data challenge) which describes a crowdsourced competition to improve on a problem in prediction. Perhaps the first and best known example of this was the Netflix prize (Jackson 2017), which, in 2006, promised one million dollars to the first team to improve the algorithm by which that company recommended movies to its customer base. The competition took several years, and inspired substantial improvements in machine learning as well as in crowdsourced science. At this writing, Kaggle hosts many active competitions - including some with prizes of one million dollars or more.” (Good luck!) Kaggle also hosts hundreds of thousands of datasets. A good place to start is to filter the datasets stored in comma separated value format (.csv) and a usability rating of 8 or more. Within psychology and behavioral science, the Open Science Framework (OSF) provides a system for hosting and sharing code and data from research articles. One OSF page is a compilation of many datasets from prominent papers in psychology and psychiatry: this now forwards to a spreadsheet which, though it does include data from a number of large and important studies, it appears insufficiently curated, with many dead links. Outside of psychology, repositories of data from many disciplines may be found at Re3data https://www.re3data.org/. There are many datasets about music - songs, artists, lyrics, etc. - at millionsongdataset. Note that many of these are quite large, but more accessible datasets are available, including here. Or just Google datasets. 13.2.1 make/extract/combine your own data Despite the petabytes (exabytes? zettabytes? yottabytes?) of data in the datasets described above, it’s possible that the dataset that you want to examine does not yet exist. But you may be able to create it, for example, by scraping data from the Web. Typically, you would use an Application Programming Interface (API) to pull data down from platforms such as Twitter or Reddit. For these and other major social media and news platforms, there are R packages which will walk you through the process of getting the data from webpage to tidy dataset. (Be aware, though, that the methods for data access on these platforms frequently changes, so that code that worked a year ago might not work today). Another source of data is … your own life. If you wear a pedometer or sleep tracker, are a calorie counter or keep other logs as a member of the quantified self movement, consider how such data might relate to aspects of the physical environment (such as temperature, or the time between sunrise and sunset) and/or the broader social and cultural context (a measure, perhaps of the sentiment, or mood, of news articles from papers like the NY Times). Finally, you might want to combine multiple datasets, such as county-level home pricing data from Zillow (https://www.zillow.com/research/data/), county-level elections data from, for example, here: https://github.com/tonmcg/US_County_Level_Election_Results_08-16, and the boundaries of Woodard’s 11 American Nations (see Lanning). In joining different datasets, or data from different sources, we can go beyond a pedagogical exercise (learning about learning) and contribute new and meaningful knowledge. 13.2.2 keep it manageable Proceed with caution - many of these datasets are likely to be quite large (for example, analyses of images) and/or in formats that for now are too challenging (JSON). I encourage you to stick with data that are available in a .csv format and that don’t have more than, say, a million data points (e.g., 50,000 observations * 20 variables). todo: move this chapter to just before text analysis; begins a new section on applied data science and computational social References Jackson, Dan. 2017. “The Netflix Prize: How a 1 Million Contest Changed Binge-Watching Forever.” Thrillist. Com. "],["strings-factors-dates-and-times.html", "14 strings, factors, dates, and times 14.1 strings 14.2 factors 14.3 dates 14.4 times", " 14 strings, factors, dates, and times This chapter discusses some of the types of data other than numeric and logical, in particular strings, factors, and dates/times. For the time being, please consider this as a supplement to R4DS, 2e, Chapter 14. 14.1 strings Strings are sets of characters which may include “123” as well as “why *DID* the chicken cross the road?” Samples of text, from names to novels, are the most interesting type of string. Among the tools that are used in examining texts are searches (do these tweets include language associated with hate speech?), validity checks (does the string correspond to a valid zip code?), and reformatting (to lower case so that BOB, Bob, and bob are all coded as identical). These ideas are simple, but quickly become challenging when, for example, the strings in which we are interested include characters that R usually interprets as code - such as commas, quotes, and slashes. See the section on string basics (14.2) for how to “escape” these characters, for example, how to treat a hashtag (#) as just a character as opposed to the beginning of a comment. These rules are codified as regular expressions (regex, sometimes regexp). Regex are not unique to R, but are shared with other languages as well. In R, particularly in the tidyverse package, regex are typically implicit, represented within commands that are part of the stringr package and that typically begin with str_. For example, str_detect returns a set of logical values: donuts &lt;- c(&quot;glazed&quot;, &quot;cakes&quot;, &quot;Pink sprinkled&quot;, &quot;cream filled&quot;, &quot;day-old frosted&quot;, &quot;chocolates&quot;) donuts %&gt;% str_detect(&quot; &quot;) ## [1] FALSE FALSE TRUE TRUE TRUE FALSE Most of the str_ functions are straightforward, but remember that str_sub provides a subset, not a substitution; to change a string, use str_replace. As Hadley points out in R4DS 2e, the autocomplete function in R_studio is very handy for helping you explore the different functions - in your console, type str_ … then scroll through the possibilities. donuts %&gt;% str_sub(1,5) ## [1] &quot;glaze&quot; &quot;cakes&quot; &quot;Pink &quot; &quot;cream&quot; &quot;day-o&quot; &quot;choco&quot; donuts %&gt;% str_replace(&quot; &quot;,&quot;_&quot;) ## [1] &quot;glazed&quot; &quot;cakes&quot; &quot;Pink_sprinkled&quot; &quot;cream_filled&quot; ## [5] &quot;day-old_frosted&quot; &quot;chocolates&quot; As you work with texts, simple problems sometimes require sophisticated codes. The regex that are used to solve these problems quickly become dense and challenging. One tool that can help you is the str_view command, which returns highlighted text showing corresponding passages. For example: slashMovieTitles &lt;- c(&quot;Face/off&quot;, &quot;8 1/2&quot;, &quot;F/X&quot;, &quot;Frost/Nixon&quot;, &quot;Victor/Victoria&quot;) slashMovieTitles %&gt;% str_view (&quot;/&quot;) ## [1] │ Face&lt;/&gt;off ## [2] │ 8 1&lt;/&gt;2 ## [3] │ F&lt;/&gt;X ## [4] │ Frost&lt;/&gt;Nixon ## [5] │ Victor&lt;/&gt;Victoria Regex statements are dense statements that allow us to work efficiently, for example, with special characters (like backslashes), repeated characters (zzz), and sets of characters [AEIOU], and characters at the beginning (^) or end ($) of strings. See R4DS 2e Chapter 15 for more. A particularly useful function in stringr is str_split, which can be used to quickly break a text into discrete words. Note that using a space and the explicit “word” boundary give different results. str_split(donuts, &quot; &quot;, simplify = TRUE) ## [,1] [,2] ## [1,] &quot;glazed&quot; &quot;&quot; ## [2,] &quot;cakes&quot; &quot;&quot; ## [3,] &quot;Pink&quot; &quot;sprinkled&quot; ## [4,] &quot;cream&quot; &quot;filled&quot; ## [5,] &quot;day-old&quot; &quot;frosted&quot; ## [6,] &quot;chocolates&quot; &quot;&quot; str_split(donuts, boundary (&quot;word&quot;), simplify = TRUE) ## [,1] [,2] [,3] ## [1,] &quot;glazed&quot; &quot;&quot; &quot;&quot; ## [2,] &quot;cakes&quot; &quot;&quot; &quot;&quot; ## [3,] &quot;Pink&quot; &quot;sprinkled&quot; &quot;&quot; ## [4,] &quot;cream&quot; &quot;filled&quot; &quot;&quot; ## [5,] &quot;day&quot; &quot;old&quot; &quot;frosted&quot; ## [6,] &quot;chocolates&quot; &quot;&quot; &quot;&quot; The output of str_split is generally a list (more on that soon), but here the lists are simplified into tibbles. In the tidyverse, str_split is typically one of the first steps in preparing text. The tidytext package (https://www.tidytextmining.com/), which is discussed at length in the computational social science course, builds on this foundation and is a powerful set of tools for all sorts of problems in formal text analysis. 14.2 factors Conditions (experimental vs control), categories (male or female), types (scorpio, “hates astrology”) and other nominal measures are categorical variables or factors. In the tidyverse, the r package for dealing with this type of measure is forcats, one of the core parts of the tidyverse. Here’s an example of a categorical variable. Why is it set up like this, and what does it do? # Example of a factor eyes &lt;- factor(x = c(&quot;blue&quot;, &quot;green&quot;, &quot;green&quot;, &quot;zombieRed&quot;), levels = c(&quot;blue&quot;, &quot;brown&quot;, &quot;green&quot;)) eyes ## [1] blue green green &lt;NA&gt; ## Levels: blue brown green In base R, string variables (“donut”, “anti-Brexit”, and “yellow”) are generally treated as factors by default. In the tidyverse, string variables are treated as strings until they are explicitly declared as factors. The syntax for working with factors-as-categories is given in Chapter 16 of R4DS 2e. I will not duplicate that here, but I will point out that factors are represented internally in R as numbers, and converting (coercing) factors to other data types can occasionally lead to nasty surprises. Sections 16.4 and 16.5 describe how factors can be cleanly reordered and modified. 14.2.1 types of babies In the babynames data, baby’s gender is a categorical variable, which is treated (because tidyverse) as a character or string. Here, we make it into a factor. We create two other factors as well. # adding third level for non-binary babies sexlevels &lt;- c(&quot;M&quot;, &quot;F&quot;, &quot;O&quot;) babynames2 &lt;- babynames %&gt;% mutate(sex = factor(sex, levels = sexlevels)) %&gt;% mutate(beginVowel = case_when( substr(name,1,1) %in% c(&quot;A&quot;,&quot;E&quot;,&quot;I&quot;,&quot;O&quot;,&quot;U&quot;) ~ &quot;Vowel&quot;, TRUE ~ &quot;Consonant&quot;)) %&gt;% mutate(beginVowel = factor(beginVowel)) %&gt;% mutate (century = case_when( year &lt; 1900 ~ &quot;19th&quot;, year &lt; 2000 ~ &quot;20th&quot;, year &gt; 1999 ~ &quot;21st&quot;)) %&gt;% mutate(century = factor(century)) Use the syntax above to create types of names for different generations (boomers, gen x, Millenials, gen z). Use https://www.kasasa.com/articles/generations/gen-x-gen-y-gen-z to determine your groupings. Say something interesting about the data - names, genders, etc. Plot this. 14.2.2 types of grown-ups If you would instead like to examine survey data, the forcats package includes a set of categorical variables. Using the discussion in Chapter 15 of R4DS as your guide, examine the relationship between two or more of these categorical variables. Again, plot these gss_cat ## # A tibble: 21,483 × 9 ## year marital age race rincome partyid relig denom tvhours ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 2000 Never married 26 White $8000 to 9999 Ind,near … Prot… Sout… 12 ## 2 2000 Divorced 48 White $8000 to 9999 Not str r… Prot… Bapt… NA ## 3 2000 Widowed 67 White Not applicable Independe… Prot… No d… 2 ## 4 2000 Never married 39 White Not applicable Ind,near … Orth… Not … 4 ## 5 2000 Divorced 25 White Not applicable Not str d… None Not … 1 ## 6 2000 Married 25 White $20000 - 24999 Strong de… Prot… Sout… NA ## 7 2000 Never married 36 White $25000 or more Not str r… Chri… Not … 3 ## 8 2000 Divorced 44 White $7000 to 7999 Ind,near … Prot… Luth… NA ## 9 2000 Married 44 White $25000 or more Not str d… Prot… Other 0 ## 10 2000 Married 47 White $25000 or more Strong re… Prot… Sout… 3 ## # ℹ 21,473 more rows 14.3 dates The challenges of combining time-demarcated data (Chapter 17 of R4DS 2e) are significant. For dates, a variety of different formats (3-April, October 23, 1943, 10/12/92) must be made sense of. Sometimes we are concerned with durations (how many days, etc.); on other occasions, we are concerned with characteristics of particular dates (as in figuring out the day of the week on which you were born). And don’t forget about leap years. In R, the lubridate package (a non-core part of the tidyverse, i.e., one that you must load separately) helps to handle dates and times smoothly. It anticipates many of the problems we might encounter in extracting date and time information from strings. Lubridate generally works well to simplify files with dates and times, and can be used to help in data munging. For example, in my analyses of the Corona data, dates and times were reported in four different ways. The code below decodes these transparently and combines them into a common date/time format . 2/3/20 6 PM 2/3/20 18:00 2/3/20 18:00:00 2020-02-03 18:00:00 # not run #coronaData2 &lt;- coronaData %&gt;% mutate # (`FixedDate = # parse_date_time(`Last Update`, # c(&#39;mdy hp&#39;,&#39;mdy HM&#39;, # &#39;mdy HMS&#39;,&#39;ymd HMS&#39;))) 14.4 times Working with temporal data is often challenging. The existence of, for example, 12 versus 24 hour clocks, time zones, and daylight savings, can make a simple question about duration quite challenging. Imagine that Fred was born in Singapore at the exact moment of Y2K. He now lives in NYC. How many hours has he been alive as of right now? How would you solve this? # find timezones for Singapore and NYC # a = get datetime for Y2K in Singapore in UTC # b = get datetime for now in NYC in UTC # compute difference and express in sensible metric "],["lists.html", "15 lists", " 15 lists Up until now, we have we have thought about ‘data structures’ as matrices (rectangles, two-dimensional arrays), in which columns typically correspond to variables and rows to observations, and in which each variable has a particular type, such as numeric or character (or, in the last chapter, special types such as factors and dates/times). In base R, data matrices are typically represented as data frames (type = df). In the Tidyverse, we have been using a special type of data frame, the tibble (type = df and tbl_df). The diamonds dataset, for example, is a tibble: babynames2 &lt;- babynames %&gt;% mutate(generation = case_when( (1944 &lt;= year) &amp; (year &lt;= 1964) ~ &quot;boomer&quot;, (1965 &lt;= year) &amp; (year &lt;= 1979) ~ &quot;genX&quot;, (1980 &lt;= year) &amp; (year &lt;= 1994) ~ &quot;millenial&quot;, (1995 &lt;= year) &amp; (year &lt;= 2019) ~ &quot;genZ&quot;)) %&gt;% mutate(generation = factor(generation)) str(babynames2$generation) ## Factor w/ 4 levels &quot;boomer&quot;,&quot;genX&quot;,..: NA NA NA NA NA NA NA NA NA NA ... babynames3 &lt;- babynames2 %&gt;% # count(generation) %&gt;% na.omit(generation) Within the diamonds tibble, we can examine the types of each variable. This uses the sapply function to SIMPLY APPLY a function (class) to the columns of a data frame or tibble. Here, each column (such as $carat) is a vector, and each vector is homogeneous (of one particular type): diamonds %&gt;% sapply(class) ## $carat ## [1] &quot;numeric&quot; ## ## $cut ## [1] &quot;ordered&quot; &quot;factor&quot; ## ## $color ## [1] &quot;ordered&quot; &quot;factor&quot; ## ## $clarity ## [1] &quot;ordered&quot; &quot;factor&quot; ## ## $depth ## [1] &quot;numeric&quot; ## ## $table ## [1] &quot;numeric&quot; ## ## $price ## [1] &quot;integer&quot; ## ## $x ## [1] &quot;numeric&quot; ## ## $y ## [1] &quot;numeric&quot; ## ## $z ## [1] &quot;numeric&quot; Beyond these atomic vectors, data can take more complex forms, such as hierarchical or tree-like structures such as the following. Wilkes Honors College Courses ├───Area: Psychology ├───Name: Personality and Social Development └───Term: Spring 2025 └───Instructors: Lanning └───Students: └───Al └───Year: Freshman └───Concentration: Psychology └───Barb └───etc. ├───Name: Political Psychology └───Term: Fall 2020 └───Instructors: Lanning ├───etc. Nested data sets such as these are common across the Internet. They describe the structure of the webpage you are looking at (which you can see, depending upon your browser, by clicking on something like ‘developer tools’). Data formats for representing nested structures include XML (Extensible Markup Language) and JSON (Java Script Object Notation). Many datasets of interest, such as this set of ratings of 10,000 books on Goodreads are structured as XML as well. In R, XML and JSON files will (typically after some massaging), be represented as lists. Lists are recursive, that is, they may include other lists. In addition to external data sources, the results of many procedures within R may also be represented as lists. Consider the following code. What does it do? What is in ‘mod’? Why is it stored like this? mod &lt;- lm(price ~ carat, data = diamonds) In R studio, you can inspect the structure of the list by clicking on it in the global environment window, by using the View tab, or with the command str(mod). You can extract rows of your list by including them in single brackets (which will return another list), or double brackets (which will return a vector or data frame). Compare the structure of the following data sets: b1 &lt;- mod[&#39;coefficients&#39;] b2 &lt;- mod[[&#39;coefficients&#39;]] c1 &lt;- mod[&#39;model&#39;] c2 &lt;- mod[[&#39;model&#39;]] Lists are, in a sense, containers. A single bracket gives us the wrapper as well as what is inside; the double bracket extracts only the inner element. Lists can be challenging, but they are necessary in a world where data is complexly structured. The R package purrr, a core part of the tidyverse, includes functions which simplify working with lists; to learn more, there is a tutorial here, and an overview of the package (with a cheatsheet) here. Lists are also discussed in Chapter 23 of R4DS 2e. todo: additional iscussion of purrr; review "],["loops-functions-and-beyond.html", "16 loops, functions, and beyond 16.1 loops 16.2 from loop to apply to purrr::map 16.3 some examples of functions 16.4 how many bottles of what?", " 16 loops, functions, and beyond In one of the most important contemporary theoretical models of intelligence, (Robert J. Sternberg 1999) has argued that the ability to automatize, that is, to work efficiently on repeated or habitual tasks, is a key component of intelligent behavior. Solving habitual problems efficiently - whether it is making a cup of coffee or finding the shortest path to complete a shopping list in a supermarket or a series of errands across town - allows us to focus our limited resources on other challenging tasks that, in turn, may determine whether we survive, or at least prosper. In programming, loops and functions are essential tools for making repetitive tasks simple. Simplifying your code is one of the more intellectually satisfying aspects of working in R or in any programming language. In R, loops are supplemented by additional tools for simplifying and avoiding repetition in code, including the ‘apply’ family in Base R and the map function in the tidyverse. Functions (and, beyond this, custom libraries) can further streamline your work. 16.1 loops Consider the task of printing out a series of numbers. Here’s a simple example of how this could be done in a loop in Base R. It prints numbers between 1995 and 1998 (inclusive). for (i in 1995:1998) { # i is an index code print(i) # print the ith value in the sequence } # go to the next one until the range is complete ## [1] 1995 ## [1] 1996 ## [1] 1997 ## [1] 1998 Let’s expand on this a little, connecting back to the babynames data. This will print counts of the number of babies for each year in GenerationZ, which includes years from 1995-2015. genZ &lt;- (1995:2015) # this reduces the (big) babynames to a simple file of years and counts babyCounts &lt;- babynames %&gt;% group_by(year) %&gt;% filter(year %in% genZ) %&gt;% summarize(nbabies = sum(n)) # and this uses a for loop to print each row in turn # for (i in (1:nrow(babyCounts))) { for (i in seq_along(nrow(babyCounts))) { # filename[i,j] == ith row and jth column print(paste((babyCounts[i,1]), babyCounts[i,2])) } ## [1] &quot;1995 3661351&quot; The syntax of loops, including where to put parentheses in index statements, can be tricky. Expect to refer to Google and StackExchange often in order to get your code running. Another good source is Chapter 26 of R4DS (2e). There, Wickham goes in to additional detail, including a description of seq_along(df) as a tool for creating an index corresponding to 1:ncol(df). Here’s an example with the diamonds dataset: diamonds2 &lt;- diamonds %&gt;% select_if(., is.numeric) # for (i in (1:ncol(diamonds2))) { for (i in seq_along(diamonds2)) { print(paste(names(diamonds2[i]), round(mean(diamonds2[[i]]),2))) } ## [1] &quot;carat 0.8&quot; ## [1] &quot;depth 61.75&quot; ## [1] &quot;table 57.46&quot; ## [1] &quot;price 3932.8&quot; ## [1] &quot;x 5.73&quot; ## [1] &quot;y 5.73&quot; ## [1] &quot;z 3.54&quot; Chapter 26 of R4DS also considers some extensions to related problems such as loops of indefinite length, which can often be addressed using the ‘while’ command. 16.2 from loop to apply to purrr::map Understanding for loops is fundamental in programming, but in R they should often be sidestepped. If the order of iteration isn’t important (if, for example, it doesn’t matter which of the diamonds variables we take the mean of first), then using one of the measures from the apply family can generally be used to make your code simpler and more efficient. The logic is that one takes a dataframe (df or tibble), then applies a function to its rows or columns: # apply = apply the function (mean) to the columns(2) of the df diamonds2 %&gt;% apply(2,mean) ## carat depth table price x y ## 0.7979397 61.7494049 57.4571839 3932.7997219 5.7311572 5.7345260 ## z ## 3.5387338 # sapply = simply apply - guesses that you are looking for col. means diamonds2 %&gt;% sapply(mean) ## carat depth table price x y ## 0.7979397 61.7494049 57.4571839 3932.7997219 5.7311572 5.7345260 ## z ## 3.5387338 The many variants of the apply family, including lapply [list apply] and tapply [table apply] as well as sapply, each have their own uses and can be quite efficient but, again, can be syntactically challenging. In the evolving tidyverse, the map family of commands is supplementing if not supplanting apply; these commands (part of the purrr package in the core tidyverse) may prove to be more convenient and clear. For example, the map_df function will apply a function and return a dataframe (tibble), which can be handy for further analysis. diamonds2 %&gt;% map_df(mean) ## # A tibble: 1 × 7 ## carat depth table price x y z ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.798 61.7 57.5 3933. 5.73 5.73 3.54 16.3 some examples of functions If you repeat a series of lines of code several times in your program, it is often best to wrap this into a function. The first example is from a preregistered study I recently started of language and politics. For the preregistration, I ran analyses using simulated data, both to increase the likelihood that the code will run without error on real data and to help anticipate the analyses which are to be run on ‘real’ data. I began by getting a real body of text from the net and scrambling it, then constructing fake ‘Republican’ and ‘Democratic’ texts from this. Here, I illustrate this by constructing 50 sample documents, each consisting of between 5 and 20 words. The project is given in four steps: 16.3.1 preliminaries Here is the preliminary stuff, where I pull the data off the net and initialize the variables sampledata &lt;- read_csv(&quot;data/sentiment-words-DFE-785960.csv&quot;)#url( # &quot;https://www.crowdflower.com/wp-content/uploads/2016/03/sentiment-words-DFE-785960.csv&quot;)) #&quot;https://raw.githubusercontent.com/totalgood/hope/master/data/corpora/sentiment-words-DFE-785960.csv&quot;)) # pulls off four words sampledata &lt;- sampledata[22:25] %&gt;% na.omit() ndocs &lt;- 50 minDocLength &lt;- 5 maxDocLength &lt;- 20 doc &lt;- vector(mode = &quot;character&quot;, length = ndocs) 16.3.2 the function Here’s the simple function which pulls a random word out of the matrix of sampledata. set.seed(33458) # a random seed is used to allow reproducible results getword &lt;- function() { rowid &lt;- sample(1:nrow(sampledata), 1) colid &lt;- sample(1:ncol(sampledata), 1) sampledata[rowid,colid] } 16.3.3 applying the function The function is applied, first, to extract one word, then, in successive loops, to build up one phrase and then many. # combine words into docs # establish length of first phrase docLength &lt;- sample(minDocLength:maxDocLength,1) # initialize with one word sampleCorpus &lt;- getword() # loop to build up first phrase for (i in 1:docLength) { addWord &lt;- getword() sampleCorpus &lt;- paste(sampleCorpus, addWord) } #add additional simulated documents for (j in 2:ndocs) { docLength &lt;- sample(minDocLength:maxDocLength,1) newdoc &lt;- getword() for (i in 1:docLength - 1) { addWord &lt;- getword() newdoc &lt;- paste(newdoc, addWord) } sampleCorpus &lt;- rbind(newdoc,sampleCorpus) } Finally, the results are combined with a vector of alternating labels of ‘Dem’ and ‘Rep’: row.names(sampleCorpus) &lt;- NULL evenOdd &lt;- rep(c(&quot;Dem&quot;,&quot;Rep&quot;),length.out = nrow(sampleCorpus)) workingCorpus &lt;- as_tibble(cbind(evenOdd,sampleCorpus)) head(workingCorpus,5) ## # A tibble: 5 × 2 ## evenOdd V2 ## &lt;chr&gt; &lt;chr&gt; ## 1 Dem wild at heart nobody loves me lost art miss the sun need a doctor kin… ## 2 Rep great player awfully nice back hurts not paying pretty rubbish found … ## 3 Dem yahooo make no promises horrible daughter ew crazy evening fit ## 4 Rep can&#39;t sleep lazy afternoon sucks #fedup great shame positive break fa… ## 5 Dem problems nice funniest hurt not necessarily sad nose eat no problem d… 16.4 how many bottles of what? To put the fun back into function, here’s a solution to the “99 bottles of beer” function described in r4DS 21.2.1. Study the code. Ask or answer a question about it in class or on Slack. beerSong &lt;- function(liquid = &quot;beer&quot;, count = 99, surface = &quot;wall&quot;) { songtext &lt;- &quot;&quot; for (i in (count:1)) { thisLine = (paste0(i, &quot; bottles of &quot;, liquid, &quot; on the &quot;, surface, &quot;, you take one down and pass it around,\\n&quot;)) songtext = c(songtext, thisLine) } songtext = c(songtext, (paste0(&quot;no more bottles of &quot;, liquid,&quot; on the &quot;, surface,&quot;...&quot;))) cat(songtext) # cat prints without line numbers } #beerSong() And here are solutions proposed by some of your classmates over the last few years. How do they differ from each other, and from the solution given above? beersng &lt;- function(n) { if (n == 1) { cat(&quot;\\n&quot;,n,&quot; bottle of beer on the wall, &quot;,n, &quot; bottles of beer.\\nTake one down and pass it around,&quot;, &quot; no more bottles of beer on the wall.\\n&quot;, sep = &quot;&quot;) cat(&quot;\\nNo more bottles of beer on the wall, no more bottles of beer.\\n&quot;, &quot;Go to the store and buy some more, 99 bottles of beer on the wall.\\n&quot;, sep=&quot;&quot;) } else { cat(&quot;\\n&quot;,n,&quot; bottles of beer on the wall, &quot;,n, &quot; bottles of beer.\\nTake one down and pass it around, &quot;, n-1, &quot; more bottles of beer on the wall.\\n&quot;, sep=&quot;&quot;) return(beersng(n-1)) } } #beersng(99) moreBeer &lt;- function () { for (i in 0:100){ starting_number &lt;- 100 if (starting_number - i == 0) { print(&quot;No more bottles of beer on the wall, no more bottles of beer, Go to the store and buy some more, 99 bottles of beer on the wall.&quot;) break } print(paste(starting_number - i, &quot;bottles of beer on the wall&quot;, &quot;Take one down and pass it around,&quot;, starting_number - i - 1, &quot;bottles of beer on the wall.&quot;)) } } # moreBeer() song &lt;- function(bottlesofbeer){ for(i in bottlesofbeer:1){ cat(bottlesofbeer,&quot; bottles of beer on the wall \\n&quot;, bottlesofbeer,&quot; bottles of beer \\nTake one down, pass it around \\n&quot;, bottlesofbeer-1, &quot; bottles of beer on the wall \\n&quot;,&quot; \\n&quot;) bottlesofbeer = bottlesofbeer - 1 } } #song(99) Which code is ‘best’? Good code is clear, but it is also efficient. We probably shouldn’t expect much in the way of differences between these functions in terms of speed (as each includes 99 iterations of a simple print command), but here’s a simple way to compare. Note that I’ve used the “sink” command to write my output to files rather than consoles: start_time &lt;- Sys.time() sink (file = &quot;f1.txt&quot;) beerSong(&quot;beer&quot;,99) sink() end_time &lt;- Sys.time() beerSongtime &lt;- end_time - start_time start_time &lt;- Sys.time() sink (file = &quot;f2.txt&quot;) beersng(99) sink() end_time &lt;- Sys.time() beersngtime &lt;- end_time - start_time start_time &lt;- Sys.time() sink (file = &quot;f3.txt&quot;) song(99) sink() end_time &lt;- Sys.time() songtime &lt;- end_time - start_time start_time &lt;- Sys.time() sink (file = &quot;f4.txt&quot;) moreBeer() sink() end_time &lt;- Sys.time() moreBeertime &lt;- end_time - start_time Here is the first line of output from each function, together with table showing the elapsed times: readLines(&quot;f1.txt&quot;,1) ## [1] &quot; 99 bottles of beer on the wall, you take one down and pass it around,&quot; readLines(&quot;f2.txt&quot;,2) ## [1] &quot;&quot; ## [2] &quot;99 bottles of beer on the wall, 99 bottles of beer.&quot; readLines(&quot;f3.txt&quot;,4) ## [1] &quot;99 bottles of beer on the wall &quot; &quot; 99 bottles of beer &quot; ## [3] &quot;Take one down, &quot; &quot; pass it around &quot; readLines(&quot;f4.txt&quot;,1) ## [1] &quot;[1] \\&quot;100 bottles of beer on the wall Take one down and pass it around, 99 bottles of beer on the wall.\\&quot;&quot; tibble(functionName = c(&quot;beerSongtime&quot;, &quot;beersngtime&quot;, &quot;songtime&quot;, &quot;moreBeertime&quot;), time = c(beerSongtime,beersngtime, songtime, moreBeertime)) %&gt;% kable(digits = 2) functionName time beerSongtime 0.02 secs beersngtime 0.01 secs songtime 0.01 secs moreBeertime 0.02 secs file.remove(c(&quot;f1.txt&quot;, &quot;f2.txt&quot;, &quot;f3.txt&quot;, &quot;f4.txt&quot;), echo = FALSE) ## [1] TRUE TRUE TRUE TRUE FALSE References Sternberg, Robert J. 1999. “The Theory of Successful Intelligence.” Review of General Psychology 3 (4): 292–316. https://doi.org/10/cqrkxh. "],["from-correlation-to-multiple-regression.html", "17 from correlation to multiple regression 17.1 bivariate analysis: Galton’s height data 17.2 multivariate data", " 17 from correlation to multiple regression In the previous chapters, we have learned how to summarize and visualize data. We have seen that we can summarize data using descriptive statistics and visualize data using plots. We can distinguish between analyses of just one variable (the univariate case), two variables (bivariate), and multivariate (many variables). 17.1 bivariate analysis: Galton’s height data (Note that this section is excerpted directly from From https://github.com/datasciencelabs). Francis Galton, a polymath and cousin of Charles Darwin, is one of the fathers of modern statistics. Galton liked to count - his motto is said to have been “whenever you can, count”. He collected data on the heights of families in England, and he found that there was a strong correlation between the heights of fathers and their sons. We have access to Galton’s family height data through the HistData package. We will create a dataset with the heights of fathers and the first son of each family. Here are the key univariate statistics for the two variables of father and son height, each taken alone: data(&quot;GaltonFamilies&quot;) galton_heights &lt;- GaltonFamilies %&gt;% filter(childNum == 1 &amp; gender == &quot;male&quot;) %&gt;% select(father, childHeight) %&gt;% rename(son = childHeight) galton_heights %&gt;% summarise(mean(father), sd(father), mean(son), sd(son)) ## mean(father) sd(father) mean(son) sd(son) ## 1 69.09888 2.546555 70.45475 2.557061 This univariate description fails to capture the key characteristic of the data, namely, the idea that there is a relationship between the two variables. To summarize this relationship, we can compute the correlation between the two variables. galton_heights %&gt;% summarize(cor(father, son)) %&gt;% round(3) ## cor(father, son) ## 1 0.501 In these data, the correlation (r) is about .50. (This means that for every standard deviation increase in the father’s height, we expect the son’s height to increase by about half a standard deviation). Incidentally, if we want to save correlations as a matrix, we can use the correlate() function from the corrr package. This function computes the correlation matrix for all pairs of variables in a data frame, which can be easily saved and formatted as a table. The fashion() function can be used to easily clean up the output. (And the parentheses around the whole statement allows us to print out the result to the console / RMarkdown document, as well aas saving rmatrix in our environment). (rmatrix &lt;- galton_heights %&gt;% select(father, son) %&gt;% correlate() %&gt;% fashion(decimals = 3)) ## term father son ## 1 father .501 ## 2 son .501 17.1.1 correlations based on small samples are unstable: A Monte Carlo demonstration Correlations based on small samples can bounce around quite a bit. Consider what happens when, for example, we sample just 25 cases from Galton’s data, and compute the correlation within this sample. Note that I begin by setting a seed for the random sequence. I repeat this 1000 times, then plot the distribution of these sample rs: set.seed(33458) # why do I do this? nTrials &lt;- 1000 nPerTrial &lt;- 25 replications &lt;- replicate(nTrials, { sample_n(galton_heights, nPerTrial, replace = TRUE) %&gt;% # we sample with replacement here summarize(r=cor(father, son)) %&gt;% .$r }) replications %&gt;% as_tibble() %&gt;% ggplot(aes(replications)) + geom_histogram(binwidth = 0.05, col = &quot;blue&quot;, fill = &quot;cyan&quot;) These sample correlations range from -0.002 to 0.882. Their average, however, at 0.503 is almost exactly that of the overall population. Often in data science, we will estimate population parameters in this way - by repeated sampling, and by studying the extent to which results are consistent across samples. More on that later. 17.1.2 from correlation to regression In bivariate analysis, there is often an asymmetry between the two variables - one is often considered the predictor (or independent variable, typically x) and the other the response (or dependent variable, y). In these data, we are likely to consider the father’s height as the predictor and the son’s height as the response. As noted above, one way of thinking about a correlation between variables like heights of fathers (x) and sons (y), is that for every one standard deviation increase in x (father’s height), we expect the son’s height to increase by about \\(r\\) times the standard deviation of y (the son’s height). We can compute all of these things manually and plot the points with a regression line. (We use the pull function to extract the values from the statistics from tibbles into single values). mu_x &lt;- galton_heights |&gt; summarise(mean(father)) |&gt; pull() mu_y &lt;- galton_heights |&gt; summarise(mean(son)) |&gt; pull() s_x &lt;- galton_heights |&gt; summarise(sd(father)) |&gt; pull() s_y &lt;- galton_heights |&gt; summarise(sd(son)) |&gt; pull() r &lt;- galton_heights %&gt;% summarize(cor(father, son)) |&gt; pull () m &lt;- r * s_y / s_x b &lt;- mu_y - m*mu_x galton_heights %&gt;% ggplot(aes(father, son)) + geom_point(alpha = 0.5) + geom_abline(intercept = b, slope = m, col = &quot;blue&quot;) Finally, if we first standardize the variables, then the regression line has intercept 0 and slope equal to the correlation \\(\\rho\\). Here, the slope, regression line, and correlation are all equal (I’ve made the plot square to better indicate this). galton_heights %&gt;% ggplot(aes(scale(father), scale(son))) + geom_point(alpha = 0.5) + geom_abline(intercept = 0, slope = r, col = &quot;blue&quot;) 17.2 multivariate data For the Galton data, we examined the relationship between two variables - one a predictor (father’s height) and the other a response (son’s height). In this section (drawn from Peng, Caffo, and Leek’s treatment from Coursera - the Johns Hopkins Data Science Program), we will extend our analysis to consider multiple predictors of a single response or outcome variable. You may need to install the packages “UsingR”, “GGally” and/or”Hmisc”. We begin with a second dataset. You can learn about it by typing ?swiss in the console. data(swiss) str(swiss) ## &#39;data.frame&#39;: 47 obs. of 6 variables: ## $ Fertility : num 80.2 83.1 92.5 85.8 76.9 76.1 83.8 92.4 82.4 82.9 ... ## $ Agriculture : num 17 45.1 39.7 36.5 43.5 35.3 70.2 67.8 53.3 45.2 ... ## $ Examination : int 15 6 5 12 17 9 16 14 12 16 ... ## $ Education : int 12 9 5 7 15 7 7 8 7 13 ... ## $ Catholic : num 9.96 84.84 93.4 33.77 5.16 ... ## $ Infant.Mortality: num 22.2 22.2 20.2 20.3 20.6 26.6 23.6 24.9 21 24.4 ... Here’s a scatterplot matrix of the Swiss data. Look at the first column of plots (or first row of the correlations). What is the relationship between fertility and each of the other variables? # ds_theme_set() set.seed(0) ggpairs (swiss, lower = list( continuous = &quot;smooth&quot;), axisLabels =&quot;none&quot;, switch = &#39;both&#39;) Here, we predict fertility from all of the remaining variables together in a single regression analysis, using the lm (linear model) command. Note that the result of this analysis is a list. We can pull out the key features of the data using the summary() command. How do you interpret this? swissReg &lt;- lm(Fertility ~ ., data=swiss) summary(swissReg) ## ## Call: ## lm(formula = Fertility ~ ., data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.2743 -5.2617 0.5032 4.1198 15.3213 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66.91518 10.70604 6.250 1.91e-07 *** ## Agriculture -0.17211 0.07030 -2.448 0.01873 * ## Examination -0.25801 0.25388 -1.016 0.31546 ## Education -0.87094 0.18303 -4.758 2.43e-05 *** ## Catholic 0.10412 0.03526 2.953 0.00519 ** ## Infant.Mortality 1.07705 0.38172 2.822 0.00734 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.165 on 41 degrees of freedom ## Multiple R-squared: 0.7067, Adjusted R-squared: 0.671 ## F-statistic: 19.76 on 5 and 41 DF, p-value: 5.594e-10 Regression is a powerful tool for understanding the relationship between a response variable and one or more predictor variables. We can use it where our variables are not normally distributed, as in the case of dichotomous variables (yes/no, true/false), as well as counts, which often are skewed. Here’s a second dataset, the marital affairs data, which is also included in the Ecdat package. We’ll apply ggpairs here, but for clarity will show only half of the data at a time. The dependent variable of interest (nbaffairs) will be included in both plots: Fair1 &lt;- Fair %&gt;% select(sex:child, nbaffairs) ggpairs(Fair1, # if you wanted to jam all 9 vars onto one page you could do this # upper = list(continuous = wrap(ggally_cor, size = 10)), lower = list(continuous = &#39;smooth&#39;), axisLabels = &quot;none&quot;, switch = &#39;both&#39;) Fair2 &lt;- Fair %&gt;% select(religious:nbaffairs) ggpairs(Fair2, lower = list(continuous = &#39;smooth&#39;), axisLabels = &quot;none&quot;, switch = &#39;both&#39;) affairReg &lt;- lm(nbaffairs ~ ., data=Fair) summary(affairReg) ## ## Call: ## lm(formula = nbaffairs ~ ., data = Fair) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.0503 -1.7226 -0.7947 0.2101 12.7036 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.87201 1.13750 5.162 3.34e-07 *** ## sexmale 0.05409 0.30049 0.180 0.8572 ## age -0.05098 0.02262 -2.254 0.0246 * ## ym 0.16947 0.04122 4.111 4.50e-05 *** ## childyes -0.14262 0.35020 -0.407 0.6840 ## religious -0.47761 0.11173 -4.275 2.23e-05 *** ## education -0.01375 0.06414 -0.214 0.8303 ## occupation 0.10492 0.08888 1.180 0.2383 ## rate -0.71188 0.12001 -5.932 5.09e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.095 on 592 degrees of freedom ## Multiple R-squared: 0.1317, Adjusted R-squared: 0.12 ## F-statistic: 11.23 on 8 and 592 DF, p-value: 7.472e-15 "],["cross-validation.html", "18 cross-validation 18.1 revisiting the affairs data 18.2 avoiding capitalizing on chance 18.3 an example of cross-validated linear regression", " 18 cross-validation The problem of capitalizing on chance is a significant one in prediction, and one should always be skeptical of models which are untested beyond the sample from which they were derived. 18.1 revisiting the affairs data Let’s go back and do what we should have done earlier, that is, examine and think about the “number of affairs” variable. What does the distribution look like? data(Fair) Fair %&gt;% count(nbaffairs) ## nbaffairs n ## 1 0 451 ## 2 1 34 ## 3 2 17 ## 4 3 19 ## 5 7 42 ## 6 12 38 We note that the distribution is skewed - and we realize that perhaps we should think about it differently: The meaning of the difference between 0 and 1 is not the same as that between 1 and 2, or perhaps even 1 and 12. We can transform the data in several ways. We might perform a root transform, which we could then round to an integer: Fair &lt;- Fair %&gt;% mutate(rootAffair = as.integer(sqrt(nbaffairs))) Fair %&gt;% count(rootAffair) ## rootAffair n ## 1 0 451 ## 2 1 70 ## 3 2 42 ## 4 3 38 Or we could simply distinguish between those that do and don’t have affairs. Fair &lt;- Fair %&gt;% mutate(affairYN = ifelse (nbaffairs &gt; 0,1,0)) Fair %&gt;% count(affairYN) ## affairYN n ## 1 0 451 ## 2 1 150 We’ll examine each of these in the following paragraphs. 18.2 avoiding capitalizing on chance One lesson from the last class was that correlations (and regression coefficients) drawn from small samples were not stable. In regression analysis, as progressively smaller samples were drawn from the Fair data, the ability to predict the outcome increased. In the limiting case, when the number of predictors (variables) was equal to the number of cases in the sample (rows), prediction became perfect. 18.2.1 splitting the data into training and test subsamples The most basic solution to this problem is to split the data into two groups, a training sample from which we extract our model, and a test sample on which you will assess it. (Often, the logic of this will be extended to include a third group, a validation sample which would be used to tune or select the results of the training run before the test data are examined). Here, we will consider the simpler approach, splitting the Fair data into training and test samples. The critical feature of this analysis is that we will hold out the test data, and not even look at it until after our model building is complete. # establish a seed for your data-split # so that your results will be reproducible set.seed(33458) n &lt;- nrow(Fair) # create a set of line numbers # of size corresponding to the # desired training sample trainIndex &lt;- sample(1:n, size = round(0.6*n), replace=FALSE) # create training and test samples trainFair &lt;- Fair[trainIndex ,] testFair &lt;- Fair[-trainIndex ,] 18.3 an example of cross-validated linear regression We first predict the variable “rootAffair,” using just the training data: trainFair2 &lt;- trainFair %&gt;% select(-nbaffairs, -affairYN) testFair2 &lt;- testFair %&gt;% select(-nbaffairs, -affairYN) model2 &lt;- lm(rootAffair ~ ., data=trainFair2) summary(model2) ## ## Call: ## lm(formula = rootAffair ~ ., data = trainFair2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.6655 -0.4923 -0.2180 0.1444 2.7593 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.974758 0.385662 5.120 5.03e-07 *** ## sexmale -0.045067 0.106127 -0.425 0.6713 ## age -0.014193 0.007835 -1.812 0.0709 . ## ym 0.038914 0.013901 2.799 0.0054 ** ## childyes 0.027497 0.116926 0.235 0.8142 ## religious -0.090619 0.038503 -2.354 0.0191 * ## education -0.018241 0.021963 -0.831 0.4068 ## occupation 0.061293 0.030904 1.983 0.0481 * ## rate -0.265213 0.042161 -6.291 9.39e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8224 on 352 degrees of freedom ## Multiple R-squared: 0.1783, Adjusted R-squared: 0.1597 ## F-statistic: 9.549 on 8 and 352 DF, p-value: 5.595e-12 To assess the effectiveness of this model on an independent sample, we write a simple function, which assesses R2, then applies it to both the training data and the test data: R2.model.dep.data &lt;- function(myModel,myDep,myData) { errorscores &lt;- myDep - predict(myModel,myData) SS.error &lt;- sum(errorscores^2) deviations &lt;- scale(myDep,scale = FALSE)#) SS.total &lt;- sum(deviations^2) 1 - (SS.error/SS.total) } R2Train &lt;- R2.model.dep.data(model2,trainFair2$rootAffair,trainFair2) (R2Test &lt;- R2.model.dep.data(model2,testFair2$rootAffair,testFair2)) ## [1] 0.003453435 The R2 on the training sample is 0.1783303, on the test sample it is 0.0034534. The difference between these is sometimes referred to as shrinkage. Shrinkage will be problematic particularly when there are a small number of observations, a large number of predictors (and consequently a complex model), or both of these. 18.3.1 applying logistic regression analysis to the training data Let’s go back to the “number of affairs” variable and consider a second way of thinking about it, that is, simply comparing those who do and don’t have affairs: Fair &lt;- Fair %&gt;% mutate(affairYN = ifelse (nbaffairs &gt; 0,1,0)) Fair %&gt;% count(affairYN) ## affairYN n ## 1 0 451 ## 2 1 150 With this change, the regression problem becomes more clearly a problem in classification. How can we best predict which ‘type’ (affair-ers vs. not) a given person falls in to? With this change, we have moved from a variable which is essentially continuous (nbaffairs) to one which is dichotomous and therefore distributed binomially. The desired regression is now a logistic one. We begin by running this analysis using only the training data. model2 &lt;- glm(affairYN ~ ., data = trainFair, family = &quot;binomial&quot;) summary (model2) ## ## Call: ## glm(formula = affairYN ~ ., family = &quot;binomial&quot;, data = trainFair) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.601e+01 1.434e+05 0.000 1.000 ## sexmale 3.818e-02 3.891e+04 0.000 1.000 ## age -1.303e-02 3.176e+03 0.000 1.000 ## ym 1.490e-02 5.403e+03 0.000 1.000 ## childyes 2.288e-02 4.479e+04 0.000 1.000 ## religious 2.225e-02 1.508e+04 0.000 1.000 ## education 3.888e-02 8.532e+03 0.000 1.000 ## occupation -4.976e-02 1.113e+04 0.000 1.000 ## rate -1.357e-01 1.591e+04 0.000 1.000 ## nbaffairs -3.191e+01 4.714e+04 -0.001 0.999 ## rootAffair 1.455e+02 1.626e+05 0.001 0.999 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 4.0981e+02 on 360 degrees of freedom ## Residual deviance: 3.2775e-09 on 350 degrees of freedom ## AIC: 22 ## ## Number of Fisher Scoring iterations: 25 Our “predicted scores” are continuous, corresponding to the probability that a given person will have an affair. Here’s how they are distributed (still on the training data here): predictTrain &lt;- predict(model2, trainFair, type = &quot;response&quot;) summary (predictTrain) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.0000 0.0000 0.2548 1.0000 1.0000 As scores on predictTrain increase, the predicted likelihood of an affair increases. If you are considering marriage to a potential partner, what score would be too high? How do we distinguish “ok” from “not”? "],["prediction-and-classification.html", "19 prediction and classification 19.1 from regression to classification: selection of a threshold 19.2 another approach to classification: k-nearest neighbor", " 19 prediction and classification If we want to classify people, we will need to create a decision threshold at which we will change our prediction from ‘no’ to ‘yes’. 19.1 from regression to classification: selection of a threshold We will continue to work with the affairs data; here’s the relevant code from last chapter: # code from last chapter on Fair data Fair &lt;- Fair %&gt;% mutate(rootAffair = as.integer(sqrt(nbaffairs))) %&gt;% mutate(affairYN = ifelse (nbaffairs &gt; 0,1,0)) set.seed(33458) n &lt;- nrow(Fair) trainIndex &lt;- sample(1:n, size = round(0.6*n), replace=FALSE) trainFair &lt;- Fair[trainIndex ,] testFair &lt;- Fair[-trainIndex ,] trainFair2 &lt;- trainFair %&gt;% select(-nbaffairs, -affairYN) testFair2 &lt;- testFair %&gt;% select(-nbaffairs, -affairYN) model2 &lt;- lm(rootAffair ~ ., data = trainFair2) summary(model2) ## ## Call: ## lm(formula = rootAffair ~ ., data = trainFair2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.6655 -0.4923 -0.2180 0.1444 2.7593 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.974758 0.385662 5.120 5.03e-07 *** ## sexmale -0.045067 0.106127 -0.425 0.6713 ## age -0.014193 0.007835 -1.812 0.0709 . ## ym 0.038914 0.013901 2.799 0.0054 ** ## childyes 0.027497 0.116926 0.235 0.8142 ## religious -0.090619 0.038503 -2.354 0.0191 * ## education -0.018241 0.021963 -0.831 0.4068 ## occupation 0.061293 0.030904 1.983 0.0481 * ## rate -0.265213 0.042161 -6.291 9.39e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8224 on 352 degrees of freedom ## Multiple R-squared: 0.1783, Adjusted R-squared: 0.1597 ## F-statistic: 9.549 on 8 and 352 DF, p-value: 5.595e-12 predictTrain &lt;- predict(model2, trainFair, type = &quot;response&quot;) To maximise overall prediction, we will create a threshold equal to the actual proportion of people who don’t have affairs in our sample: (threshold &lt;- mean(trainFair$affairYN)) ## [1] 0.2548476 This is equal to both the mean of our predicted scores (above) and the mean of our actual scores, and, because this is a dichotomous variable, the proportion of people in the sample who have affairs. We’ll predict that if a person has a predicted score more than this we’ll predict that s/he will be unfaithful, else we will “PredictOK.” Then we will create a confusion matrix, to compare our correct predictions (PredictOK and affairYN = 1, Predictunfaithful and affairYN = 0) with the remainder. classification &lt;- ifelse(predictTrain &gt; threshold, &quot;Predictunfaithful&quot;, &quot;PredictOK&quot;) (b &lt;- table(classification, trainFair$affairYN)) ## ## classification 0 1 ## PredictOK 107 18 ## Predictunfaithful 162 74 (correct &lt;- b[1,1] + b[2,2]) ## [1] 181 (errors &lt;- b[1,2] + b[2,1]) ## [1] 180 (accuracy &lt;- correct/(correct + errors)) ## [1] 0.501385 19.1.1 applying the model to the test data We shouldn’t trust these results, though, because the model is based on the same data that it is tested upon. Now we will apply the model to the test data. With the linear regression model above, we examined ‘shrinkage’ in the overall R-square. Here, we can assess shrinkage in terms of the percent of erroneous classifications. Typically (but not invariably), the percent of accurate classifications will decline, especially if the model is a complex one with many variables or if the number of observations is low. predictTest &lt;- predict(model2, testFair, type = &quot;response&quot;) classification &lt;- ifelse(predictTest &gt; threshold, &quot;Predictunfaithful&quot;, &quot;PredictOK&quot;) (b &lt;- table(classification, testFair$affairYN)) ## ## classification 0 1 ## PredictOK 73 9 ## Predictunfaithful 109 49 (correct &lt;- b[1,1] + b[2,2]) ## [1] 122 (errors &lt;- b[1,2] + b[2,1]) ## [1] 118 (accuracy &lt;- correct/(correct + errors)) ## [1] 0.5083333 On the test data, we are correct 51 percent of the time. This describes our out of sample risk. 19.1.2 changing our decision threshold In many decision problems, there is an asymmetry in the cost of different types of errors: if you are foraging for mushrooms, for example, an error of the form (you decide its safe and it is poisonous) is more costly than the converse (you decide its poisonous and it is safe). This may be true in the present example as well. Consider someone who is looking for a spouse, but is really averse to the idea of getting hurt by an affair. That person might feel like the cost of marrying an unfaithful person is much greater than the cost of not marrying a faithful one. So we adjust the threshold downwards: threshold &lt;- .05 classification &lt;- ifelse(predictTest &gt; threshold, &quot;Predictunfaithful&quot;, &quot;PredictOK&quot;) (b &lt;- table(classification, testFair$affairYN)) ## ## classification 0 1 ## PredictOK 27 3 ## Predictunfaithful 155 55 (correct &lt;- b[1,1] + b[2,2]) ## [1] 82 (errors &lt;- b[1,2] + b[2,1]) ## [1] 158 (accuracy &lt;- correct/(correct + errors)) ## [1] 0.3416667 The “overall accuracy” - that is, number of correct classifications - drops. But that’s not what we are really interested in, rather, we are interested in minimizing hurt. Here’s another example: Someone who is very lonely might feel the opposite, and be willing to accept greater substantially greater risk. threshold &lt;- .5 classification &lt;- ifelse(predictTest &gt; threshold, &quot;Predictunfaithful&quot;, &quot;PredictOK&quot;) (b &lt;- table(classification, testFair$affairYN)) ## ## classification 0 1 ## PredictOK 111 21 ## Predictunfaithful 71 37 (correct &lt;- b[1,1] + b[2,2]) ## [1] 148 (errors &lt;- b[1,2] + b[2,1]) ## [1] 92 (accuracy &lt;- correct/(correct + errors)) ## [1] 0.6166667 Prediction is higher here - but not much higher than it would be if we raised the threshold even further, and just assumed that everyone can be trusted. Then, our error rate would be 25 percent. When overall predictability is low, it’s often the case that you can maximize correct predictions by simply predicting that everyone will be in the most popular category. Predicting rare events, such as suicides, is particularly difficult. 19.1.3 more confusion There is a nice shortcut to generating confusion matrices such as those above using the caret package. This function describes outcomes in several ways, as there are many languages for describing outcomes in 2 x 2 tables, including Type I vs. Type II errors, Hits vs. False Alarms/False Positives, and Sensitivity vs. Specificity. In these data, it’s been set up so that hit rate ~ sensitivity ~ (“no affair” | no affair) correct rejection ~ specificity ~ (“affair” | affair) threshold &lt;- .5 # syntax for classification in caret is a little different # (the labels for the actual and predicted scores have to be the same) #classification &lt;- ifelse(predictTest &gt; threshold, # &quot;Predictunfaithful&quot;, &quot;PredictOK&quot;) classification &lt;- ifelse(predictTest &gt; threshold, 1,0) # caret package (newest) requires explicit matching of factors classification &lt;- as.factor(classification) testFair$affairYN &lt;- as.factor(testFair$affairYN) confusionMatrix(classification, testFair$affairYN) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 111 21 ## 1 71 37 ## ## Accuracy : 0.6167 ## 95% CI : (0.5519, 0.6785) ## No Information Rate : 0.7583 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : 0.1916 ## ## Mcnemar&#39;s Test P-Value : 3.245e-07 ## ## Sensitivity : 0.6099 ## Specificity : 0.6379 ## Pos Pred Value : 0.8409 ## Neg Pred Value : 0.3426 ## Prevalence : 0.7583 ## Detection Rate : 0.4625 ## Detection Prevalence : 0.5500 ## Balanced Accuracy : 0.6239 ## ## &#39;Positive&#39; Class : 0 ## 19.1.4 ROCs and AUC Each of these decision thresholds describes the performance of a model at a particular point. We can combine the thresholds and plot them in Receiver Operating Characteristic (ROC) curves. The area under the curve (AUC) is a great measure of model accuracy, in that it summarizes how effective a classifier is across all possible thresholds. # fig.width and fig.height specified to get square plots # colAUC function gets stats etc AUCModel &lt;- colAUC(predictTest, testFair[[&quot;affairYN&quot;]], plotROC = TRUE) AUCModel ## [,1] ## 0 vs. 1 0.6662562 19.2 another approach to classification: k-nearest neighbor Real-life social predictions are often guided not by induction or the (optimized) combination of a set of predictor variables. Rather, we often reason by analogy - we might think, for example, that I won’t go out with Fred because he reminds me of Larry, and Larry was kind of a jerk. If regression analysis is an approach to prediction based in our set of variables, k-nearest neighbor analysis instead makes predictions based on observations. Formally, as described in the documentation for the knn package, For each row of the test set, the k nearest (in Euclidean distance) training set vectors are found, and the classification is decided by majority vote, with ties broken at random. (ref). In the simplest form of this analysis, we find the nearest thing to a “doppelganger” (a look-alike or near double) for a given observation. So, in the affairs data, if a person is most like someone else in the dataset who has had an affair, we predict an affair, else not. 19.2.1 application: the affairs data Begin by loading the affairs data from last time. Using the same seed (33458) means that the same set of training and test cases will be extracted as in the prior analysis data(Fair) # one change here: Note the bidirectional pipe to simplify code # use only when you are sure that your file update is ok Fair %&lt;&gt;% # &lt;- Fair %&gt;% mutate(affairYN = # nbaffairs is set up as a factor # to allow confusionmatrix to run as.factor(ifelse(nbaffairs &gt; 0,1,0))) %&gt;% # unlike the lm and glm commands, knn will not automatically create our dummy # variables for us. so we need to do this manually. mutate(sexMale = ifelse(sex == &quot;female&quot;, 0, 1)) %&gt;% mutate(childyes = ifelse(child == &quot;no&quot;, 0, 1)) %&gt;% select(-(c(sex,child,nbaffairs))) set.seed(33458) n &lt;- nrow(Fair) trainIndex &lt;- sample(1:n, size = round(0.6*n), replace = FALSE) # create training and test samples trainFair &lt;- Fair[trainIndex ,] testFair &lt;- Fair[-trainIndex ,] To run a k-nearest neighbor analysis, we need three inputs: our predictors in the training data, our predictors in the test data, and our outcome/classes in the training data. Here, as in the regression analysis in the last chapter, we can generate a confusion matrix to assess the accuracy of prediction: set.seed(33458) trainPredictors &lt;- trainFair %&gt;% select(-affairYN) testPredictors &lt;- testFair %&gt;% select(-affairYN) knnAffair &lt;- knn(trainPredictors, # training data testPredictors, # test data trainFair$affairYN, # class 1 # number of neighbors ) b&lt;-confusionMatrix(knnAffair, testFair$affairYN) b ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 143 32 ## 1 39 26 ## ## Accuracy : 0.7042 ## 95% CI : (0.6421, 0.7611) ## No Information Rate : 0.7583 ## P-Value [Acc &gt; NIR] : 0.9772 ## ## Kappa : 0.2247 ## ## Mcnemar&#39;s Test P-Value : 0.4764 ## ## Sensitivity : 0.7857 ## Specificity : 0.4483 ## Pos Pred Value : 0.8171 ## Neg Pred Value : 0.4000 ## Prevalence : 0.7583 ## Detection Rate : 0.5958 ## Detection Prevalence : 0.7292 ## Balanced Accuracy : 0.6170 ## ## &#39;Positive&#39; Class : 0 ## b[[&quot;overall&quot;]][[&quot;Accuracy&quot;]] ## [1] 0.7041667 19.2.2 from one doppelganger to many In the above code, we used a k-nearest neighbor analysis based on a single ‘neighbor’ (k = 1). Can we improve prediction by considering more than 1 neighbor? 19.2.3 the Bayesian classifier Lisa is a doctor whose patients often have post-surgical pain. Their suffering is real, but effective pain medications such as OxyContin have a high likelihood of leading to abuse and addiction. She has two bits of information about a patient, his or her age (say that we have this in ten levels, corresponding to decades of age, so that the first level is “under 20” and that the last level is “over 100”), and his or her self-report of pain, also on a 10 point scale. Assume that Lisa wants to prescribe the medication to all patients who would not abuse it, and not prescribe the medication to anyone who would. Assume that Lisa knew the entire matrix of conditional probabilities, P (addiction | “age &lt; 20” &amp; “pain = 1”) = .34 P (addiction | “age &lt; 20” &amp; “pain = 2”) = .26 … P (addiction | “age &gt; 100” &amp; “pain = 10”) = .09 In this case she would prescribe the drug for every case where the conditional probability was greater than .5, and never otherwise. This is called the Bayesian classifier, and if we have the entire matrix of conditional probabilities we could do no better. In real world problems, we are typically dealing with many predictors, and we don’t have the full matrix of conditional probabilities. But this two predictor case “sets up” the illustration drawn from James et al. (2013). Figure 19.1: Comparing two values of k. From James et al. (2013). In the figure above, assume that the horizontal and vertical axes correspond to scores on the two predictors (age and pain). The orange and blue colored dots correspond to cases of abuse and non-abuse in the training data. The dashed line is the Bayesian classifier. The solid line is the k-nn decision boundary, which distinguishes the regions in which we will predict abuse and non abuse in the test data. We see that when k is small (a single neighbor), prediction is flexible, non-linear, and that as k increases, the boundary differentiating the decision to prescribe and not prescribe becomes more nearly linear. But what value of k is optimal? 19.2.4 Back to the affairs data To test a range of values, we can first set up our knn analysis as a function (compare this code with the code in the prior section). trainPredictors &lt;- trainFair %&gt;% select(-affairYN) testPredictors &lt;- testFair %&gt;% select(-affairYN) knnFairdata &lt;- function (k) { set.seed(33458) knnAffair &lt;- knn(trainPredictors, # training data testPredictors, # test data trainFair$affairYN, # class k # number of neighbors ) b&lt;-confusionMatrix(knnAffair, testFair$affairYN) b[[&quot;overall&quot;]][[&quot;Accuracy&quot;]] } We run the function on k = 1 and k = 2 to test it: knnFairdata(1) ## [1] 0.7041667 knnFairdata(2) ## [1] 0.6958333 Now we can apply it to as many as 100 values of k, using the purrr::map command: kAccuracy &lt;- (map_dbl(1:100,knnFairdata)) %&gt;% # map the knnFunction onto numbers 1-100 as_tibble() %&gt;% # then a tibble so we can do a quick plot rename(Accuracy = 1) %&gt;% mutate(k = seq_along(Accuracy)) We can graph this, using the syntax from the beginning of the class: kAccuracy %&gt;% ggplot(aes(k, Accuracy)) +#%&gt;% geom_point() + ggtitle(&quot;Overall accuracy for varying levels of k&quot;) # This pulls out the maximum accuracy, and the value of k for which it occurs: (ka &lt;- which.max(kAccuracy$Accuracy)) ## [1] 33 (kb &lt;- max(kAccuracy$Accuracy)) ## [1] 0.7583333 19.2.5 avoiding capitalization on chance (again) In these data, with this split of training and test (and this initial seed) the maximum predictability occurs at k = 33, with an overall accuracy of 0.7583333. Would this hold if we used a different random split? Remember, here, we have tested not one model, but 100 of them, then chosen the best one. The peak in the curve at 33 may well be due to chance characteristics of the test data. We could address this empirically using one of several techniques. One approach is to have a third independent sample on which to test the accuracy of prediction at k = 33. This would require the separate validation sample that was introduced in the last chapter. In the following block, I resplit the Fair data, using proportions of 60%, 30%, 10%. These values are likely not optimal given the (relatively small) size of the Fair data, but will work to illustrate the approach: set.seed(33458) threeWaySplit &lt;- sample(1:3, size = nrow(Fair), prob = c(0.6,0.3,0.1), replace = TRUE) trainFair2 &lt;- Fair[threeWaySplit == 1,] testFair2 &lt;- Fair[threeWaySplit == 2,] validFair2 &lt;- Fair[threeWaySplit == 3,] I tweak my function here to use the new data, then run it 100 times as before. trainPredictors &lt;- trainFair2 %&gt;% select(-affairYN) testPredictors &lt;- testFair2 %&gt;% select(-affairYN) knnFairdata2 &lt;- function (k) { set.seed(33458) knnAffair &lt;- knn(trainPredictors, # training data testPredictors, # test data trainFair2$affairYN, # class k # number of neighbors ) b&lt;-confusionMatrix(knnAffair, testFair2$affairYN) b[[&quot;overall&quot;]][[&quot;Accuracy&quot;]] } kAccuracy &lt;- (map_dbl(1:100,knnFairdata2)) %&gt;% # map the knnFunction onto numbers 1-100 as_tibble() %&gt;% # then a tibble so we can do a quick plot rename(Accuracy=value) %&gt;% mutate (k = seq_along(Accuracy)) (ka2 &lt;- which.max(kAccuracy$Accuracy)) ## [1] 15 (kb2 &lt;- max(kAccuracy$Accuracy)) ## [1] 0.7802198 So here, on the (contaminated) test data, the maximum predictability occurs at k = 15, with an overall accuracy of 0.7802198. We apply this back to the validation data by pulling the knn code out of the function, and running it just once against the validation data: knnAffair &lt;- knn(trainFair2[,-7], # training data validFair2[,-7], # VALIDATION data trainFair2$affairYN, # class ka2 # number of neighbors ) b &lt;- confusionMatrix(knnAffair, validFair2$affairYN) (kb3 &lt;- b[[&quot;overall&quot;]][[&quot;Accuracy&quot;]]) ## [1] 0.703125 The overall predictability using the k-nearest neighbor analysis on the clean validation data, is now 0.703125. You might note that in our two splits of the Fair data - the two-way split of 60% Training and 40% Test, and the three-way split of 60% Training, 30% Test, 10% Validation, we got two different solutions to the question of the ‘optimal k’ (i.e., 33 and 15). With larger sample sizes, these values would be more stable. 19.2.6 the multinomial case A final comment on the k-nearest neighbors approach: You can extend this to classification problems in which we are predicting not just a dichotomous outcome, but a multinomial one - such as a personality type or college concentration. References James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 103. Springer Texts in Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4614-7138-7. "],["machine-learning-chihuahuas-vs-muffins-and-other-distinctions-and-ideas.html", "20 machine learning: chihuahuas vs muffins, and other distinctions and ideas 20.1 supervised versus unsupervised 20.2 prediction versus classification 20.3 understanding versus prediction 20.4 bias versus variability 20.5 compensatory versus non-compensatory problems 20.6 a postscript: The Tidymodels packages", " 20 machine learning: chihuahuas vs muffins, and other distinctions and ideas In the last few chapters, we have considered linear and logistic regression and k-nearest neighbor analysis as tools for prediction and classification. We’ve shown how to split the data into training, test and (in some cases) validation samples, then how to assess the robustness or accuracy of a model on these new datasets. We’ve also considered measures such as R-squared, overall accuracy, and area under the ROC as measures of validity. These ideas and techniques form a starting point for the study of machine learning. My approach is drawn largely from James et al. (2013), which is available freely on the web and includes links to additional materials and R-based exercises for those who would like to study this further. 20.1 supervised versus unsupervised One problem that we haven’t yet considered is the distinction between supervised and unsupervised problems, arguably the most fundamental distinction in machine learning. In both the Swiss and the Fair problems, we had a known outcome (fertility, infidelity) which we were trying to predict from a set of independent variables. In these problems, we have an a priori split of the variables into two sets (outcomes and predictors). These are considered supervised problems. In these problems, we can think of the known outcome or criterion as guiding (supervising) the work of model-building. There is a second type of problem in which we don’t have an outcome, which would guide or supervise our model. Without an outcome or criterion, we must rely on the internal structure of the data. These are considered unsupervised problems. Methods used to address unsupervised problems include cluster analysis (of which there are many subtypes), component analysis, and exploratory factor analysis. In the unsupervised approach, objectives include finding unknown patterns, developing a set of types (or a taxonomy), and assessing the dimensionality of a latent set of variables. In psychology, a focal problems involves assessing the factor structure of personality (if you have taken introductory psychology, you are likely familiar with the five-factor or Big Five model of personality). The unsupervised approach is also used to solve problems of community detection in the study of social and scientific networks (see Figure 20.1, from Lanning (2017)). I think that questions about dimensionality and internal structure can be compelling (Lanning 1994, 1996; Lanning and Rosenberg 2009), but I will not consider them further here. Figure 20.1. Part of the structure of personality research. From Lanning (2017) 20.2 prediction versus classification Within the category of supervised problems, we can distinguish problems in classification (those in which the outcome is nominal or discrete) from problems where the outcome is ordered or numeric. We examined the Fair data in both these ways, first treating the outcome as the rescaled number of reported affairs, then as a distinction between those who did and did not have affairs. Somewhere in between these two approaches are problems in which the criterion is an ordered set of categories - such as small, medium, and large pizza sizes or, to consider a problem in my own area of study, a sequence of seven levels of social maturity or personality development. Working with several colleagues, I constructed “dictionaries” which empirically distinguished these levels; the object is to allow assessment of the level of maturity or ego development of a given text. In the diagram below, the words in these initial dictionaries are arranged clockwise, with those characteristic of the earliest (Impulsive) level in the upper right quadrant (i.e., at 1 o’clock), then moving through the middle stages at the bottom, etc.(Lanning (2018)). 20.3 understanding versus prediction In machine learning, we are in general concerned with the problem of finding the function which relates an outcome (Y) to a set of predictors (X). This general principle spans two opposing, but overlapping, use-cases: understanding and prediction. For example, in the ‘Fair’ data, we were initially concerned with the question of “what predicts infidelity?,” which involves or suggests an interest in understanding. With the k-nn analysis, our focus increasingly shifted away from this towards the more pragmatic goal of increasing our hit rate or overall accuracy, away from a concern with inference and understanding to a position where we were satisfied to treat the algorithm as a black box from which we were only concerned with the accuracy of outputs. These two objectives of “understanding and thinking in terms of equations and models” and “prediction and thinking only in terms of optimization” can be thought of as two points on a continuum of interpretability. Some techniques used in machine learning give results that are quite interpretable (including multiple regression, particularly restricted regression techniques such as the lasso). Others, including support vector machines and, in particular, deep neural networks sacrifice interpretability in the service of prediction. For complex approaches in image recognition, such as the chihuahua versus muffin problem, deep neural networks provide the best solutions, but are particularly challenging to understand (Kumar, Wong, and Taylor 2017). Fig 21.1: The chihuahua-muffin problem. Exercise 21.1: Consider the Chihuahua-Muffin problem illustrated above. What is the outcome variable ‘Y’? What are some of the predictors ‘X’? (Note that in image recognition and natural language processing, predictors are typically called “features”). Why is this problem interesting? That is, are there problems similar to this that have important social uses? Can you describe a simple algorithm or decision rule which works more than 50% of the time on the (training) data above (i.e., “if X then”Chihuahua” else “Muffin”)? What might an effective algorithm on new (test) data look like? finally, what do you think goes on on your mind as you evaluate the photos in the “chihuahua-muffin problem”? 20.4 bias versus variability If you read further about machine learning, you are likely to encounter the phrase “bias-variability tradeoff.” You may remember that, in our initial analyses of the Swiss fertility data, we discussed how reducing the number of observations increases the fit of the model on the (training) data - and that, ultimately, the fit of the model would become perfect when we reduce the number of observations to the number of variables in the model. Here, we construct two independent samples, each with 9 subjects. data(Fair) set.seed(33458) FairSample1 &lt;- Fair %&gt;% sample_n(9) set.seed(94611) FairSample2 &lt;- Fair %&gt;% sample_n(9) In the following chunk, we run a regression analysis witin each of these samples, predicting the number of affairs from the 9 predictors (including the intercept). affairReg1 &lt;- lm(nbaffairs ~ ., data=FairSample1) affairReg2 &lt;- lm(nbaffairs ~ ., data=FairSample2) R21 &lt;- summary(affairReg1)$r.squared R22 &lt;- summary(affairReg2)$r.squared t(c(sample1 = R21, sample2 = R22)) %&gt;% kable(caption = &quot;R^2 from two small samples&quot;) Table 20.1: R^2 from two small samples sample1 sample2 1 1 names(affairReg1[[&quot;coefficients&quot;]]) %&gt;% cbind(round(affairReg1[[&quot;coefficients&quot;]], 2)) %&gt;% cbind(round(affairReg2[[&quot;coefficients&quot;]], 2)) %&gt;% as_tibble() %&gt;% rename(variable = 1, sample1 = 2, sample2 = 3) %&gt;% kable(caption = &quot;Regression coefficients from two small samples&quot;) Table 20.1: Regression coefficients from two small samples variable sample1 sample2 (Intercept) 19.93 87.53 sexmale -3.7 49.39 age -0.16 -2.96 ym 0.35 2.5 childyes 0.72 -4.45 religious -0.79 -4.55 education -0.49 -0.88 occupation 2.33 2.95 rate -3.81 -1.5 Note that in each case we have perfect predictability, but with a very different set of predictors. The difference between these two sets of coefficients is an illustration of how coefficients in overfit models will vary from one sample to another. At the opposite extreme are underfit models, which are likely to provide relatively stable coefficients across samples, but which aren’t very effective at prediction. This is the bias-variability trade-off, and it occurs not only in tiny data sets such as these, but in bigger datasets where there is a large number of predictor variables, as is often the case in, for example, artificial intelligence (including image recognition) and bioinformatics (including statistical genetics). The problem of “too many predictors” can be addressed, in part, by “preprocessing,” or trying to restructure the data to effectively increase the number of rows in the data (e.g., by imputing missing values), or decrease the number of columns (that is, using component or factor analysis as an initial step in data analysis). Another approach is to use resampling. 20.4.1 resampling: beyond test, training, and validation samples We’ve considered one approach to avoiding chance-inflated models and prediction estimates, and that is the approach of ‘holding out’ test (and possibly validation) samples. An extension of this approach is k-fold cross validation, in which the sample is divided into k (e.g., ten) parts, each of which is used as a validation sample in k different analyses. To assess the overall performance of the model, the results of these are averaged. This is a sophisticated and relatively easy to implement approach which can be used, for example, to assess the relative performance of different models, such as linear versus non-linear models. Another approach to resampling is bootstrapping, in which model parameters (e.g., regression coefficients) are taken as the average of many (for example 1,000) analyses of subsamples of the data. In the bootstrap, sampling is done with replacement, so that the same case may appear in many samples. The averaged coefficients arrived at using bootstrapping are less variable than the results of a single analysis. 20.5 compensatory versus non-compensatory problems Consider two simple hypothetical real world problems. In the first, you are deciding where you should apply to graduate school. Assume that, in the sample of schools under consideration, only two variables matter to you, say “program quality” and “program cost,” and that you weight the first of these positively and the second negatively. We might anticipate that these would be related in a compensatory way, so that you would be willing to pay more for a better program. In the second problem, you are looking to hire a group of commercial jet pilots. Again, there are only two predictors; here, they are eyesight and responsibility. Unlike in the first problem, these are related in a noncompensatory way - for example, no matter how good someone’s eyesight is, if they are irresponsible you will not want to hire them. Similarly, even if they are very responsible, poor eyesight might make them ineligible. Exercise 21.2: For the grad school and airline pilot problems, draw a pair of coordinate axes, together with points representing a few dozen “positive” (apply or hire) and “unacceptable” (don’t apply, don’t hire) observations. What can you say about the boundary between these two regions in each problem? We can think of problems such as the grad school problem in a regression framework, while the pilot problem is instead a decision tree or “multiple cutoff” model. Very simple decision trees, including “fast and frugal trees,” can be useful tools both to describe how people decide and how they might make better decisions (Phillips et al. 2017). In machine learning problems, there may be hundreds or thousands of predictors (features). The logic of decision trees is extended in techniques including bootstrap aggregation or bagging (in which a large number of trees are considered, then averaged), and random forests (which sequentially examines subsets of predictors in an effort to increase the reliability of predictions). These models are often predictively useful, but can be complex and difficult to interpret. Chapter 8 of (James et al. 2013) goes into more detail on these methods. Two additional methods bear mention: Support vector machines (SVM) are classifiers which attempt to find optimal margins (hyperplanes) between two or more sets of criteria. Finally, neural networks are models which typically include multiple layers (akin to biological neurons) each of which can be activated based on the sum of the inputs of the prior layer. The chain of prediction from input (pixel) to output (“chihuahua”) is not a simple forward path, rather, errors of prediction are used to tune weights of intermediate layers in a process known as backpropagation. Artificial neural networks are, at present, the most important set of techniques in artificial intelligence problems including image, speech, and even taste recognition. 20.6 a postscript: The Tidymodels packages The new Tidymodels packages provide a handful of new, tidyverse-compliant approaches to problems in prediction, modeling, and machine learning. The website, particularly the Get Started page, is straightforward, and I urge you to review it systematically. Here’s a quick summary of some key points from the first few pages of that section: The parsnip package presents a tidyverse-compliant approach to writing syntax of models; it appears to be particularly useful for examining multiple models on the same data sets. The broom package includes the tidy() function, which can be used to present the results of your analyses in data frames - this is handy for making tables of your data. Results of these models can be represented as data frames, which can then be easily tabled or used to create figures. You can see how these are used on the Build a model page. Perhaps the most useful tool introduced in these pages is the Skimr package. Skimr, though not part of the Tidymodels, is handy for getting a quick set of summary statistics - it’s substantially more convenient than the summarise function in the Tidyverse and, unlike the describe package in the psych package, is Tidyverse-compliant. The preprocessing of data includes several concepts that we have previously considered, including splitting data into training and test subsamples, the creation of dummy variables, and how we can treat time data. The recipes package include several functions that make it more convenient to work with only subsets of our variables, that is, to assign “roles” to include or exclude variables in our models. This section also includes an introduction to the yardstick package, which gives an easier way to implement ROC analysis than the code I used in the prior chapters. Resampling is a process through which we can estimate model parameters by inspecting a series of subsets of the data. It is particularly useful when we want to compare a set of competing models on a small-ish dataset (Lanning 2018). This section also includes an application of the random forests model introduced above. References James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 103. Springer Texts in Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4614-7138-7. Kumar, Devinder, Alexander Wong, and Graham W Taylor. 2017. “Explaining the Unexplained: A Class-Enhanced Attentive Response (Clear) Approach to Understanding Deep Neural Networks.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 36–44. ———. 1994. “Dimensionality of Observer Ratings on the California Adult Q-set.” Journal of Personality and Social Psychology 67 (July): 151–60. https://doi.org/10/drnkvm. ———. 1996. “Robustness Is Not Dimensionality: On the Sensitivity of Component Comparability Coefficients to Sample Size.” Multivariate Behavioral Research 31 (1): 33–46. https://doi.org/10/dt6gb3. ———. 2017. “What Is the Relationship Between ‘Personality’ and ‘Social’ Psychologies? Network, Community, and Whole Text Analyses of the Structure of Contemporary Scholarship.” Collabra: Psychology 3 (1): 8. ———. 2018. “Data Visualizations in Personality and Social Psychology: Challenges in Representing Taxonomic, Community, and Developmental Structures.” Association of Psychological Science Annual Convention, San Francisco, May 25. Lanning, Kevin, and Ari Rosenberg. 2009. “The Dimensionality of American Political Attitudes: Tensions Between Equality and Freedom in the Wake of September 11.” Behavioral Sciences of Terrorism and Political Aggression 1 (2): 84–100. https://doi.org/10/fckr37. Phillips, Nathaniel D., Hansjörg Neth, Jan K. Woike, and Wolfgang Gaissmaier. 2017. “FFTrees: A Toolbox to Create, Visualize, and Evaluate Fast-and-Frugal Decision Trees.” Judgment and Decision Making 12 (4): 344–68. "],["working-with-text-a-case-study.html", "21 working with text: a case study 21.1 federal workers 21.2 finding Reddit data 21.3 some initial observations 21.4 preprocessing 21.5 comparing the words and stems 21.6 Construction of differential word clouds 21.7 bigrams 21.8 categories of words 21.9 Code for assessing LIWC effect sizes between 2 groups 21.10 exercise: what would you do next?", " 21 working with text: a case study In this chapter, I present a rudimentary case study in which a contemporary issue is explored using R. I examine Reddit4 data, create comparison word-clouds from single words and two-word strings (bigrams), and consider the results of an external proprietary tool for examining categories of language. The chapter is based on the tidy approach to text analysis, for which a clear and thorough introduction has been provided in two books by Silge and her colleagues, each of which is available online Hvitfeldt and Silge (2021). A word of warning: This chapter is light on the grunt work of data wrangling and preprocessing, which were introduced in Chapter 14. Keep in mind that that work is typically very time-consuming. 21.1 federal workers On March 19, 2025, the New York Times ran an article with the title “Will I Lose My Job?’ Federal Workers Flock to Reddit for Answers.” The gist of the article was that US Federal workers, facing job insecurity in the opening days of President Trump’s second term, were using the r/fedworkers subreddit to connect with others and communicate their anxieties, concerns, and strategies. But did they, though? Is there evidence that the r/fedworkers subreddit was being used in these ways? In the analysis that follows, I will examine posts5 to the community for the period from January 1, 2024 through March 24, 2025. We’ll consider the following: What specific words differentiate Trump era posts (since his second inauguration in January 2025) from those from the Biden era (prior to the November 2024 election)? For this, we’ll rely primarily on visualizations, in particular, differential word clouds of single words and bigrams. What categories of speech differentiate the two epochs? For this, we’ll use a proprietary tool, Linguistic Inquiry and Word Count. LIWC is a widely used measure for extracting grammatical, social, and psychological variables from text [LIWC; Ryan L. Boyd et al. (2022)]. We’ll generate tables for the LIWC categories and examine the effect size and statistical significance of these differences. This approach can optionally be extended to additional comparisons, including: Are similar results obtained when we examine comments rather than posts? (Here, we could compare Trump-era words and categories with Biden-era words and categories). How are comments and posts different from each other? (Here, we could compare words and categories for r/fedworkers posts with words and categories for r/fedworkers comments) Does the content of posts (or comments) between the election and the inaugural more closely resemble that of the Biden era (as Biden was still president) or the coming Trump administration? In order to address these questions, we’ll briefly touch on some auxiliary technical questions as well, including: Should we ‘stem’ words prior to analysis? That is, should we treat terms such as (donut and donuts) as the same word or as different? How about (go and going)? A similar question can be asked about case - should all characters be set to lowercase? That is, should (I’m and i’m) be treated as the same? How about (DOGE and doge)? Should we include common stop words (a, and, if, or, me) in our analysis, or disregard them? 21.2 finding Reddit data Although Reddit data became less accessible beginning in 2023, archives of Reddit posts and comments remain readily available following some simple Web sleuthing. In this block, I begin with a set of posts that I have already downloaded. We initially look at a very small set of data - 100 lines - both to make sure the code works and, if it does, to make an initial determination about what fields are of interest. The file is large, so it will ultimately be read in through streaming rather than all-at-once, hence the readLines -&gt; stream_in syntax. posts &lt;- read_lines(&quot;data/r_fednews_posts.jsonl&quot;, n_max = 100) a1 &lt;- stream_in(textConnection( gsub(&quot;\\\\n&quot;,&quot;&quot;, posts))) ## Found 100 records... Imported 100 records. Simplifying... glimpse(a1) ## Rows: 100 ## Columns: 113 ## $ `_meta` &lt;df[,1]&gt; &lt;data.frame[26 x 1]&gt; ## $ all_awardings &lt;list&gt; [], [], [], [], [], [], [], [], [], [… ## $ allow_live_comments &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALS… ## $ approved_at_utc &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ approved_by &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ archived &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ author &lt;chr&gt; &quot;Unlikely_Story1424&quot;, &quot;lakepirate1775&quot;, … ## $ author_flair_background_color &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ author_flair_css_class &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ author_flair_richtext &lt;list&gt; [], [], [], [], [], [], [], [], [], [], … ## $ author_flair_template_id &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ author_flair_text &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ author_flair_text_color &lt;chr&gt; NA, NA, NA, NA, &quot;dark&quot;, NA, NA, NA, NA, … ## $ author_flair_type &lt;chr&gt; &quot;text&quot;, &quot;text&quot;, &quot;text&quot;, &quot;text&quot;, &quot;text&quot;, … ## $ author_fullname &lt;chr&gt; &quot;t2_dvmjwod3g&quot;, &quot;t2_3z3p2fng&quot;, &quot;t2_r0erp… ## $ author_is_blocked &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ author_patreon_flair &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ author_premium &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ awarders &lt;list&gt; [], [], [], [], [], [], [], [], [], [], … ## $ banned_at_utc &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ banned_by &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ can_gild &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ can_mod_post &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ category &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ clicked &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ content_categories &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ contest_mode &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ created &lt;int&gt; 1704074487, 1704075982, 1704083189, 1704… ## $ created_utc &lt;int&gt; 1704074487, 1704075982, 1704083189, 1704… ## $ discussion_type &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ distinguished &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ domain &lt;chr&gt; &quot;self.fednews&quot;, &quot;self.fednews&quot;, &quot;self.fe… ## $ downs &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ edited &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ gilded &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ gildings &lt;df[,0]&gt; &lt;data.frame[26 x 0]&gt; ## $ hidden &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ hide_score &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T… ## $ id &lt;chr&gt; &quot;18vmmmu&quot;, &quot;18vn1fo&quot;, &quot;18vox52&quot;, &quot;18voyk… ## $ is_created_from_ads_ui &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ is_crosspostable &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, … ## $ is_meta &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ is_original_content &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ is_reddit_media_domain &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ is_robot_indexable &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, … ## $ is_self &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE… ## $ is_video &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ likes &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ link_flair_background_color &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, NA, &quot;&quot;, &quot;&quot;, … ## $ link_flair_css_class &lt;chr&gt; NA, NA, &quot;e&quot;, NA, &quot;c&quot;, &quot;c&quot;, NA, &quot;c&quot;, NA, … ## $ link_flair_richtext &lt;list&gt; [&lt;data.frame[0 x 0]&gt;], [&lt;data.frame[0 x … ## $ link_flair_text &lt;chr&gt; NA, NA, &quot;Misc&quot;, NA, &quot;Pay &amp; Benefits&quot;, &quot;P… ## $ link_flair_text_color &lt;chr&gt; &quot;dark&quot;, &quot;dark&quot;, &quot;dark&quot;, &quot;dark&quot;, &quot;dark&quot;,… ## $ link_flair_type &lt;chr&gt; &quot;text&quot;, &quot;text&quot;, &quot;richtext&quot;, &quot;text&quot;, &quot;ric… ## $ locked &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ media &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ media_embed &lt;df[,0]&gt; &lt;data.frame[26 x 0]&gt; ## $ media_only &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ mod_note &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ mod_reason_by &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mod_reason_title &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ mod_reports &lt;list&gt; [], [], [], [], [], [], [], [], [], [], … ## $ name &lt;chr&gt; &quot;t3_18vmmmu&quot;, &quot;t3_18vn1fo&quot;, &quot;t3_18vox52&quot;… ## $ no_follow &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE… ## $ num_comments &lt;int&gt; 1, 15, 1, 1, 1, 19, 1, 61, 220, 95, 1, … ## $ num_crossposts &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ num_reports &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ over_18 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ parent_whitelist_status &lt;chr&gt; &quot;all_ads&quot;, &quot;all_ads&quot;, &quot;all_ads&quot;, &quot;all_ad… ## $ permalink &lt;chr&gt; &quot;/r/fednews/comments/18vmmmu/historical_… ## $ pinned &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ previous_selftext &lt;chr&gt; &quot;[removed]&quot;, NA, NA, NA, NA, NA, NA, NA,… ## $ pwls &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6… ## $ quarantine &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ removal_reason &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ removed_by &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ removed_by_category &lt;chr&gt; &quot;automod_filtered&quot;, NA, &quot;moderator&quot;, &quot;mo… ## $ report_reasons &lt;list&gt; [], [], [], [], [], [], [], [], [], [], … ## $ retrieved_on &lt;int&gt; 1704074505, 1704075998, 1704083206, 1704… ## $ saved &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ score &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 19, 1, 1, 1, 1, 1,… ## $ secure_media &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ secure_media_embed &lt;df[,0]&gt; &lt;data.frame[26 x 0]&gt; ## $ selftext &lt;chr&gt; &quot;Anyone else here use Mint to track thei… ## $ send_replies &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE… ## $ spoiler &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ stickied &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FA… ## $ subreddit &lt;chr&gt; &quot;fednews&quot;, &quot;fednews&quot;, &quot;fednews&quot;, &quot;fednew… ## $ subreddit_id &lt;chr&gt; &quot;t5_2xy8z&quot;, &quot;t5_2xy8z&quot;, &quot;t5_2xy8z&quot;, &quot;t5_… ## $ subreddit_name_prefixed &lt;chr&gt; &quot;r/fednews&quot;, &quot;r/fednews&quot;, &quot;r/fednews&quot;, &quot;… ## $ subreddit_subscribers &lt;int&gt; 72988, 72990, 73009, 73010, 73027, 73027… ## $ subreddit_type &lt;chr&gt; &quot;public&quot;, &quot;public&quot;, &quot;public&quot;, &quot;public&quot;, … ## $ suggested_sort &lt;chr&gt; &quot;confidence&quot;, &quot;confidence&quot;, &quot;confidence&quot;… ## $ thumbnail &lt;chr&gt; &quot;self&quot;, &quot;self&quot;, &quot;self&quot;, &quot;self&quot;, &quot;self&quot;, … ## $ thumbnail_height &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ thumbnail_width &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ title &lt;chr&gt; &quot;Historical TSP Data + Mint Shutdown&quot;, &quot;… ## $ top_awarded_type &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ total_awards_received &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ treatment_tags &lt;list&gt; [], [], [], [], [], [], [], [], [], [], … ## $ ups &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 19, 1, 1, 1, 1, 1, … ## $ upvote_ratio &lt;dbl&gt; 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00… ## $ url &lt;chr&gt; &quot;https://www.reddit.com/r/fednews/commen… ## $ user_reports &lt;list&gt; [], [], [], [], [], [], [], [], [], [],… ## $ view_count &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ visited &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ whitelist_status &lt;chr&gt; &quot;all_ads&quot;, &quot;all_ads&quot;, &quot;all_ads&quot;, &quot;all_ad… ## $ wls &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, … ## $ link_flair_template_id &lt;chr&gt; NA, NA, &quot;998fe156-1729-11e4-9ea3-12313b0… ## $ media_metadata &lt;df[,1]&gt; &lt;data.frame[26 x 1]&gt; ## $ post_hint &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ preview &lt;df[,2]&gt; &lt;data.frame[26 x 2]&gt; ## $ url_overridden_by_dest &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … Only a few columns are of interest to us. We select those columns in the next chunk, for a sample of 10000 posts. We also take note of how long it takes to read a sample of this size by noting start-time, end.time, and the difference between these. Finally we make the date-time field (created_utc) field “human readable.” posts &lt;- read_lines(&quot;data/r_fednews_posts.jsonl&quot;, n_max = 10000) start.time &lt;- Sys.time() a1 &lt;- stream_in(textConnection( gsub(&quot;\\\\n&quot;,&quot;&quot;, posts))) |&gt; select(author, title, selftext, id, link_flair_text, num_comments, ups, downs, subreddit, created_utc) |&gt; mutate (date = (as_datetime(created_utc))) |&gt; select(-created_utc) ## Found 500 records... Found 1000 records... Found 1500 records... Found 2000 records... Found 2500 records... Found 3000 records... Found 3500 records... Found 4000 records... Found 4500 records... Found 5000 records... Found 5500 records... Found 6000 records... Found 6500 records... Found 7000 records... Found 7500 records... Found 8000 records... Found 8500 records... Found 9000 records... Found 9500 records... Found 10000 records... Imported 10000 records. Simplifying... end.time &lt;- Sys.time() (end.time - start.time) ## Time difference of 7.90551 secs Looks good - the date/time field is clean, the columns are those that we want, and it only took a few seconds to read in 10,000 posts. Let’s read them all in, then take a look at a small random sample - 10 - of the texts of the posts. posts &lt;- read_lines(&quot;data/r_fednews_posts.jsonl&quot;) start.time &lt;- Sys.time() a1 &lt;- stream_in(textConnection( gsub(&quot;\\\\n&quot;,&quot;&quot;, posts))) |&gt; select(author, title, selftext, id, link_flair_text, num_comments, score, subreddit, created_utc) |&gt; mutate (date = (as_datetime(created_utc))) |&gt; select(-created_utc) ## Found 500 records... Found 1000 records... Found 1500 records... Found 2000 records... Found 2500 records... Found 3000 records... Found 3500 records... Found 4000 records... Found 4500 records... Found 5000 records... Found 5500 records... Found 6000 records... Found 6500 records... Found 7000 records... Found 7500 records... Found 8000 records... Found 8500 records... Found 9000 records... Found 9500 records... Found 10000 records... Found 10500 records... Found 11000 records... Found 11500 records... Found 12000 records... Found 12500 records... Found 13000 records... Found 13500 records... Found 14000 records... Found 14500 records... Found 15000 records... Found 15500 records... Found 16000 records... Found 16500 records... Found 17000 records... Found 17500 records... Found 18000 records... Found 18500 records... Found 19000 records... Found 19500 records... Found 20000 records... Found 20500 records... Found 21000 records... Found 21500 records... Found 22000 records... Found 22500 records... Found 23000 records... Found 23500 records... Found 24000 records... Found 24500 records... Found 25000 records... Found 25500 records... Found 26000 records... Found 26500 records... Found 27000 records... Found 27500 records... Found 28000 records... Found 28500 records... Found 29000 records... Found 29500 records... Found 30000 records... Found 30500 records... Found 31000 records... Found 31500 records... Found 32000 records... Found 32500 records... Found 33000 records... Found 33500 records... Found 34000 records... Found 34500 records... Found 35000 records... Found 35500 records... Found 36000 records... Found 36500 records... Found 37000 records... Found 37500 records... Found 38000 records... Found 38500 records... Found 39000 records... Found 39500 records... Found 40000 records... Found 40500 records... Found 41000 records... Found 41500 records... Found 42000 records... Found 42500 records... Found 43000 records... Found 43500 records... Found 44000 records... Found 44500 records... Found 45000 records... Found 45500 records... Found 46000 records... Found 46500 records... Found 47000 records... Found 47500 records... Found 48000 records... Found 48500 records... Found 49000 records... Found 49500 records... Found 50000 records... Found 50500 records... Found 51000 records... Found 51500 records... Found 52000 records... Found 52500 records... Found 53000 records... Found 53500 records... Found 54000 records... Found 54500 records... Found 55000 records... Found 55500 records... Found 56000 records... Found 56500 records... Found 57000 records... Found 57500 records... Found 58000 records... Found 58500 records... Found 59000 records... Found 59500 records... Found 59651 records... Imported 59651 records. Simplifying... end.time &lt;- Sys.time() (end.time - start.time) ## Time difference of 2.655577 mins set.seed(33458) a1 |&gt; select (selftext) |&gt; slice_sample(n=10) ## selftext ## 1 ACA and IRA provided alternate funding sources for several agencies to hire new FTEs. In my agency even when there was a hiring freeze from the general “congress appropriations” hiring budget, we were allowed to bring new people on through the IRA as direct hires. The ACA is also very contentious. If those laws get repealed what happens to those employees? ## 2 [removed] ## 3 [removed] ## 4 I am a supervisor in USDA, more specifically FPAC. I have asked the higher ups for HR guidance on a problem employee (this has gone on for 3-4 years) and have received none. I have sent in numerous emails, detailing running records - the only documentation they have or considered doing anything on said employee was 3 years ago when she assaulted another employee. \\n\\nI have become excellent writing running records, I’ve learned valuable leadership and management skills when they said “maybe I am the one who needed to change.” They continue to fail to take any action. I asked for training on issues that pop up with her and nothing. I felt like I was crazy but visiting with EAP helped, the therapist was able to spot her out and personality quickly. I felt relieved. I try to stay positive but she brings down the whole office. She started bullying the other employees in another department (we’re open concept so they sit near each other) and I’m fed up. \\n\\nCan anyone point me to specific policies/ procedures from FPAC that I can utilize to track her performance and conduct? \\nYes, I keep detailed running records but I don’t know what “counts” that can be written up. \\n\\n*Please note* I am not looking for things to write her up on, employee goes through “cycles” her mood in the office is like a roller coaster, and after working with her for 4 years and 5 years of managing her, I can predict with precision what she will do next. So this outburst she had with the other employees is just the start of what’s to come. ## 5 [removed] ## 6 ## 7 If the Supreme Court overturns Humphrey’s Executor, would all civil service protections be at risk? I understand that principal officers would no longer be protected from presidential whims, but what about the rank and file? Would the overturning of Humphrey’s Executor mean that the Pendleton Act and all subsequent civil service protections are unconstitutional and void? ## 8 Other than sharing how we lost our jobs because of the illegal firings, what should we demand them to do? Argue that we should have a government shutdown next month? What emergency countermeasures, if any, do Democrats have right now that they can use? ## 9 [removed] ## 10 We are NTEU 335, the union representing 1,100 CFPB employees, and we will be rallying at HQ today to protest the attack on the CFPB and its workers, who were locked out of their offices over the weekend. Please join us!\\n\\nWhere: CFPB Headquarters, 1700 G St NW, Washington DC 20552\\n\\nWhen: 4:15 pm, TODAY, February 10\\n\\nWho: YOU!!! These 59651 posts include all of those which were available for the period from January 1, 2024 through March 24, 2025. 21.3 some initial observations There are a few interesting properties of the data: One is that it appears that many of the posts have been removed, including 4 out of this (very small) sample of 10. It would be interesting to compare the proportion of deleted posts to this subreddit with those of other subreddits during the same period. (I would anticipate that this proportion is much higher in this subreddit, likely because of fears of retribution that may or may not be warranted). We should keep in mind that the removed or deleted posts are unlikely to be a random sample of posts on the subreddit - they may be more angry, more polarized, and/or more easily de-anonymized. We should return to this when we consider our results. For now, we will filter out the removed posts. One way to examine this empirically would be to examine the ‘titles’ field. The removed posts still include titles, so the nature of the removed posts in comparison with the non-removed posts could be examined by using the “compare words and categories” approach on the title fields of the two sets. The second is that there are, at least in this small sample, a lot of abbreviations and acronyms - including ACA, IRA, USDA, FPAC, NTEU, CFPB, HQ, and EAP. Often, at this point in an analysis of text, we would reformat the text so that all words were lowercase (with the tolower) function. We’ll instead modify this a bit - retaining capitalization when a token is in ALL CAPS (and is more than a single character like “I” or “A”) - and making into lowercase otherwise. 21.4 preprocessing In this next chunk, we create a new variable called ‘epoch,’ with three values, Biden (during the Biden presidency, but before the November election, Lameduck (the period between the election and Trump’s second inauguration), and Trump. We treat the ‘lameduck’ period separately because, although Biden was President, Trump, and concerns about the coming Trump presidency, was dominant on the airwaves. In these initial comparisons, we’ll exclude that lameduck period from analysis We also take the content of the posts and unravel them, with one line per word (so that a post which had 100 words would be 100 lines in the new, tidy datafile). Call this file tidyCorpus0 (a corpus is a body of text). We then selectively set the text to lowercase (retaining uppercase only when a multi-character token, or word, is in ALL CAPS). This gives us tidyCorpus1. We also remove stopwords from the text in order to simplify and shorten the texts; acknowledging that there are often good reasons not to do this Pennebaker et al. (2014). We’ll remove numbers from the text in the same step. Call this file tidyCorpus2. Finally, we stem the text, reducing each word to its common stem, using the SnowballC library. This gives is tidyCorpus3. If we were to stem the text, we would move ahead with this file rather than tidyCorpus2. For more on stemming, please consider Hvitfeldt and Slige(2021), which is available online. set.seed(33458) a2 &lt;- a1 |&gt; filter (selftext != &quot;[removed]&quot;) |&gt; mutate (epoch = case_when ( date &lt; &quot;2024-11-06 00:00:00&quot; ~ &quot;Biden&quot;, date &gt; &quot;2025-01-20 12:00:00&quot; ~ &quot;Trump&quot;, TRUE ~ &quot;Lameduck&quot; )) tidyCorpus0 &lt;- a2 |&gt; unnest_tokens(word, selftext, to_lower = FALSE) tidyCorpus1 &lt;- tidyCorpus0 |&gt; mutate (word = case_when( # if changing a word to uppercase has no effect ... (toupper(word) == word) &amp; # amd the word is more than 1 character long, keep it as is nchar(word) &gt; 1 ~ word, # else change it to lowercase TRUE ~ tolower(word))) # tidyCorpus2 gets rid of numbers and removes stop words tidyCorpus2 &lt;- tidyCorpus1 |&gt; anti_join(stop_words) |&gt; mutate(word = gsub(x = word, pattern = &quot;[0-9]+|[[:punct:]]|\\\\(.*\\\\)&quot;, replacement = &quot;&quot;)) |&gt; filter (word != &quot;&quot;) # tidyCorpus3 takes the additional step of stemming the data tidyCorpus3 &lt;- tidyCorpus2 |&gt; mutate(word = wordStem(word)) 21.5 comparing the words and stems We reduce each of the the three tidy corpora (bodies of text) to simple frequency tables nwordsCorpus0 &lt;- tidyCorpus0 |&gt; count (word) |&gt; nrow() nwordsCorpus1 &lt;- tidyCorpus1 |&gt; count (word) |&gt; nrow() nwordsCorpus2 &lt;- tidyCorpus2 |&gt; count (word) |&gt; nrow() nwordsCorpus3 &lt;- tidyCorpus3 |&gt; count (word) |&gt; nrow() Whereas there were 58154 words in the initial data, this drops to 49447 when we (mostly) represent all words as lower case. It drops further to 41586 when we remove numbers and stop words, and finally to 29338 when we stem the data. 21.6 Construction of differential word clouds We first take the corpus and reshape it to a matrix with row names (words) and counts (one column for each group). wideCorpus2 &lt;- tidyCorpus2 |&gt; filter (epoch != &quot;Lameduck&quot;) |&gt; count(epoch, word, sort = TRUE) |&gt; pivot_wider(names_from = epoch, values_from = n, values_fill = 0) |&gt; column_to_rownames(var = &quot;word&quot;) |&gt; # filter out words that don&#39;t appear at least a few times in each group # and also removes words that are too rare to be useful for comparison mutate (mincount = pmin(Biden, Trump)) |&gt; filter (mincount &gt; 4) |&gt; select (-mincount) |&gt; as.matrix(rownames = TRUE) head(wideCorpus2,5) ## Trump Biden ## federal 7200 2346 ## employees 6351 1008 ## people 4930 1246 ## https 4801 762 ## im 3975 3281 The comparison.cloud function from the wordcloud package is used to show the greatest relative proportions of words in each group (in that function, the scale parameters will need to be adjusted to get the best visual representation). comparison.cloud(wideCorpus2, scale = c(1.5,.5), max.words=100, random.order=FALSE, rot.per = 0, use.r.layout=FALSE, family=&quot;sans&quot;, title.size=1) Many of the words which appear disproportionately in the Trump corpus (employees, https, federal, government) appear relatively neutral in emotional tone, although there are also terms such as fired and resignation here. On the Biden side, the abbreviation GS (for General Schedule, describing the level of various positions in civil service), together with position, job, HR, step, hours, and supervisor indicates that most posts refer to employment issues. Interesting, but perhaps less than we might have expected. Will an analysis of two-word strings (bigrams) provide more insight? 21.7 bigrams The comparison cloud illustrates differences in single-word occurrences between the Biden and Trump epochs in r/fedworkers. Here, we repeat the analysis, investigating two-word strings. Note that in order to remove stopwords and numbers, we fist combine adjacent words (tokens) into bigrams, then separate these into pairs of adjacent words, filter on each of these adjacent words, and rejoin them. tidyBigrams &lt;- a2 |&gt; filter (epoch != &quot;Lameduck&quot;) |&gt; unnest_tokens(bigram, selftext, token = &quot;ngrams&quot;, n = 2) |&gt; separate(bigram,c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) |&gt; anti_join(stop_words, by = join_by(&quot;word1&quot; == &quot;word&quot;)) |&gt; anti_join(stop_words, by = join_by(&quot;word2&quot; == &quot;word&quot;)) |&gt; mutate(word1 = gsub(x = word1, pattern = &quot;[0-9]+|[[:punct:]]|\\\\(.*\\\\)&quot;, replacement = &quot;&quot;)) |&gt; mutate(word2 = gsub(x = word2, pattern = &quot;[0-9]+|[[:punct:]]|\\\\(.*\\\\)&quot;, replacement = &quot;&quot;)) |&gt; drop_na() |&gt; filter (word1 != &quot;&quot;) |&gt; filter (word2 != &quot;&quot;) |&gt; unite(bigram, word1, word2, sep = &quot; &quot;) bigramCounts &lt;- tidyBigrams |&gt; count(epoch, bigram, sort = TRUE) |&gt; pivot_wider(names_from = epoch, values_from = n, values_fill = 0) |&gt; column_to_rownames(var = &quot;bigram&quot;) |&gt; as.matrix(rownames = TRUE) comparison.cloud(bigramCounts, scale = c(1.5,.5), max.words=100, random.order=FALSE, rot.per = 0, use.r.layout=FALSE, family=&quot;sans&quot;, title.size=1) Here, the language of the two epochs comes into clearer focus, with phrases such as deferred resignation characterizing the Trump epoch and the relatively benign pay period characterizing the Biden period. 21.8 categories of words In my own research, I rely on a proprietary tool, Linguistic Inquiry and Word Count. Though LIWC can be run within R on a machine that has the license for the software, here I run the program externally, then reimport it into R. write_csv(a2, &quot;data/fedworkersposts.csv&quot;) # we run LIWC outside of R here. # the code is run on the raw data files fedworkersLIWC &lt;- read_csv(&quot;data/fedworkersLIWC.csv&quot;) 21.9 Code for assessing LIWC effect sizes between 2 groups There are many measures, or categories, in LIWC. In order to assess the statistical significance of differecnces between the Biden and Trump epochs on LIWC, the simplest approach is to adjust for the number of comparisons using the Bonferroni correction. The number of LIWC categories is 117, so the Bonferroni p-value should be .05/117 = .000427. This chunk uses three different libraries describe function from the psych package to get means, standard deviations, and sample sizes for each LIWC category. Then it computes the effect sizes and associated statistics using the mes function from the compute.es package and makes tables for the LIWC variables with the largest effects. BonferroniP &lt;- (.05/117) # 117 is the number of LIWC categories# simple descriptives BidenEpoch &lt;- fedworkersLIWC |&gt; filter (epoch == &quot;Biden&quot;) |&gt; select(WC:OtherP) |&gt; describe() |&gt; # from the psych package select(BidenN=n, BidenM = mean, BidenSD = sd) |&gt; round(2) TrumpEpoch &lt;- fedworkersLIWC |&gt; filter (epoch == &quot;Trump&quot;) |&gt; select(WC:OtherP) |&gt; describe() |&gt; select(TrumpN=n, TrumpM = mean, TrumpSD = sd) |&gt; round(2) poststats &lt;- BidenEpoch |&gt; bind_cols (TrumpEpoch) # from the compare.es package. Effects &lt;- mes(poststats[,2], poststats[,5], poststats[,3], poststats[,6], poststats[,1],poststats[,4], level = BonferroniP, verbose = FALSE) |&gt; select(d, l.d, u.d, pval.d) LIWCSummary &lt;- poststats |&gt; bind_cols(Effects) |&gt; select(-BidenN, -TrumpN) |&gt; write_csv(&quot;data/LIWCeffects.csv&quot;) Nsignificant &lt;- Effects |&gt; mutate (sig = case_when (pval.d &lt; BonferroniP ~ 1, TRUE ~ 0)) |&gt; summarise(sum = sum(sig)) |&gt; as.numeric() BidenN &lt;- min(poststats$BidenN) TrumpN &lt;- min(poststats$TrumpN) LIWCSummary |&gt; slice_max(d, n = 10) |&gt; kable(caption = &quot;LIWC categories in r/fedworkers most associated with the Biden era&quot;) |&gt; kable_styling() %&gt;% add_footnote(paste0( &quot;Note: BidenNs &gt;= &quot;, min(poststats$BidenN), &quot; TrumpNs &gt;= &quot;, min(poststats$TrumpN))) Table 21.1: Table 21.2: LIWC categories in r/fedworkers most associated with the Biden era BidenM BidenSD TrumpM TrumpSD d l.d u.d pval.d Authentic 72.81 31.07 52.05 36.62 0.59 0.59 0.59 0 i 6.33 4.44 3.68 4.64 0.58 0.58 0.58 0 Tone 41.36 29.32 31.28 28.63 0.35 0.35 0.35 0 Dic 89.69 7.90 86.08 12.16 0.33 0.33 0.33 0 function 54.47 10.41 50.53 15.62 0.28 0.28 0.28 0 money 1.55 2.50 0.91 2.23 0.28 0.28 0.28 0 Linguistic 69.16 10.82 65.14 16.25 0.27 0.27 0.27 0 conj 6.46 3.26 5.52 3.79 0.26 0.26 0.26 0 time 5.33 4.21 4.23 4.36 0.26 0.26 0.26 0 achieve 1.67 2.02 1.23 1.87 0.23 0.23 0.23 0 Lifestyle 6.82 4.34 5.75 4.81 0.23 0.23 0.23 0 a Note: BidenNs &gt;= 9010 TrumpNs &gt;= 19068 LIWCSummary |&gt; slice_min(d, n = 10) |&gt; kable(caption = &quot;LIWC categories in r/fedworkers most associated with the Trump era&quot;) |&gt; kable_styling() %&gt;% add_footnote(paste0( &quot;Note: BidenNs &gt;= &quot;, min(poststats$BidenN), &quot; TrumpNs &gt;= &quot;, min(poststats$TrumpN))) Table 21.1: Table 21.1: LIWC categories in r/fedworkers most associated with the Trump era BidenM BidenSD TrumpM TrumpSD d l.d u.d pval.d Clout 23.33 28.67 42.22 33.36 -0.59 -0.59 -0.59 0 Culture 1.26 2.39 2.70 4.10 -0.40 -0.40 -0.40 0 we 0.46 1.29 1.14 2.34 -0.33 -0.33 -0.33 0 tone_neg 1.03 2.05 2.05 3.73 -0.31 -0.31 -0.31 0 tech 0.56 1.68 1.44 3.23 -0.31 -0.31 -0.31 0 Social 8.76 5.96 10.92 7.60 -0.30 -0.30 -0.30 0 socrefs 4.93 4.43 6.47 5.37 -0.30 -0.30 -0.30 0 affiliation 1.13 2.31 1.91 3.02 -0.28 -0.28 -0.28 0 power 1.63 2.55 2.48 3.29 -0.28 -0.28 -0.28 0 emo_neg 0.56 1.47 1.28 3.13 -0.27 -0.27 -0.27 0 a Note: BidenNs &gt;= 9010 TrumpNs &gt;= 19068 LIWCSummary |&gt; arrange(desc(d)) |&gt; head(10) |&gt; kable() BidenM BidenSD TrumpM TrumpSD d l.d u.d pval.d Authentic 72.81 31.07 52.05 36.62 0.59 0.59 0.59 0 i 6.33 4.44 3.68 4.64 0.58 0.58 0.58 0 Tone 41.36 29.32 31.28 28.63 0.35 0.35 0.35 0 Dic 89.69 7.90 86.08 12.16 0.33 0.33 0.33 0 function 54.47 10.41 50.53 15.62 0.28 0.28 0.28 0 money 1.55 2.50 0.91 2.23 0.28 0.28 0.28 0 Linguistic 69.16 10.82 65.14 16.25 0.27 0.27 0.27 0 conj 6.46 3.26 5.52 3.79 0.26 0.26 0.26 0 time 5.33 4.21 4.23 4.36 0.26 0.26 0.26 0 achieve 1.67 2.02 1.23 1.87 0.23 0.23 0.23 0 There are just over 19000 posts in the (roughly four month long) Trump epoch, and just over 9000 in the (roughly ten months long) Biden epoch. Judging from the number of posts, the r/fedworkers subreddit has been substantially more active since the Trump presidency began. In comparisons of the two “epochs,” the LIWC categories which were particularly characteristic of the Biden posts included Authentic, a summary variable indicating apparent honesty or genuineness, and I, including first-person singular pronouns (Ryan L. Boyd et al. 2022). For each of these, the effect size (difference in standard deviations between the two distributions), was more than .5, indicating a moderate effect. The LIWC categories which were particularly characteristic of the Trump posts included Clout, a summary category of words associated with leadership or status, followed by Culture, which includes language about politics, technology, and ethnicity, and we, second-person singular pronouns). The shift from the first-person singular to plural is what we might expect in a community of individuals subjectively under siege. Altogether, of the 117 LIWC Categories, 98 showed a statistically significant difference between the two epochs. These analyses support the argument presented in the Times article referenced above, including that the r/fedworkers subreddit had grown, in the Trump epoch, into a supportive community (shift from i to we), in which workers were sharing anxieties (LIWC categories such as emo_neg and tone_neg). 21.10 exercise: what would you do next? This chapter has provided a brief example of how to work with text from Reddit. If you were to work with these data, what would you do next? If you were to work with another dataset, what would it look like, what would you expect to find, and how would you study it? References Boyd, Ryan L, Ashwini Ashokkumar, Sarah Seraj, and James W Pennebaker. 2022. “The Development and Psychometric Properties of LIWC-22.” Austin, TX: University of Texas at Austin 10: 1–47. Hvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. Chapman and Hall/CRC. Pennebaker, James W., Cindy K. Chung, Joey Frazee, Gary M. Lavergne, and David I. Beaver. 2014. “When Small Words Foretell Academic Success: The Case of College Admissions Essays.” Edited by Qiyong Gong. PLoS ONE 9 (12): e115844. https://doi.org/10/f6z8q5. Reddit is an electronic discussion board or website which is structured as a set of communities or subreddits; each subreddit includes posts, and, typically, submitted by users (redditors).↩︎ The selection of posts rather than comments is somewhat arbitrary; one reason for examining posts is that the simple number of comments in the period under study would be relatively unwieldy.↩︎ "],["an-introduction-to-networks.html", "22 an introduction to networks 22.1 key network concepts", " 22 an introduction to networks “No man is an island, entire of itself; every man is a piece of the continent, a part of the main.” -John Donne. Networks are everywhere. For engineers, networks are the structures that underlie power grids and transportation systems. For epidemiologists, networks describe the paths through which infectious diseases spread. For social scientists, networks are a key unit of analysis in understanding the structure of social groups. Phenomena as diverse as altruism (Apicella et al. 2012), kidney donations (Nikzad et al. 2021), and obesity (Christakis and Fowler 2007) can be understood, in part, as network phenomena. Networks can be nested. The internet can be seen as a network of machines linked by cables and various cellular and satellite technologies. Social network applications such as X and Instagram operate within these networks, and your own network of acquaintances In this chapter, we briefly introduce some key network concepts. We then consider how network structures can be examined and understood in the tidyverse. In the next chapter, we will consider a case study in network analysis, examining a bibliometric network structure of a scholarly discipline. 22.1 key network concepts Networks involve two types of objects nodes (or vertices) and edges (or links). Networks may be directed or undirected. Edges in a directed network might describe relationships such as likes, follows, has heard of, cites, or clicks on. We can represent them by arrows between nodes. Edges in an undirected network are symmetrical, as in the case of sharing a common property or ancestor. Undirected relationships are marked by phrases such as are siblings, are allies, know each other, are married, have been cited by the same source, and have both been exposed to. We represent undirected networks by lines without arrowheads. Unrequited love is a directed relationship, requited or reciprocal love can be modeled as undirected one. Paths are series of connected edges between nodes. Networks may be unsigned (only positive links) or signed (with positive and negative links). In some networks, there is just one type of node. In addition to these single-mode networks, we will also encounter bipartite (two-mode) networks. A network of enrollments, for example, might include two types of nodes (e.g., classes and students). Edges in such a network describe relationships such as is enrolled in. In bipartite networks, there can only be links between different types of nodes (so classes can’t be enrolled in classes for example). 22.1.1 a simple example: networks and balance theory Consider an undirected, signed network consisting of just three nodes and the edges between them. The nodes might represent, for example, persons (Harary 1959) or countries (Estrada 2019). Relationships between the nodes might include things such as gets along with/does not get along with, trusts/doesn’t trust, etc. For networks (graphs) such as these, what can we say about the case where there are three negative edges? Two? One? Zero? Which ones seem to ‘make sense’ and might be expected to be stable over time? A recent NY Times article illustrates how the complex geopolitical situation in the contemporary Middle East can be modeled using a simple signed network. 22.1.2 centrality Within networks, nodes differ in centrality, connectedness, or importance. There are different measures and types of centrality - for example, the number (degree) of a node describes its number of connections. In directed networks, we can further distinguish between indegree centrality, for incoming links, and outdegree centrality, for outgoing ones. On some social media platforms, this might be described by ‘followers’ and ‘is following’, respectively. Other measures of centrality gauge the importance of a node by the number of other nodes it connects (betweenness centrality). The most interesting measures of centrality, eigenvector centrality and PageRank, are recursive. That is, a node is central or important to the extent that the nodes to which it is linked are themselves central or important. PageRank was the original foundation of modern search engine on the Web. (The “Page” in PageRank does not refer to Webpage, but to Larry Page, one of the founders (with Surgey Brin) of Google (Page et al. 1999). 22.1.3 components and communities Networks vary in their size (e.g., big vs small), number of discrete components (zero for the empty set, one for a network in which all nodes are directly or indirectly connected, more than this otherwise), and also in their community structure or clustering. Whereas the number of components in a network is relatively unambiguous, communities in networks are often fuzzy or difficult to discern, in much the same way that constellations in the night sky might seem arbitrarily ‘drawn.’ Just as there are many notions of centrality, there are also many approaches to thinking about communities. For example, we might use a “top-down” approach in which we begin with all of the nodes in a network and cleave them into two groups that both (a) minimize within-group distances and (b) maximize between-group distances. We could then repeat this strategy until we met some criterion. This approach gives a tree-like picture of community structure. Alternatively, we can use a “bottom-up” approach to thinking about categories - combining the two closest nodes into a single community, then repeating this as appropriate. Communities in which all members are directly connected to all others are referred to as cliques. In the bottom-up approach, unlike the top-down approach, there are typically left-over nodes which are not connected to any community. In some applications of the bottom-up approach, communities are allowed to overlap, giving rise to complex structures such as that illustrated previously in Figure 20.1. 22.1.4 another example: it’s a small world Consider a bipartite network in which there are two types of nodes, students and classes. An edge in this network represents a particular student who is enrolled in a particular class. Make a sketch of this network for you and a friend who is not in this class. Is there a path between the two of you? How about if we add another friend - or ten friends - to the network? What do we notice about the component structure of the network and the paths between nodes as the network grows bigger? (Milgram 1967). In data science, it’s generally easiest to begin with very small datasets and to work towards bigger ones. With networks, this isn’t always the case, because very small networks may be structurally more complex than larger ones. 22.1.5 static and dynamic networks Finally, we can distinguish between static networks (assessed at just one moment in time) and dynamic ones, which change. In dynamic networks, several important phenomena can be seen. One of these is contagion - not just of diseases, but also of emotions (Kelly, Iannone, and McCarty 2016) and a range of socially desirable and undesirable phenomena (Christakis and Fowler 2013). Another phenomena seen in dynamic networks is preferential attachment in which the most important nodes become still more important over time (Watts 2004). Preferential attachment is reflected in many aspects of contemporary culture; we see it when a handful of restaurants or products ‘go viral’ while others struggle to attract customers. I believe that the acceleration of inequality of wealth and power in contemporary society is, in part, a product of the connectedness of the network in which we live, an unfortunate byproduct of the digital age. ba &lt;- play_barabasi_albert(n = 3, power = 1, appeal_zero = 0, growth = 1, directed = FALSE) |&gt; ggraph(layout = &quot;kk&quot;) + geom_node_point(aes(size = 1, color = &quot;BLUE&quot;)) + geom_edge_link(alpha = 0.5) + theme_graph() + theme(legend.position = &quot;none&quot;) bb &lt;- play_barabasi_albert(n = 10, power = 1, appeal_zero = 0, growth = 1, directed = FALSE) |&gt; ggraph(layout = &quot;kk&quot;) + geom_node_point(aes(size = 1, color = &quot;BLUE&quot;)) + geom_edge_link(alpha = 0.5) + theme_graph() + theme(legend.position = &quot;none&quot;) bc &lt;- play_barabasi_albert(n = 20, power = 1, appeal_zero = 0, growth = 1, directed = FALSE) |&gt; ggraph(layout = &quot;kk&quot;) + geom_node_point(aes(size = 1, color = &quot;BLUE&quot;)) + geom_edge_link(alpha = 0.5) + theme_graph() + theme(legend.position = &quot;none&quot;) plot_grid (ba, bb, bc, nrow = 1, ncol = 3) + draw_figure_label(&quot;Preferential attachment: As networks grow, there is increasing inequality in centrality (degree)&quot;) #if needed, install.packages(&quot;&quot;) library(tidyverse) library(kableExtra) # for tables library(bibliometrix) # for generating network from bib data &amp; reducing to one mode #library(qgraph) # for drawing simple graphs library(tidygraph) # for representing graphs as matrices of nodes/edges library(igraph) # needed by tidygraph library(ggraph) References Apicella, Coren L., Frank W. Marlowe, James H. Fowler, and Nicholas A. Christakis. 2012. “Social Networks and Cooperation in Hunter-Gatherers.” Nature 481 (7382): 497–501. https://doi.org/10/fz3v4v. Christakis, Nicholas A., and James H. Fowler. 2007. “The Spread of Obesity in a Large Social Network over 32 Years.” N Engl J Med 357: 3709. https://doi.org/10/dmrgt6. ———. 2013. “Social Contagion Theory: Examining Dynamic Social Networks and Human Behavior.” Statistics in Medicine 32 (4): 556–77. https://doi.org/10/ck2j. Estrada, Ernesto. 2019. “Rethinking Structural Balance in Signed Social Networks.” Discrete Applied Mathematics 268 (September): 70–90. https://doi.org/10.1016/j.dam.2019.04.019. Harary, Frank. 1959. “On the Measurement of Structural Balance.” Behavioral Science 4 (4): 316–23. https://doi.org/10/cp9nfp. Kelly, Janice R., Nicole E. Iannone, and Megan K. McCarty. 2016. “Emotional Contagion of Anger Is Automatic: An Evolutionary Explanation.” British Journal of Social Psychology 55 (1): 182–91. https://doi.org/10/gf6mn3. Milgram, Stanley. 1967. “The Small World Problem.” Psychology Today 2 (1): 6067. Nikzad, Afshin, Mohammad Akbarpour, Michael A. Rees, and Alvin E. Roth. 2021. “Global Kidney Chains.” Proceedings of the National Academy of Sciences of the United States of America 118 (36): e2106652118. https://doi.org/10.1073/pnas.2106652118. Page, Lawrence, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. “The PageRank Citation Ranking: Bringing Order to the Web.” Watts, Duncan J. 2004. “The \"New\" Science of Networks.” Annual Review of Sociology 3: 243–70. https://www.jstor.org/stable/29737693. "],["case-study-the-network-structure-of-computational-social-science.html", "23 case study: the network structure of computational social science", " 23 case study: the network structure of computational social science Bibliometric networks are models of the structure of scholarly disciplines. There are a variety of methods to developing such networks. We’ll use an approach which is both simple and powerful, and that is to examine shared references or bibliographic couplings. In such a network, papers will be strongly linked if they have many references in common, and distant from each other when they share no or few references. We begin with a collection of records of research articles that we have downloaded from the Web of Science, using the phrase “computational social science” as a search term. (A video on YouTube illustrates each step of this process). A second video accompanies the code below, which was originally developed for the Summer Institute of Computational Social Science held in Jupiter, Florida in 2023. After loading the needed libraries, we begin with a dataframe consisting of a list of records downloaded from the Web of Science. These are all papers which included the phrase ‘computational social science,’ as downloaded on 6/5/2023. The data include a ‘source’ field and a set of cited references as well as a bunch of other fields (metadata). # for large jobs, set to TRUE to run this on only one reference file debugging &lt;- FALSE dataDir &lt;- &quot;data/CompSocSci2023&quot; filenames &lt;- list.files(dataDir, pattern = &quot;*.txt&quot;, full.names = TRUE) if (debugging == TRUE) {filenames = filenames[1]} biblioDF &lt;- data.frame(as.list(filenames)) %&gt;% convert2df() ## ## Converting your wos collection into a bibliographic dataframe ## ## Done! ## ## ## Generating affiliation field tag AU_UN from C1: Done! biblioDF %&gt;% rownames_to_column(&quot;source&quot;) %&gt;% select(source,CR) %&gt;% head(1) %&gt;% kable(caption = &quot;Cited refs in one document&quot;) %&gt;% kable_styling() Table 23.1: Table 23.2: Cited refs in one document source CR HOX JJ, 2017, METHODOLOGY-EUR ATTEWELL P., 2015, DATA MINING SOCIAL S;AXELROD R, 1981, SCIENCE, V211, P1390, DOI 10.1126/SCIENCE.7466396;BOND RM, 2012, NATURE, V489, P295, DOI 10.1038/NATURE11421;CAMSTRA A, 1992, SOCIOL METHOD RES, V21, P89, DOI 10.1177/0049124192021001004;CIOFFI-REVILLA CLAUDIO., 2014, INTRO COMPUTATIONAL;DAAS P., 2012, NYENR S BIG DAT;DAWSON R. J. M., 1995, J STAT ED, V3;DONOHO D., 2015, 50 YEARS DATA SCI;FOSTER I., 2016, BIG DATA SOCIAL SCI;FRED MORSTATTER, 2013, IS SAMPLE GOOD ENOUG;GHANI R, 2017, STAT SOC BEHAV SCI, P147;GINSBERG J, 2009, NATURE, V457, P1012, DOI 10.1038/NATURE07634;GONCALVES-SA J., 2015, 2015 WINT S COMP SOC;GRANOVETTER MS, 1973, AM J SOCIOL, V78, P1360, DOI 10.1086/225469;GRISHENKO A., 2014, TWITTER ARCHITECTU 1;IBM, 2016, 4 VS BIG DAT;KOLLANYI B., 2016, 2016219 COMPROP;LANEY D., 2001, 3 D DATA MANAGEMENT, DOI DOI 10.1016/J.INFSOF.2008.09.005;LAZER D, 2014, SCIENCE, V343, P1203, DOI 10.1126/SCIENCE.1248506;LOEHLIN JC, 1965, J PERS SOC PSYCHOL, V2, P580, DOI 10.1037/H0022457;MADDEN M., 2013, TEENS SOCIAL MEDIA P;ONNELA JP, 2007, P NATL ACAD SCI USA, V104, P7332, DOI 10.1073/PNAS.0610245104;OREILLY RC, 2000, COMPUTATIONAL EXPLOR;PHELPS S., 2012, EMERGENCE SOCIAL NET, DOI 10.2139/SSRN.2109553, DOI 10.2139/SSRN.2109553;PRESENTI J., 2015, 5 NEW SERVICES EXPAN;ROBERTS M. E., 2016, COMPUTATIONAL SOCIAL, P51;SILBERZAHN R., 2015, CROWDSOURCING DATA A;TUKEY JW, 1962, ANN MATH STAT, V33, P1, DOI 10.1214/AOMS/1177704711;TUKEY JW, 1977, EXPLORATORY DATA ANA;VO H, 2017, STAT SOC BEHAV SCI, P125;WEBB EJ, 1966, UNOBTRUSIVE MEASURES;ZHU XIAOJIN, 2009, INTRO SEMISUPERVISED, DOI 10.2200/S00196ED1V01Y200906AIM006 23.0.1 from citation network to structural network A citation network is a type of two-mode or bipartite network which consists of source papers, referenced papers, and the directed edges which link a subset of these. Here’s a fragment of our citation network, first as a list of citations (edges), then as a graph showing links between the two types of vertices or nodes (i.e., source papers in red, cited papers in green). Notice that the edges are directed (e.g., Hox 2017 cites Lazer 2014). # illustration of a small fragment of the citation network someSources &lt;- biblioDF %&gt;% # take the first four sources slice (1:4) smallBiblioDF &lt;- someSources %&gt;% # make them into a rectangular source * citation matrix cocMatrix(Field = &quot;CR&quot;, sep = &quot;;&quot;) %&gt;% as.matrix() %&gt;% # then a list of edges reshape2::melt() %&gt;% filter (value &gt; 0) %&gt;% rename(source = 1, citation = 2) %&gt;% select(-value) %&gt;% group_by(citation) %&gt;% # then choose only papers cited more than once filter(n()&gt;1) %&gt;% ungroup smallBiblioDF %&gt;% kable (caption = &quot;A small citation network: Edge list of papers cited &gt; 1 time in four sources in CSS&quot;) %&gt;% kable_styling() Table 23.3: Table 23.4: A small citation network: Edge list of papers cited &gt; 1 time in four sources in CSS source citation TORNBERG P, 2021, BIG DATA SOC CENTOLA D 2010 SCIENCE ZHANG J, 2020, BIG DATA RES CENTOLA D 2010 SCIENCE TORNBERG P, 2021, BIG DATA SOC CONTE R 2012 EUR PHYS J-SPEC TOP BALTAR R, 2021, SIMBIOTICA CONTE R 2012 EUR PHYS J-SPEC TOP TORNBERG P, 2021, BIG DATA SOC EDELMANN A 2020 ANNU REV SOCIOL BALTAR R, 2021, SIMBIOTICA EDELMANN A 2020 ANNU REV SOCIOL TORNBERG P, 2021, BIG DATA SOC KITCHIN R 2014 BIG DATA SOC BALTAR R, 2021, SIMBIOTICA KITCHIN R 2014 BIG DATA SOC TORNBERG P, 2021, BIG DATA SOC KRAMER ADI 2014 P NATL ACAD SCI USA ZHANG J, 2020, BIG DATA RES KRAMER ADI 2014 P NATL ACAD SCI USA TORNBERG P, 2021, BIG DATA SOC LAZER D 2009 SCIENCE ZHANG J, 2020, BIG DATA RES LAZER D 2009 SCIENCE HOX JJ, 2017, METHODOLOGY-EUR LAZER D 2014 SCIENCE ZHANG J, 2020, BIG DATA RES LAZER D 2014 SCIENCE TORNBERG P, 2021, BIG DATA SOC LAZER DMJ 2020 SCIENCE BALTAR R, 2021, SIMBIOTICA LAZER DMJ 2020 SCIENCE # plotted using qgraph smallgraph &lt;- smallBiblioDF %&gt;% qgraph::qgraph(labels = T, label.cex = 4, vTrans = 30, border.color = &quot;white&quot;, edge.width = 4, color = c(2,2,2,2,3,3,3,3,3,3,3,3,3), title = &quot;A small citation network: Common cites among four papers&quot;) The two-mode citation network can be collapsed to one of two one-mode networks. A co-citation network is one which extracts citations (green nodes), and links them based on the number of times that they are cited by the same authors. We will look instead at bibliometric couplings, or links between the (pink) source papers. This will be a single mode, weighted, undirected network, with source papers as vertices and the number of shared references as edge weights. Here’s a graph showing the collapsed network based on just the papers cited above: smallOneModeMatrix &lt;- biblioNetwork(someSources, analysis = &quot;coupling&quot;, network = &quot;references&quot;, sep = &quot;;&quot;) %&gt;% # Normalize adjusts weights; Salton algorithm takes shared refs / product of geometric means of n refs. This is what I have always done. normalizeSimilarity(type = &quot;salton&quot;) %&gt;% as.matrix() %&gt;% round(2) diag(smallOneModeMatrix) &lt;- 0 smallOneModeMatrix %&gt;% kable(caption = &quot;A small structural network: Matrix showing shared references among 4 papers&quot;) %&gt;% kable_styling() Table 23.5: Table 23.6: A small structural network: Matrix showing shared references among 4 papers HOX JJ, 2017 TORNBERG P, 2021 ZHANG J, 2020 BALTAR R, 2021 HOX JJ, 2017 0.00 0.00 0.01 0.00 TORNBERG P, 2021 0.00 0.00 0.02 0.05 ZHANG J, 2020 0.01 0.02 0.00 0.00 BALTAR R, 2021 0.00 0.05 0.00 0.00 Based on the structure of shared references, Baltar and Hox appear to be quite different, distant from each other. (But don’t trust the result too much - small networks like this aren’t particularly stable). Regardless, this is a beginning of an understanding of the structure of scholarship in computational social science. 23.0.2 looking at the whole citation network The bibliometrix package can quickly give summary statistics for citation networks. Here are some characteristics of the entire set of papers. twoModeStats &lt;- biblioAnalysis(biblioDF)#, sep = &quot;;&quot;) twoModeSummary &lt;- twoModeStats |&gt; summary() # summary returns a list. look at it. ## ## ## MAIN INFORMATION ABOUT DATA ## ## Timespan 1999 : 2023 ## Sources (Journals, Books, etc) 310 ## Documents 794 ## Annual Growth Rate % 18.76 ## Document Average Age 5.71 ## Average citations per doc 21.14 ## Average citations per year per doc 2.273 ## References 35298 ## ## DOCUMENT TYPES ## article 676 ## article; book chapter 3 ## article; data paper; early access 1 ## article; early access 45 ## article; proceedings paper 2 ## book review 7 ## book review; early access 1 ## correction 5 ## editorial material 28 ## editorial material; book chapter 1 ## letter 1 ## letter; early access 1 ## review 22 ## review; early access 1 ## ## DOCUMENT CONTENTS ## Keywords Plus (ID) 1400 ## Author&#39;s Keywords (DE) 2228 ## ## AUTHORS ## Authors 2055 ## Author Appearances 2555 ## Authors of single-authored docs 136 ## ## AUTHORS COLLABORATION ## Single-authored docs 149 ## Documents per Author 0.386 ## Co-Authors per Doc 3.22 ## International co-authorships % 32.87 ## ## ## Annual Scientific Production ## ## Year Articles ## 1999 1 ## 2000 1 ## 2002 2 ## 2005 2 ## 2006 2 ## 2008 2 ## 2009 2 ## 2010 3 ## 2011 7 ## 2012 10 ## 2013 13 ## 2014 21 ## 2015 26 ## 2016 38 ## 2017 49 ## 2018 78 ## 2019 82 ## 2020 83 ## 2021 133 ## 2022 177 ## 2023 62 ## ## Annual Percentage Growth Rate 18.76 ## ## ## Most Productive Authors ## ## Authors Articles Authors Articles Fractionalized ## 1 CIOFFI-REVILLA C 12 CIOFFI-REVILLA C 6.10 ## 2 MOAT HS 12 BAIL CA 3.95 ## 3 PREIS T 12 O&#39;BRIEN DT 3.83 ## 4 PENTLAND A 9 MOAT HS 3.70 ## 5 BAIL CA 8 PREIS T 3.70 ## 6 FERRARA E 7 FARRELL J 3.00 ## 7 JUNGHERR A 7 ITO N 2.92 ## 8 ITO N 6 JUNGHERR A 2.92 ## 9 JURGENS P 6 SASAHARA K 2.67 ## 10 KOSINSKI M 6 MA J 2.64 ## ## ## Top manuscripts per citations ## ## Paper DOI TC TCperYear NTC ## 1 LAZER D, 2009, SCIENCE 10.1126/science.1167742 1825 107.4 1.98 ## 2 KOSINSKI M, 2013, P NATL ACAD SCI USA 10.1073/pnas.1218772110 1183 91.0 8.08 ## 3 LUKE S, 2005, SIMUL-T SOC MOD SIM 10.1177/0037549705058073 584 27.8 2.00 ## 4 WU YY, 2015, P NATL ACAD SCI USA 10.1073/pnas.1418680112 469 42.6 8.70 ## 5 BAIL CA, 2018, P NATL ACAD SCI USA 10.1073/pnas.1804840115 467 58.4 14.43 ## 6 VARGO CJ, 2018, NEW MEDIA SOC 10.1177/1461444817712086 228 28.5 7.04 ## 7 FARRELL J, 2016, P NATL ACAD SCI USA 10.1073/pnas.1509433112 221 22.1 5.17 ## 8 WANG Y, 2018, J PERS SOC PSYCHOL 10.1037/pspa0000098 218 27.2 6.73 ## 9 CHANG RM, 2014, DECIS SUPPORT SYST 10.1016/j.dss.2013.08.008 196 16.3 3.61 ## 10 CONTE R, 2012, EUR PHYS J-SPEC TOP 10.1140/epjst/e2012-01697-8 182 13.0 4.07 ## ## ## Corresponding Author&#39;s Countries ## ## Country Articles Freq SCP MCP MCP_Ratio ## 1 USA 307 0.3906 238 69 0.225 ## 2 UNITED KINGDOM 72 0.0916 41 31 0.431 ## 3 CHINA 51 0.0649 29 22 0.431 ## 4 GERMANY 49 0.0623 32 17 0.347 ## 5 JAPAN 42 0.0534 34 8 0.190 ## 6 ITALY 33 0.0420 16 17 0.515 ## 7 NETHERLANDS 20 0.0254 8 12 0.600 ## 8 SWITZERLAND 18 0.0229 13 5 0.278 ## 9 CANADA 17 0.0216 11 6 0.353 ## 10 AUSTRALIA 16 0.0204 7 9 0.562 ## ## ## SCP: Single Country Publications ## ## MCP: Multiple Country Publications ## ## ## Total Citations per Country ## ## Country Total Citations Average Article Citations ## 1 USA 8628 28.10 ## 2 UNITED KINGDOM 3051 42.38 ## 3 ITALY 861 26.09 ## 4 GERMANY 732 14.94 ## 5 CHINA 420 8.24 ## 6 NETHERLANDS 331 16.55 ## 7 SWITZERLAND 304 16.89 ## 8 DENMARK 249 24.90 ## 9 KOREA 232 25.78 ## 10 SPAIN 227 18.92 ## ## ## Most Relevant Sources ## ## Sources Articles ## 1 JOURNAL OF COMPUTATIONAL SOCIAL SCIENCE 199 ## 2 PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA 30 ## 3 EPJ DATA SCIENCE 21 ## 4 SOCIAL SCIENCE COMPUTER REVIEW 21 ## 5 ROYAL SOCIETY OPEN SCIENCE 14 ## 6 JASSS-THE JOURNAL OF ARTIFICIAL SOCIETIES AND SOCIAL SIMULATION 11 ## 7 BIG DATA &amp; SOCIETY 9 ## 8 NATURE HUMAN BEHAVIOUR 9 ## 9 PLOS ONE 9 ## 10 SCIENTIFIC REPORTS 8 ## ## ## Most Relevant Keywords ## ## Author Keywords (DE) Articles Keywords-Plus (ID) Articles ## 1 COMPUTATIONAL SOCIAL SCIENCE 367 MODEL 43 ## 2 SOCIAL MEDIA 91 SCIENCE 43 ## 3 BIG DATA 62 BEHAVIOR 42 ## 4 MACHINE LEARNING 60 SOCIAL MEDIA 42 ## 5 TWITTER 47 COMMUNICATION 37 ## 6 NATURAL LANGUAGE PROCESSING 38 BIG DATA 36 ## 7 SOCIAL NETWORKS 29 DYNAMICS 36 ## 8 COVID-19 28 MEDIA 36 ## 9 AGENT-BASED MODELING 26 TWITTER 35 ## 10 DATA SCIENCE 21 NETWORKS 34 twoModeSummary |&gt; pluck(1) |&gt; unlist() |&gt; kable( caption = &quot;Citation network: Key features&quot;) %&gt;% kable_styling() Table 23.7: Table 23.8: Citation network: Key features x MAIN INFORMATION ABOUT DATA Timespan 1999 : 2023 Sources (Journals, Books, etc) 310 Documents 794 Annual Growth Rate % 18.76 Document Average Age 5.71 Average citations per doc 21.14 Average citations per year per doc 2.273 References 35298 DOCUMENT TYPES article 676 article; book chapter 3 article; data paper; early access 1 article; early access 45 article; proceedings paper 2 book review 7 book review; early access 1 correction 5 editorial material 28 editorial material; book chapter 1 letter 1 letter; early access 1 review 22 review; early access 1 DOCUMENT CONTENTS Keywords Plus (ID) 1400 Author’s Keywords (DE) 2228 AUTHORS Authors 2055 Author Appearances 2555 Authors of single-authored docs 136 AUTHORS COLLABORATION Single-authored docs 149 Documents per Author 0.386 Co-Authors per Doc 3.22 International co-authorships % 32.87 Here’s a manually-constructed table of the “most cited journals.” sourceJournals &lt;- biblioDF %&gt;% citations(field = &quot;article&quot;, sep = &quot;;&quot;) %&gt;% # output of above is a list... pluck gets the first element # which is a table of sources pluck(3) %&gt;% as_tibble() %&gt;% rename(CitedJournal = 1) sourceJournals %&gt;% count(CitedJournal) %&gt;% arrange(desc(n)) %&gt;% head(10) %&gt;% kable (caption = &quot;Most cited journals in Comp Soc Sci&quot;) %&gt;% kable_styling() Table 23.9: Table 23.10: Most cited journals in Comp Soc Sci CitedJournal n PLOS ONE 425 P NATL ACAD SCI USA 388 NA 319 SCIENCE 250 AM SOCIOL REV 227 J PERS SOC PSYCHOL 224 NATURE 223 AM J SOCIOL 182 LECT NOTES COMPUT SC 159 SCI REP-UK 152 23.0.2.1 the need to inspect/clean data One drawback of bibliometrix and packages like it is that they remove us somewhat from the data. Here, ‘NA’ is the third most common source of papers. We look more closely at these records to see if there is a problem. Here’s one way… # there are no papers where the cited field is empty or NA # so this gets us nowhere # sourceJournals %&gt;% # filter(CitedJournal == &quot;&quot; | # CitedJournal == &quot;NA&quot; ) set.seed(33458) countsonly &lt;- sourceJournals %&gt;% group_by(CitedJournal) %&gt;% count(CitedJournal) countNA &lt;- countsonly %&gt;% filter(is.na(CitedJournal)) %&gt;% as_tibble %&gt;% select(n) %&gt;% as.integer() sources &lt;- sourceJournals %&gt;% left_join(countsonly) biblioDF %&gt;% citations(field = &quot;article&quot;, sep = &quot;;&quot;) %&gt;% pluck(1) %&gt;% as_tibble() %&gt;% bind_cols(sources) %&gt;% rename(papercites = 2, journalcites = 4) %&gt;% filter(journalcites == countNA) %&gt;% select(CR) %&gt;% sample_n(20) %&gt;% kable (caption = &quot;Random sample of references with source journal coded as NA&quot;) %&gt;% kable_styling() Table 23.11: Table 23.12: Random sample of references with source journal coded as NA CR ANONYMOUS, REPRESENTING SOCIAL THE UNITED NATIONS OFFICE FOR DISASTER, DISASTER STATISTICS ANONYMOUS, ALLSIDES MED BIAS RA U.S. DEPARTMENT OF HEALTH AND HUMAN SERVICES ADMINISTRATION FOR CHILDREN AND FAMILIES ADMINISTRATION ON CHILDREN YOUTH AND FAMILIES CHILDREN’S BUREAU (2021), CHILD MALTREATMENT WORLD BANK, POPULATION DENSITY ANONYMOUS, PARTITIONING PREDICT WHO, WHO COR COVID 19 DAS ADRIAN M., TERADATA MAGAZINE ANONYMOUS, 10 INT AAAI C WEB SO ROMIR, DYN HAPP IND RUSS WO MACARTHUR A., LIFEWIRE 0206 NATIONAL AUDUBON SOCIETY, CHRISTM BIRD COUNT ARTEFACT GROUP, ETH EXPL PACK NAI J., J PERSONALITY SOCIAL CRANDALL D., P 14 ACM SIGKDD INT HARTFORD, FINANCIAL TIMES STEPHENS N, PALGRAVE COMMUNICATI BHARAT K, GOOGLE NEWS COVERAGE TERRA DOTTA, TACKLING GENDER GAP STATISTA, MOST POPS SOC NETW W These NA appear to be mostly detritus, cites to anonymous sources, etc. There are better ways of cleaning the data (including restricting the set of papers to articles and reviews), but we’ll ignore these for now. 23.0.3 the structural network: Centrality and community structure We now reduce the two mode network to a one-mode for the whole dataset, normalizing the edge weights as before, and setting the diagonal entries to 0. We then use the tidygraph package to compute several measures of centrality and community structure. # random seed to ensure reproducible results # esp for centrality and community analyses set.seed(33458) oneModeMatrix &lt;- biblioNetwork(biblioDF, analysis = &quot;coupling&quot;, network = &quot;references&quot;, sep = &quot;;&quot;) %&gt;% normalizeSimilarity(type = &quot;salton&quot;) %&gt;% as.matrix() str(oneModeMatrix) ## num [1:794, 1:794] 1 0 0.0128 0 0 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:794] &quot;HOX JJ, 2017&quot; &quot;TORNBERG P, 2021&quot; &quot;ZHANG J, 2020&quot; &quot;BALTAR R, 2021&quot; ... ## ..$ : chr [1:794] &quot;HOX JJ, 2017&quot; &quot;TORNBERG P, 2021&quot; &quot;ZHANG J, 2020&quot; &quot;BALTAR R, 2021&quot; ... # save it as a graph for igraph/tidygraph oneModeGraph &lt;- oneModeMatrix %&gt;% graph_from_adjacency_matrix(mode = &quot;undirected&quot;, diag = FALSE, weighted = TRUE) # tidygraph allows us to look at graphs # as data frames of nodes and edges tidyBibGraph &lt;- oneModeGraph %&gt;% as_tbl_graph() %&gt;% # we activate nodes to assign new measures for each node activate(nodes) %&gt;% mutate(centralPR = centrality_pagerank(weights = weight)) %&gt;% mutate(nodePRRank = rank(-centralPR)) %&gt;% # mutate(centralWD = centrality_degree(weights = weight)) %&gt;% mutate(central0D = centrality_degree(weights = NULL)) %&gt;% mutate(communityLouv = group_louvain(weights = weight)) %&gt;% mutate(group = as.factor(group_louvain())) %&gt;% # mutate(communityWalk = group_walktrap(weights = weight)) %&gt;% mutate(ID = row_number()) edgeList &lt;- tidyBibGraph %&gt;% activate(edges) %&gt;% as_tibble() nodeList &lt;- tidyBibGraph %&gt;% activate(nodes) %&gt;% as_tibble() 23.0.4 visualizing the network I use ggraph to try a few visualizations within R - it plays nicely with tidygraph. In the first visualization, all but the 22 isolates are plotted. It suggests that there is a community structure, but nothing beyond this. nNodesToPlot &lt;- 794 minEdgeWeightToPlot &lt;- .00001 library (ggraph) tidyBibGraph %&gt;% activate(nodes) %&gt;% filter(central0D &gt; 0) %&gt;% filter(nodePRRank &lt; nNodesToPlot) %&gt;% activate(edges) %&gt;% filter(weight &gt; minEdgeWeightToPlot) %&gt;% ggraph(layout = &#39;stress&#39;) + geom_edge_link(alpha = .1) + geom_node_point(aes(size = (centralPR), color = as.factor(communityLouv))) + # geom_node_label(aes(label = name, # color = as.factor(communityLouv))) + theme(legend.position = &quot;none&quot;) In the second plot, I show just the top 20 nodes, but also include labels for these. nNodesToPlot &lt;- 20 minEdgeWeightToPlot &lt;- .05 tidyBibGraph %&gt;% activate(nodes) %&gt;% filter(nodePRRank &lt; nNodesToPlot) %&gt;% activate(edges) %&gt;% filter(weight &gt; minEdgeWeightToPlot) %&gt;% ggraph(layout = &#39;stress&#39;) + geom_edge_link(alpha = .1) + geom_node_point(aes(size = (centralPR), color = as.factor(communityLouv))) + geom_node_label(aes(label = name, color = as.factor(communityLouv))) + theme(legend.position = &quot;none&quot;) The graph shows papers in four communities, three of which are represented by just one or two papers. Note that some of the papers are odd - a book review, for example. nodeInfo &lt;- biblioDF %&gt;% mutate(ID = row_number()) %&gt;% select(AB, TI, DE, ID, J9) %&gt;% # ID is a text field of keywords in the bib data # I combine it with the other text fields, then # create a new ID of text number to combine with # the node centrality etc mutate(alltext = paste(TI, AB, DE, ID, sep = &quot; &quot;)) %&gt;% select(-AB, -DE, -ID) %&gt;% mutate(ID = row_number()) allNodeInfo &lt;- nodeList %&gt;% # filter(nodePRRank &lt; 21) %&gt;% left_join(nodeInfo, by = &#39;ID&#39;) nodeList %&gt;% filter(nodePRRank &lt; 21) %&gt;% left_join(nodeInfo) %&gt;% select (name, nodePRRank, communityLouv, TI) %&gt;% arrange(communityLouv, nodePRRank) %&gt;% kable(caption = &quot;Top papers in CSS by PR and community&quot;) %&gt;% kable_styling() Table 23.13: Table 23.14: Top papers in CSS by PR and community name nodePRRank communityLouv TI KEUSCHNIGG M, 2018 3 1 ANALYTICAL SOCIOLOGY AND COMPUTATIONAL SOCIAL SCIENCE KOSINSKI M, 2016 12 1 MINING BIG DATA TO EXTRACT PATTERNS AND PREDICT REAL-LIFE OUTCOMES BADHAM J, 2020 1 2 INTRODUCTION TO COMPUTATIONAL SOCIAL SCIENCE, 2ND EDITION PREIS T, 2014 4 2 ADAPTIVE NOWCASTING OF INFLUENZA OUTBREAKS USING GOOGLE SEARCHES BOTTA F, 2015-1 5 2 QUANTIFYING CROWD SIZE WITH MOBILE PHONE AND TWITTER DATA CEBRIAN M, 2011 6 2 ENGINEERING TRADE-OFFS IN SOCIAL ORGANIZATION: THE BEGINNINGS OF A COMPUTATIONAL SOCIAL SCIENCE MANN A, 2016 7 2 COMPUTATIONAL SOCIAL SCIENCE BAIL CA, 2017-1 8 2 TAMING BIG DATA: USING APP TECHNOLOGY TO STUDY ORGANIZATIONAL BEHAVIOR ON SOCIAL MEDIA LAZER D, 2017 10 2 DATA EX MACHINA: INTRODUCTION TO BIG DATA LETCHFORD A, 2015 11 2 THE ADVANTAGE OF SHORT PAPER TITLES GUBA K, 2018 13 2 BIG DATA IN SOCIOLOGY: NEW DATA, NEW SOCIOLOGY? STELLA M, 2018 14 2 BOTS INCREASE EXPOSURE TO NEGATIVE AND INFLAMMATORY CONTENT IN ONLINE SOCIAL SYSTEMS CURME C, 2014 15 2 QUANTIFYING THE SEMANTICS OF SEARCH BEHAVIOR BEFORE STOCK MARKET MOVES JUNGHERR A, 2017-1 16 2 THE EMPIRICIST’S CHALLENGE: ASKING MEANINGFUL QUESTIONS IN POLITICAL SCIENCE IN THE AGE OF BIG DATA DE ZUNIGA 2017 18 2 CITIZENSHIP, SOCIAL MEDIA, AND BIG DATA: CURRENT AND FUTURE RESEARCH IN THE SOCIAL SCIENCES THEOCHARIS Y, 2021 19 2 COMPUTATIONAL SOCIAL SCIENCE AND THE STUDY OF POLITICAL COMMUNICATION PREIS T, 2013 20 2 QUANTIFYING THE DIGITAL TRACES OF HURRICANE SANDY ON FLICKR WENG LL, 2013 2 3 VIRALITY PREDICTION AND COMMUNITY STRUCTURE IN SOCIAL NETWORKS COTA W, 2019 17 3 QUANTIFYING ECHO CHAMBER EFFECTS IN INFORMATION SPREADING OVER POLITICAL COMMUNICATION NETWORKS CONTE R, 2016 9 5 TOWARDS COMPUTATIONAL AND BEHAVIORAL SOCIAL SCIENCE 23.0.5 exploring the communities One way to see and understand this community structure is to explore the datausing an interactive network tool (Gephi). In order to do this, we’ll write the nodes and edges to disk in two separate writes. Another way to understand the communities is to compare the language of different communities, using the combined text fields. That is explored in a second r markdown script; CSS2_NetworkToText.Rmd. allNodeInfo %&gt;% write_csv(file.path(dataDir, &quot;CSSNodesText.csv&quot;)) edgeList %&gt;% rename(source = from, target = to) %&gt;% write_csv(file.path(dataDir, &quot;CSSEdges.csv&quot;)) "],["some-ethical-concerns-for-the-data-scientist.html", "24 some ethical concerns for the data scientist 24.1 ethics and personality harvesting 24.2 the law of unintended consequences 24.3 your privacy is my concern 24.4 who should hold the digital keys? 24.5 contact-tracing and COVID-19 24.6 the digital divide 24.7 still more case studies 24.8 some potential remedies 24.9 technology, change, and risk", " 24 some ethical concerns for the data scientist The Latin word scientia is commonly translated as both “science” and “knowledge.” Modern science or modern knowledge has led not only to dramatic improvements in public health and human life expectancy, but also to weapons of war that allow the slaughter of millions at a distance. The idea that science or knowledge is ethically fraught is not new: Adam and Eve are said to have been cast out of the Garden of Eden after tasting the forbidden fruit from the tree of knowledge (Shattuck 1997). And, as we have seen in our discussion of the “most dangerous equation,” the lack of knowledge can also be dangerous - perhaps particularly today, as we are in the grips of a pandemic with leaders who govern by intuition rather than science, by hunch rather than data (Howard Wainer 2007). In this final chapter, I consider a few case studies which exemplify some of the ethical issues and concerns that frame data science. Note the word “frame” - this is little more than the beginning of a scaffolding. But I hope that it is sufficient to bolster the claim that ethical concerns are or should be at the foundation of data science, and that individuals with training in the liberal arts (and not just math and engineering) should play an important role. 24.1 ethics and personality harvesting In personality psychology, concerns about how personality tests might invade privacy have been raised for over fifty years. Today, these concerns are newly relevant. One issue is measurement and experimentation without consent (Gleibs 2014). Here, the most dramatic and arguably consequential example of this occurred when Cambridge Analytica scraped the online activity of individuals whose friends had participated in a study of personality, then leveraged this further to assess the personality of all or nearly all American adults. Cambridge Analytica, Facebook, and other data-rich firms such as Experian engage in practices such as “shadow profiling,” which is the nonconsensual profiling of individuals through their social connections (Garcia et al. 2018). This profiling leads to the selective tailoring of messages (ads, political appeals) which, at the very least, attenuates freedom of choice. At a societal level, it can lead to manipulations of consumer and political behavior, including voting decisions (Bond et al. 2012; Matz et al. 2017). Tailored messaging can not only harm our social fabric, deepening partisan divides among us, but can have consequences for social institutions as well. In a recent paper, several of us expressed our concern that the harvesting of personality information is likely to become more common in the years to come (Ryan L. Boyd, Pasca, and Lanning 2020). This argument derives from a series of claims, which are listed below: It takes very little data to identify someone. We can often discover the identity of an individual from a very small number of data points (Sweeney 2005). There is a great deal of data available. For example, credit card purchases, Uber trips, exercise routes tracked on platforms such as Strava, turnpike itineraries, medical records - are becoming increasingly digital, and so the potential for linking data increases. Combining data leads to new value. As we have seen this term, there can be meaningful value, new knowledge, when different datasets are joined. There are few constraints on data collection and synthesis. The collection of digital data is typically undertaken by private companies motivated by profit, who are often unconstrained by ethical concerns that might be raised by, for example, human subjects review boards. Information is valuable to a range of parties - from potential employers to insurance companies to potential romantic partners. For these and other reasons, we anticipate that there will be a growing market for data about our personalities (Wu 2019). 24.2 the law of unintended consequences One of the recurring issues in digital ethics has been the so-called “law of unintended consequences” (Merton 1936). When we initiate a new policy, or collect a new dataset, or give permission to a social media application to share our information, or investigate our family tree using a service such as Ancestry.com or 23andMe.com, we do not know what will happen with the information that we share or create. For example, assume that Fred signs up to learn about his family and health on 23andMe. He finds that he has a health vulnerability with an already known, or even a soon-to-be-discovered, genetic predisposition. That information has consequences not just for him but, potentially, for his offspring. If that genetic information is shared, it takes little in the way of a dystopian imagination to consider that Fred ’s grandchildren might have to pay higher health insurance premiums, be prohibited from migrating to certain other countries, be seen as less desirable partners, etc. These examples might seem extreme, but unintended consequences are typically unforeseeable. For example, when the good folks at Netflix shared some of their movie-preference data and offered a million-dollar prize to anyone who could substantially improve on their recommendation-engine, they did not intend to “out” individuals including a closeted lesbian mom who was identified by several data scientists (Narayanan and Shmatikov 2008). 24.3 your privacy is my concern It is not paradoxical that I should be concerned with your (right to) privacy. I’m invested in your ability to choose what to reveal about yourself, when, and to whom, in part because I want you to be able to live in and contribute to the world. Even if I am not that concerned about having details of my own life revealed, I must be sympathetic to your concerns. The concern for privacy is not a fetish, but is critical if we are to live and work in a just and decent society. 24.4 who should hold the digital keys? Let’s shift gears somewhat, and consider some of the issues surrounding autonomous vehicles. In a data-dependent world, who should be the guardians of the code that connects us? To consider just one example, the computer systems in modern cars typically run millions of lines of code. As cars become increasingly autonomous, this complexity will only increase. (Incidentally, the Society of Automotive Engineers, or SAE, describes 6 levels of ‘auto autonomy.’ At this writing, the most sophisticated systems available to consumers, such as Tesla Autopilot, are at level 2. What lies ahead are cars which are self-driving on carefully selected, geo-fenced roads, and ultimately cars “which can operate on any road… a human driver could negotiate”). Our roads and highways will become an Internet of Vehicles (IOV), which will include not just connections between cars and an intelligent cloud ‘above us’ but also direct links between a distributed system of intelligent cars, stoplights, and road sensors in a fog ‘around us’ (Bonomi et al. 2012). Fog computing and the IOV will reduce travel times and increase both fuel efficiency and automotive safety. Obviously, there are cybersecurity concerns. While the prospects for a chaotic, choreographed hack of hundreds of vehicles on the streets of Manhattan, such as that in the 2017 movie “The Fate of the Furious”, are remote at best (or worst), there have been examples of “white-hat hackers” who have successfully infiltrated (and thereby helped secure) car information systems. As the IOV develops, there will be vulnerabilities to privacy as well as safety, and the security of the system will be paramount. Different car manufacturers are taking different approaches to developing secure information systems, with many using a closed or proprietary approach. But the scope of the problem is so large that there is a movement towards pooling resources and encouraging collaboration among industry partners, academics, and citizen scientists in the development of an open-source autonomous driving platform, such as Apollo. Perhaps counterintuitively, there may be significant security advantages to using source code that is open to all (Clarke, Dorwin, and Nash 2009; FitzGerald, Levin, and Parziale 2016). 24.5 contact-tracing and COVID-19 On April 10, 2020, Google and Apple announced that they would collaborate on a contact-tracing system to try and slow the COVID-19 pandemic. Keeping in mind the ethical issues of (a) the law of unintended consequences, (b) prior failures to maintain anonymity, (c) arguments that data should be best secured by industry, government, or crowdsourcing, as well as the tech issues of (d) the potential methods for contact tracing, and (e) the costs of the pandemic to public health and to world economies, to what extent should digital tracking be used to assess and limit the spread of the novel coronavirus? You can learn more about the Google-Apple collaboration at their FAQ and this piece at techcrunch. Consider some of the ethical issues raised in this essay at fivethirtyeight.com, and in this more recent piece at the Verge. 24.6 the digital divide Not all are benefiting equally from the birth of the digital age. Unfortunately, the “Matthew-effect,” by which resources flow to those who have them most and need them less, is a fundamental property of networked, complex systems. As we become more interconnected, the gap between rich and poor is accelerating. The primary path to a successful life is to find a “scalable” occupation - that is, one in which you can serve many people with little effort. But that means that fewer will “serve.” So yes, spend your summer trying to become a social media influencer. Better still, make a commitment to working to try to reduce the growing digital divide, and more broadly to address issues of equality and social justice in your own applications of data science. Google, which once had the mantra “don’t be evil” in its core of conduct, has now moved to a set of positive (do’s) as well as negative (don’ts) guidelines for their work in Artificial Intelligence. Information can be empowering (for those who have it). 24.7 still more case studies As we face ubiquitous observation as our world becomes an Internet of Things, and live in a world in which decisions will increasingly be made for us by applications of machine learning and artifical intelligence, new questions will be raised about the social impact of our science. Some of these are illustrated in these six hypothetical case studies proposed by an interdisciplinary team at Princeton. They warrant your consideration. 24.8 some potential remedies In the European Union, the General Data Protection Regulation (GDPR) has been in effect since 2018, and provides guidelines for protecting people and personal data in the digital age. In a related piece, (Loukides 2018) provided a briefer set of guidelines. They argued that it is not enough that people provide consent, but also that there must be clarity: That it, it’s not enough that people agree to share their data, they must also understand what they are agreeing to. Other issues that they highlight include the need for data security (as stolen data often include, for example, SSNs and passwords), and protection of vulnerable populations such as children. Finally, they provide a checklist for those developing data products: ❏ Have we listed how this technology can be attacked or abused? ❏ Have we tested our training data to ensure it is fair and representative? ❏ Have we studied and understood possible sources of bias in our data? ❏ Does our team reflect diversity of opinions, backgrounds, and kinds of thought? ❏ What kind of user consent do we need to collect to use the data? ❏ Do we have a mechanism for gathering consent from users? ❏ Have we explained clearly what users are consenting to? ❏ Do we have a mechanism for redress if people are harmed by the results? ❏ Can we shut down this software in production if it is behaving badly? ❏ Have we tested for fairness with respect to different user groups? ❏ Have we tested for disparate error rates among different user groups? ❏ Do we test and monitor for model drift to ensure our software remains fair over time? ❏ Do we have a plan to protect and secure user data? 24.9 technology, change, and risk A zero-sum enterprise (or game) is one in which for every winner and for every gain, there is a loser and loss, and vice-versa. Gambling is zero-sum. So too is speculation in bitcoin and other cryptocurrencies. Innovations in science and industry are not zero-sum, for new inventions and methods can lead to change in the bottom line. Human interaction is also often not zero-sum, for they can lead to synergies of feeling and cognition, the blossoming of friendship and love and the creation of new knowledge through dialog and collaboration. When we think of these non-zero-sum games, we are likely to think of these scenarios where there is progress, or an overall gain as when a new construction material leads to the production of better and cheaper homes and shelters to improve the human condition. But it can also be negative, as when the invention of a new herbicide leads to the production of dioxins and related toxins. Data science is a non-zero-sum game. As previously discussed, it is a relatively easy to combine two datasets in new ways to create new knowledge. But the cost of this knowledge may be great. References Bond, Robert M., Christopher J. Fariss, Jason J. Jones, Adam D. I. Kramer, Cameron Marlow, Jaime E. Settle, and James H. Fowler. 2012. “A 61-Million-Person Experiment in Social Influence and Political Mobilization.” Nature 489 (7415): 295–98. https://doi.org/10/f3689v. Bonomi, Flavio, Rodolfo Milito, Jiang Zhu, and Sateesh Addepalli. 2012. “Fog Computing and Its Role in the Internet of Things.” In Proceedings of the First Edition of the MCC Workshop on Mobile Cloud Computing, 13–16. ACM. https://doi.org/10/gft9b9. Boyd, Ryan L., Paola Pasca, and Kevin Lanning. 2020. “The Personality Panorama: Conceptualizing Personality Through Big Behavioural Data: The Personality Panorama.” Edited by John Rauthmann. European Journal of Personality, April. https://doi.org/10/gg4t8j. Clarke, Russell, David Dorwin, and Rob Nash. 2009. “Is Open Source Software More Secure?” Homeland Security/Cyber Security. FitzGerald, Ben, Peter L Levin, and Jacqueline Parziale. 2016. Open Source Software &amp; the Department of Defense. Center for a New American Security. Garcia, David, Mansi Goel, Amod Kant Agrawal, and Ponnurangam Kumaraguru. 2018. “Collective Aspects of Privacy in the Twitter Social Network.” EPJ Data Science 7: 1–13. https://doi.org/10/cjhr. Gleibs, Ilka H. 2014. “Turning Virtual Public Spaces into Laboratories: Thoughts on Conducting Online Field Studies Using Social Network Sites.” Analyses of Social Issues and Public Policy 14 (1): 352–70. https://doi.org/10/f6t7gd. Loukides, Hilary, Mike. 2018. Ethics and Data Science. O’Reilly. Matz, S. C., M. Kosinski, G. Nave, and D. J. Stillwell. 2017. “Psychological Targeting as an Effective Approach to Digital Mass Persuasion.” Proceedings of the National Academy of Sciences 114 (48): 12714–19. https://doi.org/10.1073/pnas.1710966114. Merton, Robert K. 1936. “The Unanticipated Consequences of Purposive Social Action.” American Sociological Review 1 (6): 894–904. https://doi.org/10/fjg8hf. Narayanan, Arvind, and Vitaly Shmatikov. 2008. “Robust De-anonymization of Large Sparse Datasets.” In 2008 IEEE Symposium on Security and Privacy (Sp 2008), 111–25. Oakland, CA, USA: IEEE. https://doi.org/10.1109/SP.2008.33. Shattuck, Roger. 1997. Forbidden Knowledge: From Prometheus to Pornography. Houghton Mifflin Harcourt. Sweeney, Latanya. 2005. “Privacy-Enhanced Linking.” ACM SIGKDD Explorations Newsletter 7 (2): 72–75. https://doi.org/10/bjvpjh. Wainer, Howard. 2007. “The Most Dangerous Equation.” American Scientist 95 (3): 249. https://doi.org/10.1511/2007.65.249. Wu, Tim. 2019. “How Capitalism Betrayed Privacy.” The New York Times, 5. "],["exercises-3.html", "25 exercises 25.1 generating correlated data (GPA and SAT) and putting it in a Google Sheet 25.2 now look at your data in the spreadsheet 25.3 using AI to help us here 25.4 Getting data from a package like MDSR", " 25 exercises 25.1 generating correlated data (GPA and SAT) and putting it in a Google Sheet It’s simple to generate a single random variable, but we often want to examine data for several variables that are correlated. The MVRnorm function, in the MASS package, does this. We use this to generate two variables with mean 0 and standard deviation 1, a given correlation, and a given sample size (number of rows). We then fiddle with these variables, setting the means, standard deviations, ranges, and number of significant digits, to make them look like GPA and SAT scores. And we use the randomnames package to make up fake names to go along with the fake scores. Finally, we upload the data to a Google Sheet. This last command will require first establishing permissions for R to read and write to your Google Drive. knitr::opts_chunk$set(echo = TRUE) library(tidyverse) library(babynames) # for set of first names library(randomNames) # for initial set of last names library(googlesheets4) # to read and write to Google sheets set.seed(33458) # for reproducibility correlation &lt;- 0.4 nrows &lt;- 1000 names &lt;- randomNames(nrows, which.names = &quot;first&quot;) %&gt;% bind_cols(randomNames(nrows, which.names = &quot;last&quot;)) %&gt;% rename (FirstName = 1, LastName = 2) hsdata &lt;- MASS::mvrnorm(n = nrows, mu = c(0.0, 0), Sigma = matrix( c(1, correlation, correlation, 1), nrow = 2, ncol = 2)) %&gt;% as_tibble() %&gt;% select (gpa = 1, sat = 2) %&gt;% mutate (sat = 500 + (100 * round(sat, 1))) %&gt;% mutate (gpa = 3 + round(gpa, 2)) %&gt;% mutate (gpa = case_when( gpa &lt; 0 ~ 0, gpa &gt; 4.5 ~ 4.5, TRUE ~ gpa)) %&gt;% mutate (sat = case_when( sat &lt; 200 ~ 200, gpa &gt; 800 ~ 800, TRUE ~ sat)) hsdata &lt;- names %&gt;% bind_cols(hsdata) %&gt;% arrange(FirstName) #gs4_create(name=&quot;RandomHSGPASAT&quot;, # sheets = hsdata) slice(hsdata, 1:3) ## # A tibble: 3 × 4 ## FirstName LastName gpa sat ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aaliyah Guttadore 0.72 510 ## 2 Aaqil Beltran 2.72 400 ## 3 Aaqil Robbe 0.75 400 25.2 now look at your data in the spreadsheet In this spreadsheet, what are the means for GPA and SAT? What are the standard deviations? what is the difference between ‘sort range’ and ‘sort sheet’ in Google Sheets? What do you think about the randomnames package? Do the names appear “representative” (of what)? If you wanted to generate a more representative sample of names, how might you proceed? 25.3 using AI to help us here Our intuitions about the idiosyncratic nature of names in the randomNames package is supported (not necessarily confirmed) by MS Copilot, which notes that “the randomNames package in R samples from a uniform distribution, which doesn’t reflect real-world name frequencies.” For first names, one solution is to use the babynames data, and then to weight the probability of including the name in our sample by the actual frequency in (some) population: # load babynames and Filter for a specific year or range names_data &lt;- babynames %&gt;% filter(year &gt;= 2000) %&gt;% group_by(name) %&gt;% summarise(freq = sum(n)) %&gt;% ungroup() # Sample names with probability proportional to frequency set.seed(33458) sampled_first_names &lt;- sample(names_data$name, size = nrows, replace = TRUE, prob = names_data$freq) |&gt; as_tibble() |&gt; rename(NewFirstName = 1) slice(sampled_first_names, 1:10) ## # A tibble: 10 × 1 ## NewFirstName ## &lt;chr&gt; ## 1 Tea ## 2 Emma ## 3 Olivia ## 4 Noemi ## 5 Ruger ## 6 Madelyn ## 7 Jeffrey ## 8 Caleb ## 9 Audrey ## 10 Elizabeth 25.3.1 for last names, it’s a little bit trickier For last names, we’ll also use census data. There’s not an R package that has these, but there is Census data that is available in a zip file. It’s described at https://www2.census.gov/topics/genealogy/2010surnames/surnames.pdf. There are two files that are available - the top 1000 names, and then a (much longer) file of all names that occur more than 100 times. We’ll use the latter. Why do we do this? Our approach to downloading a zip file directly into R follows: First, we specify a temporary file, then download the zip file into it. Then we unzip this and save it in a subdirectory called ‘data.’ Finally, we read it into R: temp_zip_file &lt;- tempfile(fileext = &quot;.zip&quot;) sourcefile &lt;- &quot;https://www2.census.gov/topics/genealogy/2010surnames/names.zip&quot; download.file(sourcefile, destfile = temp_zip_file, mode = &quot;wb&quot;) unzip(temp_zip_file, files = &quot;Names_2010Census.csv&quot;, exdir = &quot;data&quot;) surnames &lt;- read.csv(&quot;data/Names_2010Census.csv&quot;) |&gt; arrange(rank) slice(surnames, 1:10) ## name rank count prop100k cum_prop100k pctwhite pctblack pctapi pctaian pct2prace pcthispanic ## 1 ALL OTHER NAMES 0 29312001 9936.97 9936.97 66.65 8.53 7.97 0.86 2.32 13.67 ## 2 SMITH 1 2442977 828.19 828.19 70.9 23.11 0.5 0.89 2.19 2.4 ## 3 JOHNSON 2 1932812 655.24 1483.42 58.97 34.63 0.54 0.94 2.56 2.36 ## 4 WILLIAMS 3 1625252 550.97 2034.39 45.75 47.68 0.46 0.82 2.81 2.49 ## 5 BROWN 4 1437026 487.16 2521.56 57.95 35.6 0.51 0.87 2.55 2.52 ## 6 JONES 5 1425470 483.24 3004.80 55.19 38.48 0.44 1 2.61 2.29 ## 7 GARCIA 6 1166120 395.32 3400.12 5.38 0.45 1.41 0.47 0.26 92.03 ## 8 MILLER 7 1161437 393.74 3793.86 84.11 10.76 0.54 0.66 1.77 2.17 ## 9 DAVIS 8 1116357 378.45 4172.31 62.2 31.6 0.49 0.82 2.45 2.44 ## 10 RODRIGUEZ 9 1094924 371.19 4543.50 4.75 0.54 0.57 0.18 0.18 93.77 25.3.2 cleaning these up In the lastname data, there’s a problem. Some 10% of the names in the data - presumably rare ones - are replaced with “ALL OTHER NAMES.” We’ll just filter these out. Finally, we combine the new NewFirstName and NewLastName with our original HS data. We also replace the SHOUTING (uppercase) in the NewLastName variable with Title Case: Did we do this right? Any other issues? How might you have done this better? surnames &lt;- read.csv(&quot;data/Names_2010Census.csv&quot;) |&gt; filter(name != &quot;ALL OTHER NAMES&quot;) slice(surnames, 1:10) ## name rank count prop100k cum_prop100k pctwhite pctblack pctapi pctaian pct2prace pcthispanic ## 1 SMITH 1 2442977 828.19 828.19 70.9 23.11 0.5 0.89 2.19 2.4 ## 2 JOHNSON 2 1932812 655.24 1483.42 58.97 34.63 0.54 0.94 2.56 2.36 ## 3 WILLIAMS 3 1625252 550.97 2034.39 45.75 47.68 0.46 0.82 2.81 2.49 ## 4 BROWN 4 1437026 487.16 2521.56 57.95 35.6 0.51 0.87 2.55 2.52 ## 5 JONES 5 1425470 483.24 3004.80 55.19 38.48 0.44 1 2.61 2.29 ## 6 GARCIA 6 1166120 395.32 3400.12 5.38 0.45 1.41 0.47 0.26 92.03 ## 7 MILLER 7 1161437 393.74 3793.86 84.11 10.76 0.54 0.66 1.77 2.17 ## 8 DAVIS 8 1116357 378.45 4172.31 62.2 31.6 0.49 0.82 2.45 2.44 ## 9 RODRIGUEZ 9 1094924 371.19 4543.50 4.75 0.54 0.57 0.18 0.18 93.77 ## 10 MARTINEZ 10 1060159 359.40 4902.90 5.28 0.49 0.6 0.51 0.22 92.91 # then pull out a weighted sample of last names sampled_last_names &lt;- sample(surnames$name, size = nrows, replace = TRUE, prob = surnames$count) |&gt; as_tibble() |&gt; rename(NewLastName = 1) hsdata2 &lt;- hsdata |&gt; bind_cols(sampled_first_names) |&gt; bind_cols(sampled_last_names) |&gt; mutate(NewLastName = stringr::str_to_title(NewLastName)) slice(hsdata2, 1:10) ## # A tibble: 10 × 6 ## FirstName LastName gpa sat NewFirstName NewLastName ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Aaliyah Guttadore 0.72 510 Tea Macdonald ## 2 Aaqil Beltran 2.72 400 Emma Tornberg ## 3 Aaqil Robbe 0.75 400 Olivia Pedraja ## 4 Aaron Taylor 3.93 470 Noemi Carter ## 5 Aaron Baca 3.5 610 Ruger Roberts ## 6 Aaron Johnston 2 360 Madelyn Alicea ## 7 Aasim Warner 4.43 600 Jeffrey Tucker ## 8 Aasima Kim 3.41 430 Caleb Anderson ## 9 Aasima Lee 4.46 440 Audrey Medina ## 10 Aasiya Dillard 3.57 450 Elizabeth Vazquez 25.4 Getting data from a package like MDSR library(tidyverse) library(mdsr) x &lt;- data(package = &quot;mdsr&quot;) x &lt;- x$results |&gt; as_tibble() |&gt; select(Item, Title) "],["references.html", "References", " References Anscombe, FJ. 1973. “Graphs in Statistical Analysis.” American Statistician 27 (1): 17–21. Apicella, Coren L., Frank W. Marlowe, James H. Fowler, and Nicholas A. Christakis. 2012. “Social Networks and Cooperation in Hunter-Gatherers.” Nature 481 (7382): 497–501. https://doi.org/10/fz3v4v. Baker, Monya. 2016. “Is There a Reproducibility Crisis?” Nature 533: 26. Benjamin, Daniel J., James O. Berger, Magnus Johannesson, Brian A. Nosek, E.-J. Wagenmakers, Richard Berk, Kenneth A. Bollen, et al. 2018. “Redefine Statistical Significance.” Nature Human Behaviour 2 (1): 6–10. https://doi.org/10/cff2. Blumenthal, Arthur L. 1975. “A Reappraisal of Wilhelm Wundt.” American Psychologist 30 (11): 1081. Bond, Robert M., Christopher J. Fariss, Jason J. Jones, Adam D. I. Kramer, Cameron Marlow, Jaime E. Settle, and James H. Fowler. 2012. “A 61-Million-Person Experiment in Social Influence and Political Mobilization.” Nature 489 (7415): 295–98. https://doi.org/10/f3689v. Bonomi, Flavio, Rodolfo Milito, Jiang Zhu, and Sateesh Addepalli. 2012. “Fog Computing and Its Role in the Internet of Things.” In Proceedings of the First Edition of the MCC Workshop on Mobile Cloud Computing, 13–16. ACM. https://doi.org/10/gft9b9. Boyd, Ryan L, Ashwini Ashokkumar, Sarah Seraj, and James W Pennebaker. 2022. “The Development and Psychometric Properties of LIWC-22.” Austin, TX: University of Texas at Austin 10: 1–47. Boyd, Ryan L., Paola Pasca, and Kevin Lanning. 2020. “The Personality Panorama: Conceptualizing Personality Through Big Behavioural Data: The Personality Panorama.” Edited by John Rauthmann. European Journal of Personality, April. https://doi.org/10/gg4t8j. Broman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989. Buckheit, Jonathan B., and David L. Donoho. 1995. “Wavelab and Reproducible Research.” In Wavelets and Statistics, 55–81. Springer. Christakis, Nicholas A., and James H. Fowler. 2007. “The Spread of Obesity in a Large Social Network over 32 Years.” N Engl J Med 357: 3709. https://doi.org/10/dmrgt6. ———. 2013. “Social Contagion Theory: Examining Dynamic Social Networks and Human Behavior.” Statistics in Medicine 32 (4): 556–77. https://doi.org/10/ck2j. Clarke, Russell, David Dorwin, and Rob Nash. 2009. “Is Open Source Software More Secure?” Homeland Security/Cyber Security. Cleveland, William S., and Robert McGill. 1985. “Graphical Perception and Graphical Methods for Analyzing Scientific Data.” Science, New Series 229 (4716): 828–33. https://www.jstor.org/stable/1695272. Cox, Jonathan, and Michael Lindell. 2013. “Visualizing Uncertainty in Predicted Hurricane Tracks.” International Journal for Uncertainty Quantification 3 (2). https://doi.org/10/gjjsfw. Deane, Claudia. 2024. “Americans’ Deepening Mistrust of Institutions.” Donoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734. Donoho, David L. 2010. “An Invitation to Reproducible Computational Research.” Biostatistics 11 (3): 385–88. https://doi.org/10/bxwkns. Erdelyi, Matthew H. 1974. “A New Look at the New Look: Perceptual Defense and Vigilance.” Psychological Review 81 (1): 1–25. https://doi.org/10/cs5c5q. Estrada, Ernesto. 2019. “Rethinking Structural Balance in Signed Social Networks.” Discrete Applied Mathematics 268 (September): 70–90. https://doi.org/10.1016/j.dam.2019.04.019. FitzGerald, Ben, Peter L Levin, and Jacqueline Parziale. 2016. Open Source Software &amp; the Department of Defense. Center for a New American Security. Gandrud, Christopher. 2013. Reproducible Research with R and R Studio. CRC Press. Garcia, David, Mansi Goel, Amod Kant Agrawal, and Ponnurangam Kumaraguru. 2018. “Collective Aspects of Privacy in the Twitter Social Network.” EPJ Data Science 7: 1–13. https://doi.org/10/cjhr. Gleibs, Ilka H. 2014. “Turning Virtual Public Spaces into Laboratories: Thoughts on Conducting Online Field Studies Using Social Network Sites.” Analyses of Social Issues and Public Policy 14 (1): 352–70. https://doi.org/10/f6t7gd. Grange, JA, D Lakens, F Adolfi, C Albers, F Anvari, M Apps, S Argamon, et al. 2018. “Justify Your Alpha.” Nature Human Behavior. Harary, Frank. 1959. “On the Measurement of Structural Balance.” Behavioral Science 4 (4): 316–23. https://doi.org/10/cp9nfp. Hastie, Reid, and Robyn M Dawes. 2010. Rational Choice in an Uncertain World: The Psychology of Judgment and Decision Making. Sage. Healy, Kieran. 2017. “Data Visualization for Social Science: A Practical Introduction with r and Ggplot2.” Henrich, Joseph, Steven J. Heine, and Ara Norenzayan. 2010. “The Weirdest People in the World?” Behavioral and Brain Sciences 33 (2-3): 61–83. https://doi.org/10/c9j35b. Hicks, Stephanie C., and Rafael A. Irizarry. 2018. “A Guide to Teaching Data Science.” The American Statistician 72 (4): 382–91. https://doi.org/10/gfr5tf. Hornik, Kurt, and The R Core Team. 2022. “R FAQ: The Comprehensive R Archive Network.” Hullman, Jessica, Paul Resnick, and Eytan Adar. 2015. “Hypothetical Outcome Plots Outperform Error Bars and Violin Plots for Inferences about Reliability of Variable Ordering.” Edited by Elena Papaleo. PLOS ONE 10 (11): e0142444. https://doi.org/10.1371/journal.pone.0142444. Hvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. Chapman and Hall/CRC. Ioannidis, John PA. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124. https://doi.org/10/chhf6b. Isaacson, Walter. 2014. The Innovators: How a Group of Hackers, Geniuses, and Geeks Created the Digital Revolution. Simon and Schuster. Jackson, Dan. 2017. “The Netflix Prize: How a 1 Million Contest Changed Binge-Watching Forever.” Thrillist. Com. James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 103. Springer Texts in Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4614-7138-7. Kelly, Janice R., Nicole E. Iannone, and Megan K. McCarty. 2016. “Emotional Contagion of Anger Is Automatic: An Evolutionary Explanation.” British Journal of Social Psychology 55 (1): 182–91. https://doi.org/10/gf6mn3. Kondo, Marie. 2016. Spark Joy: An Illustrated Master Class on the Art of Organizing and Tidying up. Ten Speed Press. Kumar, Devinder, Alexander Wong, and Graham W Taylor. 2017. “Explaining the Unexplained: A Class-Enhanced Attentive Response (Clear) Approach to Understanding Deep Neural Networks.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 36–44. Lakatos, Imre. 1969. “Falsification and the Methodology of Scientific Research Programmes.” Criticism and the Growth of Knowledge. Cambridge University Press: Cambridge. Lanning, Kevin. 1987. “Some Reasons for Distinguishing Between ‘Non-normative Response’ and ‘Irrational Decision’.” The Journal of Psychology 121 (2): 109–17. https://doi.org/10/fv4hh5. ———. 1994. “Dimensionality of Observer Ratings on the California Adult Q-set.” Journal of Personality and Social Psychology 67 (July): 151–60. https://doi.org/10/drnkvm. ———. 1996. “Robustness Is Not Dimensionality: On the Sensitivity of Component Comparability Coefficients to Sample Size.” Multivariate Behavioral Research 31 (1): 33–46. https://doi.org/10/dt6gb3. ———. 2017. “What Is the Relationship Between ‘Personality’ and ‘Social’ Psychologies? Network, Community, and Whole Text Analyses of the Structure of Contemporary Scholarship.” Collabra: Psychology 3 (1): 8. ———. 2018. “Data Visualizations in Personality and Social Psychology: Challenges in Representing Taxonomic, Community, and Developmental Structures.” Association of Psychological Science Annual Convention, San Francisco, May 25. Lanning, Kevin, and Ari Rosenberg. 2009. “The Dimensionality of American Political Attitudes: Tensions Between Equality and Freedom in the Wake of September 11.” Behavioral Sciences of Terrorism and Political Aggression 1 (2): 84–100. https://doi.org/10/fckr37. Leek, Jeffrey T, and Roger D Peng. 2015. “Statistics: P Values Are Just the Tip of the Iceberg.” Nature 520 (7549): 612. https://doi.org/10/gfb8jm. Loevinger, Jane. 1957. “Objective Tests as Instruments of Psychological Theory.” Psychological Reports 3 (3): 635–94. https://doi.org/10/b27jpk. Loukides, Hilary, Mike. 2018. Ethics and Data Science. O’Reilly. Matz, S. C., M. Kosinski, G. Nave, and D. J. Stillwell. 2017. “Psychological Targeting as an Effective Approach to Digital Mass Persuasion.” Proceedings of the National Academy of Sciences 114 (48): 12714–19. https://doi.org/10.1073/pnas.1710966114. McShane, Blakeley B., David Gal, Andrew Gelman, Christian Robert, and Jennifer L. Tackett. 2017. “Abandon Statistical Significance.” arXiv:1709.07588 [Stat], September. https://arxiv.org/abs/1709.07588. Merton, Robert K. 1936. “The Unanticipated Consequences of Purposive Social Action.” American Sociological Review 1 (6): 894–904. https://doi.org/10/fjg8hf. Miguel, Edward, Colin Camerer, Katherine Casey, Joshua Cohen, Kevin M Esterling, Alan Gerber, Rachel Glennerster, et al. 2014. “Promoting Transparency in Social Science Research.” Science 343 (6166): 30–31. https://doi.org/10/gdrcpz. Milgram, Stanley. 1967. “The Small World Problem.” Psychology Today 2 (1): 6067. Munafò, Marcus R., Brian A. Nosek, Dorothy V. M. Bishop, Katherine S. Button, Christopher D. Chambers, Nathalie Percie du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J. Ware, and John P. A. Ioannidis. 2017. “A Manifesto for Reproducible Science.” Nature Human Behaviour 1 (1): 0021. https://doi.org/10.1038/s41562-016-0021. Narayanan, Arvind, and Vitaly Shmatikov. 2008. “Robust De-anonymization of Large Sparse Datasets.” In 2008 IEEE Symposium on Security and Privacy (Sp 2008), 111–25. Oakland, CA, USA: IEEE. https://doi.org/10.1109/SP.2008.33. Nikzad, Afshin, Mohammad Akbarpour, Michael A. Rees, and Alvin E. Roth. 2021. “Global Kidney Chains.” Proceedings of the National Academy of Sciences of the United States of America 118 (36): e2106652118. https://doi.org/10.1073/pnas.2106652118. Ondaatje, Michael, and Walter Murch. 2002. The Conversations: Walter Murch and the Art of Editing Film. A&amp;C Black. Open Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716–16. https://doi.org/10/68c. Page, Lawrence, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. “The PageRank Citation Ranking: Bringing Order to the Web.” Peng, Roger. 2018. “Teaching r to New Users - from Tapply to the Tidyverse.” Peng, Roger D. 2014. R Programming for Data Science. Pennebaker, James W., Cindy K. Chung, Joey Frazee, Gary M. Lavergne, and David I. Beaver. 2014. “When Small Words Foretell Academic Success: The Case of College Admissions Essays.” Edited by Qiyong Gong. PLoS ONE 9 (12): e115844. https://doi.org/10/f6z8q5. Phillips, Nathaniel D., Hansjörg Neth, Jan K. Woike, and Wolfgang Gaissmaier. 2017. “FFTrees: A Toolbox to Create, Visualize, and Evaluate Fast-and-Frugal Decision Trees.” Judgment and Decision Making 12 (4): 344–68. Poulin, Michael J., and Claudia M. Haase. 2015. “Growing to Trust: Evidence That Trust Increases and Sustains Well-Being Across the Life Span.” Social Psychological and Personality Science 6 (6): 614–21. https://doi.org/10.1177/1948550615574301. Reinsel, David, John Gantz, and John Rydning. 2025. “The Digitization of the World.” Shattuck, Roger. 1997. Forbidden Knowledge: From Prometheus to Pornography. Houghton Mifflin Harcourt. Silge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. \" O’Reilly Media, Inc.\". Slovic, Paul, David Zionts, Andrew K Woods, Ryan Goodman, and Derek Jinks. 2013. “Psychic Numbing and Mass Atrocity.” The Behavioral Foundations of Public Policy, 126–42. https://doi.org/10/gk4945. Sternberg, Robert J. 1999. “The Theory of Successful Intelligence.” Review of General Psychology 3 (4): 292–316. https://doi.org/10/cqrkxh. Sternberg, Robert J. 2018. “Theories of Intelligence.” In, edited by Steven I. Pfeiffer, Elizabeth Shaunessy-Dedrick, and Megan Foley-Nicpon, 145161. American Psychological Association. https://doi.org/10.1037/0000038-010. Sullivan, J. L., and J. E. Transue. 1999. “THE PSYCHOLOGICAL UNDERPINNINGS OF DEMOCRACY: A Selective Review of Research on Political Tolerance, Interpersonal Trust, and Social Capital.” Annual Review of Psychology 50 (1): 625–50. https://doi.org/10/cmthvk. Sweeney, Latanya. 2005. “Privacy-Enhanced Linking.” ACM SIGKDD Explorations Newsletter 7 (2): 72–75. https://doi.org/10/bjvpjh. Szucs, Denes, and John Ioannidis. 2017. “When Null Hypothesis Significance Testing Is Unsuitable for Research: A Reassessment.” Frontiers in Human Neuroscience 11: 390. https://doi.org/10/gc6vws. Thies, Justus, Michael Zollhöfer, Matthias Nießner, Levi Valgaerts, Marc Stamminger, and Christian Theobalt. 2015. “Real-Time Expression Transfer for Facial Reenactment.” ACM Trans. Graph. 34 (6): 183–81. https://doi.org/10/f7wqz7. Tufte, Edward R. 2001. The Visual Display of Quantitative Information. 2nd ed. Cheshire, CT: Graphics Press. Tukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67. https://doi.org/10/d48nqg. ———. 1977. “EDA: Exploratory Data Analysis.” Reading, Mass. Tversky, Amos, and Daniel Kahneman. 1974. “Judgment Under Uncertainty: Heuristics and Biases.” Science 185 (4157): 1124–31. https://doi.org/10/gwh. Wainer, Howard. 2007. “The Most Dangerous Equation.” American Scientist 95 (3): 249. https://doi.org/10.1511/2007.65.249. Wainer, H, and D Thissen. 1981. “Graphical Data Analysis,” 51. Watts, Duncan J. 2004. “The \"New\" Science of Networks.” Annual Review of Sociology 3: 243–70. https://www.jstor.org/stable/29737693. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science 2e. \" O’Reilly Media, Inc.\". Wu, Tim. 2019. “How Capitalism Betrayed Privacy.” The New York Times, 5. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
