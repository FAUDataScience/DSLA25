---
title: "Data science for the liberal arts"
author: "Kevin Lanning"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: 
    bookdown::gitbook:
        config:
            fontsettings:
                theme: white
documentclass: "book"
bibliography: references.bib
biblio-style: apalike
# description: "Test"
link-citations: yes
github-repo: kevinlanning/DSLA25
# classoption: "openany"
# cover-image: images/chihuahuamuffin.png
urlcolor: blue
linkcolor: blue
---
# {-} 

<!--chapter:end:index.Rmd-->

```{r preface, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# preface {.unnumbered}

This work-in-progress will ultimately serve as a textbook for introductory undergraduate courses in data sciences, and is particularly aimed at students in the liberal arts. No prior knowledge of computer programming is presumed, though, ideally, students will have had college algebra (or its equivalent) and an introductory course in statistics, methods, or data analysis.

## the role of the liberal arts in data science {.unnumbered}

Data science is still a relatively new field of study, and there are multiple approaches to teaching it and to its place in the college curriculum. This book is intended to serve courses such as [*Introduction to Data Science*](https://kevinlanning.github.io/DataSciSpring2025/) at the Wilkes Honors College of Florida Atlantic University which, in turn, was initially based on data science classes at the universities of [North Carolina](https://idc9.github.io/stor390/), [British Columbia](https://stat545.com/), [Duke](http://www2.stat.duke.edu/courses/Fall18/sta112.01/), [Maryland](http://www.hcbravo.org/IntroDataSci/calendar/), [Wisconsin](http://pages.stat.wisc.edu/~yandell/R_for_data_sciences/syllabus.html), [Stanford](https://github.com/dcl-2017-04/curriculum), [BYU](https://byuistats.github.io/M335/syllabus.html), [Harvard](http://datasciencelabs.github.io/), and [UC Berkeley](http://www.gastonsanchez.com/stat259/). At each of these schools, the Introduction to Data Science is, to my eyes at least, closer to Statistics than to Computer Science.

Statistics is itself a broad field, and our approach is aligned with its most applied and pragmatic form. From this perspective, the choice of statistical methods should follow from the data and problem at hand - in other words, statistics should serve the needs of the user rather than dictate them [@loevinger1957].

Pragmatism, in turn, can serve various goals, ranging from maximizing the revenues generated by an online ad to minimizing the carbon footprint of a travel itinerary. Data science for the liberal arts may be seen as a fusion of the pragmatism of data science with social and humanistic concerns; we stand beside programs in [Computational Social Science](https://sicss.io/) as it has been taught at schools including [Chicago](https://dssg.uchicago.edu/), [Georgia Tech](https://github.com/jacobeisenstein/gt-css-class), [UC Santa Barbara](https://github.com/raviolli77/dataScience-UCSBProjectGroup-Syllabus), [Princeton](http://www.princeton.edu/~mjs3/soc596_f2016/), [UC Berkeley](https://github.com/rochelleterman/PS239T), at [Berlin's Hertie School of Governance](https://github.com/HertieDataScience/SyllabusAndLectures), and in [Columbia's School of Journalism](https://github.com/tommeagher/data1-fall2015).

Data science for the liberal arts begins with the person and society rather than with the algorithm and network. In its concern with the liberal arts, it is intended to provide a modest counterbalance to the inherently centripetal, or inequality-accelerating, force of modern information technology.[^00-preface-1]

[^00-preface-1]: I hope to return to this in a later chapter, but in the meantime consider the discussion of the "Matthew Effect" in sociology and network science [@watts2004].

## some features of the text {.unnumbered}

There are a number of different approaches to teaching data science for the liberal arts. The present text includes several distinguishing features.

**R**

About ten years ago, in an informal survey of introductory data science courses, I found a pretty even split between those which began with Python and those which began with the statistical programming language R. This difference corresponds, very loosely, to the split noted above: Computer science based approaches to data science were frequently grounded in Python, while statistics-based approaches were generally grounded in R.

Today, the picture is a little different: Python is increasing in popularity. Schools including Berkeley and Harvard have shifted largely (but not entirely) away from R towards Python in training undergraduates. But R is still taught as a first course or in certificate programs at [Harvard](https://pll.harvard.edu/course/data-science-r-basics), [Oxford](https://www.lifelong-learning.ox.ac.uk/courses/an-introduction-to-programming-in-r), and [Johns Hopkins](https://www.coursera.org/specializations/jhu-data-science).

Python is used more than R in industry, but R remains popular in academic settings, particularly in areas like epidemiology/public health and psychology. R is generally easier than Python for those who have some statistics but no programming, and Python is easier for those who have some programming but limited statistics. Literacy in R is a valuable skill for those planning on applying to graduate study in disciplines including behavioral sciences (psychology, economics), social sciences (anthropology, political science, public policy), and some health sciences (nursing and public health).

Our course will be based in R.

**Reproducible science**

The course will provide an introduction to some of the methods and tools of reproducible science. We will consider the replication crisis in the natural and social sciences, and then consider three distinct approaches which serve as partial solutions to the crisis. The first of these is training in a notebook-based approach to writing analyses, reports and projects (using R markdown documents). The second is using public repositories (such as the [Open Science Framework](https://osf.io/) and, to a limited extent, [GitHub](https://github.com/)) to provide snapshots of projects over time. Finally, the third is to consider the place of significance testing in the age of Big Data, and to provide training in the use of descriptive, exploratory techniques of data analysis.

**Good visualizations**

As I note in the first chapter, communication is a distinguishing concern of data science for the liberal arts. "Communication" includes not just writing up results, but also designing data displays that incisively convey the key ideas or features in a flood of data. We'll examine and develop data visualizations such as plots, networks and text clouds. More advanced topics may include maps, interactive displays, and animations.

**A little data**

There are plenty of data sources for us to examine, and we'll consider existing datasets from disciplines ranging from literature to economics to public health, with sizes ranging from a few dozen to millions of data points. We will also clean and create new datasets.

**A few tools**

One feature of Data Science is that it is changing rapidly. The tools, methods, data sources, and ethical concerns that face us today are different from those which shaped the field just one or two years ago.

In fields that are undergoing rapid change, there is some trade-off between building expertise with existing (older) tools and trying the newer approaches. Partly because I want to equip you with skills which will not be obsolete, partly because some of these new approaches promise more accessibility, elegance, and/or power, and partly because of my own interest in staying current, we'll be using some of the latest packages and programs.

In the last few years, I've shifted the class from the standard R dialect (as I learned it from the [Johns Hopkins-Coursera Data Science Specialization](https://www.coursera.org/specializations/jhu-data-science)) to the Tidyverse, a dialect of R that I find to be relatively clear and concise. A few years ago, I shifted our primary platform from individual laptops to a [cloud-based R platform](https://posit.cloud/); while this approach has its advantages, I found that these did not outweigh the costs of the approach, so we will go back to the standalone method.

We'll explore different approaches to learning R syntax, including the [learnr](https://rstudio.github.io/learnr/) package, [Swirl](https://swirlstats.com/), and DataCamp.

In the past, I've recommended using dedicated markdown editors such as [Obsidian](https://obsidian.md/). While I still think that these are worth considering for some text-editing and note-taking applications, we'll do our work instead with the editor in the latest variant of RStudio on our laptops, as this allows WYSIWIG (what you see is what you get) formatting of documents - such as this one - that are intended as "publication-ready" texts.

We'll use, and explore the advantages and disadvantages, of spreadsheets such as Excel or Google Sheets as well.

## the book is for you {.unnumbered}

It's my intention that this text should serve every college student, regardless of concentration or college major. The skills and insights that you will gain in this course will help you in graduate and professional schools, will help you in your careers, and will help you in your goal of making a better world. And it will help you train the next generation of data scientists as well.

<!--chapter:end:00-preface.Rmd-->

```{r datasci, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# PART I: Introduction {.unnumbered}

# what is "data science for the liberal arts?"

Hochster, in Hicks and Irizarry [-@hicks2018], describes two broad types of data scientists: Type A (Analysis) data scientists, whose skills are like those of an applied **statistician**, and Type B (Building) data scientists, whose skills lie in problem solving or coding, using the skills of the **computer scientist**. This view arguably omits a critical component of the field, as data science is driven not just by statistics and computer science, but also by "domain expertise:"

![*The iconic data science Venn diagram*](images/dataVenn.png)

The iconic [Venn diagram model of data science](https://www.google.com/search?q=venn+diagram+model+of+data+science&newwindow=1&safe=active&rlz=1C1CHBF_enUS762US763&tbm=isch&tbo=u&source=univ&sa=X&ved=0ahUKEwiM_abBtY7XAhXDQCYKHdgyB58QsAQIOg&biw=1378), as shown above, suggests that there are not two but three focal areas in the field, one of which begins not with math or computer science, but with "domain expertise." Data science for the liberal arts is a 'Type C' approach, where 'C' might refer to a concentration of concern in the arts, humanities, social and/or natural sciences. For the Type C data scientist, coding is in the service of applied problems and concerns.

Type C data science does not merely integrate 'domain expertise' with statistics and computing, it places content squarely at the center. We can appreciate the compelling logic and power of statistics as well as the elegance of well-written code, but for our purposes these are means to an end. Programming and statistics are tools in the service of social and scientific problems and cultural concerns. Type C data science aims for work which is not merely cool, efficient, or elegant, but responsible and meaningful.

At the risk of oversimplifying:

-   Type **A** data scientists focus on **Analysis** and questions about 'how?'

-   Type **B** data scientists focus on **Building** and questions of 'what?'

-   Type **C** data scientists focus on **Consideration** and questions of 'why?', 'who?', 'what for?', and 'at what (social) cost?'

## the incompleteness of the data science Venn diagram

Data visualizations are starting points which can provide insights, typically highlighting big truths or effects by obscuring other, presumably smaller ones. The Venn diagram model of data science is no exception: As with other graphs, figures, and maps, it allows us to see by showing only part of the picture. What does it omit? That is, beyond mathematics, computing, and domain expertise, what other skills contribute to the success of the data scientist?

### additional domains

For the liberal arts data scientist, we can note at least three additional important domains, that is, **communication**, **collaboration**, and **citizenship**.

**Communication,** including writing and the design and display of quantitative data, is central to data science because results are inconsequential unless they are recognized, understood, and built upon. Facets of communication include oral presentations, written texts and good data visualizations.

**Collaboration** is important because problems in data science are sufficiently complex so that any single individual will typically have expertise in some, but not all, facets of the area. Collaboration, even more than statistical and technical sophistication, is arguably the most distinctive feature of contemporary scholarship in the natural and social sciences as well as in the private sector [@isaacson2014].

**Citizenship** is important because we are humans living in a social world; it includes serving the public good, overcoming the digital divide, furthering social justice, increasing public health, diminishing human suffering, and making the world a more beautiful place. The Type C data scientist is aware of the fact that the world and workforce are undergoing massive **change**: This puts the classic liberal arts focus of "learning how to learn" (as opposed to memorization) at center stage. Finally, the Type C data scientist is sensitive to the **creepiness** of living increasingly in a measured, observed world. These real-world goals should be informed by ethical **concerns** including a respect for the privacy and autonomy of our fellow humans.

### an additional dimension

Cutting across these various facets (statistics, computing, domain expertise, collaboration, communication, and citizenship), a second dimension can be articulated. No one of us can excel in all of these domains, rather, we might aim towards a hierarchy of goals ranging from **literacy** (can comprehend) through **proficiency** (can communicate and contribute) to **fluency** (can practice) to **leadership** (can create new solutions or methods).

That is, we can think of **a *continuum* of ability, including knowledge, skills, interests, and goals.** That continuum ranges from the data *consumer* to the data *citizen* to the data science *contributor.* A Type C data science includes this dimension of 'depth' as well.

## the importance of data science for society

Communication, collaboration, and citizenship are each associated with the concept of trust. Trust is an important social good because it is associated with both individual well being [@poulin2015] and the stability of democratic institutions [@sullivan1999]. But interpersonal and institutional trust, including trust in science, have declined in recent years [@deane2024]. The decline in trust in science has been exacerbated by the so-called reproducibility (or replication) crisis, in which many scientific results initially characterized as "statistically significant" have been found not to hold up under scrutiny, that is, aren't reproducible. The reasons for the reproducibility crisis are many and contentious, but there is substantial consensus that one path towards better science involves the public sharing of methods and data. A second path towards better and more trustworthy science involves the use of larger datasets: With large datasets, effects are more stable and "statistical significance" is rarely a concern. Other indices, such as measures of accuracy and effect size are typically of primary interest. Data science, with its tools for reproducible analysis and its use of big data sets, can make science more trustworthy and improve the quality of our lives.

### intelligence, artificial intelligence (AI), and careers

It is difficult to predict the consequences of advances in AI for career selection. Some areas are likely to be impacted by AI more than others, but because AI is still in its infancy, we can only speculate. But one way to approach this question is to begin by considering what constitutes "intelligence." According to @sternberg2018intelligence, intelligence can be thought of as having three components. One of these is **Analytical Intelligence**, which includes things like planning, reasoning, acquiring knowledge, and problem solving. A second is **Creative Intelligence**, which includes both the ability to deal with novel situations effectively, and the ability to "automatize" or efficiently execute familiar tasks. The third is **Practical or Contextual Intelligence**, which includes the ability to adapt to new environments.

AI engines or agents (or simply AIs) are, for now at least, effective at only some of these. They are better than humans at knowledge acquisition, logical reasoning and some kinds of problem solving. All of these are parts of Analytical Intelligence. They are also better than us at one part of Sternberg's Creative Intelligence: They excel at many types of automatized routines.

Beyond this, they struggle. My desktop AI (Microsoft Copilot 365 Version 19.2508.31121.0) tells me that the strengths of AI, understood in the context of Sternberg's theory, make it ideal for things like "Data Analysis, Financial modeling, Diagnostics, Scheduling and logistics, and Repetitive administrative tasks." The first and last of these are core parts of data science, but not the whole of it. AI struggles (again, according to Copilot), with "Leadership and strategic decision-making, Counseling, therapy, and social work, Cross-cultural negotiation, [and] Artistic innovation requiring deep emotional resonance." Careers that are likely to grow include creative professions (design, storytelling), human-centered roles (coaching, diplomacy), and also hybrid roles, that is, "Professionals who use AI tools to enhance their work (e.g., data-informed decision-makers)."

All of this should be taken with a grain of salt. Today's AI capabilities are quite limited compared to what they will be a year from now, and the AI that you and I might use on our desktops is, in all likelihood, a simple version of what is already available to large-scale users.

> Oliver is an embodied AI described as a "helperbot" in the recent Broadway musical *Maybe Happy Ending*. We can imagine that a helperbot like Oliver could be built on a model that synthesized millions of interactions between people in asymmetrical roles, such as butlers and their employers, secretaries and bosses, and assistants and supervisors. In the musical, Oliver appears to have been an ideal servant, one who anticipated his owner's needs consistently, promptly and sensitively.
>
> I won't spoil it for you, but *Maybe Happy Ending* can be seen as portraying the challenges AI agents face, and how Artificial Intelligence is different from human intelligence. In Sternberg's (2018) terms, Oliver struggles not just with the question of how to adapt to a new environment, but also how to choose an environment altogether. How will it react when it encounters problems for which it has simply no frame of reference?

![Darren Criss as Oliver in *Maybe Happy Ending*](images/helperbot.jpg)

### the challenge of TMI

The challenges of data science are many, but perhaps the most fundamental is the problem of (literally) TMI. When we compare traditional statistics with modern data science, we realize that the former is typically concerned with making inferences from datasets that are too *small*, while the latter is concerned with making sense of data that is or are too *big* [@donoho2017]. The basic challenge of working in data science is the challenge of "too much information," or TMI.

The challenge of TMI is not new, or restricted to data science. In the 19th Century, Wilhelm Wundt argued that attention was the distinguishing act of the human mind [@blumenthal1975]. That is, in attending to (or focusing on) something, we must overlook everything else, consequently, *selection* is the essence of human perception [@erdelyi1974]. Selection is important not just in psychology, but in the arts as well, for *editing,* or choosing what not to write or show, is at the core of the creation of works including novels and film [@ondaatje2002].

Although the problem of TMI is not new, today it exists at a much greater scale, for there is simply more information around us. Indeed, over the last 20 years, the amount of digital information in the world has increased roughly 200-fold.[^01-datasci-1]

[^01-datasci-1]: According to Wikipedia, the Zettabyte (ZB) Era began in 2012, when the amount of digital information in the world first exceeded 1 ZB (or 10^21^ bytes). In 2025, it is estimated that the world will house 175 ZBs of digital data [@reinsel2025], hence a 175X increase in in 13 years. My estimate of a 200X increase in 20 years is a conservative extrapolation from these numbers. Incidentally, one ZB = 1,000,000,000,000,000,000,000 bytes, which could be stored on roughly 250 billion DVDs, or 500 million 2 TB hard drives.

Like perception in psychology and editing in the arts, data science is concerned with extracting meaning from information. Because the amount of information around us has mushroomed and its nature has become more important, our need to extract meaning has become more ubiquitous and more urgent. For these reasons, data science is a foundational discipline in 21st century inquiry.

## discussion: what are your objectives in data science?

<!--chapter:end:01-dataSci.Rmd-->

```{r setup, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# getting started

------------------------------------------------------------------------

We begin with a brief self-assessment, asking you to reflect on your own knowledge of data science, including the necessary-but-not-sufficient areas of computer programming and statistics. We then move to a description of some rudimentary tools that we will be using, then install the R programming environment on our laptops.

## are you already a programmer and statistician?

Regarding **programming**, you may know more than you think you do. Here's a simple program - a set of instructions - for producing a cup of coffee:

> add water to the kettle and turn it on
>
> if it's morning, put regular coffee in the French press, otherwise use decaf
>
> if the water has boiled, add it to the French press, else keep waiting
>
> if the coffee has steeped for four minutes, depress (smash) piston/plunger, else keep waiting
>
> pour coffee into cup
>
> enjoy

As a post-millennial student from a WEIRD culture, or Western, Educated, Industrialized, Rich Democracy [@henrich2010], you've 'programmed' computers, too, if only to enter a password, open an app, and upload a photo on your cell phone.

**Statistics** is of fundamental importance, not just for understanding abstract trends, but for making decisions about everyday life. Consider the case of a senior undergraduate who wants to go to med school. How many schools should she apply to?

> **Exercise 2_1** *Susie is applying to two med schools. At School A, 25% of students are accepted, and at School B, 25% are accepted as well. You are Susie. Are you going to get in to at least one of these programs? What is the probability? Does your estimate depend upon any assumptions?*

Questions such as these are important for us. If the combined probability is low, it will *likely* (another probability concept) make sense for Susie to spend the time, money, energy, and ego-involvement to apply to additional programs. If the probability is higher, it may not. But problems like this are hard - our estimates of probability are frequently poorly calibrated, and combining probability estimates is challenging. See, e.g., Tversky and Kahneman [-@tversky1974], and consider taking a course in *Behavioral Economics* or *Thinking and Decision Making* to learn more.

## spreadsheets - some best practices

Spreadsheets are handy tools, particularly for smaller datasets. You may have worked with data in spreadsheets such as Microsoft Excel or Google Sheets. If you haven't here's a start:

> **Exercise 2_2** Open the Google Sheet at <http://bit.ly/dslaX2_1>. Save a copy and edit it, entering the following in cell B7:
>
> *=SUM (B2:B6)*
>
> What is the result?
>
> Now **copy cell B7 to C7**
>
> What happens? Is this the result you expected?

Spreadsheets are great tools - the first one, VisiCalc, was the first "killer app" to usher in the personal computer revolution. But spreadsheets have limitations as well. **Best practices** have been proposed for using spreadsheets in data science such as, for example, including only data (and not calculations) in spreadsheets, using what we will recognize as a 'tidy' format in which data are in a simple rectangle (that is, avoid combining cells and using multi-line headers), and saving spreadsheets as simple text files, typically in comma-delimited or CSV format [@broman2018]. Typically, this means that each worksheet will be in a separate file, rather than in workbooks with multiple tabs.

There are good reasons for these recommendations: When we manipulate data in spreadsheets, we typically will not have a record of what was (and wasn't) changed, and this compromises the reproducibility of our work. Similarly, when we sort data in spreadsheets, we risk chaos, for example, if we sort only certain columns, the integrity of spreadsheet-rows will be lost.

**In general, spreadsheets should generally be used to store data rather than to analyze it.** But don't be a slave to this - if you are making some quick calculations that are inconsequential and/or to be used on just one occasion, working in Excel or Google Sheets is often the way to go.

## setting up your laptop: some basic tools

Documents produced in word processors typically include invisible characters used for formatting, security, etc. These characters, when fed into a computer program, lead to unpredictable results. *Markdown* (MD) is a simple syntax for producing documents that are easily read by both humans and machines. Most of the documents you will produce and work with this term, including the present chapter, are and will be written in some version of Markdown. Two markdown editors are worth considering. The first of these is [Obsidian](https://obsidian.md/), which is not just a markdown editor, but a tool for task management and linking ideas - a "second brain" according to some advocates. The drawback of Obsidian is that it can do so much that it can be overwhelming, especially for the beginner. The other markdown editor is RStudio - the environment within which we will be using R - which has a handy built in visual markdown editor as well.

Collaboration and communication are integral to data science. In the world beyond universities, the most important messaging and collaboration platform is [Slack](https://slack.com/). Slack is a commercial app, but it has a free tier. Slack is handy for group work of all forms, but most universities use collaborative tools embedded in Learning Management Systems such as Canvas instead.

While Microsoft Word has the advantages of familiarity, ease-of-use offline, and extensive formatting capabilities, **Google Docs** has several advantages over Word. Google Docs is free, it is convenient for collaborative work (as it allows simultaneous editing), and it provides a solid framework for *version control,* a critical skill in information management. Version control can help you avoid the chaos and confusion of having a computer (or several computers) full of files that look like Cham's (2012) comic:

![*Never call anything 'final.doc'.*](images/final.jpg)

Version control is an important concept in data science. Collaboratively built programs and platforms, including most of the add-ons (libraries, packages) which make R so powerful, are open-source projects built by many individuals over time. For projects such as these, both the current build and its associated history are typically maintained on [GitHub](https://github.com/), a website for hosting code. When we contribute to these projects, we will first mirror the web-based GitHub site using a program on our own Macs or Windows PCs called [Git](https://git-scm.com/), then upload our proposed changes. Keeping remote and local branches of files in sync can be challenging, however, and you will not be expected to use this technology in this class. But if you are curious, or want to learn more, an introduction to using Git and R together may be found [here](https://happygitwithr.com/).

## a (modified) 15-minute rule

While AI tools for coding, including Microsoft Copilot, are helpful for addressing the idiosyncacies of coding syntax, at some point, you will run into problems - if you don't you aren't learning enough. An important determinant of your success will be the balance you maintain between persistence and help-seeking.

The 15-minute rule is one guideline for this balance: It has been cleverly summarized as "[You must try, and then you must ask](https://www.mattringel.com/2013/09/30/you-must-try-and-then-you-must-ask/)." That is, if you get stuck, keep trying for 15 minutes, then reach out to others. I think that this rule is basically sound, particularly if it is applied with cognitive flexibility, social sensitivity, and reciprocity. So when you get stuck, make a note of the problem, then move to another part of your project (that's the cognitive flexibility part): This allows your problem to percolate and still make progress. When you ask others for help, ask in a way that shows an awareness of the demands on their time (social sensitivity): Part of this means that you should explain your problem in as detailed a fashion as possible - in technical terms, a "reprex" or reproducible example. Finally, you should be willing to provide as well as give help (reciprocity).

## installing R and RStudio desktop

Finally, if you have not already done so, install the latest version of R on your own Windows or Mac laptop. Then install RStudio Desktop. (RStudio is our interface, the environment we will use to write, test, and run R code).

<!--chapter:end:02-setup.Rmd-->

```{r standsfor, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# what R stands for ...

R was initially developed by Ross Ihaka and Robert Gentleman as a tool to help teach university-level statistics at the University of Auckland. At one level, the name 'R' simply stands for the first initial of these two founders [@hornik2022]. But, just as we noted that the 'C' in Type C Data Analysis stands for concepts such as concentration, communication, collaboration, the 'R' in our programming language means much more:

R is a system for **reproducible** analysis, and reproducibility is essential. When we write R code, we'll use R markdown documents. An R markdown document can include text (comments or explanations), 'chunks' of code, and output including graphs and tables. Having explanations, code, and results in a single document facilitates reproducible work. (Jupyter notebooks in the Python world are similar in this respect).

R is for **research**. Research is not just an end-product, not just a published paper or book:

> ... these documents are not the research [rather] these documents are the "advertising". The research is the "full software environment, code, and data that produced the results" [@buckheit1995; @donoho2010, 385].

Published works (including theses as well as books, scholarly papers, and business reports) are summaries; R markdown documents are the raw materials from which these are derived. When we consider only summaries (or the 'advertising'), we make it difficult for others to verify, or build upon, the findings by reproducing them [@gandrud2013].

R is a system for **representing** data in cool, insight-facilitating ways, a tool for creating (reproducible) data visualizations which can provide insights and communicate results. The power of R to make clear, honest, and reproducible data visualizations is widely seen as a major strength of the language.

R is **really popular**, and this matters, because learning R will make you a more attractive candidate for many graduate programs as well as jobs in the private sector.

Because R is popular, there are many **resources**, including, for example -

> Online resources include the simple (and less simple) lessons of SwirlR, which offers the possibility of "learning R in R," as well as DataCamp, the Data Science Certificate Program at Johns Hopkins, and other MOOCs.
>
> Books include [@peng2014] - which includes not only videos of his lectures in the program at Hopkins, but also a brief list of still more resources - and [@wickham2023]

You'll also learn (more directly) from people, including your classmates, as well as the broader community of people around the world. There are hundreds if not thousands of people, young and old, who are on the road with you. I am as well, just a step or two (hopefully) ahead.

R might stand for **relatively high level**. Programming languages can be described along a continuum from high to low level, the former (like R) are more accessible to humans, the latter (like assembly language) more accessible to machines. Python, Java, and C++ are all more towards the middle of this continuum.

R does not stand for '[**arggh**](https://www.urbandictionary.com/define.php?term=ARGH),' although you may proclaim this in frustration ('arggh, why can't I get this to work?) or, perhaps, in satisfaction ('arggh, matey, that be a clever way of doing this').[^03-whatrstandsfor-1]

[^03-whatrstandsfor-1]: Actually, pirates have little use for R, as pirates love the C (programming language).

But R does stand for **rewarding**. A language is a way of thinking about the world, and this is true for computer languages as well. You'll be challenged by its complexity, its idiosyncracy, its alien logic. But you will succeed, and you will find that you can do things that you did not believe possible.

### base R and packages

R is a programming language. It can be seen as including two parts, a simple core (Base R) and a large number of additional packages. These packages (libraries) are customized add-ons which simplify certain tasks, such as text analysis. There are, at this writing, 21,861 available packages on the CRAN package repository (as well as additional useful packages that, for one reason or another, do not appear on CRAN. Packages on CRAN are partially indexed by "task view pages." The [task view page for natural language processing](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html) or text analysis includes, at this writing, over 60 separate packages.

So how do you choose, and where do you begin? For our purposes, we will start with the curated list of packages which jointly comprise the tidyverse [@wickham2019], which is effectively a dialect of R. You can learn more about the tidyverse and its place in data science in the [introduction to the Wickham text](https://r4ds.hadley.nz/intro.html)[@wickham2023].

To download the tidyverse package from the 'net, open RStudio, find the 'console' window on the left side of your screen, and enter the command followed by \<enter\> or \<return\>

> install.packages("tidyverse")

## cha-cha-cha-changes

R is constantly changing, not just in the proliferation of packages, but also in the organization of the R community. While R is free and open source, RStudio is a commercial product. The company (and website) that develops the RStudio IDE is undergoing a name change (from RStudio to Posit). This is motivated, in part, by the need to make the RStudio platform more welcoming for other languages including Python.

Similarly, the R markdown programming language is slowly being replaced with newer, and ultimately more capable, software called Quarto. Quarto is back-compatible with R markdown, but can be used with other languages including Python as well. A description of the differences between R markdown and Quarto may be found [here](https://quarto.org/docs/faq/rmarkdown.html). For our purposes, you can treat Quarto files (.qmd suffix) as R markdown files (.rmd), and vice-versa.

One more change: Posit (the company) is developing a new IDE called 'Positron.' Positron may ultimately be a more useful environment for data science than the RStudio IDE, but it is in the beta testing stage at this writing. The RStudio environment, and the Rmarkdown documents that are produced within it, will continue to be available, and widely used, for the foreseeable future.

## some technical characteristics

R is an object-oriented language - one conceptually organized around objects and data rather than actions and logic.

In R, at the most basic or atomic level, "objects" include characters, real numbers, integers, complex numbers, and logicals.

These atomic objects may be combined into vectors, which generally include objects of the same type [one kind of object, 'lists,' is an exception to this; @peng2014]. Vectors can be further combined into data frames, which are two-dimensional tables or arrays. A *tibble* is a particular type of data frame which is used in the tidyverse. Tibbles are, in some ways, handier to work with than other data frames. We'll be working extensively with data frames in general, and tibbles in particular, as we move forward.

Objects have attributes. Attributes of R include such things as name, dimensions (for vectors and arrays), class (that's the type of object described in the previous paragraph), length, etc.

Real world data sets are messy, and frequently have missing values. In R, missing values may be represented by NA (not available) or NaN (not a number, implying an undefined or impossible value).

## finding help

One does not simply 'learn R.' Unlike, say, learning to ride a bicycle, fry an egg, or drive a car with a manual transmission, learning R is not a discrete accomplishment that one can be said to have mastered and from which one then moves on. Rather, R is an evolving, open system of applications and tools which is so vast that there is always more that one can achieve, new lessons that one can learn. And, the complexity of R syntax is such that, for almost all of us, we will need help for coding on any non-trivial task.

For us, the key ideas in "looking for help" will include not just the tools on the RStudio IDE, but also (a) using Google searches wisely, (b) judicious use of AI assistance, and (c) reaching out to your classmates and instructor.

Here, as in the real world, there is an etiquette for help-seeking which is based on consideration. Your search for help should begin by making sure that others will encounter the same result, then by stripping the problem down to its essence. Once you have reduced the problem to this minimal, reproducible essence, you will often be able to spot the problem yourself - and, if not, you will make it easier for others to help you. There is an R package (reprex) which will likely facilitate this, but I haven't tried it yet. [Here](https://www.tidyverse.org/help/) is a good introduction.

Finally, to get a sense of the power and versatility of R Markdown documents, you might explore this [tutorial](https://rmarkdown.rstudio.com/lesson-1.html). Note that, if you want to work interactively with the tutorial, you will need to first sign up for an account on RStudio cloud. Go to [posit.cloud](https://posit.cloud), click on "learn more" in the "Free" column, then sign up.

When you encounter obstacles, remember the 15-minute rule.

<!--chapter:end:03-whatRstandsfor.Rmd-->

```{r exploring, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# exploring R world

There are many sources for learning the basics of R. A few of these follow. Please **spend at least 180 mins exploring at least two of the following.** Be prepared to discuss your progress next class: You will be asked which source(s) you used, what you struggled with, what questions you have, and what you would recommend to your classmates. **Hint:** If you find the material too challenging, remember the 15 minute rule, take a break away from your machine and other screens, clear your head, then try a different approach.

## go to the movies

About nine years ago, Iain Carmichael used data from the Internet Movies Database (IMDB) to introduce R. You can see his introduction [here](https://idc9.github.io/stor390/notes/getting_started/getting_started.html).

You can consider his report from the standpoint of style (formatting, organization), coding (how he did this), data (the part of the IMDB data he is looking at), and his results (plots of distributions and relationships).

Do you have any questions about the movie data? How might you ask these?

A minimal amount of sleuthing - a click on the STOR 390 link at the top of the page, then a quick scroll - reveals that "all of [Carmichael's] course material is on the github repo" - or repository.

-   Can you find the Rmd document that generated his work?

-   Can you download it on to your machine?

-   If you try to run it, what happens?

-   If you were working on your thesis and came across a problem like this, what would you do next?

## go into the clouds

In addition to the desktop version of R (and Rstudio) we will be using in this class, there is a cloud-based environment as well. As mentioned in the last chapter, you can sign up for a starter account at [posit.cloud](https://posit.cloud).

When you open posit.cloud, you should see a column on the left of the screen that includes four sections - spaces, learn, help, and info. (If you don't see these, click on the hamburger menu in the upper left corner and it will appear). Browse through the recipes tab, particularly the ones in the left most column, to start to get a sense of how you might solve some common challenges in R.

## open the box

Go to [datasciencebox/content](https://datasciencebox.org/content). Click on the "Hello world" link; this will take you to the beginning of Mine Çetinkaya-Rundel's introductory lessons on R. These include slides, the source code for the slides (these are written in R markdown), and videos of her lectures, including the one we watched on the first day of class.

Midway down this page, you'll find a link to "the RStudio Cloud workspace for Data Science Course in a Box project." Open it up and begin to explore the code and data behind her presentation.

## go to (data)camp

Many folks swear by (and others, I presume, at) DataCamp, which kind of gamifies learning software. As a student in this class, you have access to all of their stuff... free. You can even do lessons on your phone. Use the link given to you in class to enroll, then explore the introductory-level classes in R at <https://www.datacamp.com/category/r>

## learn to knit

In *literate programming*, comments, code, and results are integrated in a clear and reproducible way - they *document* our work.

'Markdown' is a simple language for adding formatting to text. 'R' is a statistical language. 'R Markdown' is a variant of R that you can use to produce or publish complex documents like this one, as well as the Carmichael page described above.

To create an R markdown (Rmd) document, open up Rstudio, click on (**File -\> New File -\> R Markdown**). A window will open up with a file that begins with a block of *YAML* (Yet Another Markdown Language). You can edit this as needed:

```         
---
title: "Here's an R Markdown Document"
author: "Frankie McFrank Frank"
date: "1/12/2025"
output: html_document
---
```

Go ahead and click on the clever "knit" icon in the bar just above the source window to create a sample document. You'll need to save the file with a new name, then R will finally create an HTML (Hyper Text Markdown Language ) page. Compare the R Markdown document (your code) with the result (the HTML).

The second chapter of [Healy's online book about Data visualizations](https://socviz.co/) provides a more thorough explanation of R Markdown as well as an introduction to R and R studio which largely parallels the discussions here and in the [Wickham et al. text](https://r4ds.hadley.nz/index.html) [@healy2017; @wickham2023]. We'll be discussing R Markdown (and its cousin Quarto) in Chapter 6.

## read (parts of) another introductory book

In addition to the Wickham text and the Baumer et al. [Modern Data Science with R](https://mdsr-book.github.io/mdsr3e/) book, one more warrants study: ModernDive (v2), aka [Statistical Inference via Data Science](https://moderndive.com/v2/index.html) is a new introduction to R and data science authored by Chester Ismay, Albert Kim, and Arturo Valdivia. It's contents largely parallel those in Wickham and Baumer; if you decide to read all or part of it, please let me know what you think.

## older approaches

I, like thousands of others, learned R in the process of completing the Johns Hopkins [Data Science Specialization](https://jhudatascience.org/courses.html) offered through [Coursera](https://www.coursera.org/). The sequence can be challenging, but their introduction to R used an accessible, interactive R package called *Swirl.* You can read about swirl ("learn R in R") at <https://swirlstats.com/>.

### **using Swirl**

After loading R (and opening R studio), you will get to the Swirl lessons with the following steps:

1)  Install the Swirl package on your computer (you only need to do this once). Type the following into your console window in R studio (typically left hand side of your screen or lower left)

> install.packages

2)  Then load the package into your workspace (you'll need to do this at the beginning of every session you use Swirl)

> library (swirl)

2)  Then run it!

> swirl ()

Swirl will ask a few questions then give you the option of choosing one of several courses. You'll choose the R Programming option, which leads to 15 separate lessons.

At the end of each lesson, you'll be asked

> *Would you like to receive credit for completing this course on Coursera.org?*

Answer no... then do another lesson.

### reading/watching Roger Peng's text and/or videos

Finally, you might consider the text and videos from the Coursera R class. Most of the material from that class can be found in Roger Peng's [-@peng2014] text, a slightly updated version of which can be found [here](https://bookdown.org/rdpeng/rprogdatascience/). The videos in the series may be found in this [playlist](https://www.youtube.com/watch?v=wy0h1f5awRI&list=PL_bgmYHGITt-8Noh1fZpxTHJ-GrBb626R). Here's an introduction:

[![Roger Peng](https://img.youtube.com/vi/wy0h1f5awRI/0.jpg)](https://youtu.be/wy0h1f5awRI).

*Roger Peng introducing R*

<!--chapter:end:04-exploring.Rmd-->

```{r owl, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Part II: Towards data literacy {.unnumbered}

# now draw the rest of the owl

![*Draw the rest of the owl.*](images/InkedrCr9A_LI.jpg)

In the prior chapter, you explored several different sources for learning how to code in R. Now it's time to explore other approaches. Take a break from reading, and spend some time coding and consolidating, reviewing tutorials, or playing with data.

## time for hands-on experience

If you want to work actively with a dataset, here are two possibilities. (You are not limited to just these, so if you want to look at something else that's fine too). Each of these datasets has been supplied as its own package.

### consider loading the tidyverse

The tidyverse allows the use of the 'pipe' operator, ("%\>%"), which is useful for combining commands. Now there is a native pipe in Base R ("\|\>"), which does the same thing. But we will be using the tidyverse for a number of reasons, so go ahead and install it if you haven't already, then load it.

> Remember that any package needs to be installed on your machine once before progressing. That is, if you installed the tidyverse previously, you don't need to do the first line here. If you haven't installed the tidyverse, you should remove the octothorpe or pound sign (\#) on the second line before running this next chunk:

```{r}
# install.packages("tidyverse")
library(tidyverse)
```

### now explore the babynames package

The babynames dataset is described [here](https://cran.r-project.org/web/packages/babynames/readme/README.html). What is in the data? What interesting questions might you ask about the dataset?

```{r}
# install.packages("babynames")
library(babynames)
data(babynames)
str(babynames)
babynames %>% slice_sample(n = 5)
```

### or the (ggplot2)movies package

The index page for the movies dataset is [here](https://cran.r-project.org/web/packages/ggplot2movies/index.html).

```{r}
# install.packages("ggplot2movies")
library(ggplot2movies)
data(movies)
str(movies)
movies %>% slice_sample(n = 5)
```

Regardless of whether you have played with one or both of these datasets, worked with the tutorials, or something else, please be prepared to share your experiences with the class at our next meeting.

## assignment

In our next meeting, go as far as you can with the following:

1.  Open the dataset. Describe the data in a paragraph based on one or more R functions (such as str, glimpse, and slice).

    a.  What are the variables? What are the observations? What are the data types? What are the ranges of the variables? Are there missing values?

2.  After looking at the data, describe one or more questions of interest that you would like to ask about the data. (I do mean "of interest" - something that has meaning, that people would actually like to know).

    a.  Write each question in a separate paragraph. Use headings to structure your document.

3.  Describe, in words, how you would do look at your question. Be as specific as possible, but don't worry about R syntax (e.g., I would pull out such-and-such variables, or such-and-such observations, and I would compare them with x, or I would like this with 'y'). Explain what you might find, and why (again) that would be interesting.

    a.  Now draw the rest of the owl - translate your words into code, and run the analysis.

4.  If appropriate, describe what a graph or visualization of the data might look like.

    a.  go for it if you can.

5.  Save your work as an R markdown document, and knit it to an html file.

<!--chapter:end:05-owl.Rmd-->

```{r literate, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# literate programming

Showing your work, to (future) you as well as to others, is a key part of reproducible science. R Markdown documents facilitate this, as they allow you to include comments, code, and results in a single place. But before we consider R markdown, we begin with two more elemental ideas: *projects* and *scripts* (R4DS, Chapter 6) .

## projects are directories

You should save your work in *projects.* These isolate your data and scripts into discrete directories.

There are two reasons I begin with 'projects:' The first is that students who are new to coding will often struggle to find their datasets and code on their personal machines; having a project directory makes things easier. The second is that , down the road, it's likely that you will be working on R for different things in parallel - for this and another class, for this class and your thesis, or perhaps for two distinct types of analysis within your thesis.

![*Left panel: Files pane in RStudio for this manuscript. Right panel: Menu showing some other recent projects.*](images/CompositScreenGrab.png)

When you open up an R project, you'll be in the right directory, with the relevant files (and only the relevant files) at your fingertips in the files pane. It is yet another way in which the notion of 'tidiness' facilitates our work.

## scripts are files of code

To do simple exercises in R, you can enter code directly in the Console pane (the default is in the lower left of the RStudio screen), then get an instant response. This (interactive) approach to coding is quick, but it is difficult to recreate.

> For example, imagine that I were doing an analysis between age and a personality trait that, in one dataset, is referred to as 'Neuroticism' (N) and, in a second, the same trait is reverse scored as 'Emotional Stability' (ES). If I want to combine measures of N and ES from two different datasets, each of which has scores for the trait on a 1-7 (or 7-1) scale, I could reverse one of these. My code might look like this:

##### combine two small datasets, reverse one of them, print first and last few rows:

```{r }
library(tidyverse)
file1 <- read_csv("data/datawithN.csv")
glimpse (file1)
file2 <- read_csv("data/datawithES.csv")
glimpse (file2)
combinedfile <- file1 %>% 
    mutate(ES = 8 - N) %>% # Creates 'ES' from 'N' for file1
    select (-N) %>% 
    bind_rows(file2) 
combinedfile %>% 
    slice(c(1:3,18:20)) # the 'c' is for combine or concatenate
```

You'll want a record of your code for even simple transformations such as this one. R4DS Chapter 6 shows the R studio interface and encourages you to save your work in scripts. These are written in the source (editor) window in the upper left quadrant of the default R studio screen.

## R markdown / Quarto documents combine scripts with comments and (once knit) results

The objectives described in the prior section lead naturally to a consideration of R Markdown documents, which allow you to include comments, scripts, and results in a single place. In R4DS, Wickham [\@-wickham2023] describes the use of Quarto rather than R markdown. Regardless of whether you use Quarto (see Chapter 28 of R4DS, or the tutorial [here](https://quarto.org/docs/get-started/)) or R Markdown (see the tutorial [here](https://rmarkdown.rstudio.com/lesson-1.html)), I encourage you to use one of these powerful, organizing approaches for nearly everything you do in R.

There are as many as four parts of an R markdown or Quarto document:

-   A YAML (yet another markdown language) header or metadata
-   Text formatted in markdown
-   R code (chunks) surrounded by code fences
-   and, occasionally, inline code

## some elements of coding style

Good coding is often a combination of several skills ranging from puzzle-solving to communication. I can't claim that these are *the* elements of coding style (apologies to Strunk & White), but rather that these are merely some of the elements.

Good coding is **clear** and thus commented. You are writing for your future self as well as others, so be explicit about the purpose of each chunk of code.

Good coding is **concise**. When you can write code in 3 lines instead of 30, your code may be more clear and efficient. Take pleasure in writing parsimonious, efficient code. But where efficiency and clarity conflict, choose the latter.

Good code should be **complete**, including all steps from reading the data to producing output. Where appropriate, comment-out (rather than delete) informative errors, again for the future you.

Good code may be **creative**. The coolest solutions are those which pull from and synthesize a number of ideas. Creativity often requires walking away from a problem in order to ultimately arrive at a solution (Wertheimer's *Productive Thinking*).

Finally, good code should be **considered**. Reflect on the impacts of your work - just because you can analyze something doesn't mean that you should.

## What to do when you are stuck

-   Google. pay attention to your error messages

-   Ask for help, make your questions clear and reproducible (see R4DS Chapter 1)

-   Take a break, think outside the box and [kludge](https://www.google.com/search?newwindow=1&safe=active&rlz=1C1SQJL_enUS782US782&q=Dictionary#dobs=kludge) something together if you have to

-   Document your struggles and your cleverness for a future you

<!--chapter:end:06-literateRmd.Rmd-->

```{r viz, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# principles of data visualization

## some opening thoughts

Graphs aren't just to inform, but to make you reflect.

We are concerned not just with the design of graphs, but with our role in society as leaders, as arbiters of facts, as educators. We want to foster data literacy.

Graphs and other data visualizations are arguably the most important tool we have in scientific communication.

How do people understand graphs? How can you use graphs to tell people the truth in a way that they understand? How can you convey uncertainty in a graph?

When you see a graph, what do you notice, what do you wonder, and what is the story? Is "story-telling" what visualizations should be about?

## some early graphs

Visual displays of information reach back to prehistoric times; modern graphs date back, arguably, to [Playfair's 1786 Political Atlas](https://robots.thoughtbot.com/analyzing-minards-visualization-of-napoleons-1812-march) - in which

> "... spatial dimensions were used to represent nonspatial, quantitative, idiographic, empirical data. Although it now seems natural to represent, for example, rising and falling imports over time as a rising and falling line, it does not seem to have been done before that time and was quite an accomplishment. Notably, in addition to the statistical line chart, Playfair at one fell swoop single-handedly invented most of the remaining forms of the statistical graphic repertoire used today-the bar chart and histogram, the surface chart, and the circle diagram or "pie chart" [@wainer1981].

![*Playfair's 1786 analysis of trade deficits*](images/playfair1786.PNG)

------------------------------------------------------------------------

The most celebrated early graph is that of [Minard](https://datavizblog.com/2013/05/30/dataviz-history-charles-minards-flow-map-of-napoleons-russian-campaign-of-1812-polotsk-smolensk-and-on-to-borodino/):

![*Minard's display of Napoleon's catastrophic assault on Moscow, 1812*](images/minard1812.PNG)

The visualization depicts the size, latitude, and longitude of Napoleon's army as they moved towards (tan line) then away (black line) from Moscow; temperature during the retreat is plotted as well. Further, vertical lines linking the temperature display to the number of troops indicate the often perilous river crossings which further decimated Napoleon's troops). [Cheng (2014)](https://robots.thoughtbot.com/analyzing-minards-visualization-of-napoleons-1812-march) decomposes the graph and provides some simpler visualizations; she also provides the following background:

> "*Czar Alexander of Russia sees that Napoleon was becoming too powerful, so he refuses to participate in this embargo [against the UK]. Angry at Czar Alexander's decision, Napoleon gathers a massive army of over 400,000 to attack Russia in June of 1812. While Russia's troops are not as numerous as France's, Russia has a plan. Russian troops keep retreating as Napoleon's troops move forward, burning everything they pass, ensuring that the French forces could not take anything from their environment. Eventually the French army follows the Russian army all the way to Moscow during October, suffering major losses from lack of food. By the time Napoleon gets to Moscow, he knows he has to retreat. As winter settles into Europe and the temperature drops, Napoleon's troops suffer even more losses, returning to France from lack of food, disease, and weather conditions."*

Of course, the casualties and retreat of Napoleon's army are immortalized not just in this graph, but also in Russian literature (Tolstoy's *War and Peace*) and music (Tchaikovsky's 1812 overture, in which five cannon shots mark the battle of Borodino and eleven more mark the arrival of Napoleon in the now-razed city of Moscow).

## Tukey and EDA

For Donoho [-@donoho2017], the publication of John Tukey's "Future of Data Analysis" [-@tukey1962] arguably marks the beginning of data science. As the first data scientist, Tukey embraced a descriptive and exploratory approach to data analysis, particularly in his publication of *Exploratory Data Analysis* [EDA, @tukey1977].

In EDA, Tukey presented an idiosyncratic, coherent approach to looking at data, beginning with tallying. The distributions of small counts of single variables, for Tukey, could best be presented in **stem and leaf displays.** Comparisons between groups can be presented in **box plots.** To examine relationships between variables and the adequacy of simple models, he argues for thoughtfully transforming data to uncover linear trends, then to examine **residuals** to find where these trends do not hold.

## approaches to graphs

A graph might begin with perception and understanding (the consumer), with knowledge and design values (the producer), but it also reflects the truth of the data. How much is each?

In thinking about how to design graphs, we can begin with abstract theory, with principles of design informed by our understanding of perception, or with empirical analyses of understanding and memory.

## Tufte: first principles

Tufte [-@tufte2001] describes **Graphical Excellence**: Graphs should, among other things, "Induce the viewer to think about the substance, rather than about methodology, graphic design, the technology of graphic productions, or something else." Graphs should "Present many numbers in a small space, make large data sets coherent, and encourage the eye to compare different pieces of data." Graphs should "serve a reasonably clear purpose: description, exploration, tabulation, or decoration [and] be closely integrated with the statistical and verbal descriptions of a data set." Tufte concludes with the following *Principles of Graphical Excellence*, which I quote verbatim:

-   Graphical excellence is the well-designed presentation of interesting data---a matter of substance, of statistics, and of design.
-   Graphical excellence consists of complex ideas communicated with clarity, precision and efficiency.
-   Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space.
-   Graphical excellence is nearly always multivariate.
-   And graphical excellence requires telling the truth.

## the politics of data visualization

On a cold morning in January, 1986, Space Shuttle Challenger lifted off from Cape Canaveral. Because of the cold weather, engineers at Morton Thiokol, who designed the rocket boosters, considered the possibility that the O-rings which sealed joints on the rockets would be too hard and brittle to prevent the release of potentially explosive fuel. They examined the relation between temperature and o-ring damage on prior flights, using the following visualization:

![*What Morton Thiokol engineers saw prior to deciding to launch the Challenger Space Shuttle in January, 1986*](images/mortonthiokolChallenger.PNG)

What, if anything, can we see here? The graphic was chaotic and poorly designed; if the engineers had wanted to systematically explore the relation between o-ring damage and temperature they could have removed the rockets and arranged them on a single axis by liftoff-temperature rather on several rows by liftoff-date. Here's what they would have seen:

![*What the engineers could have seen, perhaps, with a better graph*.](images/tufteChallenger.PNG)

The forecast for the morning of the launch was far colder than that for prior launches, and within the set of prior launches a clear relationship between temperature and o-ring damage is evident. But the Challenger did launch, exploding shortly after takeoff, killing the seven astronauts on board. With a clearer display of the data, the Challenger launch would likely have been postponed [@tufte2001].

### poor design leads to an uninformed or misinformed world

In the flood of information that surrounds us, our peripheral and sometimes our focal attention will be drawn to pictorial summaries of consequential events and effects. But while data visualizations and data journalism has become more responsible, poorly designed graphs - what Tufte has described as "chartjunk" - are still common.

Poorly designed graphs don't just confuse rocket scientists. They reach us, mislead us, and make fools of all of us as well. With better graphs, we become a better informed, better educated citizenry.

### poor design can be a tool to deceive

![*Trump as "the first datavis President"* ([Meeks, 2019](https://datastori.es/152-year-in-review-2019/#t=1:15:06.516)).](images/trumpDorian.jpg)

The apps on our smartphones and electronic devices are meticulously designed to maintain our attention. They are largely successful, but, for many of us, made it more difficult to read in depth (Wu, *The Attention Merchants*). With information all around us demanding our attention, visual representations of data have become particularly important. The data visualization is, increasingly, not just a supplement to the story, but the story itself. When this premise is coupled with the idea that the truth is malleable, the product of what our leaders say rather than empirical statements of fact, we approach a dangerous situation, one in which real threats are misrepresented and lives are again endangered.

Presenting information in self-promoting ways includes so-called "Sharpie-gate," where President Trump simply altered a hurricane prediction map in defense of a misstatement.

## the psychology of data visualization

Speaking of America, consider the following:

![*Chernoff's too-clever faces*](images/Chernoff.PNG)

In this figure, from Wainer [-@wainer1981], Chernoff's faces are used to represent multivariate data. The display is loosely based on the (psychological) premise that, from infancy, we are inordinately sensitive to facial features. Consequently, the most powerful, effective, or informative data displays should manipulate facial features. Does this graph succeed? Why or why not? Would an updated model, based on richer, more complex, and more realistic simulations of facial expressions [@thies2015] be more successful?

### the power of animation

Animated data displays bring the dimension of time into data visualization. Here are two brief (\< 5 minutes) animated data displays, each powerful in its own way, and each of which gives the viewer an appreciation of an important social phenomenon.

The first is from the late Hans Rosling, and summarizes a chunk of the Gapminder data. There is a Gapminder package in R if you would like to explore the data further, and perhaps become the next Rosling:

[![](https://img.youtube.com/vi/jbkSRLYSojo/0.jpg)](https://youtu.be/jbkSRLYSojo)

*Rosling and social progress*

The second is from Kim Rees and her ex-colleagues at [Periscopic](https://periscopic.com/) (Rees is now at CapitalOne). For me, it's an important graphic because it tries to overcome what has been called "psychic numbing" - we are sensitive to the individual case, but lose our perspective when faced with large-scale tragedies, so that effectively the more lives lost... the less we care [@slovic2013].

[![](https://img.youtube.com/vi/8R8UOjMy-5k/0.jpg)](https://youtu.be/8R8UOjMy-5k)

*Rees and stolen years*

### telling the truth when the truth is unclear

We live in an uncertain world, and a major challenge in data visualization is how to convey this uncertainty in a way that people understand readily. Examples of this are familiar. Here in Florida, projected hurricane paths are frequently represented by a "cone of uncertainty" surrounding a single track. An alternative approach is to use a display which shows a distribution of possible tracks.

![*Two approaches to displaying hurricane paths*](https://i1.wp.com/datastori.es/wp-content/uploads/2019/01/02.jpg?w=650&h=434)

### visualizing uncertainty

To further illustrate the idea of uncertainty, data visualizations may be animated. To display hurricane outcomes, for example, individual lines in a spaghetti plot might fade in and out [@cox2013]. Another use of animation is suggested by [@hullman2015] who use [hypothetical outcome plots](https://cdn-images-1.medium.com/max/600/1*vol7-537cqnpucRgBP-j9A.gif) rather than simple error bars to illustrate sampling distributions. Empirically, subjects have a better understanding of the meaning of statistical effects when these are represented by dynamic displays.

During the 2016 US Presidential Election, the NY Times graphics presented the range of likely possibilities (here, between the 25th and 75th percentiles) as a dynamically changing ["jittery gauge"](https://www.vis4.net/blog/images/old/jitter4.gif) . Following the election, there was some criticism of these gauges, as readers claimed that they tended to make them anxious. However, it appears that at least some of the anxiety was attributable not to the gauges themselves, but to the [electoral outcome itself](https://www.vis4.net/blog/2016/11/jittery-gauges-election-forecast/). The gauges were back in 2018, and will likely be used again in the future.

Finally, animation may be used not just to display uncertainty or a possible range, but also data displays which both carry a great deal of information and unfold over time, such as [the language of personality development](http://wise.fau.edu/~lanning/EgoDevelopmentsmallest.gif) in my own work.

## exercises

Please consider each of the following data visualizations.

### Smartphone sales {.unnumbered}

![*Which smartphone manufacturers are doing well?*](images/badgraph.PNG)

> Look at the smartphone sales visualization for just a moment, as if you might while reading something else on another screen, or as your ten-year old little sister might look at it, or most of your classmates in high school, or the average person you might see at a rest stop on the Florida Turnpike. What does the graph tell them?
>
> Now look at the diagram more closely. What does it actually say? How could you improve it to make the content of the graph more informative about its title?
>
> Why was the graph designed in this way?
>
> Does this matter?

### Stand your ground {.unnumbered}

![*Does "stand your ground" make us safer?*](images/standyourground.jpg)

> *Again -*
>
> Glance at the visualization, as you might glance at dozens of images on \> your phone or computer screen. What does the graph tell you?
>
> Again, look at it more closely. What does it actually say?
>
> Would you change the figure? If so, how?
>
> Would your changed visualization be "better"? If yes, why?

### Big businesses {.unnumbered}

![*Voronoi's "Top 50 Most Profitable Companies"*](images/voronoi.jpg)

> Finally, What does this visualization tell you at first glance?
>
> Do you think this is a good visualization?
>
> Would Tufte agree? Why or why not?
>
> Are Tufte's principles of data visualization sound?
>
> It's been argued that we live in an attention economy, in which 'eyes' (e.g., in a screen) are more valuable than wealth. How does this relate to the design of data visualizations?

## further reading and resources

If you'd like to learn more, Tufte [-@tufte2001] and his other books are beautiful and thought provoking. Cleveland [-@cleveland1985] examines graphs from a more rigorous psychological and empirical viewpoint. The Data Stories podcasts are often excellent, despite the challenge of an auditory medium for visual display ([especially the episode on Hans Rosling](http://datastori.es/92-a-tribute-to-hans-rosling/)). And Healy[-@healy2017] provides a comprehensive introduction that, in essence, provides the basis for a semester-long course based in the ggplot2 package in R that we will be working with in the next chapter [@wickham2023].

## notes for next revision

add some notes on Healy

<!--chapter:end:07-visualization.Rmd-->

```{r ggplot, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
install.packages("anscombier")
```

# visualization in R with ggplot

In the last chapter, we introduced data visualization, citing "vision-aries" including Edward Tufte and Hans Rosling, inspired works such as Minard's *Carte Figurative* and Periscopic's *stolen years*, as well as a few cautionary tales of misleading and confusing graphs.

Here, in playing with and learning the R package **ggplot**, we begin to move from consumers to creators of data visualizations.

As the first visualization in [@wickham2023] reminds us, data visualization is at the core of exploratory data analysis:

![*Data visualization is at the core of data analysis* ([@wickham2023])](images/dataviscycle.PNG)

In the world of data science, statistical programming is about discovering and communicating truths within your data. This **exploratory data analysis is the corner of science, particularly at a time in which confirmatory studies are increasingly found to be unreproducible**.

Most of your reading will be from Chapter 1 of [@wickham2023], this is intended only as a supplement.

## a picture \> (words, numbers)?

The chapter begins with a quote from John Tukey about the importance of graphs. Yet there is a tendency among some statisticians and scientists to discount graphs, to consider graphic representations of data as less valuable than statistical ones. It is true that, because there are many ways to graph data, and because scientists and data journalists are humans with pre-existing beliefs and values, a graphical displays should not be assumed to simply depict a singular reality. But the same can be said about statistical analyses (see Chapter 9).

To consider the value of statistical versus graphical displays, consider 'Anscombe's quartet' (screenshot below, live at <http://bit.ly/anscombe2019>):

![*An adaptation of Anscombe's "quartet"* [@anscombe1973]](images/spreadsheet61.PNG)

> **Exercise 8_1** *Consider the spreadsheet chunk presented above, which I am characterizing as data collected on a sample of ten primary school children at recess on four consecutive days. Working with your classmates, compute the mean, standard deviation, and correlation between the two measures for one day. Share your results with the class.*

The four pairs of variables in [@anscombe1973] appear statistically "the same," yet the data suggest something else. Additional examples of the problem of relying on simple statistics, in particular correlation coefficients, are considered in the first chapter of Healy [-@healy2017]. Perhaps graphs can reveal truths that statistics can hide.

> **Exercise 8_2** *The Anscombe data is included in base R~~as a library in R~~. Can you find, load, and explore it?*

## Read Wickham's opening chapter

In class, we will review and recreate the plots in section 1.2 of [@wickham2023] and exercises in 1.2.5 and 1.4.3 and 1.5.5

Savor this section: Read slowly, and play around with the RStudio interface. For example, read about the mpg data in the 'help' panel, pull up the mpg data in a view window, and sort through it by clicking on various columns.

![*A screenshot from RStudio, showing the mpg dataset*](images/rstudio62.PNG)

## explore

Try to make a cool graph - one that informs the viewer, and, to paraphrase Tukey, helps us see what we don't expect.

Try several different displays. Which fail? Which succeed? **Be prepared to share your efforts.**

Remember the 15 minute rule, and don't be afraid to screw up. *Each mistake you wisdom*.

#### some sources

The Datacamp ggplot course <https://app.datacamp.com/learn/courses/introduction-to-data-visualization-with-ggplot2>

The Gapminder data <https://cran.r-project.org/web/packages/gapminder/readme/README.html>

A graph in The Economist. Here, Andrew Couch, host of the "Tidy Tuesday" podcast, walks through the recreation of a fairly complex plot from The Economist. Follow along, beginning with the links to GitHub <https://youtu.be/gcDQ_KbXQ3o?si=wFadvTi886hQm6H->

For another example of an Economist-style visualization, there is also this analysis of Global Terrorism Data from Rpubs: <https://rpubs.com/tangerine/economist-plot>. (This appears to be a student assignment). As with the 'movies" data described in an earlier project, the link to the data is no longer valid. To access it, you can establish an account on Kaggle (which links to older data) and/or the global terrorism database at the University of Maryland.

<!--chapter:end:08-ggplot.Rmd-->

```{r probability, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# on probability and statistics

We previously considered Anscombe[-@anscombe1973] and his quartet, and how visualizing data is valuable. This week, we move to a brief discussion of principles of statistics.

## on probability

**Discrete** probability is used to understand the likelihood of categorical events. We can think of initial estimates of probability as subjective or personal. For some events (*what is the probability this plane will crash?*), an estimate of probability can be drawn from a base rate or relative frequency (e.g., *p(this plane will crash) = (number of flights with crashes/ number of flights)*.

For other events (e.g., *what is the probability that a US President will resign or be impeached before completing their term of office?*), it may be hard to arrive at a suitable base rate. Here, a number of subjective beliefs or principles may be combined to arrive at a subjective or personal probability. In a sense, all probability estimates begin with a personal belief such as this, in part because the choice of the most informative base rate is often not self-evident - in the plane crash example, maybe we should consider a reference group rates such as 'for this airline,' or 'for this type of jet' etc.

The claim that there is a simple distinction between 'relative frequencies' and 'personal probabilities' turns out to be an oversimplification. Relative frequencies are not statistical givens, because in many of not most situations there is no single base rate that is clearly superior to others. For the plane crash example, we might consider crash rates among all planes, all jets, and all carriers, or particular planes (Boeing 737 Max jets), particular carriers (United), or the intersection of some or all of these as well as other variables (United 737 Max airliners flying out of LaGuiardia at night). There is no single answer to the plane crash estimate, in other words.

Similarly, a baseball manager, in considering whether a pinch hitter might be brought in to bat at a crucial spot in a game, might consider an omnibus batting average (effectively a relative frequency of hits/opportunities), batting average at night, against this pitcher, etc.

In general, there is not a correct answer to this "problem of the reference class" in part because a more precise reference group (737 Max planes, batting against a particular pitcher) is *inherently* based on a smaller sample of data, and is therefore less stable, than a broader, but coarser reference group upon which a probability estimate might also be based [@lanning1987].

The personal origins of probability estimates should become less important as we are exposed to data and revise our estimates in accordance with Bayes theorem. But over the last 50 years, a substantial body of evidence has demonstrates that, under at least some circumstances, we don't make estimates of probability in this way.

## the rules of probability

Here's an introduction to the principles of probability. These are presented, with examples and code, in this [R markdown document](https://github.com/datasciencelabs/2020/blob/master/06_probability/01_discrete-probability.Rmd) at Harvard's datasciencelabs repository:

> **I. For any event A, 0 \<= P(A) \<= 1**
>
> **II. Let S be the sample space, or set of all possible outcomes. Then P(S) = 1, and P(not S) = 0.**
>
> **III. If P(A and B) = 0, then P(A or B) = P(A) + P(B).**
>
> **IV. P(A\|B) = P(A and B)/ P(B)**

Principle III applies for **mutually exclusive** events, such as A = you are in class this morning, B = you are at the beach this morning. For mutually exclusive (disjoint, disjunctive) events, the union is the sum of the two events. This is called the addition rule for disjoint events.

A different rule applies for events that are **mutually independent**, such as (A = I toss a coin and it lands on 'Heads') and (B = it will rain tomorrow). What we mean by independent is that our estimates of the probability of one don't change based on the state of the other - your estimate of the likelihood of rain shouldn't depend on my coin flip. Here, you multiply rather than add:

> **If P (A\|B) = P (A), then P (A and B) = P(A) P(B).**

In words - if the probability of A given B equals the probability of A, then the probability of both A and B equals the probability of A times the probability of B.

This **multiplication rule** is handy for estimating the probability of an outcome that happens following a chain of independent events, such as the probability that the next eight times I toss a coin it will land on "tails" every time:

> P (TTTTTTTT) = P(T) P(T) P(T) P(T) P(T) P(T) P(T) P(T). = .58 = 1/256.

Many sets of events are neither disjoint nor independent, so we need more general ways of thinking about pairs of events. For most of us, Venn diagrams are useful to think about combining probabilities. The **union or P(A U B)** describes the probability that A, B, or both of these will occur. Here, you will use the **general addition rule:**

> **P(A or B) = P(A) + P(B) - P(A and B)**

(the probability of A or B is the probability of A plus the probability of B minus the probability of both A and B).

For the **intersection or P(A ∩ B)**, we need to consider **conditional probabilities**. Think of the probability of two events sequentially: First, what's the probability of A? Second, what's the probability of B, given that A has occurred? Multiply these to get the likelihood of A and B:

> **P(A and B) = P(A) P(B\|A).**

*Example: The probability of you and your roommate both getting mononucleosis equals the probability of your getting mono times the probability that your roommate gets it, given that you have it also.*

This is the **general multiplication rule**. In this abstract example, the order is irrelevant. To estimate the likelihood of A and B, we could as easily take the probability of B, and multiply it by the conditional probability of A given B

> **P(A and B) = P(B) P(A\|B).**

*Use the mono example again. What are A and B here? Does it still make sense? When might P (B\|A) make more sense than P (A\|B)?*

We are often interested in estimating conditional probabilities, in which case we'll use the same equation, but solve instead for P (A\|B). This leads us back to principle IV:

> **IV. P(A\|B) = P (A and B)/ P(B)**

### keeping conditional probabilities straight

In general, P (B\|A) and P (A\|B) are not equivalent.

Here, one could use a Venn diagram to model this asymmetry. There are several R packages for doing this, including Venn and VennDiagram.

#### exercise 9.1

> In 2024, the Florida Highway Patrol won a national competition for "best looking cruiser." The winning car was a Dodge Charger.

![](images/FloridaCar.jpg)

> Not all FHP cruisers are Dodge Chargers, but some are. Assume that there are 8 million registered cars in Florida, that all cars (including all FHP cruisers) are registered, and that 80,000 of these (or 1% of all cars) are Dodge Chargers.
>
> a)  On the basis of the above information, if you see a Dodge Charger on the road, can you compute the probability that it is an FHP cruiser (i.e., P(FHP cruiser \| Dodge Charger)?
>
> b)  If you can compute this, what is the probability? If you cannot compute this, what is the minimum additional information would you need to compute this probability (P(FHP cruiser \| Dodge Charger)?
>
> c)  Provide a reasonable estimate of this additional value, then compute (P(FHP cruiser \| Dodge Charger).
>
> d)  Working with your own numbers, what is P(Dodge Charger \| FHP cruiser)?
>
> e)  How confident are you in these results? Are there any additional assumptions that you might make that would make you more confident about your results?

#### **exercise 9.2**

a)  Sketch out a Venn Diagram that accurately reflects the relationships you described in exercise 9.1.

b)  Use R to generate your Venn Diagram.

c)  Look at your figure. In general, if P (A\|B) \< P (B\|A), what must be true of the relationship of P (A) to P (B)?

### Venn diagrams should generally be asymmetrical

Answers to this exercise are at the end of the book. There are two points to these exercises. The first is that Venn diagrams should often be asymmetrical. The second is that problems about probability are often easier if we draw Venn diagrams.

## continuous probability distributions

We can also use probability with **continuous** variables such as systolic blood pressure (that's the first one), which has a mean of approximately 120 and a standard deviation of 15. Continuous probability distributions are handy tools for thinking about the meaning of scores, particularly when we express scores in standard deviations from the mean (z scores). More to the point, this way of thinking about probability is widely used in questions of scientific inference, as, for example, in testing hypotheses such that "the average systolic blood pressure among a group of people studying at a coffee shop (hence caffeinated) will be significantly greater than that of the population as a whole."

This is part of the logic of **Null Hypothesis Significance Testing (NHST)** - if the result in my coffee shop sample is sufficiently high, then I say that I have rejected the null hypothesis, and found data which support the hypothesis of interest.

## dangerous equations

Just as Tufte [-@tufte2001] demonstrated that poor data visualizations can be dangerous, leading, for example, to the loss of life in the Challenger disaster, Wainer [-@wainer2007] shows that a lack of statistical literacy is also "dangerous."

Wainer cites three specific examples of important, yet widely misunderstood, statistical laws. The first of these is deMoivre's equation, which shows that variability decreases with the square root of sample size. Because the variability of a sample decreases with the size of that sample, small samples tend to have extreme scores. For example, the counties with the highest and lowest rates of kidney cancer (or most other unexplained health measures) will be sparsely populated, typically rural places.

For Wainer, a second form of statistical illiteracy is the failure to understand the complex interdependencies that arise in multiple regresson analysis, in particular, how coefficients may change or even reverse in sign when new variables are added as predictors.

Wainer's third example of statistical illiteracy is the failure to appreciate regression to the mean. I consider this to be the most dangerous form of statistical illiteracy, in part because regression effects contribute to an overestimate of the effectiveness of punishment and an under-appreciation of the effectiveness of positive reinforcement as tools for behavior change [@hastie2010].

<!--chapter:end:09-probabilityAndInference.Rmd-->

```{r reproducibility, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# reproducibility and the replication crisis

Probability theory is elegant, and the logic of **Null Hypothesis Significance Testing (NHST)** is compelling. But philosophers of science have long recognized that this is not really how science works [@lakatos1969]. That is, science is not primarily built by the testing and rejecting of null hypotheses. (Think, for example, of how you might describe an experiment for testing the hypothesis that gravity exists, and whether you would ever reject this hypothesis).

The problem is a multifaceted one. It arises partly because, in any experiment, there is a large (infinite?) number of conditions which might be invoked to explain-away results of a failure of our work. It arises because we are human, and prone to various biases of decision making and information integration (that is, we are poor Bayesians). It arises, too, because scientific institutions such as journals, funding agencies, and universities are competitive environments which incentivize positive findings.

In recent years, the tension between **the false ideal of NHST** and the real world of science has become increasingly evident. Within psychology, experimental studies have often - even typically - failed to replicate well-known results [@opensciencecollaboration2015]. It's not just psychology [@baker2016]. One of the first important papers to shine light in the area [@ioannidis2005] came from medicine; it suggested six contributing factors, which I quote verbatim here:

> *The smaller the studies conducted in a scientific field, the less likely the research findings are to be true.*
>
> -   This stems directly from our discussion of the central limit theorem and the instability of results from small samples.
>
> *The smaller the effect sizes in a scientific field, the less likely the research findings are to be true*
>
> -   We'll talk about effect size below.
>
> *The greater the number and the lesser the selection of tested relationships in a scientific field, the less likely the research findings are to be true.* (and) *The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true.*
>
> -   The "problem" of analytic flexibility leads to 'p-hacking'
>
> *The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true* and *The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true.*
>
> -   Positive findings rise, and negative ones are ignored. And scientists are human, and subject to incentives.

Here's a video which provides some more context for the crisis:

[![Reproducibility](images/reprostill.PNG)](https://youtu.be/42QuXLucH3Q) .

\*Video 10.1: On the reproducibility crisis (12 mins)

## answers to the reproducibility crisis

For scientific progress to occur, there needs to be more than a simple rejection of what is wrong with current methods and research programs: Here, as elsewhere, criticism must be constructive to be valuable.

There have been a number of solutions proposed to the reproducibility crisis.

### partial answer 1: Tweak or abandon NHST

The first cluster of responses to the reproducibility crisis is concerned with statistics, specifically problems with Null Hypothesis Significance Testing (NHST). These include (a) justifying one's alpha - making it more stringent, for example, for counter-intuitive claims [@grange2018], (b) changing the default p value from .05 to .005 [@benjamin2018], and (c) abandoning significance testing altogether [@mcshane2017].

[@szucs2017] goes into some of these issues in more detail and discusses other limitations of significance testing, including the dichotomous, all-or-none silliness of the accept/reject decision. (If you play the NHST game, there is no 'almost' significant, 'approached significance,' 'highly significant', etc.).

[@leek2015] argue that the problems are not merely with NHST, but with the whole of data analysis. They maintain that better training in data science - courses like ours, perhaps, are part of the answer.

![leek2015](images/leek2015pipeline.PNG) (figure)

[@munafo2017] also argue that threats to reproducible science occur at a number of places in science, not just with the evaluation of hypotheses.

![munafo2017](images/munafo2017threats.PNG)

### partial answer 2: Keep a log of every step of every analysis in R markdown or Jupyter notebooks

A second cluster of responses is concerned with keeping good records. Let's say that you are running a study, say, which looks at the hypothesis of differential variation of females and males in a cognitive measure; your interest is to critically examine the hypothesis discussed by [@wainer2007] that males show more variability.

There have been *a lot* of studies related to this over the years, so that rather than collect new data you decide that you will work with existing data from several online archives. You find and download spreadsheets from two studies: In the first, gender is coded '1' for male, '2' for female. In the second, gender is coded '1' for female, '2' for male, and '3' for other. There are, in essence, two ways that you can combine the variables into a common format: The first would be to take one of the spreadsheets and do a few find-and-replace commands on the appropriate column of the data. This is quick and easy - but when someone else, or even future you, returns to this data, you will not remember if you have recoded it.

The alternative is to keep a record of your work in R markdown. This is more time consuming, and can sometimes be clumsy. But it is ~~virtuous~~ useful and clear - and when you screw up, you will have a full record of what happened.

Part of the problem of scientific reproducibility is to keep comprehensive records. This record-keeping and research transparency is at the heart of R markdown documents.

### partial answer 3: Pre-registration of your research

The third answer to the reproducibility crisis is the most comprehensive; it involves not merely keeping a record of what you have done, but preregistering your work, that is, fully specifying your planned analyses beforehand [@miguel2014]. The author, an economist, outlines his argument in a five-minute video [here](https://www.futurelearn.com/courses/open-social-science-research/0/steps/31436). For randomized controlled trials, consider socialscienceregistry.org, and for more general use, use the open science framework page.

## further readings

Finally, if you would like to learn more about the reproducibility crisis, there is a recent collection of papers in Nature [here](https://www.nature.com/collections/prbfkwmwvz/).


## notes for next revision

This chapter could include more recent scholarship on the reproducibility crisis and links to sites including OSF. It could also introduce a discussion of bootstrapping, effect-size estimation... n empirical approach to reproducibility (e.g., k-fold sampling).

<!--chapter:end:10-reproducibility.Rmd-->

```{r wrangling, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# (PART) Part III Towards data proficiency {.unnumbered}

# wrangling and tidying

![*wild horses are beautiful, wild data ... not so much*](images/horses.png)


The term 'wrangling' has been used to describe the process by which unruly, messy, and complex datasets are organized and restructured so that they can be summarized, interpreted, and understood. *Wrangling* includes finding errors, which requires looking at your data closely enough to identify problems, and transforming the data as needed to allow data analyses to take place. *Tidying* is also concerned with data preparation, but is focused on the narrower task of arranging data into simple data frames (tibbles) in which each variable is in its own column, each observation is in its own row, and each cell includes one and only one value.

In practice, the processes of data wrangling and data tidying are often overlapping. In this chapter, we will explore (a little) wrangling and (some) tidying using simulated data. First, we briefly consider some characteristics of the tidyverse

## the structure of the tidyverse

The R programming language was initially intended to serve engineers, and the tidyverse can be seen as a collection of packages which are aimed at a broader audience of statisticians and data scientists, and to help them become more productive [@peng2018]. Tidyverse packages have a common syntax that makes it easier to generate and understand code and create reproducible results. Tidyverse packages may be described as having three layers.

**The core of the tidyverse** includes nine packages. You will see the list of these packages when you load the tidyverse:

```{r echo=TRUE}
library(tidyverse)
```

Each package includes a number of functions. Many tidyverse packages are described on handy [cheatsheets](https://posit.co/resources/cheatsheets/) such as this one for dplyr:

[![Dplyr Cheatsheet](images/dplyrcheatsheet.png)](https://rstudio.github.io/cheatsheets/data-transformation.pdf)

**The periphery of the tidyverse** consists of additional packages which are less commonly invoked than those in the tidyverse core. Unlike the core packages, all of these peripheral or auxiliary packages must be loaded into memory by an explicit statement, e.g., *library(googlesheets4).* You can see the list of all (core and peripheral) packages with the tidyverse_packages command:

> *\> \# install.packages("tidyverse")\
> \> tidyverse_packages ()*

For me, the most useful packages in this peripheral or auxiliary layer have been **googledrive**, **googlesheets4**, and **readxl** (for working with Google apps and Excel files). I've also used **rvest**, **xml2**, and **jsonlite** (for web scraping and parsing more complex data structures).

In addition to the core and periphery of the tidyverse, there is a more loosely-defined third layer of what may be considered to be **tidyverse-friendly packages**. These packages, which are not installed with the core and peripheral tidyverse, include a wide range of tools that are handy for using tidy syntax in particular applications, such as **janitor** (for cleaning data), **tidytext** (for text analysis), and **tidygraph** (for network analysis). When you use these, you'll need to first make sure that they are installed on your machine. (R studio will remind you to do this). Then you can load them into memory.

At each of these three levels, the tidyverse is constantly evolving. For example, at this writing, one of the peripheral or auxiliary packages (**httr**) has been superseded by a newer package (**httr2**); despite this, httr (and not httr2) remains in the auxiliary layer.

## where should we eat?

Tidyverse functions from at least two of the core tidyverse packages, **dplyr** and **tidyr**, help wrangle, clean, and shape our data into a tidy form that can 'spark joy' [@kondo2016]. To learn more about tidy data and how we use dplyr and tidyr in everyday coding, we will construct and explore a simulated dataset, consisting of a 100 restaurants, each of which is described by just a few variables.

To do this, we'll first name the restaurants. We begin with the data from the babynames package, then will use *filter* function (from dplyr) to choose only names since 1981, then we *group* and *summarize* the data by name, then *arrange* (sort) the names by popularity, then *slice* (take only) the first 100 observations, then finally *select* only the name column. Each of these functions is part of the dplyr package:

```{r}
library(babynames)
# make restaurant names
restodata <- babynames |> 
    filter(year > 1980) |> 
    group_by(name) |> 
    summarise(n=sum(n)) |> 
    arrange(desc(n)) |> 
    slice(1:100) |> 
    select(name) 
head(restodata,3)
```

In the prior chunk of text we wrangled - or, if you prefer, massaged - the babynames data in a few ways to get us a simple list of 100 popular names. But this is only a start. Let's change the set of 'person names' into 'restaurant names.' To do this, we will use *mutate* (another dplyr function) then combine each name with the phrase *'s_place.*' To combine strings, we use the *str_c* function. This is from the **stringr** package, which is another package in the core of the tidyverse:

```{r}
restodata <- restodata |> 
    mutate (resto_name = str_c(name,"s_place")) 
head(restodata,3)
```

Now, assume that the restaurants has been graded by the Department of Health on an A to F scale. We'll assign these *health* ratings randomly, using the following three steps.

> *First, we seed the (pseudo) random number generator; this allows the result to be reproducible. If you don't do this, you will almost certainly get different ratings each time you run the code.*
>
> *Then, we declare health as a random integer for each restaurant on 1 to 5 scale. We can do this in several ways. Here, I use the sample function to directly generate my 100 integers. (I originally used runif, which generates random numbers, then rounded this score to the nearest integer, but this is more complicated, and leads to fewer observations with extreme values. It's commented out in my code.).*
>
> *Then, we recode these as letter grades using the case_when function. This function is akin to an 'if-else' statement. Note that the case_when syntax is tricky. If we think of the double equals as 'is equal to', the tilde as 'call it', and the comma as 'else,' the first part of the case when statement becomes the following:*
>
> *when health is equal to 1, call it F, else ...*

In addition to health, assume that we have cost data as well. We'll use essentially the same code to come up with our cost variable. Then we drop our original name variable.

```{r}
# add health ratings
set.seed(33458)
restodata <- restodata |>
#    mutate(health = round(runif(n=n(),1,5))) |> 
    mutate (health = sample(1:5, 100, replace=T)) |>  
    mutate(health = case_when(health == 1 ~ 'F',
                                health == 2 ~ 'D',
                                health == 3 ~ 'C',
                                health == 4 ~ 'B',
                                health == 5 ~ 'A')) |> 
    mutate(cost = sample(1:3, 100, replace=T)) |>
    mutate(cost = case_when(cost == 1 ~ '$',
                              cost == 2 ~ '$$',
                              cost == 3 ~ '$$$')) |> 
    select(-name)
head(restodata,3)
```

Each restaurant now has a restaurant name, a health rating, and a classification as cheap (\$), moderate (\$\$), or expensive (\$\$\$). Now we add an additional variable - *popularity*. This is again randomly generated on a 1 to 5 scale. But restaurants that are popular at lunch may not be popular at breakfast or dinner (they may cater to office workers, for example), and some restaurants may be closed for one or more of these meals - say, 30% at breakfast, 25% at lunch and 15% at dinner. Here, there are four separate steps:

> *Create 'breakfastdata' as a copy of the fakerestodata*
>
> *Generate a popularity score as a random variable.*
>
> *Generate a second random variable called 'meal', which is initially set to a random integer between 1 and 100.*
>
> *Then, use the case_when syntax to recode this: if mean is below a cutoff (30 for Breakfast), set the value to NA (missing), otherwise call it 'Breakfast.'*
>
> *Then do the same for 'lunchdata' and for 'dinnerdata.' We'll use different cutoffs here as we expect that more restaurants will be open for dinner, and fewer for breakfast.*
>
> *Then bind the lunchdata and the dinnerdata to the breakfastdata.*
>
> *Finally, remove the lines where the meal data is missing.*

We are calling this dataset 'tallrestodata' for reasons that will become apparent.

```{r}
# note that the same code runs three times 
# we should simplify this by creating a function
breakfastdata <- restodata |>
    mutate (popularity = sample(1:5, 100, replace=T)) |> 
    mutate (meal = sample(1:100, 100, replace=F)) |> 
    mutate (meal = case_when (meal < 31 ~ NA, 
                              TRUE ~ "Breakfast" ))
lunchdata <- restodata |>
    mutate (popularity = sample(1:5, 100, replace=T)) |> 
    mutate (meal = sample(1:100, 100, replace=F)) |> 
    mutate (meal = case_when (meal < 26 ~ NA, 
                              TRUE ~ "Lunch" ))
dinnerdata <- restodata |>
    mutate (popularity = sample(1:5, 100, replace=T)) |> 
    mutate (meal = sample(1:100, 100, replace=F)) |> 
    mutate (meal = case_when (meal < 16 ~ NA, 
                              TRUE ~ "Dinner" ))
tallrestodata <- lunchdata |> 
    bind_rows(dinnerdata, breakfastdata) |> 
    drop_na(meal)
head(tallrestodata,3)
```

### tall and wide formats

*Tallrestodata* is tidy - each column is a variable, each row is an observation, and each cell includes a unique value. But it may not be exactly what we need. I might, for example, want to go to a popular restaurant when no one else is there - perhaps to go to breakfast at a restaurant that is popular only at lunch and dinner.

One approach to this is to create a new variable in the data for the (average popularity at lunch and dinner) minus (popularity at breakfast). High scores on this would give us restaurants that might be really good or interesting, but not crowded, for a breakfast meal. But popularity at breakfast, lunch, and dinner are in different rows of the restodata; so this is a little tricky.

The easiest way to do this is to use the *pivot_wider* function from the tidyr package. Again, the syntax is challenging - you may find the [cheatsheet](https://rstudio.github.io/cheatsheets/tidyr.pdf) to be useful. This will reshape the data from tall and narrow to a format that is typically wider and shorter.

From this wider data, we use mutate twice t ultimately create a new variable called *secretBreakfastPlace*.

```{r}
widerestodata <- tallrestodata |> 
    pivot_wider(names_from = meal,
                names_prefix = "popularity_",
                values_from = 
                    c(popularity)) |> 
# we could create the secretBreakfast variable
# in a single step, but this is probably more clear
    mutate (lunchDinnerPop = (popularity_Lunch +
                popularity_Dinner)/2) |> 
    mutate (secretBreakfastPlace = lunchDinnerPop -
                popularity_Breakfast) 
head(widerestodata)
```

Although the *widerestodata* has what we want, it is not tidy: The 'popularity' variable is now in three columns rather than one, and there are many cells with missing values. So we reshape the data back to its tall and tidy form, but which now includes the variable describing the two new mutated variables. Then we sort it by our desired variable, secretBreakfastPlace

```{r}
tallrestodata <- widerestodata |> 
    pivot_longer(names_to = 'meal',
                names_prefix = "popularity_",
                 cols = c('popularity_Breakfast',
                          'popularity_Lunch',
                          'popularity_Dinner'),
                values_to = 
                    ('popularity'),
                values_drop_na = TRUE) |> 
    arrange(desc(secretBreakfastPlace))
head(tallrestodata)
```

### exercises

**Study and understand the code**. Ask and see if you can answer questions about each chunk of code. For example, there are 100 observations in each of the breakfast, lunch, and dinner datasets. How many are there in tall and wide restodata sets? Why?

**Expand on the code: Where you would *really* want to eat**? Look at the data and think about it. Make it more realistic if you can. How would you actually decide? Come up with a decision rule that you might use for choosing a restaurant. This rule might include filters and/or simple algebraic expressions (such as '+' or '-'). Express your decision rule using code, then select your best restaurant. You may also want to create new variables such as distance, ambiance, or type of cuisine.

## more on tidy coding

There are many sources for help. The help panel in R Studio is a start, but you may have better look on Google. Here are some suggestions:

1.  **work with tidy data.** Make each row an observation, and each column a variable. Complex data sets, such as samples of text, become much more manageable when reduced to simple arrays.

2.  **think in tidy; talk the talk**. For example, recognize that **%\>% or \|\>** (the pipe) means *then*. Statements with pipes begin with data, may include queries (extract, combine, arrange), and finish with a command.

3.  **search for tidyverse solutions.** When you have a problem in your code, for example, "how do I compute the mean for different groups of a variable in R?," do a Google search for *R mean groups tidyverse*, not just *R mean groups.* This will get you in the habit of working with tidy solutions where they can be found.

4.  **look for new answers**. Because R and the tidyverse are constantly evolving, consider looking at recent pages first. In your Google search bar, click on Tools -\> Past year).

5.  **adhere to good coding style.** Well-written code is reasonably parsimonious, readable, and easily debugged. There are a few style manuals for R, including one from [Hadley](http://adv-r.had.co.nz/Style.html), and this [Rchaeological Commentary] (<https://cran.r-project.org/web/packages/rockchalk/vignettes/Rstyle.pdf>).

6.  **write functions.** If you repeat a section of code, rewrite it as a function. (See the example above).

7.  **annotate your work**. Assume that you will come back to it at a later date, while working on a different project, and use portions of your current code. Your R markdown documents should be a log. When you run in to a significant, challenging problem, don't delete your mistakes, but \## comment them out - as I have done in a few places above.

```{r echo=TRUE, results="hide", message=FALSE}
library(gapminder)
b <- gapminder %>% 
# when should you comment out an error
# instead of deleting it? for me, I'll 
# comment out errors that took me a long time 
# to solve, and/or that I'll learn from. 
# Probably not here, in other words...
#  filter(lifeExp) > 70 bad parens
    filter(lifeExp > 70)
```

Finally, **maintain perspective.** Your need to solve problems (how to analyze x, etc.) should not take a back seat to your desire to write the best code. There is almost always a better way to do things. Strive reasonably to accomplish this, but be prepared to **kludge**.

<!--chapter:end:11-dataWrangling.Rmd-->

```{r findingdata, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(janitor)
library(tidyverse)
library(corrr)
library(kableExtra)
```

# finding, exploring, cleaning, and combining data

**Data science, oddly enough, begins not with R... but with *data*.** There is no shortage of datasets available to analyze, and each can give rise to a host of interesting analyses and insights.

What do you want to study? Let's begin by looking at schools

## florida educational data

Florida, like many states, makes data on school quality publicly available. Schools are assessed, in part, on student performance (scores on comprehensive tests in fields such as English Language Arts). Schools are also assessed on measures such as whether this performance has increased across years, the percent of students who graduate in four years, and the percentage of students who pass Advanced Placement and related exams. You can learn more about these measures at <https://www.fldoe.org/core/fileparse.php/18534/urlt/SchoolGradesOverview24.pdf>).

The data itself are available in an Excel spreadsheet. Here's a screenshot of the first few columns and rows of the file.

![](images/FloridaSchools.png)

It's apparent that the file is a little messy. As we saw in a prior chapter, we would like the first row of the dataset to include the variable (column) names: here, there are three rows of header prior to this. Further, many of the variable names include spaces, minus signs, and the like.

We will first download the data from the web (at this writing, you can find it at <https://www.fldoe.org/file/18534/SchoolGrades24.xlsx>). We will store it on our disk in a subdirectory of our project folder called 'data.'

![*Slash: Windows or Mac?*](images/slash.png)

### a digression: Slash, Windows and the world.

Once we download the data, we need to tell R where to find it. If we are working with an *Rproject*, we might keep everything - code, data, and output - in the same directory. But we often need or want to store data in a separate place, in which case you will need to specify a file path, which will include one or more slashes (and not the Guns N' Roses guitarist kind).

In most of the computing world, including Macs, filepaths are delineated by forward slashes ("/"). On a Windows machine, they instead include backwards slashes ("\\"). To further complicate matters, **the backwards slash has a special significance as an** **'escape' character** - this means, as we will see briefly below as well as in the chapter on text analysis, that it tells the system to interpret the following character literally (for example, a comma is read as a *comma*) rather than symbolically (where a comma might be read as a separator between two objects).

In any operating system, we can locate files using relative paths (starting in your project directory) or absolute ones (starting in your computer's root directory; see below). Relative paths generally work better, as you can use the code on multiple machines. But if you can't find your datafile, try the absolute path as a kludge.

### getting data from our machine into R

Ones we have specified the datapath, we can use the read_excel command, which is in the readxl library, which is part of the peripheral tidyverse (so you do not need to first install it on your computer). We'll tell R to skip the first three lines of text.

We'll continue the pipe with a simple command from the janitor package - which you will need to load on to your machine. That command gets rid of spaces in variable names and replaces them with *camelCase* or, the default, **snake_case**. (To see why you want to do this try omitting this line from your code).

```{r}
# relative path with backward slashes replaced by forward ones
datadir <- "data/" # this is a relative path
# absolute path with backward slashes made literal with escapes
# datadir <- "C:\\Users\\me\\OneDrive\\GitRepos\\DSLA25\\data\\"

FloridaSchools <- read_excel(
# here, we could also just do 
#   "data/SchoolGrades24.xlsx"), 
    paste0(datadir, "SchoolGrades24.xlsx"),
    skip = 3) |>
    clean_names() 
```

### which ones are "high schools"?

There are a few ways that we could reduce this to just High Schools - one is to include only schools which report a graduation rate that is not a missing value; the other is to include just schools that are explicitly named "high school." (We could also use both of these, or something else). Here, we will use the latter - filtering on schools which have "HIGH SCHOOL" in the school_name.

We then reduce the columns to a handful of measures of interest.

```{r}
FloridaHighSchools <- FloridaSchools |> 
#   drop_na('graduation_rate_2022_23') |> 
    filter(str_detect(school_name,
                      "HIGH SCHOOL")) |> 
    select (district_number, district_name, 
            school_number, school_name,
            english_language_arts_achievement,
            mathematics_achievement,
            science_achievement,
            social_studies_achievement,
            graduation_rate_2022_23,
            grade_2024,
            percent_of_economically_disadvantaged_students)
head(FloridaHighSchools)

```

### can we compute district (county) means from these data?

We can reduce the set of high schools to one line per district, with scores the simple means of all schools in the district.

```{r}
FloridaHighSchoolsbyDistict <- FloridaHighSchools |> 
 #   select(-grade, -school_name) |> 
    group_by(district_name) |> 
    summarise_if(is.numeric, mean, na.rm=TRUE)
```

These average scores should be viewed with skepticism, because it treats small and large schools as equal. Consider graduation rates: If a district has just two schools, one with just 10 students (and graduates all of them), and a larger school with 990 students (but graduates only half - 495 - of them), we would get an estimated graduation rate of 75% ((1.0 + .5)/2). But actually, the district-wide graduation rate would be 50.5%. School enrollment data is needed to accurately estimate district effects from individual schools.

### estimating school enrollments

At this writing (March 2025), I can't find a dataset on Florida HS enrollments that is free, recent, easy to pull down, and reasonably comprehensive. For our purposes, we can work with an estimate of this: There are measures of school size in football league data (<https://fhsaa.com/news/2023/12/21/football-classifications-available-for-2024-25-2025-26.aspx>).

The names for schools there are formatted differently from our other dataset, so this will take a little work. We will first read in the data, then estimate enrollments based on the class of the school (Rural, 1R, 1A, 2A, 3A, 4A, 5A, 6A, 7A).

Can you describe what is happening in each line of code in this section?

Note the use of the escape character in this chunk

```{r}
EnrollmentsFromFHSAA <- read_excel(
    paste0(datadir, "Football_2024_26.xlsx"),
        skip = 1) |>
    clean_names()
head(EnrollmentsFromFHSAA)

EnrollmentsFromFHSAA <- EnrollmentsFromFHSAA |> 
    separate(col =  "school_name",
             into =  c("school_name", "school_place"), 
             sep = "\\(", extra = "merge") |> 
    mutate (est_enrollment =
            case_when(
                class == "Rural" ~ mean(111,558),
                class == "1R" ~ mean(111,558),
                class == "1A" ~ mean(61,643),
                class == "2A" ~ mean(644,1166),
                class == "3A" ~ mean(1167,1542),
                class == "4A" ~ mean(1543,1822),
                class == "5A" ~ mean(1823,2135),
                class == "6A" ~ mean(2136,2512),
                class == "7A" ~ mean(2512,4627), 
                TRUE ~ NA)) |> 
    mutate(school_name = toupper(school_name)) |> 
    mutate(school_place = str_remove(school_place,"\\)"))
```

When we run this chunk, we get a warning. What is it about? Is it ok?

## combining datasets

The enrollment data are now in the EnrollmentsFromFHSAA dataset. We first edit the school names from the FloridaHighSchools file to see if we can get them to match. Then we try to merge (left join) these with the FloridaHighSchools data, using the school_name variable as the key.

```{r}

FloridaHighSchools <- FloridaHighSchools |>
    mutate(school_name = 
               str_replace(school_name,
                           "JUNIOR/SENIOR HIGH SCHOOL","")) |>
    mutate(school_name = 
               str_replace(school_name,
                           "MIDDLE/HIGH SCHOOL","")) |>
    mutate(school_name = 
               str_replace(school_name,
                           "SENIOR HIGH SCHOOL","")) |> 
    mutate(school_name = 
               str_replace(school_name,
                           "HIGH SCHOOL",""))
FloridaHighSchools2 <- FloridaHighSchools |>
    left_join(EnrollmentsFromFHSAA, by = "school_name")
```

But when we do this, we get another warning message. Why is it there?

### challenges in joining datasets

In each of the two datasets, there are duplicate names - for example, there are two "Atlantic" High Schools in the both the Florida High School (FHS) and the Florida High School Athletic Association (FHSAA) datasets. We can see this by looking at the output of the following commands:

```{r}
SchoolsWithSameNamesInFHS <- FloridaHighSchools |> 
    group_by(school_name) |>
    filter(n() > 1) |> 
#    select (school_name, district_name) |> 
    ungroup() |> 
    arrange(school_name)

SchoolsWithSameNamesInEnrollments <- EnrollmentsFromFHSAA |> 
    group_by(school_name) |>
    filter(n() > 1)  |> 
#    select (school_name, school_place) |> 
    ungroup() |> 
    arrange(school_name)
```

There are 12 schools (6 pairs) in the FHS data, and 26 schools (13 pairs) in the FHSAA data. Ten of these schools are in common across the two sets, the remainder are unique to one or the other.

### some approaches to fixing the data

In dealing with large datasets, it is fairly common that code will work correctly for a large majority of the cases, and that creativity is needed to efficiently fix the remainder. Here, there are at least four (non-mutually-exclusive) approaches that might work. In decreasing order of comprehensiveness:

-   The first approach would be to dig deeper, and to find another source for the school enrollment data. The data we have are imperfect in at least three ways:

    -   Enrollment data are missing for many schools (i.e., "independent" schools).

    -   Many schools on this list do not show up in the FHS list

    -   The enrollment data we have from these schools are estimates, not actual counts

-   The second approach would be to find a geographic dataset that includes towns and counties, then to use this as a key to join our two high school files. If we were working with a larger dataset, this would be worthwhile to try.

-   The third approach would be to manually edit the datasets so that we could include the schools with multiple names.

-   The fourth approach would be to run an initial analysis on the data we have, putting aside the duplicate schools. In the event that the results warrant closer analysis, we could then move to a more comprehensive solution. This is the place to begin:

#### the simplest approach

In order to merge the data, we first need to make sure that there are no spaces or tabs in the "school_name" variable that will join the datasets.

```{r}
SchoolsWithUniqueNamesInFHS <- FloridaHighSchools |> 
    group_by(school_name) |>
    mutate(school_name = str_trim(school_name)) |> 
    filter(n() == 1) |> 
    ungroup()

SchoolsWithUniqueNamesInEnrollments <- EnrollmentsFromFHSAA |> 
    group_by(school_name) |>
    mutate(school_name = str_trim(school_name)) |> 
    filter(n() == 1) |> 
    ungroup()

FloridaHighSchools2 <- SchoolsWithUniqueNamesInFHS |>
    left_join(SchoolsWithUniqueNamesInEnrollments, 
              by = "school_name")
```

We compute the estimated graduation rate for districts (adjGradRate) by weighing schools by their estimated enrollments as follows

```{r}

GradCountsRates <- FloridaHighSchools2 |> 
    mutate(est_Ngrads = 
               round(est_enrollment * .01 *
               graduation_rate_2022_23),0) |>
    select(district_name, est_enrollment, est_Ngrads) |> 
    group_by(district_name) |> 
    summarise_if(is.numeric,sum, na.rm = TRUE) |> 
    mutate(adjGradRate = est_Ngrads / est_enrollment)
```

#### adding in the schools with duplicate names

If we wanted to examine the high school enrollment and graduation data more closely, we can manually rename the duplicate school names. Here's one approach:

Begin by cleaning up the white space in the school name field (str_trim). Then, in the FHS data, rename schools by concatenating (str_c) school name and district name. Then manually edit the relevant cases in the FHSAA data (using mutate + case_when), and finally concatenating school and district name here as well.

```{r}
FixNamesInFHS <- SchoolsWithSameNamesInFHS |> 
    mutate(school_name = str_trim(school_name)) |> 
    mutate(school_name = str_c(school_name, "_",district_name))
FixNamesInFHSAA <- SchoolsWithSameNamesInEnrollments |> 
    mutate(school_name = str_trim(school_name)) |> 
    mutate(district_name = case_when (
        school_place == "Delray Beach" ~
            "PALM BEACH", 
        school_place == "Port Orange" ~
            "VOLUSIA", 
        school_place == "Fort Myers" ~
            "LEE", 
        school_place == "Kissimmee" ~
            "OSCEOLA",
        school_place == "Oakland Park" ~
            "BROWARD", 
        school_place == "St. Petersburg" ~
            "PINELLAS", 
        school_place == "Riverview" ~
            "HILLSBOROUGH", 
        school_place == "Sarasota" ~
            "SARASOTA", 
        school_place == "Sanford" ~
            "PINELLAS", 
        school_place == "Seminole" ~
            "SEMINOLE", 
        TRUE ~ "")
    ) |> 
        mutate(school_name = str_c(school_name, "_",district_name)) |> select(-district_name)
```

then we add these data back in with the FHS and FHSAA data and rerun the analysis

```{r}
FHSdata <- SchoolsWithUniqueNamesInFHS |> 
    bind_rows(FixNamesInFHS)

FHSAAdata <- SchoolsWithUniqueNamesInEnrollments |> 
    bind_rows (FixNamesInFHSAA)

FloridaHighSchools3 <- FHSdata  |>
    left_join(FHSAAdata, 
              by = "school_name")

GradCountsRates <- FloridaHighSchools3 |> 
    mutate(est_Ngrads = 
               round(est_enrollment * .01 *
               graduation_rate_2022_23),0) |>
    select(district_name, est_enrollment, est_Ngrads) |> 
    group_by(district_name) |> 
    summarise_if(is.numeric,sum, na.rm = TRUE) |> 
    mutate(adjGradRate = est_Ngrads / est_enrollment)


```

### estimating the relationship between economic disadvantage and graduation rates

We now have two approaches to estimating graduation rates at the level of school districts - the first is based on the simple average by schools, the second is based on the estimated enrollment data. How do these relate to each other and to other measures in the data such as economic disadvantage?

Here, we join the two district-level files, rename our two graduation measures, and compute the correlations among the measures. We use the correlate function (in the corrr package), which makes the correlation matrix into a tibble, round the entries to two decimal places, and make into a table, which we format using the kable function from the kableExtra package.

```{r}
FloridaHighSchoolsbyDistict |>      
    left_join(GradCountsRates) |>          
    select(graduation_rate_2022_23,
           adjGradRate, 
           percent_disadvantaged_students = percent_of_economically_disadvantaged_students) |>
    rename(grad_rate_raw = graduation_rate_2022_23,
           grad_rate_weighted = adjGradRate) |>
    correlate() |>
    mutate_if(is.numeric, round, 2) |> 
    kable(table.attr = "style='width:30%;'") |> 
    column_spec(1:4, width = "14em") |> 
    kable_styling(font_size = 12)
```

The two measures of graduation rates are similar but not identical (r = .88), and each is negatively associated with economic disadvantage (rs of -.38 and -.51). These correlations are high.

## recap / on joining files

In this chapter, we've examined Florida High School data and found an (expected) negative relationship between economic disadvantage and graduation rates. We also saw a picture of the guitarist from the hair band Guns 'n' Roses. We used a number of functions, such as *select*, *filter*, *mutate*, *group_by*, *summarise*, and *case_when,* that we also considered in the last chapter. We also used some new libraries, including readxl, corrr, and kableExtra, and their functions read_excel, correlate, and kable.

The core of this chapter is to introduce some of the challenges of combining files, including both (a) the frequent need to wrangle datasets so that they can be correctly put together and (b) the use of the *left_join* function. The left_join function is one of many different ways of joining data.

In our left_join, we kept all of the rows in the first dataset (x, FHSdata), and linked them to y (FHSAAdata) if and only if they have a match on the key variable (school_name). In a right join, we keep all of y, and rows in x if and only if they have a match in y. So we could also have written our code as

> FloridaHighSchools3 \<- FHSAAdata \|\>\
> right_join(FHSdata,\
> by = "school_name")

There are also *inner_joins* (which can be thought of as the intersection of the two datasets, including only rows in each dataset which match) , and *full_joins* (unions, which keep all rows in both datasets). And *anti_joins* include only the rows in x which do not have a match in y. We will return to anti-joins in our discussion of text analysis.

In combining datasets, joins are but one approach. We can also simply bind files together, without regard for a common key. Here, (and in the last chapter) we used *bind_rows* to add new observations to an existing dataset - this is typically used when two datasets have the same set of variables. Finally, bind_cols can be used to add new variables when two datasets have the same observations and in the same order.

<!--chapter:end:12-data.Rmd-->

```{r applieddata, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# applied data science

From the standpoint of applied data science, data analysis should have meaning, and should lead to consequential social or practical consequences. Here, we consider a few examples of applications of data science.

## public health and covid

In Winter 2020, as the COVID-19 pandemic began to take hold, I began chronicling the course of the virus. In a prior version of this book, I wrote the following:

> **tracking the Novel Coronavirus (from Feb 2020)**
>
> Here, I want to consider a timely (but challenging) dataset.
>
> The Novel Coronavirus is an emerging health crisis, particularly in Wuhan (a Chinese city larger than New York City) and the surrounding province of Hubei. **It is not yet a threat in the United States - there have, at this writing (02/11/20) been zero cases in Florida.** Still, tracking the spread of the virus - the unfolding number of people infected and recovered, as well as the number of deaths - is a fascinating exercise.

I then went on to provide code for accessing data provided by the Johns Hopkins Center for Systems Science and Engineering (JHU/CSSE). The data was initially provided on a [GitHub site](https://github.com/CSSEGISandData/COVID-19), then moved to a dashboard here. You can learn more about the efforts of this team [here](https://coronavirus.jhu.edu/).

### COVID data in 2025

Today, there are a number of R packages intended to help analyze COVID data. The COVID19 package provides records of the outbreak on a global scale. Here's some sample code:

```{r}
# install.packages("COVID19")
library(COVID19)
allCovid <- covid19()
CovidbyDate <- allCovid |>
    select(date,confirmed, deaths, recovered, people_vaccinated,
           place = administrative_area_level_1) |>
    group_by(date, place) |>
    summarise_if(is.numeric, sum, na.rm=TRUE)
CovidbyDate |>
    filter (date < "2023-01-01") |>
    filter(place == "United States") |>
    summarize(confirmed = sum(confirmed)) %>%
    ggplot(aes(x=date)) +  geom_line(aes(y=confirmed))

```

A second package can be used to examine the impact of COVID more indirectly, but possibly more accurately, by looking at excess mortality data.

### a brief digression on causality

Human deaths, like most events, are multiply determined. In the case of COVID, many of those who contracted the disease suffered from other vulnerabilities including diabetes, obesity, pre-existing heart disease, and "old age." They may have contracted pneumonia as a proximal cause in a pathway that might have included, for example, chronic cigarette smoking -\> emphysema -\> chronic obstructive pulmonary disease (COPD) -\> COVID -\> death.

In these cases, like most cases, isolating an individual "cause" can be difficult if not arbitrary, Determinations as to the cause of death may be difficult to make, particularly in an environment in which political or economic considerations may be non-trivial.

### the excess mortality package

The R excess mortality package (excessmort) can be used to calculate the expected number of deaths in a region and time period. Among other things, it can be used to estimate the effects of the pandemic on mortality in individual states based on historical data rather than particular diagnoses. You can learn more about the package at <https://cran.r-project.org/web/packages/excessmort/vignettes/excessmort.html>.

```{r}

# install.packages("excessmort")
library(excessmort)
exclude_dates <- c(seq(make_date(2017, 12, 16), make_date(2018, 1, 16), by = "day"),
                   seq(make_date(2020, 1, 1), max(cdc_state_counts$date), by = "day")) 
counts <- cdc_state_counts %>%
    filter(state == "Florida") %>%
    compute_expected(exclude = exclude_dates)
expected_plot(counts, title = "Expected (blue) and actual (grey) Weekly Mortality Counts in Florida")
counts <- cdc_state_counts %>%
    filter(state == "California") %>%
    compute_expected(exclude = exclude_dates)
expected_plot(counts, title = "Expected (blue) and actual (grey) Weekly Mortality Counts in California")
```

## other datasets in and beyond R

The datasets library in R includes about 90 datasets of this writing. Many of these (e.g., iris, cars) are ubiquitous in R training; they are typically small and easy to work with.

The Fivethirtyeight and fivethirtyeightdata packages include another 150 or so datasets on politics and popular culture.

A set of 2500+ datasets which are in R packages may be found at <https://vincentarelbundock.github.io/Rdatasets/datasets.html> (you can find this in a sortable spreadsheet [here](https://github.com/vincentarelbundock/Rdatasets/blob/master/datasets.csv)). These range in scope from the small ("Death By Horse Kicks", 5 rows and 2 columns) to the large "US Military Demographics", 1.4 million rows and 6 columns. Please consider explorinfg this site.

The openintro package includes several hundred datasets; they are described [here](https://cran.r-project.org/web/packages/openintro/openintro.pdf)

Note also that the **R fivethirtyeight library** provides access to a number of clever, clean, and largely manageable datasets, each of which underlies the empirical analyses and reports of Nate Silver and his team (You can learn more at <https://data.fivethirtyeight.com/>).

[Kaggle](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2F)is a noun (a community, a website, a challenge), and a verb (to kaggle is to participate in a data challenge) which describes a crowdsourced competition to improve on a problem in prediction. Perhaps the first and best known example of this was the Netflix prize [@jackson2017], which, in 2006, promised one million dollars to the first team to improve the algorithm by which that company recommended movies to its customer base. The competition took several years, and inspired substantial improvements in machine learning as well as in crowdsourced science. At this writing, Kaggle hosts many active competitions - including some with prizes of one million dollars or more." (Good luck!) Kaggle also hosts hundreds of thousands of datasets. A good place to start is to filter the datasets stored in comma separated value format (.csv) and a usability rating of 8 or more.

Within psychology and behavioral science, the Open Science Framework (OSF) provides a system for hosting and sharing code and data from research articles. One OSF page is a compilation of many datasets from prominent papers in psychology and psychiatry: this now forwards to a [spreadsheet](https://docs.google.com/spreadsheets/d/1ejOJTNTL5ApCuGTUciV0REEEAqvhI2Rd2FCoj7afops/edit?gid=0#gid=0) which, though it does include data from a number of large and important studies, it appears insufficiently curated, with many dead links.

Outside of psychology, repositories of data from many disciplines may be found at Re3data <https://www.re3data.org/>.

There are many datasets about music - songs, artists, lyrics, etc. - at millionsongdataset. Note that many of these are quite large, but more accessible datasets are available, including [here](https://github.com/mdeff/fma/blob/master/README.md).

Or just [**Google datasets**](https://toolbox.google.com/datasetsearch).

### make/extract/combine your own data

Despite the petabytes (exabytes? zettabytes? [yottabytes](https://en.wikipedia.org/wiki/Yottabyte)?) of data in the datasets described above, it's possible that the dataset that you want to examine does not yet exist. But you may be able to create it, for example, by **scraping** data from the Web. Typically, you would use an Application Programming Interface (API) to pull data down from platforms such as Twitter or Reddit. For these and other major social media and news platforms, there are R packages which will walk you through the process of getting the data from webpage to tidy dataset. (Be aware, though, that the methods for data access on these platforms frequently changes, so that code that worked a year ago might not work today).

Another source of data is ... your own life. If you wear a pedometer or sleep tracker, are a calorie counter or keep other logs as a member of the **quantified self** movement, consider how such data might relate to aspects of the physical environment (such as temperature, or the time between sunrise and sunset) and/or the broader social and cultural context (a measure, perhaps of the sentiment, or mood, of news articles from papers like the *NY Times*).

Finally, you might want to **combine multiple datasets**, such as county-level home pricing data from Zillow (<https://www.zillow.com/research/data/>), county-level elections data from, for example, here: <https://github.com/tonmcg/US_County_Level_Election_Results_08-16>, and the boundaries of Woodard's 11 American Nations (see Lanning). In joining different datasets, or data from different sources, we can go beyond a pedagogical exercise (learning about learning) and contribute new and meaningful knowledge.

### keep it manageable

Proceed with caution - many of these datasets are likely to be quite large (for example, analyses of images) and/or in formats that for now are too challenging (JSON). I encourage you to stick with data that are available in a .csv format and that don't have more than, say, a million data points (e.g., 50,000 observations \* 20 variables).

***todo: move this chapter to just before text analysis; begins a new section on applied data science and computational social***

<!--chapter:end:13-appliedDataScience.Rmd-->

```{r otherdata, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(babynames)
#library(lubridate)
```

# strings, factors, dates, and times

This chapter discusses some of the types of data other than numeric and logical, in particular strings, factors, and dates/times.

For the time being, please consider this as a supplement to R4DS, 2e, Chapter 14.

## strings

Strings are sets of characters which may include "123" as well as "why \*DID\* the chicken cross the road?" Samples of text, from names to novels, are the most interesting type of string.

Among the tools that are used in examining texts are searches (do these tweets include language associated with hate speech?), validity checks (does the string correspond to a valid zip code?), and reformatting (to lower case so that BOB, Bob, and bob are all coded as identical). These ideas are simple, but quickly become challenging when, for example, the strings in which we are interested include characters that R usually interprets as code - such as commas, quotes, and slashes. See the section on string basics (14.2) for how to "escape" these characters, for example, how to treat a hashtag (\#) as just a character as opposed to the beginning of a comment. These rules are codified as **regular expressions** (regex, sometimes regexp). Regex are not unique to R, but are shared with other languages as well.

In R, particularly in the tidyverse package, regex are typically implicit, represented within commands that are part of the stringr package and that typically begin with str\_. For example, str_detect returns a set of logical values:

```{r message=FALSE}
donuts <- c("glazed", "cakes", "Pink sprinkled",
            "cream filled",
            "day-old frosted", "chocolates")
donuts %>% 
    str_detect(" ")
```

Most of the str\_ functions are straightforward, but remember that str_sub provides a subset, not a substitution; to change a string, use str_replace. As Hadley points out in R4DS 2e, the autocomplete function in R_studio is very handy for helping you explore the different functions - in your console, type str\_ ... then scroll through the possibilities.

```{r}
donuts %>% str_sub(1,5)
donuts %>% str_replace(" ","_")
```

As you work with texts, simple problems sometimes require sophisticated codes. The regex that are used to solve these problems quickly become dense and challenging.

One tool that can help you is the str_view command, which returns highlighted text showing corresponding passages. For example:

```{r}
slashMovieTitles <- c("Face/off", 
                     "8 1/2", 
                     "F/X",
                     "Frost/Nixon",
                     "Victor/Victoria")
slashMovieTitles %>% str_view ("/")
```

Regex statements are dense statements that allow us to work efficiently, for example, with special characters (like backslashes), repeated characters (zzz), and sets of characters [AEIOU], and characters at the beginning (\^) or end (\$) of strings. See R4DS 2e Chapter 15 for more.

A particularly useful function in stringr is str_split, which can be used to quickly break a text into discrete words. Note that using a space and the explicit "word" boundary give different results.

```{r}
str_split(donuts, " ", simplify = TRUE)
str_split(donuts, boundary ("word"), simplify = TRUE)
```

The output of str_split is generally a list (more on that soon), but here the lists are simplified into tibbles. In the tidyverse, str_split is typically one of the first steps in preparing text. The tidytext package (<https://www.tidytextmining.com/>), which is discussed at length in the computational social science course, builds on this foundation and is a powerful set of tools for all sorts of problems in formal text analysis.

## factors

Conditions (experimental vs control), categories (male or female), types (scorpio, "hates astrology") and other nominal measures are categorical variables or factors. In the tidyverse, the r package for dealing with this type of measure is *forcats*, one of the core parts of the tidyverse.

Here's an example of a categorical variable. Why is it set up like this, and what does it do?

```{r factors.0314}
# Example of a factor
eyes <- factor(x = c("blue", "green",
                     "green", "zombieRed"), 
               levels = c("blue", "brown",
                          "green"))
eyes
```

In base R, string variables ("donut", "anti-Brexit", and "yellow") are generally treated as factors by default. In the tidyverse, string variables are treated as strings until they are explicitly declared as factors.

The syntax for working with factors-as-categories is given in Chapter 16 of R4DS 2e. I will not duplicate that here, but I will point out that factors are represented internally in R as numbers, and converting (coercing) factors to other data types can occasionally lead to nasty surprises. Sections 16.4 and 16.5 describe how factors can be cleanly reordered and modified.

### types of babies

In the babynames data, baby's gender is a categorical variable, which is treated (because tidyverse) as a character or string. Here, we make it into a factor. We create two other factors as well.

```{r echo=TRUE, results="hide", message=FALSE}
# adding third level for non-binary babies
sexlevels <- c("M", "F", "O")
babynames2 <- babynames %>% 
    mutate(sex = factor(sex,
               levels =  sexlevels)) %>% 
    mutate(beginVowel = case_when(
        substr(name,1,1) %in%
            c("A","E","I","O","U") ~ "Vowel",
        TRUE ~ "Consonant")) %>% 
    mutate(beginVowel = factor(beginVowel)) %>% 
    mutate (century = case_when(
        year < 1900 ~ "19th",
        year < 2000 ~ "20th",
        year > 1999 ~ "21st")) %>% 
    mutate(century = factor(century))
```

> Use the syntax above to create types of names for different generations (boomers, gen x, Millenials, gen z). Use <https://www.kasasa.com/articles/generations/gen-x-gen-y-gen-z> to determine your groupings.

> Say something interesting about the data - names, genders, etc. Plot this.

### types of grown-ups

If you would instead like to examine survey data, the forcats package includes a set of categorical variables.

> Using the discussion in Chapter 15 of R4DS as your guide, examine the relationship between two or more of these categorical variables. Again, plot these

```{r}
gss_cat
```

## dates

The challenges of combining time-demarcated data (Chapter 17 of R4DS 2e) are significant. For dates, a variety of different formats (3-April, October 23, 1943, 10/12/92) must be made sense of. Sometimes we are concerned with durations (how many days, etc.); on other occasions, we are concerned with characteristics of particular dates (as in figuring out the day of the week on which you were born). And don't forget about leap years.

In R, the *lubridate* package (a non-core part of the tidyverse, i.e., one that you must load separately) helps to handle dates and times smoothly. It anticipates many of the problems we might encounter in extracting date and time information from strings. Lubridate generally works well to simplify files with dates and times, and can be used to help in data munging. For example, in my analyses of the Corona data, dates and times were reported in four different ways. The code below decodes these transparently and combines them into a common date/time format .

> 2/3/20 6 PM
>
> 2/3/20 18:00
>
> 2/3/20 18:00:00
>
> 2020-02-03 18:00:00

```{r echo=TRUE, eval = FALSE}
# not run
#coronaData2 <- coronaData %>% mutate
#	(`FixedDate = 
#           parse_date_time(`Last Update`,
#                           c('mdy hp','mdy HM',
#                             'mdy HMS','ymd HMS'))) 
```

## times

Working with temporal data is often challenging. The existence of, for example, 12 versus 24 hour clocks, time zones, and daylight savings, can make a simple question about duration quite challenging.

> Imagine that Fred was born in Singapore at the exact moment of Y2K. He now lives in NYC. How many hours has he been alive as of right now? How would you solve this?

```{r}
# find timezones for Singapore and NYC
# a = get datetime for Y2K in Singapore in UTC
# b = get datetime for now in NYC in UTC
# compute difference and express in sensible metric
```

<!--chapter:end:14-otherDataTypes.Rmd-->

```{r lists, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(babynames)
```

# lists

Up until now, we have we have thought about 'data structures' as matrices (rectangles, two-dimensional arrays), in which columns typically correspond to variables and rows to observations, and in which each variable has a particular type, such as numeric or character (or, in the last chapter, special types such as factors and dates/times).

In base R, data matrices are typically represented as data frames (type = df). In the Tidyverse, we have been using a special type of data frame, the tibble (type = df and tbl_df). The diamonds dataset, for example, is a tibble:

```{r echo=FALSE}
knitr::opts_chunk$set(message = FALSE)
```

```{r tibble.0319, warning=FALSE}
babynames2 <- babynames %>% 
    mutate(generation = case_when( 
        (1944 <= year) & (year <= 1964) ~ "boomer",
        (1965 <= year) & (year <= 1979) ~ "genX",
        (1980 <= year) & (year <= 1994) ~ "millenial",
        (1995 <= year) & (year <= 2019) ~ "genZ")) %>%
    mutate(generation = factor(generation))
str(babynames2$generation)
babynames3 <- babynames2 %>% 
 #   count(generation) %>% 
    na.omit(generation)
    
```

Within the diamonds tibble, we can examine the types of each variable. This uses the *sapply* function to SIMPLY APPLY a function (class) to the columns of a data frame or tibble. Here, each column (such as \$carat) is a vector, and each vector is homogeneous (of one particular type):

```{r classesColumns.0319}
diamonds %>% 
    sapply(class) 
```

Beyond these *atomic vectors,* data can take more complex forms, such as hierarchical or tree-like structures such as the following.

```         
Wilkes Honors College Courses
├───Area: Psychology
    ├───Name: Personality and Social Development
    └───Term: Spring 2025
    └───Instructors: Lanning
    └───Students: 
        └───Al
            └───Year: Freshman
            └───Concentration: Psychology 
        └───Barb
        └───etc.
    ├───Name: Political Psychology
        └───Term: Fall 2020
        └───Instructors: Lanning
    ├───etc.
        
```

Nested data sets such as these are common across the Internet. They describe the structure of the webpage you are looking at (which you can see, depending upon your browser, by clicking on something like 'developer tools'). Data formats for representing nested structures include XML (Extensible Markup Language) and JSON (Java Script Object Notation). Many datasets of interest, such as this [set of ratings of 10,000 books on Goodreads](http://fastml.com/goodbooks-10k) are structured as XML as well.

In R, XML and JSON files will (typically after some massaging), be represented as lists. Lists are recursive, that is, they may include other lists.

In addition to external data sources, the results of many procedures within R may also be represented as lists.

> Consider the following code. What does it do? What is in 'mod'? Why is it stored like this?

```{r}
mod <- lm(price ~ carat, 
          data = diamonds)
```

In R studio, you can inspect the structure of the list by clicking on it in the global environment window, by using the View tab, or with the command str(mod).

You can extract rows of your list by including them in single brackets (which will return another list), or double brackets (which will return a vector or data frame).

Compare the structure of the following data sets:

```{r}
b1 <- mod['coefficients'] 
b2 <- mod[['coefficients']]  
c1 <- mod['model']
c2 <- mod[['model']]
```

Lists are, in a sense, containers. A single bracket gives us the wrapper as well as what is inside; the double bracket extracts only the inner element.

Lists can be challenging, but they are necessary in a world where data is complexly structured. The R package purrr, a core part of the tidyverse, includes functions which simplify working with lists; to learn more, there is a tutorial [here](https://jennybc.github.io/purrr-tutorial/index.html), and an overview of the package (with a cheatsheet) [here](https://purrr.tidyverse.org/). Lists are also discussed in Chapter 23 of R4DS 2e.

***todo: additional iscussion of purrr; review***

<!--chapter:end:15-lists.Rmd-->

```{r, include=FALSE, message = FALSE }
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
library(kableExtra)
library(tidyverse)
library(babynames)
```

# loops, functions, and beyond

In one of the most important contemporary theoretical models of intelligence, [@sternberg1999] has argued that the ability to automatize, that is, to work efficiently on repeated or habitual tasks, is a key component of intelligent behavior. Solving habitual problems efficiently - whether it is making a cup of coffee or finding the shortest path to complete a shopping list in a supermarket or a series of errands across town - allows us to focus our limited resources on other challenging tasks that, in turn, may determine whether we survive, or at least prosper.

In programming, loops and functions are essential tools for making repetitive tasks simple. Simplifying your code is one of the more intellectually satisfying aspects of working in R or in any programming language.

In R, loops are supplemented by additional tools for simplifying and avoiding repetition in code, including the 'apply' family in Base R and the map function in the tidyverse.

Functions (and, beyond this, custom libraries) can further streamline your work.

## loops

Consider the task of printing out a series of numbers. Here's a simple example of how this could be done in a loop in Base R. It prints numbers between 1995 and 1998 (inclusive).

```{r }
for (i in 1995:1998) { # i is an index code
    print(i) # print the ith value in the sequence 
} # go to the next one until the range is complete
```

Let's expand on this a little, connecting back to the babynames data. This will print counts of the number of babies for each year in GenerationZ, which includes years from 1995-2015.

```{r}

genZ <- (1995:2015)
# this reduces the (big) babynames to a simple file of years and counts
babyCounts <-
    babynames %>% 
    group_by(year) %>% 
    filter(year %in% genZ) %>% 
    summarize(nbabies = sum(n))
    
# and this uses a for loop to print each row in turn
# for (i in (1:nrow(babyCounts))) { 

for (i in seq_along(nrow(babyCounts))) { 
    # filename[i,j] == ith row and jth column
    print(paste((babyCounts[i,1]), babyCounts[i,2]))
}
```

The syntax of loops, including where to put parentheses in index statements, can be tricky. Expect to refer to Google and StackExchange often in order to get your code running.

Another good source is Chapter 26 of R4DS (2e). There, Wickham goes in to additional detail, including a description of seq_along(df) as a tool for creating an index corresponding to 1:ncol(df). Here's an example with the diamonds dataset:

```{r}
diamonds2 <- diamonds %>% 
    select_if(., is.numeric)
# for (i in (1:ncol(diamonds2))) {
for (i in seq_along(diamonds2)) {
    print(paste(names(diamonds2[i]),
                round(mean(diamonds2[[i]]),2)))
}
```

Chapter 26 of R4DS also considers some extensions to related problems such as loops of indefinite length, which can often be addressed using the '*while*' command.

## from loop to apply to purrr::map

Understanding for loops is fundamental in programming, but in R they should often be sidestepped. If the order of iteration isn't important (if, for example, it doesn't matter which of the diamonds variables we take the mean of first), then using one of the measures from the **apply** family can generally be used to make your code simpler and more efficient.

The logic is that one takes a dataframe (df or tibble), then applies a function to its rows or columns:

```{r}
# apply = apply the function (mean) to the columns(2) of the df 
diamonds2  %>% apply(2,mean) 
# sapply = simply apply - guesses that you are looking for col. means
diamonds2  %>% sapply(mean) 
```

The many variants of the apply family, including lapply [list apply] and tapply [table apply] as well as sapply, each have their own uses and can be quite efficient but, again, can be syntactically challenging.

In the evolving tidyverse, the **map** family of commands is supplementing if not supplanting apply; these commands (part of the purrr package in the core tidyverse) may prove to be more convenient and clear. For example, the **map_df** function will apply a function and return a dataframe (tibble), which can be handy for further analysis.

```{r}
diamonds2 %>% 
  map_df(mean)

```

## some examples of functions

If you repeat a series of lines of code several times in your program, it is often best to wrap this into a function.

The first example is from a preregistered study I recently started of language and politics. For the preregistration, I ran analyses using **simulated data,** both to increase the likelihood that the code will run without error on real data and to help anticipate the analyses which are to be run on 'real' data. I began by getting a real body of text from the net and scrambling it, then constructing fake 'Republican' and 'Democratic' texts from this. Here, I illustrate this by constructing 50 sample documents, each consisting of between 5 and 20 words.

The project is given in four steps:

### preliminaries

Here is the preliminary stuff, where I pull the data ~~off the net~~ and initialize the variables

```{r simdata}
sampledata <- read_csv("data/sentiment-words-DFE-785960.csv")#url(
#     "https://www.crowdflower.com/wp-content/uploads/2016/03/sentiment-words-DFE-785960.csv"))
#"https://raw.githubusercontent.com/totalgood/hope/master/data/corpora/sentiment-words-DFE-785960.csv"))
  # pulls off four words
sampledata <- sampledata[22:25] %>% na.omit()
ndocs <- 50
minDocLength <-  5
maxDocLength <- 20
doc <- vector(mode = "character", length = ndocs)
```

### the function

Here's the simple function which pulls a random word out of the matrix of sampledata.

```{r}
set.seed(33458) # a random seed is used to allow reproducible results
getword <- function() {
      rowid <- sample(1:nrow(sampledata), 1)
      colid <- sample(1:ncol(sampledata), 1)
      sampledata[rowid,colid]
}
```

### applying the function

The function is applied, first, to extract one word, then, in successive loops, to build up one phrase and then many.

```{r}
# combine words into docs
# establish length of first phrase
docLength <- sample(minDocLength:maxDocLength,1)
# initialize with one word
sampleCorpus <- getword()
# loop to build up first phrase
for (i in 1:docLength) {
      addWord <- getword()
      sampleCorpus <- paste(sampleCorpus, addWord)
}
#add additional simulated documents
for (j in 2:ndocs) {
      docLength <- sample(minDocLength:maxDocLength,1)
      newdoc <- getword()
      for (i in 1:docLength - 1) {
            addWord <- getword()
            newdoc <- paste(newdoc, addWord)
      }
      sampleCorpus <- rbind(newdoc,sampleCorpus)
}
```

Finally, the results are combined with a vector of alternating labels of 'Dem' and 'Rep':

```{r}
row.names(sampleCorpus) <- NULL
evenOdd <- rep(c("Dem","Rep"),length.out = nrow(sampleCorpus))
workingCorpus <- as_tibble(cbind(evenOdd,sampleCorpus))
head(workingCorpus,5)
```

## how many bottles of what?

To put the fun back into function, here's a solution to the "99 bottles of beer" function described in r4DS 21.2.1. Study the code. Ask or answer a question about it in class or on Slack.

```{r results=FALSE}
beerSong <- function(liquid = "beer", count = 99, surface = "wall") {
    songtext <- "" 
    for (i in (count:1)) { 
        thisLine = (paste0(i, " bottles of ", liquid, 
        	" on the ", surface,
        	", you take one down and pass it around,\n"))
        songtext = c(songtext, thisLine)
    }
    songtext = c(songtext, 
                 (paste0("no more bottles of ",
                         liquid," on the ", surface,"...")))
    cat(songtext) # cat prints without line numbers
}
#beerSong() 
```

And here are solutions proposed by some of your classmates over the last few years. How do they differ from each other, and from the solution given above?

```{r results ='hide'}
beersng <- function(n) {
  if (n == 1) {
    cat("\n",n," bottle of beer on the wall, ",n,
        " bottles of beer.\nTake one down and pass it around,",
        " no more bottles of beer on the wall.\n", sep = "")
    cat("\nNo more bottles of beer on the wall, 
        no more bottles of beer.\n",
        "Go to the store and buy some more, 
        99 bottles of beer on the wall.\n", sep="")
  } else {
    cat("\n",n," bottles of beer on the wall, ",n,
        " bottles of beer.\nTake one down and pass it around, ",
        n-1, " more bottles of beer on the wall.\n", sep="")
    return(beersng(n-1))
  }
}
#beersng(99)
moreBeer <- function () {
  for (i in 0:100){
  starting_number <- 100
  if (starting_number - i == 0) {
    print("No more bottles of beer on the wall, 
          no more bottles of beer, 
          Go to the store and buy some more, 
          99 bottles of beer on the wall.")
    break
  }
  print(paste(starting_number - i,
              "bottles of beer on the wall",
              "Take one down and pass it around,",
              starting_number - i - 1,
              "bottles of beer on the wall."))
  }
}
# moreBeer()
song <- function(bottlesofbeer){
  for(i in bottlesofbeer:1){ 
     cat(bottlesofbeer," bottles of beer on the wall \n",
        bottlesofbeer," bottles of beer \nTake one down, 
        pass it around \n",
        bottlesofbeer-1, " bottles of beer on the wall \n"," \n")       
        bottlesofbeer = bottlesofbeer - 1 
  }
}
#song(99)
```

Which code is 'best'? Good code is clear, but it is also efficient. We probably shouldn't expect much in the way of differences between these functions in terms of speed (as each includes 99 iterations of a simple print command), but here's a simple way to compare. Note that I've used the "sink" command to write my output to files rather than consoles:

```{r results = "hide"}
start_time <- Sys.time()
sink (file = "f1.txt")
beerSong("beer",99)
sink()
end_time <- Sys.time()
beerSongtime <- end_time - start_time

start_time <- Sys.time()
sink (file = "f2.txt")
beersng(99)
sink()
end_time <- Sys.time()
beersngtime <- end_time - start_time

start_time <- Sys.time()
sink (file = "f3.txt")
song(99)
sink()
end_time <- Sys.time()
songtime <- end_time - start_time

start_time <- Sys.time()
sink (file = "f4.txt")
moreBeer()
sink()
end_time <- Sys.time()
moreBeertime <- end_time - start_time
```

Here is the first line of output from each function, together with table showing the elapsed times:

```{r message=FALSE}
readLines("f1.txt",1)
readLines("f2.txt",2)
readLines("f3.txt",4)
readLines("f4.txt",1)
tibble(functionName = c("beerSongtime",
           "beersngtime",
           "songtime",
           "moreBeertime"), time =
             c(beerSongtime,beersngtime,
           songtime,
           moreBeertime)) %>% 
          kable(digits = 2)
file.remove(c("f1.txt", "f2.txt", "f3.txt", "f4.txt"), echo = FALSE)
```

<!--chapter:end:16-loopsFunctions.Rmd-->

```{r, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warn = FALSE, message = FALSE) # defaults for all chunks after this one
library(tidyverse)
library(HistData) # for Galton data
library(corrr) # for tidy corr matrix output
#library(dslabs)
#library(Ecfun)
#library(broom) # unlist regression results into tibbles
#library(modelr)
library(GGally) # for scatterplot matrix
library(Ecdat) # Econometric data incl. affairs dataset 
```


# from correlation to multiple regression

In the previous chapters, we have learned how to summarize and visualize data. We have seen that we can summarize data using **descriptive statistics** and visualize data using **plots**.

We can distinguish between analyses of just one variable (the univariate case), two variables (bivariate), and multivariate (many variables).

## bivariate analysis: Galton's height data

(Note that this section is excerpted directly from From <https://github.com/datasciencelabs>). [Francis Galton](https://en.wikipedia.org/wiki/Francis_Galton), a polymath and cousin of Charles Darwin, is one of the fathers of modern statistics. Galton liked to count - his motto is said to have been "whenever you can, count". He collected data on the heights of families in England, and he found that there was a strong correlation between the heights of fathers and their sons.

We have access to Galton's family height data through the `HistData` package. We will create a dataset with the heights of fathers and the first son of each family. Here are the key univariate statistics for the two variables of father and son height, each taken alone:

```{r}
data("GaltonFamilies")
galton_heights <- GaltonFamilies %>%
  filter(childNum == 1 & gender == "male") %>%
  select(father, childHeight) %>%
  rename(son = childHeight)
galton_heights %>% 
  summarise(mean(father), sd(father), mean(son), sd(son))
```

This **univariate** description fails to capture the key characteristic of the data, namely, the idea that there is a relationship between the two variables. To summarize this relationship, we can compute the **correlation** between the two variables. 

```{r}
galton_heights %>%
  summarize(cor(father, son)) %>% 
  round(3)
```

In these data, the correlation (r) is about .50. (This means that for every standard deviation increase in the father's height, we expect the son's height to increase by about half a standard deviation). Incidentally, if we want to save correlations as a matrix, we can use the `correlate()` function from the `corrr` package. This function computes the correlation matrix for all pairs of variables in a data frame, which can be easily saved and formatted as a table. The `fashion()` function can be used to easily clean up the output. (And the parentheses around the whole statement allows us to print out the result to the console / RMarkdown document, as well aas saving rmatrix in our environment).

```{r}
(rmatrix <- galton_heights %>% 
  select(father, son) %>% 
  correlate() %>% 
  fashion(decimals = 3))
```

### correlations based on small samples are unstable: A Monte Carlo demonstration

Correlations based on small samples can bounce around quite a bit. Consider what happens when, for example, we sample just 25 cases from Galton's data, and compute the correlation within this sample. Note that I begin by setting a seed for the random sequence. I repeat this 1000 times, then plot the distribution of these sample rs:

```{r}
set.seed(33458) # why do I do this?
nTrials <- 1000
nPerTrial <- 25
replications <- replicate(nTrials, {
  sample_n(galton_heights, nPerTrial, replace = TRUE) %>% # we sample with replacement here
    summarize(r=cor(father, son)) %>% 
    .$r
})
replications %>% 
  as_tibble() %>%
  ggplot(aes(replications)) +
  geom_histogram(binwidth = 0.05, col = "blue", fill = "cyan")
```

These sample correlations range from `r round(min(replications),3)` to `r round(max(replications),3)`. Their average, however, at `r round(mean(replications),3)` is almost exactly that of the overall population. Often in data science, we will estimate population parameters in this way - by repeated sampling, and by studying the extent to which results are consistent across samples. More on that later.

### from correlation to regression

In bivariate analysis, there is often an asymmetry between the two variables - one is often considered the **predictor** (or independent variable, typically x) and the other the **response** (or dependent variable, y). In these data, we are likely to consider the father's height as the predictor and the son's height as the response. 

As noted above, one way of thinking about a correlation between variables like heights of fathers (x) and sons (y), is that for every one standard deviation increase in x (father's height), we expect the son's height to increase by about $r$ times the standard deviation of y (the son's height). We can compute all of these things manually and plot the points with a regression line. (We use the pull function to extract the values from the statistics from tibbles into single values). 
 

```{r}
mu_x <- galton_heights |> summarise(mean(father)) |> pull()
mu_y <- galton_heights |> summarise(mean(son)) |> pull()
s_x <- galton_heights |> summarise(sd(father)) |> pull()
s_y <- galton_heights |> summarise(sd(son)) |> pull()
r <- galton_heights %>% summarize(cor(father, son))  |> pull ()

m <-  r * s_y / s_x
b <- mu_y - m*mu_x

galton_heights %>% 
  ggplot(aes(father, son)) + 
  geom_point(alpha = 0.5) +
  geom_abline(intercept = b, slope = m, col = "blue") 
```

Finally, if we first standardize the variables, then the regression line has intercept 0 and slope equal to the correlation $\rho$. Here, the slope, regression line, and correlation are all equal (I've made the plot square to better indicate this).

```{r  fig.width = 4, fig.height = 4}
galton_heights %>% 
  ggplot(aes(scale(father), scale(son))) + 
  geom_point(alpha = 0.5) +
  geom_abline(intercept = 0, slope = r,  col = "blue") 
```


## multivariate data

For the Galton data, we examined the relationship between two variables - one a predictor (father's height) and the other a response (son's height). In this section (drawn from Peng, Caffo, and Leek's treatment from Coursera - the Johns Hopkins Data Science Program), we will extend our analysis to consider multiple predictors of a single response or outcome variable. You may need to install the packages "UsingR", "GGally" and/or"Hmisc".

We begin with a second dataset. You can learn about it by typing `?swiss` in the console. 

```{r swiss data}
data(swiss)
str(swiss)
```

Here's a *scatterplot matrix* of the Swiss data. Look at the first column of plots (or first row of the correlations). What is the relationship between fertility and each of the other variables?

```{r scatterplot matrix with ggpairs, swiss}
# ds_theme_set()
set.seed(0)
ggpairs (swiss,
        lower = list(
            continuous = "smooth"),
        axisLabels ="none",
        switch = 'both')
```

Here, we predict fertility from all of the remaining variables together in a single regression analysis, using the lm (linear model) command. Note that the result of this analysis is a list. We can pull out the key features of the data using the summary() command. How do you interpret this?


```{r multiple regression - swiss data}
swissReg <- lm(Fertility ~ ., data=swiss)
summary(swissReg)
```

Regression is a powerful tool for understanding the relationship between a response variable and one or more predictor variables. We can use it where our variables are not normally distributed, as in the case of dichotomous variables (yes/no, true/false), as well as counts, which often are skewed.  Here's a second dataset, the marital affairs data, which is also included in the Ecdat package.
We'll apply ggpairs here, but for clarity will show only half of the data at a time. The dependent variable of interest (nbaffairs) will be included in both plots:

```{r out.width = "100%", warning= FALSE, message = FALSE}
Fair1 <- Fair %>% 
  select(sex:child, nbaffairs)
ggpairs(Fair1,  
# if you wanted to jam all 9 vars onto one page you could do this
#        upper = list(continuous = wrap(ggally_cor, size = 10)),
         lower = list(continuous = 'smooth'),
         axisLabels = "none",
        switch = 'both')
Fair2 <- Fair %>% 
  select(religious:nbaffairs) 
ggpairs(Fair2, lower = list(continuous = 'smooth'),
           axisLabels = "none",
           switch = 'both')
```

```{r multiple regression - affairs}
affairReg <- lm(nbaffairs ~ ., data=Fair)
summary(affairReg)
```

<!--chapter:end:17-correlationRegression.Rmd-->

```{r, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warn = FALSE, message = FALSE) # defaults for all chunks after this one
library(tidyverse)
library(modelr)
library(Ecdat) 
library (caret)# machine learning package
library (caTools) # ditto
```

# cross-validation

*The problem of capitalizing on chance is a significant one in prediction, and one should always be skeptical of models which are untested beyond the sample from which they were derived.*

## revisiting the affairs data

Let's go back and do what we should have done earlier, that is, examine and think about the "number of affairs" variable. What does the distribution look like?

```{r}
data(Fair)
Fair %>% count(nbaffairs)
```

We note that the distribution is skewed - and we realize that perhaps we should think about it differently: The *meaning* of the difference between 0 and 1 is not the same as that between 1 and 2, or perhaps even 1 and 12.

We can transform the data in several ways. We might perform a *root transform*, which we could then round to an integer:

```{r}
Fair <- Fair %>% 
    mutate(rootAffair = 
              as.integer(sqrt(nbaffairs)))
Fair %>% count(rootAffair)
```

Or we could simply distinguish between those that do and don't have affairs.

```{r}
Fair <- Fair %>% 
    mutate(affairYN =
               ifelse (nbaffairs > 0,1,0))
Fair %>% count(affairYN)
```

We'll examine each of these in the following paragraphs.

## avoiding capitalizing on chance

One lesson from the last class was that correlations (and regression coefficients) drawn from small samples were not stable. In regression analysis, as progressively smaller samples were drawn from the Fair data, the ability to predict the outcome increased. In the limiting case, when the number of predictors (variables) was equal to the number of cases in the sample (rows), prediction became perfect.

### splitting the data into training and test subsamples

The most basic solution to this problem is to split the data into two groups, a *training* sample from which we extract our model, and a *test* sample on which you will assess it. (Often, the logic of this will be extended to include a third group, a *validation* sample which would be used to tune or select the results of the training run before the test data are examined). Here, we will consider the simpler approach, splitting the Fair data into training and test samples. The critical feature of this analysis is that we will hold out the test data, and not even look at it until after our model building is complete.

```{r}
# establish a seed for your data-split
# so that your results will be reproducible
set.seed(33458)
n <- nrow(Fair)
# create a set of line numbers 
# of size corresponding to the 
# desired training sample 
trainIndex <- sample(1:n, size = round(0.6*n), replace=FALSE)
# create training and test samples
trainFair <- Fair[trainIndex ,]
testFair <- Fair[-trainIndex ,]
```

## an example of cross-validated linear regression

We first predict the variable "rootAffair," using just the training data:

```{r}
trainFair2 <- trainFair %>%
    select(-nbaffairs, -affairYN)
testFair2 <- testFair %>%
    select(-nbaffairs, -affairYN)
model2 <- lm(rootAffair ~ ., data=trainFair2)
summary(model2)
```

To assess the effectiveness of this model on an independent sample, we write a simple function, which assesses R2, then applies it to both the training data and the test data:

```{r}
R2.model.dep.data <- function(myModel,myDep,myData) {
    errorscores <- myDep - predict(myModel,myData)
    SS.error <- sum(errorscores^2)
    deviations <- scale(myDep,scale = FALSE)#)
    SS.total <- sum(deviations^2)
    1 - (SS.error/SS.total)
}    

R2Train <- R2.model.dep.data(model2,trainFair2$rootAffair,trainFair2)
(R2Test <- R2.model.dep.data(model2,testFair2$rootAffair,testFair2))
```

The R2 on the training sample is `r R2Train`, on the test sample it is `r R2Test`. The difference between these is sometimes referred to as shrinkage. Shrinkage will be problematic particularly when there are a small number of observations, a large number of predictors (and consequently a complex model), or both of these.

### applying logistic regression analysis to the training data

Let's go back to the "number of affairs" variable and consider a second way of thinking about it, that is, simply comparing those who do and don't have affairs:

```{r}
Fair <- Fair %>% 
    mutate(affairYN =
               ifelse (nbaffairs > 0,1,0))
Fair %>% count(affairYN)
```

With this change, the regression problem becomes more clearly a problem in classification. How can we best predict which 'type' (affair-ers vs. not) a given person falls in to?

With this change, we have moved from a variable which is essentially continuous (nbaffairs) to one which is dichotomous and therefore distributed binomially. The desired regression is now a logistic one.

We begin by running this analysis using only the training data.

```{r}
model2 <- glm(affairYN ~ ., data = trainFair,
              family = "binomial")
summary (model2)
```

Our "predicted scores" are continuous, corresponding to the probability that a given person will have an affair. Here's how they are distributed (still on the training data here):

```{r}
predictTrain <- predict(model2, trainFair, type = "response")
summary (predictTrain)
```

As scores on predictTrain increase, the predicted likelihood of an affair increases. If you are considering marriage to a potential partner, what score would be too high? How do we distinguish "ok" from "not"?

<!--chapter:end:18-cross-validation.Rmd-->

# prediction and classification

```{r, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warn = FALSE, message = FALSE) # defaults for all chunks after this one
library(tidyverse)
# library(dslabs)
# library(broom) 
library(modelr)
library(Ecdat) 
library (class) # for knn
library (caret) # machine learning package
library (caTools) # ditto
library(magrittr) # allows bidirectional pipe for updating file easily %<>% 
```

If we want to classify people, we will need to create a decision threshold at which we will change our prediction from 'no' to 'yes'.

## from regression to classification: selection of a threshold

We will continue to work with the affairs data; here's the relevant code from last chapter:

```{r}
# code from last chapter on Fair data
Fair <- Fair %>% 
    mutate(rootAffair = 
              as.integer(sqrt(nbaffairs))) %>% 
    mutate(affairYN =
               ifelse (nbaffairs > 0,1,0))
set.seed(33458)
n <- nrow(Fair)
trainIndex <- sample(1:n, size = round(0.6*n),
                     replace=FALSE)
trainFair <- Fair[trainIndex ,]
testFair <- Fair[-trainIndex ,]
trainFair2 <- trainFair %>%
    select(-nbaffairs, -affairYN)
testFair2 <- testFair %>%
    select(-nbaffairs, -affairYN)
model2 <- lm(rootAffair ~ .,
             data = trainFair2)
summary(model2)
predictTrain <- predict(model2, trainFair, 
                        type = "response")
```

To maximise overall prediction, we will create a threshold equal to the actual proportion of people who don't have affairs in our sample:

```{r}
(threshold <- mean(trainFair$affairYN))
```

This is equal to both the mean of our predicted scores (above) and the mean of our actual scores, and, because this is a dichotomous variable, the proportion of people in the sample who have affairs. We'll predict that if a person has a predicted score more than this we'll predict that s/he will be unfaithful, else we will "PredictOK." Then we will create a *confusion matrix,* to compare our correct predictions (PredictOK and affairYN = 1, Predictunfaithful and affairYN = 0) with the remainder.

```{r}
classification <- ifelse(predictTrain > threshold,
                         "Predictunfaithful", "PredictOK")
(b <- table(classification, trainFair$affairYN))
(correct <- b[1,1] + b[2,2])
(errors <- b[1,2] + b[2,1])
(accuracy <- correct/(correct + errors))
```

### applying the model to the test data

We shouldn't trust these results, though, because the model is based on the same data that it is tested upon. Now we will apply the model to the test data. With the linear regression model above, we examined 'shrinkage' in the overall R-square. Here, we can assess shrinkage in terms of the percent of erroneous classifications.

Typically (but not invariably), the percent of accurate classifications will decline, especially if the model is a complex one with many variables or if the number of observations is low.

```{r}
predictTest <- predict(model2, testFair, type = "response")
classification <- ifelse(predictTest > threshold,
                         "Predictunfaithful", "PredictOK")
(b <- table(classification, testFair$affairYN))
(correct <- b[1,1] + b[2,2])
(errors <- b[1,2] + b[2,1])
(accuracy <- correct/(correct + errors))
```

On the test data, we are correct `r 100*(round(accuracy,2))` percent of the time. This describes our *out of sample risk.*

### changing our decision threshold

In many decision problems, there is an asymmetry in the cost of different types of errors: if you are foraging for mushrooms, for example, an error of the form (you decide its safe and it is poisonous) is more costly than the converse (you decide its poisonous and it is safe).

This may be true in the present example as well. Consider someone who is looking for a spouse, but is really averse to the idea of getting hurt by an affair. That person might feel like the cost of marrying an unfaithful person is much greater than the cost of not marrying a faithful one. So we adjust the threshold downwards:

```{r}
threshold <- .05
classification <- ifelse(predictTest > threshold,
                         "Predictunfaithful", "PredictOK")
(b <- table(classification, testFair$affairYN))
(correct <- b[1,1] + b[2,2])
(errors <-  b[1,2] + b[2,1])
(accuracy <- correct/(correct + errors))
```

The "overall accuracy" - that is, number of correct classifications - drops. But that's not what we are really interested in, rather, we are interested in minimizing hurt.

Here's another example: Someone who is very lonely might feel the opposite, and be willing to accept greater substantially greater risk.

```{r}
threshold <- .5
classification <- ifelse(predictTest > threshold,
                         "Predictunfaithful", "PredictOK")
(b <- table(classification, testFair$affairYN))
(correct <- b[1,1] + b[2,2])
(errors <- b[1,2] + b[2,1])
(accuracy <- correct/(correct + errors))
```

Prediction is higher here - but not much higher than it would be if we raised the threshold even further, and just assumed that everyone can be trusted. Then, our error rate would be `r 100*(round(mean(trainFair$affairYN),2))` percent. When overall predictability is low, it's often the case that you can maximize correct predictions by simply predicting that everyone will be in the most popular category. Predicting rare events, such as suicides, is particularly difficult.

### more confusion

There is a nice shortcut to generating confusion matrices such as those above using the caret package.

This function describes outcomes in several ways, as there are many languages for describing outcomes in 2 x 2 tables, including Type I vs. Type II errors, Hits vs. False Alarms/False Positives, and Sensitivity vs. Specificity.

In these data, it's been set up so that

-   hit rate \~ sensitivity \~ ("no affair" \| no affair)
-   correct rejection \~ specificity \~ ("affair" \| affair)

```{r}
threshold <- .5
# syntax for classification in caret is a little different
# (the labels for the actual and predicted scores have to be the same)
#classification <- ifelse(predictTest > threshold,
#                         "Predictunfaithful", "PredictOK")
classification <- ifelse(predictTest > threshold, 1,0)
# caret package (newest) requires explicit matching of factors
classification <- as.factor(classification)
testFair$affairYN <- as.factor(testFair$affairYN)
confusionMatrix(classification, testFair$affairYN)
```

### ROCs and AUC

Each of these decision thresholds describes the performance of a model at a particular point. We can combine the thresholds and plot them in Receiver Operating Characteristic (ROC) curves. The area under the curve (AUC) is a great measure of model accuracy, in that it summarizes how effective a classifier is across all possible thresholds.

```{r 0414model1ROC, fig.width=5, fig.height=5}
# fig.width and fig.height specified to get square plots
# colAUC function gets stats etc
AUCModel <- colAUC(predictTest, testFair[["affairYN"]], plotROC = TRUE)
AUCModel
```

## another approach to classification: k-nearest neighbor

Real-life social predictions are often guided not by induction or the (optimized) combination of a set of predictor variables. Rather, we often reason by analogy - we might think, for example, that *I won't go out with Fred because he reminds me of Larry, and Larry was kind of a jerk.*

If regression analysis is an approach to prediction based in our set of variables, *k-nearest neighbor* analysis instead makes predictions based on observations.

Formally, as described in the documentation for the knn package,

> For each row of the test set, the k nearest (in Euclidean distance) training set vectors are found, and the classification is decided by majority vote, with ties broken at random. (ref).

In the simplest form of this analysis, we find the nearest thing to a "doppelganger" (a look-alike or near double) for a given observation. So, in the affairs data, if a person is most like someone else in the dataset who has had an affair, we predict an affair, else not.

### application: the affairs data

Begin by loading the affairs data from last time. Using the same seed (33458) means that the same set of training and test cases will be extracted as in the prior analysis

```{r inChap19}
data(Fair)
# one change here: Note the bidirectional pipe to simplify code
# use only when you are sure that your file update is ok
Fair %<>% # <- Fair %>% 
    mutate(affairYN =
# nbaffairs is set up as a factor
# to allow confusionmatrix to run
               as.factor(ifelse(nbaffairs > 0,1,0))) %>%
# unlike the lm and glm commands, knn will not automatically create our dummy # variables for us. so we need to do this manually.
    mutate(sexMale = ifelse(sex == "female", 0, 1)) %>% 
    mutate(childyes = ifelse(child == "no", 0, 1)) %>% 
    select(-(c(sex,child,nbaffairs)))

set.seed(33458)
n <- nrow(Fair)
trainIndex <- sample(1:n, size = round(0.6*n), replace = FALSE)
# create training and test samples
trainFair <- Fair[trainIndex ,]
testFair <- Fair[-trainIndex ,]
```

To run a k-nearest neighbor analysis, we need three inputs: our predictors in the training data, our predictors in the test data, and our outcome/classes in the training data.

Here, as in the regression analysis in the last chapter, we can generate a confusion matrix to assess the accuracy of prediction:

```{r amiok}
set.seed(33458)
trainPredictors <- trainFair %>%
    select(-affairYN)
testPredictors <- testFair %>%
    select(-affairYN)
knnAffair <- knn(trainPredictors, # training data
             testPredictors, # test data
             trainFair$affairYN, # class
             1 # number of neighbors
             )
b<-confusionMatrix(knnAffair, testFair$affairYN)
b
b[["overall"]][["Accuracy"]]
```

### from one doppelganger to many

In the above code, we used a k-nearest neighbor analysis based on a single 'neighbor' (k = 1). Can we improve prediction by considering more than 1 neighbor?

### the Bayesian classifier

Lisa is a doctor whose patients often have post-surgical pain. Their suffering is real, but effective pain medications such as OxyContin have a high likelihood of leading to abuse and addiction. She has two bits of information about a patient, his or her age (say that we have this in ten levels, corresponding to decades of age, so that the first level is "under 20" and that the last level is "over 100"), and his or her self-report of pain, also on a 10 point scale.

Assume that Lisa wants to prescribe the medication to all patients who would not abuse it, and not prescribe the medication to anyone who would.

Assume that Lisa knew the entire matrix of conditional probabilities,

> P (addiction \| "age \< 20" & "pain = 1") = .34
>
> P (addiction \| "age \< 20" & "pain = 2") = .26
>
> ... P (addiction \| "age \> 100" & "pain = 10") = .09

In this case she would prescribe the drug for every case where the conditional probability was greater than .5, and never otherwise. This is called the **Bayesian classifier**, and if we have the entire matrix of conditional probabilities we could do no better.

In real world problems, we are typically dealing with many predictors, and we don't have the full matrix of conditional probabilities. But this two predictor case "sets up" the illustration drawn from @james2013.

> ![Figure 19.1: Comparing two values of k. From @james2013.](images/knnTwovaluesfromJames2013.PNG)

In the figure above, assume that the horizontal and vertical axes correspond to scores on the two predictors (age and pain). The orange and blue colored dots correspond to cases of abuse and non-abuse in the training data. The dashed line is the Bayesian classifier. The solid line is the k-nn decision boundary, which distinguishes the regions in which we will predict abuse and non abuse in the test data. We see that when k is small (a single neighbor), prediction is flexible, non-linear, and that as k increases, the boundary differentiating the decision to prescribe and not prescribe becomes more nearly linear.

But what value of k is optimal?

### Back to the affairs data

To test a range of values, we can first set up our knn analysis as a function (compare this code with the code in the prior section).

```{r c117}
trainPredictors <- trainFair %>%
    select(-affairYN)
testPredictors <- testFair %>%
    select(-affairYN)
knnFairdata <- function (k) {
    set.seed(33458)
    knnAffair <- knn(trainPredictors, # training data
                 testPredictors, # test data
                 trainFair$affairYN, # class
                 k # number of neighbors
                 )
    b<-confusionMatrix(knnAffair, testFair$affairYN)
    b[["overall"]][["Accuracy"]]
}
```

We run the function on k = 1 and k = 2 to test it:

```{r c118}
knnFairdata(1)
knnFairdata(2)
```

Now we can apply it to as many as 100 values of k, using the purrr::map command:

```{r c119}
kAccuracy <- (map_dbl(1:100,knnFairdata)) %>% # map the knnFunction onto numbers 1-100
    as_tibble() %>%  # then a tibble so we can do a quick plot
    rename(Accuracy = 1) %>% 
    mutate(k = seq_along(Accuracy))
```

We can graph this, using the syntax from the beginning of the class:

```{r}
kAccuracy %>% 
    ggplot(aes(k, Accuracy)) +#%>% 
    geom_point() +
    ggtitle("Overall accuracy for varying levels of k")
# This pulls out the maximum accuracy, and the value of k for which it occurs:
(ka <- which.max(kAccuracy$Accuracy))
(kb <- max(kAccuracy$Accuracy))
```

### avoiding capitalization on chance (again)

In these data, with this split of training and test (and this initial seed) the maximum predictability occurs at k = `r ka`, with an overall accuracy of `r kb`.

Would this hold if we used a different random split? Remember, here, we have tested not one model, but 100 of them, then chosen the best one. The peak in the curve at `r ka` may well be due to chance characteristics of the test data.

We could address this empirically using one of several techniques. One approach is to have a third independent sample on which to test the accuracy of prediction at k = `r ka`. This would require the separate validation sample that was introduced in the last chapter.

In the following block, I resplit the Fair data, using proportions of 60%, 30%, 10%. These values are likely not optimal given the (relatively small) size of the Fair data, but will work to illustrate the approach:

```{r knnwork2}
set.seed(33458)
threeWaySplit <- sample(1:3, size = nrow(Fair),
                        prob = c(0.6,0.3,0.1),
                        replace = TRUE)
trainFair2 <- Fair[threeWaySplit == 1,]
testFair2 <- Fair[threeWaySplit == 2,]
validFair2 <- Fair[threeWaySplit == 3,]
```

I tweak my function here to use the new data, then run it 100 times as before.

```{r knnwork3}
trainPredictors <- trainFair2 %>%
    select(-affairYN)
testPredictors <- testFair2 %>%
    select(-affairYN)
knnFairdata2 <- function (k) {
    set.seed(33458)
    knnAffair <- knn(trainPredictors, # training data
             testPredictors, # test data
             trainFair2$affairYN, # class
             k # number of neighbors
             )
    b<-confusionMatrix(knnAffair, testFair2$affairYN)
    b[["overall"]][["Accuracy"]]
}
kAccuracy <- (map_dbl(1:100,knnFairdata2)) %>% # map the knnFunction onto numbers 1-100
    as_tibble() %>%  # then a tibble so we can do a quick plot
    rename(Accuracy=value) %>% 
    mutate (k = seq_along(Accuracy))
(ka2 <- which.max(kAccuracy$Accuracy))
(kb2 <- max(kAccuracy$Accuracy))
```

So here, on the (contaminated) test data, the maximum predictability occurs at k = `r ka2`, with an overall accuracy of `r kb2`. We apply this back to the validation data by pulling the knn code out of the function, and running it just once against the validation data:

```{r knnOK}
knnAffair <- knn(trainFair2[,-7], # training data
             validFair2[,-7], # VALIDATION data
             trainFair2$affairYN, # class
             ka2 # number of neighbors
             )
b <- confusionMatrix(knnAffair, validFair2$affairYN)
(kb3 <- b[["overall"]][["Accuracy"]])
```

The overall predictability using the k-nearest neighbor analysis on the clean validation data, is now `r kb3`.

You might note that in our two splits of the Fair data - the two-way split of 60% Training and 40% Test, and the three-way split of 60% Training, 30% Test, 10% Validation, we got two different solutions to the question of the 'optimal k' (i.e., `r ka` and `r ka2`). With larger sample sizes, these values would be more stable.

### the multinomial case

A final comment on the k-nearest neighbors approach: You can extend this to classification problems in which we are predicting not just a dichotomous outcome, but a multinomial one - such as a personality type or college concentration.

<!--chapter:end:19-classification.Rmd-->

```{r setup20, include=FALSE, message = FALSE}
library(tidyverse)
library(Ecdat) # for Swiss data
library(rpart) # for decision trees
library(kableExtra)
library(tidymodels)
```

# machine learning: chihuahuas vs muffins, and other distinctions and ideas

In the last few chapters, we have considered linear and logistic regression and k-nearest neighbor analysis as tools for prediction and classification. We've shown how to split the data into training, test and (in some cases) validation samples, then how to assess the robustness or accuracy of a model on these new datasets. We've also considered measures such as R-squared, overall accuracy, and area under the ROC as measures of validity.

These ideas and techniques form a starting point for the study of *machine learning*. My approach is drawn largely from @james2013, which is available freely on the web and includes links to additional materials and R-based exercises for those who would like to study this further.

## supervised versus unsupervised

One problem that we haven't yet considered is the distinction between supervised and unsupervised problems, arguably the most fundamental distinction in machine learning.

In both the Swiss and the Fair problems, we had a known outcome (fertility, infidelity) which we were trying to predict from a set of independent variables. In these problems, we have an *a priori* split of the variables into two sets (outcomes and predictors). These are considered **supervised** problems. In these problems, we can think of the known outcome or criterion as guiding (supervising) the work of model-building.

There is a second type of problem in which we don't have an outcome, which would guide or supervise our model. Without an outcome or criterion, we must rely on the internal structure of the data. These are considered **unsupervised** problems. Methods used to address unsupervised problems include cluster analysis (of which there are many subtypes), component analysis, and exploratory factor analysis.

In the unsupervised approach, objectives include finding unknown patterns, developing a set of types (or a taxonomy), and assessing the dimensionality of a latent set of variables. In psychology, a focal problems involves assessing the factor structure of personality (if you have taken introductory psychology, you are likely familiar with the five-factor or Big Five model of personality). The unsupervised approach is also used to solve problems of community detection in the study of social and scientific networks (see Figure 20.1, from @lanning2017). I think that questions about dimensionality and internal structure can be compelling [@lanning1994; @lanning1996; @lanning2009], but I will not consider them further here.

![Figure 20.1. Part of the structure of personality research. From @lanning2017](images/SoniaFigure.PNG)

## prediction versus classification

Within the category of supervised problems, we can distinguish problems in classification (those in which the outcome is nominal or discrete) from problems where the outcome is ordered or numeric. We examined the Fair data in both these ways, first treating the outcome as the rescaled number of reported affairs, then as a distinction between those who did and did not have affairs.

Somewhere in between these two approaches are problems in which the criterion is an ordered set of categories - such as small, medium, and large pizza sizes or, to consider a problem in my own area of study, a sequence of seven levels of social maturity or personality development. Working with several colleagues, I constructed "dictionaries" which empirically distinguished these levels; the object is to allow assessment of the level of maturity or ego development of a given text. In the diagram below, the words in these initial dictionaries are arranged clockwise, with those characteristic of the earliest (Impulsive) level in the upper right quadrant (i.e., at 1 o'clock), then moving through the middle stages at the bottom, etc.(@lanning2018). ![Figure 20.2: Differential word cloud illustrating ego development (from Lanning et al 2018).](images/diffprop_cloud_top_250_wordsyda.png)

## understanding versus prediction

In machine learning, we are in general concerned with the problem of finding the **function** which relates an outcome (Y) to a set of predictors (X). This general principle spans two opposing, but overlapping, use-cases: understanding and prediction.

For example, in the 'Fair' data, we were initially concerned with the question of "what predicts infidelity?," which involves or suggests an interest in understanding. With the k-nn analysis, our focus increasingly shifted away from this towards the more pragmatic goal of increasing our hit rate or overall accuracy, away from a concern with inference and understanding to a position where we were satisfied to treat the algorithm as a **black box** from which we were only concerned with the accuracy of outputs.

**These two objectives of "understanding and thinking in terms of equations and models" and "prediction and thinking only in terms of optimization" can be thought of as two points on a continuum of** **interpretability.** Some techniques used in machine learning give results that are quite interpretable (including multiple regression, particularly restricted regression techniques such as the **lasso**). Others, including **support vector machines** and, in particular, **deep neural networks** sacrifice interpretability in the service of prediction. For complex approaches in image recognition, such as the chihuahua versus muffin problem, deep neural networks provide the best solutions, but are particularly challenging to understand [@kumar2017].

![Fig 21.1: The chihuahua-muffin problem.](images/chihuahuamuffin.jpg)

> Exercise 21.1: Consider the Chihuahua-Muffin problem illustrated above.
>
> a)  What is the outcome variable 'Y'?
>
> b)  What are some of the predictors 'X'? (Note that in image recognition and natural language processing, predictors are typically called "features").
>
> c)  Why is this problem interesting? That is, are there problems similar to this that have important social uses?
>
> d)  Can you describe a simple algorithm or decision rule which works more than 50% of the time on the (training) data above (i.e., "if X then "Chihuahua" else "Muffin")?
>
> e)  What might an effective algorithm on new (test) data look like?
>
> f)  finally, what do you think goes on on your mind as you evaluate the photos in the "chihuahua-muffin problem"?

## bias versus variability

If you read further about machine learning, you are likely to encounter the phrase "bias-variability tradeoff." You may remember that, in our initial analyses of the Swiss fertility data, we discussed how reducing the number of observations increases the fit of the model on the (training) data - and that, ultimately, the fit of the model would become perfect when we reduce the number of observations to the number of variables in the model.

Here, we construct two independent samples, each with 9 subjects.

```{r}
data(Fair)
set.seed(33458)
FairSample1 <- Fair %>% sample_n(9)
set.seed(94611)
FairSample2 <- Fair %>% sample_n(9)
```

In the following chunk, we run a regression analysis witin each of these samples, predicting the number of affairs from the 9 predictors (including the intercept).

```{r}

affairReg1 <- lm(nbaffairs ~ ., data=FairSample1)
affairReg2 <- lm(nbaffairs ~ ., data=FairSample2)

R21 <- summary(affairReg1)$r.squared
R22 <- summary(affairReg2)$r.squared
t(c(sample1 = R21, sample2 = R22)) %>% 
    kable(caption = "R^2 from two small samples")
names(affairReg1[["coefficients"]]) %>% 
    cbind(round(affairReg1[["coefficients"]], 2)) %>% 
    cbind(round(affairReg2[["coefficients"]], 2)) %>% 
    as_tibble() %>% 
    rename(variable = 1, 
           sample1 = 2, 
           sample2 = 3) %>% 
    kable(caption = "Regression coefficients from two small samples") 
```

Note that in each case we have perfect predictability, but with a very different set of predictors. The difference between these two sets of coefficients is an illustration of how coefficients in overfit models will vary from one sample to another. At the opposite extreme are underfit models, which are likely to provide relatively stable coefficients across samples, but which aren't very effective at prediction. This is the **bias-variability trade-off**, and it occurs not only in tiny data sets such as these, but in bigger datasets where there is a large number of predictor variables, as is often the case in, for example, artificial intelligence (including image recognition) and bioinformatics (including statistical genetics).

The problem of "too many predictors" can be addressed, in part, by "preprocessing," or trying to restructure the data to effectively increase the number of rows in the data (e.g., by imputing missing values), or decrease the number of columns (that is, using component or factor analysis as an initial step in data analysis). Another approach is to use **resampling.**

### resampling: beyond test, training, and validation samples

We've considered one approach to avoiding chance-inflated models and prediction estimates, and that is the approach of 'holding out' test (and possibly validation) samples. An extension of this approach is **k-fold cross validation**, in which the sample is divided into k (e.g., ten) parts, each of which is used as a validation sample in k different analyses. To assess the overall performance of the model, the results of these are averaged. This is a sophisticated and relatively easy to implement approach which can be used, for example, to assess the relative performance of different models, such as linear versus non-linear models.

Another approach to resampling is **bootstrapping**, in which model parameters (e.g., regression coefficients) are taken as the average of many (for example 1,000) analyses of subsamples of the data. In the bootstrap, sampling is done with replacement, so that the same case may appear in many samples. The averaged coefficients arrived at using bootstrapping are less variable than the results of a single analysis.

## compensatory versus non-compensatory problems

Consider two simple hypothetical real world problems. In the first, you are deciding where you should apply to graduate school. Assume that, in the sample of schools under consideration, only two variables matter to you, say "program quality" and "program cost," and that you weight the first of these positively and the second negatively. We might anticipate that these would be related in a **compensatory** way, so that you would be willing to pay more for a better program.

In the second problem, you are looking to hire a group of commercial jet pilots. Again, there are only two predictors; here, they are eyesight and responsibility. Unlike in the first problem, these are related in a **noncompensatory** way - for example, no matter how good someone's eyesight is, if they are irresponsible you will not want to hire them. Similarly, even if they are very responsible, poor eyesight might make them ineligible.

> Exercise 21.2: For the grad school and airline pilot problems, draw a pair of coordinate axes, together with points representing a few dozen "positive" (apply or hire) and "unacceptable" (don't apply, don't hire) observations. What can you say about the boundary between these two regions in each problem?

We can think of problems such as the grad school problem in a regression framework, while the pilot problem is instead a decision tree or "multiple cutoff" model. Very simple decision trees, including "fast and frugal trees," can be useful tools both to describe how people decide and how they might make better decisions [@phillips2017].

In machine learning problems, there may be hundreds or thousands of predictors (features). The logic of decision trees is extended in techniques including bootstrap aggregation or **bagging** (in which a large number of trees are considered, then averaged), and **random forests** (which sequentially examines subsets of predictors in an effort to increase the reliability of predictions). These models are often predictively useful, but can be complex and difficult to interpret. Chapter 8 of [@james2013] goes into more detail on these methods.

Two additional methods bear mention: **Support vector machines (SVM)** are classifiers which attempt to find optimal margins (hyperplanes) between two or more sets of criteria. Finally, **neural networks** are models which typically include multiple layers (akin to biological neurons) each of which can be activated based on the sum of the inputs of the prior layer. The chain of prediction from input (pixel) to output ("chihuahua") is not a simple forward path, rather, errors of prediction are used to tune weights of intermediate layers in a process known as **backpropagation.** Artificial neural networks are, at present, the most important set of techniques in artificial intelligence problems including image, speech, and even taste recognition.

## a postscript: The Tidymodels packages

The new Tidymodels packages provide a handful of new, tidyverse-compliant approaches to problems in prediction, modeling, and machine learning. The [website](https://www.tidymodels.org/), particularly the [Get Started](https://www.tidymodels.org/start/) page, is straightforward, and I urge you to review it systematically. Here's a quick summary of some key points from the first few pages of that section:

-   The [parsnip](https://parsnip.tidymodels.org/) package presents a tidyverse-compliant approach to writing syntax of models; it appears to be particularly useful for examining multiple models on the same data sets.

-   The [broom](https://cran.r-project.org/web/packages/broom/vignettes/broom.html) package includes the tidy() function, which can be used to present the results of your analyses in data frames - this is handy for making tables of your data. Results of these models can be represented as data frames, which can then be easily tabled or used to create figures. You can see how these are used on the [Build a model](https://www.tidymodels.org/start/models/ "https://www.tidymodels.org/start/models/") page.

-   Perhaps the most useful tool introduced in these pages is the [Skimr](https://cran.r-project.org/web/packages/skimr/vignettes/skimr.html) package. Skimr, though not part of the Tidymodels, is handy for getting a quick set of summary statistics - it's substantially more convenient than the summarise function in the Tidyverse and, unlike the describe package in the psych package, is Tidyverse-compliant.

-   The [preprocessing](https://www.tidymodels.org/start/recipes/) of data includes several concepts that we have previously considered, including splitting data into training and test subsamples, the creation of dummy variables, and how we can treat time data. The [recipes](https://recipes.tidymodels.org/) package include several functions that make it more convenient to work with only subsets of our variables, that is, to assign "roles" to include or exclude variables in our models. This section also includes an introduction to the yardstick package, which gives an easier way to implement ROC analysis than the code I used in the prior chapters.

-   [Resampling](https://www.tidymodels.org/start/resampling/) is a process through which we can estimate model parameters by inspecting a series of subsets of the data. It is particularly useful when we want to compare a set of competing models on a small-ish dataset [@lanning2018]. This section also includes an application of the random forests model introduced above.

<!--chapter:end:20-machinelearning.Rmd-->

```{r setup21, include=FALSE, message = FALSE }
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
library(tidyverse)
library(tidyjson) # for reading in JSON files
library(jsonlite)
library(tidytext) # tidyverse-adjacent text analysis 
library(SnowballC) # for word stemming
library(wordcloud) # for comparison cloud
library(psych) # for descriptives
library(compute.es) # for quick effect size estimates
library(kableExtra) # for tables
```

# working with text: a case study

In this chapter, I present a rudimentary case study in which a contemporary issue is explored using R. I examine Reddit[^21-workingwithtext-1] data, create comparison word-clouds from single words and two-word strings (bigrams), and consider the results of an external proprietary tool for examining categories of language. The chapter is based on the tidy approach to text analysis, for which a clear and thorough introduction has been provided in two books by Silge and her colleagues, each of which is available online [@silge2017, @hvitfeldt2021].

[^21-workingwithtext-1]: Reddit is an electronic discussion board or website which is structured as a set of communities or subreddits; each subreddit includes posts, and, typically, submitted by users (redditors).

A word of warning: This chapter is light on the grunt work of data wrangling and preprocessing, which were introduced in Chapter 14. Keep in mind that that work is typically very time-consuming.

## federal workers

On March 19, 2025, the *New York Times* ran an article with the title "[*Will I Lose My Job?’ Federal Workers Flock to Reddit for Answers*](https://www.nytimes.com/2025/03/19/technology/reddit-va-federal-workers.html?unlocked_article_code=1.6k4.hnKX.dQvNDEhfhgcl&smid=url-share)*."* The gist of the article was that US Federal workers, facing job insecurity in the opening days of President Trump's second term, were using the *r/fedworkers* subreddit to connect with others and communicate their anxieties, concerns, and strategies.

But did they, though? Is there evidence that the r/*fedworkers* subreddit was being used in these ways? In the analysis that follows, I will examine posts[^21-workingwithtext-2] to the community for the period from January 1, 2024 through March 24, 2025.

[^21-workingwithtext-2]: The selection of posts rather than comments is somewhat arbitrary; one reason for examining posts is that the simple number of comments in the period under study would be relatively unwieldy.

We'll consider the following:

-   What specific words differentiate Trump era posts (since his second inauguration in January 2025) from those from the Biden era (prior to the November 2024 election)? For this, we'll rely primarily on visualizations, in particular, differential word clouds of single words and bigrams.

-   What categories of speech differentiate the two epochs? For this, we'll use a proprietary tool, *Linguistic Inquiry and Word Count.* LIWC is a widely used measure for extracting grammatical, social, and psychological variables from text [LIWC; @boyd2022]. We'll generate tables for the LIWC categories and examine the effect size and statistical significance of these differences.

This approach can optionally be extended to additional comparisons, including:

-   Are similar results obtained when we examine comments rather than posts? (Here, we could compare Trump-era words and categories with Biden-era words and categories).

-   How are comments and posts different from each other? (Here, we could compare words and categories for r/fedworkers posts with words and categories for r/fedworkers comments)

-   Does the content of posts (or comments) between the election and the inaugural more closely resemble that of the Biden era (as Biden was still president) or the coming Trump administration?

In order to address these questions, we'll briefly touch on some auxiliary technical questions as well, including:

-   Should we 'stem' words prior to analysis? That is, should we treat terms such as (*donut* and *donuts*) as the same word or as different? How about (*go* and *going*)?

-   A similar question can be asked about case - should all characters be set to lowercase? That is, should (*I'm* and *i'm*) be treated as the same? How about (*DOGE* and *doge*)?

-   Should we include common stop words (*a*, *and*, *if*, *or*, *me*) in our analysis, or disregard them?

## finding Reddit data

Although Reddit data became less accessible beginning in 2023, archives of Reddit posts and comments remain readily available following some simple Web sleuthing. In this block, I begin with a set of posts that I have already downloaded.

We initially look at a very small set of data - 100 lines - both to make sure the code works and, if it does, to make an initial determination about what fields are of interest. The file is large, so it will ultimately be read in through streaming rather than all-at-once, hence the readLines -\> stream_in syntax.

```{r}
posts <- read_lines("data/r_fednews_posts.jsonl", n_max = 100)
a1 <- stream_in(textConnection(
     gsub("\\n","", posts))) 
glimpse(a1)
```

Only a few columns are of interest to us. We select those columns in the next chunk, for a sample of 10000 posts. We also take note of how long it takes to read a sample of this size by noting start-time, end.time, and the difference between these. Finally we make the date-time field (created_utc) field "human readable."

```{r}
posts <- read_lines("data/r_fednews_posts.jsonl", n_max = 10000)
start.time <- Sys.time()
a1 <- stream_in(textConnection(
     gsub("\\n","", posts))) |> 
     select(author, title, selftext, id, link_flair_text,
            num_comments, ups, downs, subreddit, created_utc) |> 
     mutate (date = (as_datetime(created_utc))) |> 
     select(-created_utc)
end.time <- Sys.time()
(end.time - start.time)
```

Looks good - the date/time field is clean, the columns are those that we want, and it only took a few seconds to read in 10,000 posts. Let's read them all in, then take a look at a small random sample - 10 - of the texts of the posts.

```{r}
posts <- read_lines("data/r_fednews_posts.jsonl")
start.time <- Sys.time()
a1 <- stream_in(textConnection(
     gsub("\\n","", posts))) |> 
     select(author, title, selftext, id, link_flair_text,
            num_comments, score, subreddit, created_utc) |> 
     mutate (date = (as_datetime(created_utc))) |> 
     select(-created_utc)
end.time <- Sys.time()
(end.time - start.time)
set.seed(33458)
a1 |> select (selftext) |> slice_sample(n=10)
```

These 59651 posts include all of those which were available for the period from January 1, 2024 through March 24, 2025.

## some initial observations

There are a few interesting properties of the data:

1.  One is that it appears that many of the posts have been removed, including 4 out of this (very small) sample of 10. It would be interesting to compare the proportion of deleted posts to this subreddit with those of other subreddits during the same period. (I would anticipate that this proportion is much higher in this subreddit, likely because of fears of retribution that may or may not be warranted). We should keep in mind that the removed or deleted posts are unlikely to be a random sample of posts on the subreddit - they may be more angry, more polarized, and/or more easily de-anonymized. We should return to this when we consider our results. For now, we will filter out the removed posts.

    a.  *One way to examine this empirically would be to examine the 'titles' field. The removed posts still include titles, so the nature of the removed posts in comparison with the non-removed posts could be examined by using the "compare words and categories" approach on the title fields of the two sets.*

The second is that there are, at least in this small sample, a lot of abbreviations and acronyms - including *ACA*, *IRA*, *USDA*, *FPAC*, *NTEU*, *CFPB*, *HQ*, and *EAP*. Often, at this point in an analysis of text, we would reformat the text so that all words were lowercase (with the *tolower*) function. We'll instead modify this a bit - retaining capitalization when a token is in ALL CAPS (and is more than a single character like "I" or "A") - and making into lowercase otherwise.

## preprocessing

In this next chunk, we create a new variable called 'epoch,' with three values, *Biden* (during the Biden presidency, but before the November election, *Lameduck* (the period between the election and Trump's second inauguration), and *Trump*. We treat the 'lameduck' period separately because, although Biden was President, Trump, and concerns about the coming Trump presidency, was dominant on the airwaves. In these initial comparisons, we'll exclude that lameduck period from analysis

We also take the content of the posts and unravel them, with one line per word (so that a post which had 100 words would be 100 lines in the new, tidy datafile). Call this file tidyCorpus0 (a *corpus* is a body of text).

We then selectively set the text to lowercase (retaining uppercase only when a multi-character token, or word, is in ALL CAPS). This gives us tidyCorpus1.

We also remove stopwords from the text in order to simplify and shorten the texts; acknowledging that there are often good reasons not to do this [@lanning2018, @pennebaker2014]. We'll remove numbers from the text in the same step. Call this file tidyCorpus2.

Finally, we stem the text, reducing each word to its common stem, using the SnowballC library. This gives is tidyCorpus3. If we were to stem the text, we would move ahead with this file rather than tidyCorpus2. For more on stemming, please consider Hvitfeldt and Slige[-@hvitfeldt2021], which is available online.

```{r}
set.seed(33458)
a2 <- a1 |> filter (selftext != "[removed]") |> 
    mutate (epoch = case_when (
        date < "2024-11-06 00:00:00" ~ "Biden",
        date > "2025-01-20 12:00:00" ~ "Trump",
        TRUE ~ "Lameduck"
    ))
tidyCorpus0 <- a2 |> 
    unnest_tokens(word, selftext, to_lower = FALSE)
tidyCorpus1 <- tidyCorpus0 |> 
    mutate (word = case_when(
# if changing a word to uppercase has no effect ...
        (toupper(word) == word) & 
# amd the word is more than 1 character long, keep it as is
            nchar(word) > 1 ~ word,
# else change it to lowercase
        TRUE ~ tolower(word)))

# tidyCorpus2 gets rid of numbers and removes stop words
tidyCorpus2 <- tidyCorpus1 |> 
    anti_join(stop_words) |> 
    mutate(word = gsub(x = word, 
                       pattern = "[0-9]+|[[:punct:]]|\\(.*\\)",
                       replacement = "")) |> 
    filter (word != "")

# tidyCorpus3 takes the additional step of stemming the data
tidyCorpus3 <- tidyCorpus2 |> 
  mutate(word = wordStem(word))
```

## comparing the words and stems

We reduce each of the the three tidy *corpora* (bodies of text) to simple frequency tables

```{r}
nwordsCorpus0 <- tidyCorpus0 |> count (word) |> nrow()
nwordsCorpus1 <- tidyCorpus1 |> count (word) |> nrow()
nwordsCorpus2 <- tidyCorpus2 |> count (word) |> nrow()
nwordsCorpus3 <- tidyCorpus3 |> count (word) |> nrow()
```

Whereas there were 58154 words in the initial data, this drops to 49447 when we (mostly) represent all words as lower case. It drops further to 41586 when we remove numbers and stop words, and finally to 29338 when we stem the data.

## Construction of differential word clouds

We first take the corpus and reshape it to a matrix with row names (words) and counts (one column for each group).

```{r}
wideCorpus2 <- tidyCorpus2 |>
    filter (epoch != "Lameduck") |> 
    count(epoch, word, sort = TRUE) |> 
    pivot_wider(names_from = epoch, values_from = n, 
                values_fill = 0) |> 
    column_to_rownames(var = "word") |>
# filter out words that don't appear at least a few times in each group
# and also removes words that are too rare to be useful for comparison
    mutate (mincount = pmin(Biden, Trump)) |> 
    filter (mincount > 4) |> 
    select (-mincount) |> 
    as.matrix(rownames = TRUE)  
head(wideCorpus2,5)
```

The `comparison.cloud` function from the `wordcloud` package is used to show the greatest relative proportions of words in each group (in that function, the scale parameters will need to be adjusted to get the best visual representation).

```{r}
comparison.cloud(wideCorpus2,
        scale = c(1.5,.5),
        max.words=100,
        random.order=FALSE,
        rot.per = 0,
        use.r.layout=FALSE,
        family="sans",
        title.size=1)
```

Many of the words which appear disproportionately in the Trump corpus (*employees, https, federal, government)* appear relatively neutral in emotional tone, although there are also terms such as *fired* and *resignation* here. On the Biden side, the abbreviation *GS* (for General Schedule, describing the level of various positions in civil service), together with *position, job, HR, step, hours*, and *supervisor* indicates that most posts refer to employment issues.

Interesting, but perhaps less than we might have expected. Will an analysis of two-word strings (bigrams) provide more insight?

## bigrams

The comparison cloud illustrates differences in single-word occurrences between the Biden and Trump epochs in r/fedworkers. Here, we repeat the analysis, investigating two-word strings. Note that in order to remove stopwords and numbers, we fist combine adjacent words (tokens) into bigrams, then separate these into pairs of adjacent words, filter on each of these adjacent words, and rejoin them.

```{r}
tidyBigrams <- a2 |> 
    filter (epoch != "Lameduck") |> 
    unnest_tokens(bigram, selftext, token = "ngrams", n = 2) |> 
    separate(bigram,c("word1", "word2"), sep = " ") |> 
    anti_join(stop_words, by = join_by("word1" == "word")) |> 
    anti_join(stop_words, by = join_by("word2" == "word")) |> 
    mutate(word1 = gsub(x = word1, 
                       pattern = "[0-9]+|[[:punct:]]|\\(.*\\)",
                       replacement = "")) |> 
    mutate(word2 = gsub(x = word2, 
                       pattern = "[0-9]+|[[:punct:]]|\\(.*\\)",
                       replacement = "")) |> 
    drop_na() |> 
    filter (word1 != "") |> 
    filter (word2 != "") |> 
    unite(bigram, word1, word2, sep = " ")
bigramCounts <- tidyBigrams |>     
    count(epoch, bigram, sort = TRUE) |> 
    pivot_wider(names_from = epoch, values_from = n, 
                values_fill = 0) |> 
    column_to_rownames(var = "bigram") |>
    as.matrix(rownames = TRUE) 
comparison.cloud(bigramCounts,
        scale = c(1.5,.5),
        max.words=100,
        random.order=FALSE,
        rot.per = 0,
        use.r.layout=FALSE,
        family="sans",
        title.size=1)

```

Here, the language of the two epochs comes into clearer focus, with phrases such as *deferred resignation* characterizing the Trump epoch and the relatively benign *pay period* characterizing the Biden period.

## categories of words

In my own research, I rely on a proprietary tool, *Linguistic Inquiry and Word Count.* Though LIWC can be run within R on a machine that has the license for the software, here I run the program externally, then reimport it into R.

```{r}
write_csv(a2, "data/fedworkersposts.csv")
# we run LIWC outside of R here. 
# the code is run on the raw data files
fedworkersLIWC <- read_csv("data/fedworkersLIWC.csv")
```

## Code for assessing LIWC effect sizes between 2 groups

There are many measures, or categories, in LIWC. In order to assess the statistical significance of differecnces between the Biden and Trump epochs on LIWC, the simplest approach is to adjust for the number of comparisons using the Bonferroni correction. The number of LIWC categories is 117, so the Bonferroni p-value should be .05/117 = .000427.

This chunk uses three different libraries describe function from the psych package to get means, standard deviations, and sample sizes for each LIWC category. Then it computes the effect sizes and associated statistics using the mes function from the compute.es package and makes tables for the LIWC variables with the largest effects.

```{r}
BonferroniP <- (.05/117) # 117 is the number of LIWC categories# simple descriptives

BidenEpoch <- fedworkersLIWC |> 
    filter (epoch == "Biden") |> 
    select(WC:OtherP) |> 
    describe() |> # from the psych package
    select(BidenN=n, BidenM = mean, BidenSD = sd) |> 
    round(2)
TrumpEpoch <- fedworkersLIWC |>
    filter (epoch == "Trump") |> 
    select(WC:OtherP) |> 
    describe() |>
    select(TrumpN=n, TrumpM = mean, TrumpSD = sd) |> 
    round(2)
poststats <- BidenEpoch |> 
    bind_cols (TrumpEpoch)
# from the compare.es package.
Effects <- mes(poststats[,2], 
               poststats[,5],
               poststats[,3], 
               poststats[,6],
               poststats[,1],poststats[,4],
               level = BonferroniP,
               verbose = FALSE) |> 
    select(d, l.d, u.d, pval.d)
LIWCSummary <- poststats |> 
    bind_cols(Effects) |>
    select(-BidenN, -TrumpN) |> 
    write_csv("data/LIWCeffects.csv")

Nsignificant <- Effects |> 
    mutate (sig = case_when (pval.d < BonferroniP ~ 1,
                             TRUE ~ 0)) |> 
    summarise(sum = sum(sig)) |> 
    as.numeric()
BidenN <- min(poststats$BidenN)
TrumpN <- min(poststats$TrumpN)
LIWCSummary |> 
    slice_max(d, n = 10) |> 
    kable(caption = "LIWC categories in r/fedworkers most associated with the Biden era") |> 
    kable_styling() %>%
    add_footnote(paste0(
        "Note: BidenNs >= ",
        min(poststats$BidenN),
        " TrumpNs >= ",
        min(poststats$TrumpN)))
LIWCSummary |> 
    slice_min(d, n = 10) |> 
    kable(caption = "LIWC categories in r/fedworkers most associated with the Trump era") |> 
    kable_styling() %>%
    add_footnote(paste0(
        "Note: BidenNs >= ",
        min(poststats$BidenN),
        " TrumpNs >= ",
        min(poststats$TrumpN)))
        
LIWCSummary |> 
    arrange(desc(d)) |> 
    head(10) |> 
    kable()


```

There are just over 19000 posts in the (roughly four month long) Trump epoch, and just over 9000 in the (roughly ten months long) Biden epoch. Judging from the number of posts, the *r/fedworkers* subreddit has been substantially more active since the Trump presidency began.

In comparisons of the two "epochs," the LIWC categories which were particularly characteristic of the Biden posts included *Authentic,* a summary variable indicating apparent honesty or genuineness, and *I*, including first-person singular pronouns [@boyd2022]. For each of these, the effect size (difference in standard deviations between the two distributions), was more than .5, indicating a moderate effect. The LIWC categories which were particularly characteristic of the Trump posts included *Clout,* a summary category of words associated with leadership or status, followed by *Culture,* which includes language about politics, technology, and ethnicity, and *we*, second-person singular pronouns)*.* The shift from the first-person singular to plural is what we might expect in a community of individuals subjectively under siege. Altogether, of the 117 LIWC Categories, 98 showed a statistically significant difference between the two epochs.

These analyses support the argument presented in the *Times* article referenced above, including that the r/*fedworkers* subreddit had grown, in the Trump epoch, into a supportive community (shift from *i* to *we*), in which workers were sharing anxieties (LIWC categories such as *emo_neg* and *tone_neg*).

## exercise: what would you do next?

This chapter has provided a brief example of how to work with text from Reddit. If you were to work with these data, what would you do next? If you were to work with another dataset, what would it look like, what would you expect to find, and how would you study it?

<!--chapter:end:21-workingwithtext.Rmd-->

```{r setup22, include=FALSE, message = FALSE}
library(tidygraph)
library(cowplot)
library(ggraph)
```

# an introduction to networks

*"No man is an island,\
entire of itself;\
every man is a piece of the continent,\
a part of the main."*

-John Donne.

Networks are everywhere. For engineers, networks are the structures that underlie power grids and transportation systems. For epidemiologists, networks describe the paths through which infectious diseases spread. For social scientists, networks are a key unit of analysis in understanding the structure of social groups. Phenomena as diverse as altruism [@apicella2012], kidney donations [@nikzad2021], and obesity [@christakis2007] can be understood, in part, as network phenomena.

Networks can be nested. The internet can be seen as a network of machines linked by cables and various cellular and satellite technologies. Social network applications such as *X* and Instagram operate within these networks, and your own network of acquaintances

In this chapter, we briefly introduce some key network concepts. We then consider how network structures can be examined and understood in the tidyverse. In the next chapter, we will consider a case study in network analysis, examining a *bibliometric* network structure of a scholarly discipline.

## key network concepts

Networks involve two types of objects **nodes** (or vertices) and **edges** (or links). Networks may be directed or undirected. Edges in a **directed** network might describe relationships such as *likes, follows,* *has heard of, cites,* or *clicks on.* We can represent them by arrows between nodes.

Edges in an **undirected** network are symmetrical, as in the case of sharing a common property or ancestor. Undirected relationships are marked by phrases such as *are siblings, are allies, know each other, are married, have been cited by the same source,* and *have both been exposed to.* We represent undirected networks by lines without arrowheads.

Unrequited love is a directed relationship, requited or reciprocal love can be modeled as undirected one.

**Paths** are series of connected edges between nodes.

Networks may be **unsigned** (only positive links) or **signed** (with positive and negative links).

In some networks, there is just one type of node. In addition to these **single-mode networks**, we will also encounter **bipartite (two-mode) networks**. A network of enrollments, for example, might include two types of nodes (e.g., classes and students). Edges in such a network describe relationships such as *is enrolled in*. In **bipartite networks,** there can only be links between different types of nodes (so classes can't be enrolled in classes for example).

### a simple example: networks and balance theory

> Consider an undirected, signed network consisting of just three nodes and the edges between them. The nodes might represent, for example, persons [@harary1959] or countries [@estrada2019]. Relationships between the nodes might include things such as *gets along with/does not get along with, trusts/doesn't trust,* etc.
>
> For networks (graphs) such as these, what can we say about the case where there are three negative edges? Two? One? Zero? Which ones seem to 'make sense' and might be expected to be stable over time?
>
> A recent [*NY Times* article](https://www.nytimes.com/interactive/2024/04/20/opinion/a-visual-guide-to-the-middle-east.html) illustrates how the complex geopolitical situation in the contemporary Middle East can be modeled using a simple signed network.

### centrality

Within networks, nodes differ in **centrality,** connectedness, or importance. There are different measures and types of centrality - for example, the number (**degree**) of a node describes its number of connections. In directed networks, we can further distinguish between indegree centrality, for incoming links, and outdegree centrality, for outgoing ones. On some social media platforms, this might be described by 'followers' and 'is following', respectively.

Other measures of centrality gauge the importance of a node by the number of other nodes it connects (betweenness centrality). The most interesting measures of centrality, eigenvector centrality and **PageRank**, are *recursive*. That is, a node is central or important to the extent that the nodes to which it is linked are themselves central or important. PageRank was the original foundation of modern search engine on the Web. (The "Page" in PageRank does not refer to Webpage, but to Larry Page, one of the founders (with Surgey Brin) of Google [@page1999].

### components and communities

Networks vary in their size (e.g., big vs small), number of discrete components (zero for the empty set, one for a network in which all nodes are directly or indirectly connected, more than this otherwise), and also in their community structure or clustering. Whereas the number of components in a network is relatively unambiguous, communities in networks are often fuzzy or difficult to discern, in much the same way that constellations in the night sky might seem arbitrarily 'drawn.'

Just as there are many notions of centrality, there are also many approaches to thinking about communities. For example, we might use a "top-down" approach in which we begin with all of the nodes in a network and cleave them into two groups that both (a) minimize within-group distances and (b) maximize between-group distances. We could then repeat this strategy until we met some criterion. This approach gives a tree-like picture of community structure.

Alternatively, we can use a "bottom-up" approach to thinking about categories - combining the two closest nodes into a single community, then repeating this as appropriate. Communities in which all members are directly connected to all others are referred to as **cliques.** In the bottom-up approach, unlike the top-down approach, there are typically left-over nodes which are not connected to any community. In some applications of the bottom-up approach, communities are allowed to overlap, giving rise to complex structures such as that illustrated previously in Figure 20.1.

### another example: it's a small world

> Consider a bipartite network in which there are two types of nodes, students and classes. An edge in this network represents a particular student who is enrolled in a particular class. Make a sketch of this network for you and a friend who is not in this class. Is there a path between the two of you? How about if we add another friend - or ten friends - to the network? What do we notice about the component structure of the network and the paths between nodes as the network grows bigger? [@milgram1967].

In data science, it's generally easiest to begin with very small datasets and to work towards bigger ones. With networks, this isn't always the case, because very small networks may be structurally more complex than larger ones.

### static and dynamic networks

Finally, we can distinguish between static networks (assessed at just one moment in time) and dynamic ones, which change. In dynamic networks, several important phenomena can be seen. One of these is **contagion** - not just of diseases, but also of emotions [@kelly2016] and a range of socially desirable and undesirable phenomena [@christakis2013]. Another phenomena seen in dynamic networks is **preferential attachment** in which the most important nodes become still more important over time [@watts2004].

Preferential attachment is reflected in many aspects of contemporary culture; we see it when a handful of restaurants or products 'go viral' while others struggle to attract customers. I believe that the acceleration of inequality of wealth and power in contemporary society is, in part, a product of the connectedness of the network in which we live, an unfortunate byproduct of the digital age.

```{r}

ba <- play_barabasi_albert(n = 3, power = 1, 
                           appeal_zero = 0, growth = 1, 
                           directed = FALSE) |> 
  ggraph(layout = "kk") +
  geom_node_point(aes(size = 1, color = "BLUE")) +
  geom_edge_link(alpha = 0.5) +
  theme_graph() +
  theme(legend.position = "none")
bb <- play_barabasi_albert(n = 10, power = 1, 
                           appeal_zero = 0, growth = 1, 
                           directed = FALSE) |> 
  ggraph(layout = "kk") +
  geom_node_point(aes(size = 1, color = "BLUE")) +
  geom_edge_link(alpha = 0.5) +
  theme_graph() +
  theme(legend.position = "none")
bc <- play_barabasi_albert(n = 20, power = 1, 
                           appeal_zero = 0, growth = 1, 
                           directed = FALSE) |> 
  ggraph(layout = "kk") +
  geom_node_point(aes(size = 1, color = "BLUE")) +
  geom_edge_link(alpha = 0.5) +
  theme_graph() +
  theme(legend.position = "none")
plot_grid (ba, bb, bc,
         nrow = 1, ncol = 3) +
  draw_figure_label("Preferential attachment: As networks grow, there is increasing inequality in centrality (degree)")
```

<!--chapter:end:22-introtonetworks.Rmd-->

```{r setupbib, message=FALSE}
#if needed, install.packages("")
library(tidyverse)
library(kableExtra) # for tables 
library(bibliometrix) # for generating network from bib data & reducing to one mode
#library(qgraph)  # for drawing simple graphs
library(tidygraph) # for representing graphs as matrices of nodes/edges
library(igraph) # needed by tidygraph
library(ggraph) 
```

# case study: the network structure of computational social science

Bibliometric networks are models of the structure of scholarly disciplines. There are a variety of methods to developing such networks. We'll use an approach which is both simple and powerful, and that is to examine shared references or *bibliographic couplings.* In such a network, papers will be strongly linked if they have many references in common, and distant from each other when they share no or few references.

We begin with a collection of records of research articles that we have downloaded from the Web of Science, using the phrase "computational social science" as a search term. (A [video on YouTube](https://youtu.be/qD8Q08k2trs?si=n_59vdW8NXqI7mWR) illustrates each step of this process).

[A second video](https://youtu.be/cSi7-WzDkzM?si=o4UoRVIKESe3_Djt) accompanies the code below, which was originally developed for the Summer Institute of Computational Social Science held in Jupiter, Florida in 2023.

After loading the needed libraries, we begin with a dataframe consisting of a list of records downloaded from the Web of Science. These are all papers which included the phrase 'computational social science,' as downloaded on 6/5/2023. The data include a 'source' field and a set of cited references as well as a bunch of other fields (metadata).

```{r initialDF}
# for large jobs, set to TRUE to run this on only one reference file
debugging <- FALSE
dataDir <- "data/CompSocSci2023"
filenames <- list.files(dataDir, pattern = "*.txt", 
                        full.names = TRUE)
if (debugging == TRUE) {filenames = filenames[1]}
biblioDF <- data.frame(as.list(filenames)) %>% 
    convert2df()
biblioDF %>%  
    rownames_to_column("source") %>% 
    select(source,CR) %>% 
    head(1) %>% 
    kable(caption = "Cited refs in one document") %>% 
    kable_styling()
```

### from citation network to structural network

A citation network is a type of two-mode or bipartite network which consists of source papers, referenced papers, and the directed edges which link a subset of these. Here's a fragment of our citation network, first as a list of citations (edges), then as a graph showing links between the two types of vertices or nodes (i.e., source papers in red, cited papers in green). Notice that the edges are directed (e.g., Hox 2017 cites Lazer 2014).

```{r}
# illustration of a small fragment of the citation network
someSources <- biblioDF %>% 
# take the first four sources
    slice (1:4)

smallBiblioDF <- someSources %>% 
# make them into a rectangular source * citation matrix
    cocMatrix(Field = "CR", sep = ";") %>% 
    as.matrix() %>% 
# then a list of edges
    reshape2::melt() %>% 
    filter (value > 0) %>% 
    rename(source = 1, citation = 2) %>% 
    select(-value) %>% 
    group_by(citation) %>% 
# then choose only papers cited more than once
    filter(n()>1) %>% 
    ungroup

smallBiblioDF %>% 
    kable (caption = "A small citation network: 
           Edge list of papers cited > 1 time in four sources in CSS") %>% 
    kable_styling()

# plotted using qgraph
smallgraph <- smallBiblioDF %>%
    qgraph::qgraph(labels = T, 
           label.cex = 4,
           vTrans = 30,
           border.color = "white",
           edge.width = 4,
           color = c(2,2,2,2,3,3,3,3,3,3,3,3,3),
           title = "A small citation network: Common cites among four papers")
```

The two-mode citation network can be collapsed to one of two one-mode networks. A co-citation network is one which extracts citations (green nodes), and links them based on the number of times that they are cited by the same authors. We will look instead at *bibliometric couplings,* or links between the (pink) source papers. This will be a single mode, weighted, undirected network, with source papers as vertices and the number of shared references as edge weights. Here's a graph showing the collapsed network based on just the papers cited above:

```{r fromtwotoone}
smallOneModeMatrix <- biblioNetwork(someSources, analysis = "coupling", 
                           network = "references", 
                           sep = ";") %>% 
# Normalize adjusts weights; Salton algorithm takes shared refs / product of geometric means of n refs. This is what I have always done.
    normalizeSimilarity(type = "salton") %>%
    as.matrix() %>% 
    round(2)
diag(smallOneModeMatrix) <- 0
smallOneModeMatrix %>% 
    kable(caption = "A small structural network: Matrix showing shared references among 4 papers") %>% 
    kable_styling()
```


Based on the structure of shared references, Baltar and Hox *appear* to be quite different, distant from each other. (But don't trust the result too much - small networks like this aren't particularly stable). Regardless, this is a beginning of an understanding of the structure of scholarship in computational social science.

### looking at the whole citation network

The bibliometrix package can quickly give summary statistics for citation networks. Here are some characteristics of the entire set of papers.

```{r}
twoModeStats <- biblioAnalysis(biblioDF)#, sep = ";")
twoModeSummary <- twoModeStats |> 
  summary() # summary returns a list. look at it.
twoModeSummary |> 
  pluck(1) |> 
  unlist() |> 
  kable(
    caption = "Citation network: Key features") %>%  
    kable_styling() 
```

Here's a manually-constructed table of the "most cited journals."

```{r}
sourceJournals <- biblioDF %>% 
    citations(field = "article", sep = ";") %>% 
# output of above is a list... pluck gets the first element
# which is a table of sources
    pluck(3) %>%
    as_tibble() %>%
    rename(CitedJournal = 1) 

sourceJournals %>% 
    count(CitedJournal) %>% 
    arrange(desc(n)) %>% 
    head(10) %>% 
    kable (caption = "Most cited journals in Comp Soc Sci") %>% 
    kable_styling()
```

#### the need to inspect/clean data

One drawback of bibliometrix and packages like it is that they remove us somewhat from the data. Here, 'NA' is the third most common source of papers. We look more closely at these records to see if there is a problem. Here's one way...

```{r}
# there are no papers where the cited field is empty or NA
# so this gets us nowhere
# sourceJournals %>% 
#        filter(CitedJournal == "" |
#               CitedJournal == "NA" )
set.seed(33458)
countsonly <- sourceJournals %>% 
    group_by(CitedJournal) %>% 
    count(CitedJournal) 
countNA <- countsonly %>% 
    filter(is.na(CitedJournal)) %>% 
    as_tibble %>% 
    select(n) %>% 
    as.integer()
sources <- sourceJournals %>% 
    left_join(countsonly)
biblioDF %>% 
    citations(field = "article", sep = ";") %>% 
    pluck(1) %>% 
    as_tibble() %>% 
    bind_cols(sources) %>% 
    rename(papercites = 2, journalcites = 4) %>% 
    filter(journalcites == countNA) %>% 
    select(CR) %>% 
    sample_n(20) %>% 
    kable (caption = "Random sample of references with source journal coded as NA") %>% 
    kable_styling()
```

These NA appear to be mostly detritus, cites to anonymous sources, etc. There are better ways of cleaning the data (including restricting the set of papers to articles and reviews), but we'll ignore these for now.

### the structural network: Centrality and community structure

We now reduce the two mode network to a one-mode for the whole dataset, normalizing the edge weights as before, and setting the diagonal entries to 0.

We then use the tidygraph package to compute several measures of centrality and community structure.

```{r}
# random seed to ensure reproducible results
# esp for centrality and community analyses
set.seed(33458)
oneModeMatrix <- biblioNetwork(biblioDF, 
                            analysis = "coupling", 
                            network = "references", 
                            sep = ";") %>% 
    normalizeSimilarity(type = "salton") %>%
    as.matrix()  

str(oneModeMatrix)
# save it as a graph for igraph/tidygraph
oneModeGraph <- oneModeMatrix %>% 
    graph_from_adjacency_matrix(mode = "undirected", 
                                diag = FALSE,
                                weighted = TRUE)
# tidygraph allows us to look at graphs
# as data frames of nodes and edges
tidyBibGraph <- oneModeGraph %>% 
    as_tbl_graph() %>% 
# we activate nodes to assign new measures for each node
    activate(nodes) %>% 
    mutate(centralPR = centrality_pagerank(weights = weight)) %>% 
    mutate(nodePRRank = rank(-centralPR)) %>% 
#    mutate(centralWD = centrality_degree(weights = weight)) %>% 
    mutate(central0D = centrality_degree(weights = NULL)) %>% 
    mutate(communityLouv = group_louvain(weights = weight)) %>% 
    mutate(group = as.factor(group_louvain())) %>% 
#    mutate(communityWalk = group_walktrap(weights = weight)) %>% 
    mutate(ID = row_number()) 

edgeList <- tidyBibGraph %>% 
    activate(edges) %>% 
    as_tibble()
nodeList <- tidyBibGraph %>% 
    activate(nodes) %>% 
    as_tibble()
```

### visualizing the network

I use ggraph to try a few visualizations within R - it plays nicely with tidygraph. In the first visualization, all but the 22 isolates are plotted. It suggests that there is a community structure, but nothing beyond this.

```{r}
nNodesToPlot <- 794
minEdgeWeightToPlot <- .00001
library (ggraph)
tidyBibGraph %>% 
    activate(nodes) %>% 
    filter(central0D > 0) %>% 
    filter(nodePRRank < nNodesToPlot) %>% 
    activate(edges) %>% 
    filter(weight > minEdgeWeightToPlot) %>% 
  ggraph(layout = 'stress')  +
  geom_edge_link(alpha = .1) +  
  geom_node_point(aes(size = (centralPR),
                      color = as.factor(communityLouv))) +
#  geom_node_label(aes(label = name,
#                      color = as.factor(communityLouv))) +
     theme(legend.position = "none")
```

In the second plot, I show just the top 20 nodes, but also include labels for these.

```{r}
nNodesToPlot <- 20
minEdgeWeightToPlot <- .05
tidyBibGraph %>% 
    activate(nodes) %>% 
    filter(nodePRRank < nNodesToPlot) %>% 
    activate(edges) %>% 
    filter(weight > minEdgeWeightToPlot) %>% 
  ggraph(layout = 'stress')  +
  geom_edge_link(alpha = .1) +  
  geom_node_point(aes(size = (centralPR),
                      color = as.factor(communityLouv))) +
  geom_node_label(aes(label = name,
                      color = as.factor(communityLouv))) +
     theme(legend.position = "none")
```

The graph shows papers in four communities, three of which are represented by just one or two papers. Note that some of the papers are odd - a book review, for example.

```{r}
nodeInfo <- biblioDF %>% 
    mutate(ID = row_number()) %>% 
    select(AB, TI, DE, ID, J9) %>% 
# ID is a text field of keywords in the bib data
# I combine it with the other text fields, then
# create a new ID of text number to combine with
# the node centrality etc
    mutate(alltext = paste(TI, AB, DE, ID, sep = " ")) %>% 
    select(-AB, -DE, -ID) %>% 
    mutate(ID = row_number()) 
allNodeInfo <- nodeList %>% 
#    filter(nodePRRank < 21) %>% 
    left_join(nodeInfo, by = 'ID')

nodeList %>% 
    filter(nodePRRank < 21) %>% 
    left_join(nodeInfo) %>% 
    select (name, nodePRRank, communityLouv, TI) %>% 
    arrange(communityLouv, nodePRRank) %>% 
    kable(caption = "Top papers in CSS by PR and community") %>% 
    kable_styling()

```

### exploring the communities

One way to see and understand this community structure is to explore the datausing an interactive network tool (Gephi). In order to do this, we'll write the nodes and edges to disk in two separate writes.

Another way to understand the communities is to compare the language of different communities, using the combined text fields. That is explored in a second r markdown script; CSS2_NetworkToText.Rmd.

```{r}
allNodeInfo %>% 
  write_csv(file.path(dataDir, "CSSNodesText.csv"))
edgeList %>% 
  rename(source = from, target = to) %>% 
  write_csv(file.path(dataDir, "CSSEdges.csv"))
```

<!--chapter:end:23-bibliometrics.Rmd-->

# some ethical concerns for the data scientist

The Latin word *scientia* is commonly translated as both "science" and "knowledge." Modern science or modern knowledge has led not only to dramatic improvements in public health and human life expectancy, but also to weapons of war that allow the slaughter of millions at a distance. The idea that science or knowledge is ethically fraught is not new: Adam and Eve are said to have been cast out of the Garden of Eden after tasting the forbidden fruit from the tree of *knowledge* [@shattuck1997]. And, as we have seen in our discussion of the "most dangerous equation," the lack of knowledge can also be dangerous - perhaps particularly today, as we are in the grips of a pandemic with leaders who govern by intuition rather than science, by hunch rather than data [@wainer2007].

In this final chapter, I consider a few case studies which exemplify some of the ethical issues and concerns that frame data science. Note the word "frame" - this is little more than the beginning of a scaffolding. But I hope that it is sufficient to bolster the claim that ethical concerns are or should be at the foundation of data science, and that individuals with training in the liberal arts (and not just math and engineering) should play an important role.

## ethics and personality harvesting

In personality psychology, concerns about how personality tests might invade privacy have been raised for over fifty years. Today, these concerns are newly relevant. One issue is measurement and experimentation without consent [@gleibs2014]. Here, the most dramatic and arguably consequential example of this occurred when Cambridge Analytica scraped the online activity of individuals whose friends had participated in a study of personality, then leveraged this further to assess the personality of all or nearly all American adults. Cambridge Analytica, Facebook, and other data-rich firms such as Experian engage in practices such as "shadow profiling," which is the nonconsensual profiling of individuals through their social connections [@garcia2018].

This profiling leads to the selective tailoring of messages (ads, political appeals) which, at the very least, attenuates freedom of choice. At a societal level, it can lead to manipulations of consumer and political behavior, including voting decisions [@bond2012; @matz2017]. Tailored messaging can not only harm our social fabric, deepening partisan divides among us, but can have consequences for social institutions as well.

In a recent paper, several of us expressed our concern that the harvesting of personality information is likely to become more common in the years to come [@boyd2020]. This argument derives from a series of claims, which are listed below:

1.  **It takes very little data to identify someone**. We can often discover the identity of an individual from a very small number of data points [@sweeney2005].
2.  **There is a great deal of data available**. For example, credit card purchases, Uber trips, exercise routes tracked on platforms such as Strava, turnpike itineraries, medical records - are becoming increasingly digital, and so the potential for linking data increases.
3.  **Combining data leads to new value**. As we have seen this term, there can be meaningful value, new knowledge, when different datasets are joined.
4.  **There are few constraints on data collection and synthesis**. The collection of digital data is typically undertaken by private companies motivated by profit, who are often unconstrained by ethical concerns that might be raised by, for example, human subjects review boards.
5.  **Information is valuable** to a range of parties - from potential employers to insurance companies to potential romantic partners. For these and other reasons, we anticipate that there will be a growing market for data about our personalities [@wu2019].

## the law of unintended consequences

One of the recurring issues in digital ethics has been the so-called "law of unintended consequences" [@merton1936]. When we initiate a new policy, or collect a new dataset, or give permission to a social media application to share our information, or investigate our family tree using a service such as Ancestry.com or 23andMe.com, we do not know what will happen with the information that we share or create. For example, assume that Fred signs up to learn about his family and health on 23andMe. He finds that he has a health vulnerability with an already known, or even a soon-to-be-discovered, genetic predisposition. That information has consequences not just for him but, potentially, for his offspring. If that genetic information is shared, it takes little in the way of a dystopian imagination to consider that Fred 's grandchildren might have to pay higher health insurance premiums, be prohibited from migrating to certain other countries, be seen as less desirable partners, etc. These examples might seem extreme, but unintended consequences are typically unforeseeable. For example, when the good folks at Netflix shared some of their movie-preference data and offered a million-dollar prize to anyone who could substantially improve on their recommendation-engine, they did not intend to "out" individuals including a closeted lesbian mom who was identified by several data scientists [@narayanan2008].

## your privacy is my concern

It is not paradoxical that I should be concerned with your (right to) privacy. I'm invested in your ability to choose what to reveal about yourself, when, and to whom, in part because I want you to be able to live in and contribute to the world. Even if I am not that concerned about having details of my own life revealed, I must be sympathetic to your concerns. The concern for privacy is not a fetish, but is critical if we are to live and work in a just and decent society.

## who should hold the digital keys?

Let's shift gears somewhat, and consider some of the issues surrounding autonomous vehicles.

In a data-dependent world, who should be the guardians of the code that connects us? To consider just one example, the computer systems in modern cars typically run millions of lines of code. As cars become increasingly autonomous, this complexity will only increase. (Incidentally, the Society of Automotive Engineers, or SAE, describes 6 levels of 'auto autonomy.' At this writing, the most sophisticated systems available to consumers, such as Tesla Autopilot, are at level 2. What lies ahead are cars which are self-driving on carefully selected, geo-fenced roads, and ultimately cars "[which can operate on any road... a human driver could negotiate](https://www.caranddriver.com/features/a15079828/autonomous-self-driving-car-levels-car-levels/)"). Our roads and highways will become an Internet of Vehicles (IOV), which will include not just connections between cars and an intelligent *cloud* 'above us' but also direct links between a distributed system of intelligent cars, stoplights, and road sensors in a *fog* 'around us' [@bonomi2012]. **Fog computing** and the IOV will reduce travel times and increase both fuel efficiency and automotive safety.

Obviously, there are **cybersecurity** concerns. While the prospects for [a chaotic, choreographed hack of hundreds of vehicles on the streets of Manhattan](https://www.youtube.com/watch?v=OvewYslou9g), such as that in the 2017 movie "The Fate of the Furious", are remote at best (or worst), there have been examples of "white-hat hackers" who have successfully infiltrated (and thereby helped secure) car information systems.

As the IOV develops, there will be vulnerabilities to privacy as well as safety, and the security of the system will be paramount. Different car manufacturers are taking different approaches to developing secure information systems, with many using a closed or proprietary approach. But the scope of the problem is so large that there is a movement towards pooling resources and encouraging collaboration among industry partners, academics, and citizen scientists in the development of an open-source autonomous driving platform, such as [Apollo](http://apollo.auto/). Perhaps counterintuitively, there may be significant security advantages to using source code that is open to all [@clarke2009; @fitzgerald2016].

## contact-tracing and COVID-19

On April 10, 2020, Google and Apple announced that they would collaborate on a contact-tracing system to try and slow the COVID-19 pandemic. Keeping in mind the ethical issues of (a) the law of unintended consequences, (b) prior failures to maintain anonymity, (c) arguments that data should be best secured by industry, government, or crowdsourcing, as well as the tech issues of (d) the potential methods for contact tracing, and (e) the costs of the pandemic to public health and to world economies, to what extent should digital tracking be used to assess and limit the spread of the novel coronavirus?

You can learn more about the Google-Apple collaboration at [their FAQ](https://www.apple.com/covid19/contacttracing/) and this piece at [techcrunch](https://techcrunch.com/2020/04/10/apple-and-google-are-launching-a-joint-covid-19-tracing-tool/). Consider some of the ethical issues raised in this essay at [fivethirtyeight.com](https://fivethirtyeight.com/features/big-data-is-helping-us-fight-the-coronavirus-but-at-what-cost-to-our-privacy/), and in this more recent piece at [the Verge](https://www.theverge.com/2020/4/24/21234457/apple-google-coronavirus-contact-tracing-tracker-exposure-notification-shut-down).

## the digital divide

Not all are benefiting equally from the birth of the digital age. Unfortunately, the "Matthew-effect," by which resources flow to those who have them most and need them less, is a fundamental property of networked, complex systems. As we become more interconnected, the gap between rich and poor is accelerating. The primary path to a successful life is to find a "scalable" occupation - that is, one in which you can serve many people with little effort. But that means that fewer will "serve."

So yes, spend your summer trying to become a social media influencer. Better still, make a commitment to working to try to reduce the growing digital divide, and more broadly to address issues of equality and social justice in your own applications of data science. Google, which once had the mantra "don't be evil" in its core of conduct, has now moved to a set of positive (do's) as well as negative (don'ts) [guidelines for their work in Artificial Intelligence](https://ai.google/principles). Information can be empowering (for those who have it).

## still more case studies

As we face ubiquitous observation as our world becomes an Internet of Things, and live in a world in which decisions will increasingly be made for us by applications of machine learning and artifical intelligence, new questions will be raised about the social impact of our science. Some of these are illustrated in these [six hypothetical case studies](https://aiethics.princeton.edu/case-studies/case-study-pdfs/) proposed by an interdisciplinary team at Princeton. They warrant your consideration.

## some potential remedies

In the European Union, the [General Data Protection Regulation (GDPR)](https://gdpr-info.eu/) has been in effect since 2018, and provides guidelines for protecting people and personal data in the digital age. In a related piece, [@loukides2018] provided a briefer set of guidelines. They argued that it is not enough that people provide **consent**, but also that there must be **clarity**: That it, it's not enough that people agree to share their data, they must also understand what they are agreeing to. Other issues that they highlight include the need for data security (as stolen data often include, for example, SSNs and passwords), and protection of vulnerable populations such as children. Finally, they provide a checklist for those developing data products:

❏ Have we listed how this technology can be attacked or abused?

❏ Have we tested our training data to ensure it is fair and representative?

❏ Have we studied and understood possible sources of bias in our data?

❏ Does our team reflect diversity of opinions, backgrounds, and kinds of thought?

❏ What kind of user consent do we need to collect to use the data?

❏ Do we have a mechanism for gathering consent from users?

❏ Have we explained clearly what users are consenting to?

❏ Do we have a mechanism for redress if people are harmed by the results?

❏ Can we shut down this software in production if it is behaving badly?

❏ Have we tested for fairness with respect to different user groups?

❏ Have we tested for disparate error rates among different user groups?

❏ Do we test and monitor for model drift to ensure our software remains fair over time?

❏ Do we have a plan to protect and secure user data?

## technology, change, and risk

A *zero-sum enterprise* (or game) is one in which for every winner and for every gain, there is a loser and loss, and vice-versa. Gambling is zero-sum. So too is speculation in bitcoin and other cryptocurrencies. Innovations in science and industry are not zero-sum, for new inventions and methods can lead to change in the bottom line. Human interaction is also often not zero-sum, for they can lead to synergies of feeling and cognition, the blossoming of friendship and love and the creation of new knowledge through dialog and collaboration.

When we think of these non-zero-sum games, we are likely to think of these scenarios where there is progress, or an overall gain as when a new construction material leads to the production of better and cheaper homes and shelters to improve the human condition. But it can also be negative, as when the invention of a new herbicide leads to the production of dioxins and related toxins.

Data science is a non-zero-sum game. As previously discussed, it is a relatively easy to combine two datasets in new ways to create new knowledge. But the cost of this knowledge may be great.

<!--chapter:end:24-ethics.Rmd-->

```{r exercises, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# exercises

## generating correlated data (GPA and SAT) and putting it in a Google Sheet

It's simple to generate a single random variable, but we often want to examine data for several variables that are correlated. The MVRnorm function, in the MASS package, does this.

We use this to generate two variables with mean 0 and standard deviation 1, a given correlation, and a given sample size (number of rows).

We then fiddle with these variables, setting the means, standard deviations, ranges, and number of significant digits, to make them look like GPA and SAT scores. And we use the randomnames package to make up fake names to go along with the fake scores. Finally, we upload the data to a Google Sheet. This last command will require first establishing permissions for R to read and write to your Google Drive.

```{r makegooglesheet, include=TRUE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(babynames) # for set of first names
library(randomNames) # for initial set of last names
library(googlesheets4) # to read and write to Google sheets

set.seed(33458) # for reproducibility
correlation <- 0.4
nrows <- 1000

names <- randomNames(nrows, which.names = "first") %>%
    bind_cols(randomNames(nrows, 
                          which.names = "last")) %>%
       rename (FirstName = 1, LastName = 2)

hsdata <- MASS::mvrnorm(n = nrows, 
                mu = c(0.0, 0),
                Sigma = matrix(
                    c(1, correlation, correlation, 1),
                    nrow = 2, ncol = 2)) %>% 
    as_tibble() %>%
    select (gpa = 1, sat = 2) %>% 
    mutate (sat = 500 + (100 * round(sat, 1))) %>% 
    mutate (gpa = 3 + round(gpa, 2)) %>%
    mutate (gpa = case_when(
        gpa < 0 ~ 0,
        gpa > 4.5 ~ 4.5,
        TRUE ~ gpa)) %>% 
    mutate (sat = case_when(
        sat < 200 ~ 200,
        gpa > 800 ~ 800,
        TRUE ~ sat))

hsdata <- names %>% 
    bind_cols(hsdata) %>% 
    arrange(FirstName) 
#gs4_create(name="RandomHSGPASAT",
 #          sheets = hsdata)
slice(hsdata, 1:3)
```

### now look at your data in the spreadsheet

In this [spreadsheet](https://docs.google.com/spreadsheets/d/19BTya9N8_qyJLy8V27sefhxnV912WeilRLeX2zAUL-c/edit?usp=sharing), what are the means for GPA and SAT? What are the standard deviations? what is the difference between 'sort range' and 'sort sheet' in Google Sheets?

What do you think about the randomnames package? Do the names appear "representative" (of what)? If you wanted to generate a more representative sample of names, how might you proceed?

### using AI to help us here

Our intuitions about the idiosyncratic nature of names in the randomNames package is supported (not necessarily confirmed) by MS Copilot, which notes that "the randomNames package in R samples from a uniform distribution, which doesn't reflect real-world name frequencies."

For first names, one solution is to use the babynames data, and then to weight the probability of including the name in our sample by the actual frequency in (some) population:

```{r}

# load babynames and Filter for a specific year or range
names_data <- babynames %>%
  filter(year >= 2000) %>%
  group_by(name) %>%
  summarise(freq = sum(n)) %>%
  ungroup()

# Sample names with probability proportional to frequency
set.seed(33458)
sampled_first_names <- sample(names_data$name, size = nrows, replace = TRUE, prob = names_data$freq) |> 
    as_tibble() |> 
    rename(NewFirstName = 1)
slice(sampled_first_names, 1:10)
```

### for last names, it's a little bit trickier

For last names, we'll also use census data. There's not an R package that has these, but there is Census data that is available in a zip file. It's described at <https://www2.census.gov/topics/genealogy/2010surnames/surnames.pdf>. There are two files that are available - the top 1000 names, and then a (much longer) file of all names that occur more than 100 times. We'll use the latter. **Why do we do this?**

Our approach to downloading a zip file directly into R follows: First, we specify a temporary file, then download the zip file into it. Then we unzip this and save it in a subdirectory called 'data.' Finally, we read it into R:

```{r}
temp_zip_file <- tempfile(fileext = ".zip")
sourcefile <- "https://www2.census.gov/topics/genealogy/2010surnames/names.zip"
download.file(sourcefile, destfile = temp_zip_file, mode = "wb")
unzip(temp_zip_file, files = "Names_2010Census.csv", exdir = "data")
surnames <- read.csv("data/Names_2010Census.csv") |>
    arrange(rank)
slice(surnames, 1:10)
```

### cleaning these up

In the lastname data, there's a problem. Some 10% of the names in the data - presumably rare ones - are replaced with "ALL OTHER NAMES." We'll just filter these out.

Finally, we combine the new NewFirstName and NewLastName with our original HS data. We also replace the SHOUTING (uppercase) in the NewLastName variable with Title Case:

**Did we do this right? Any other issues? How might you have done this better?**

```{r}

surnames <- read.csv("data/Names_2010Census.csv") |>
    filter(name != "ALL OTHER NAMES")
slice(surnames, 1:10)
# then pull out a weighted sample of last names
sampled_last_names <- sample(surnames$name, size = nrows, 
                            replace = TRUE, prob = surnames$count) |> 
    as_tibble() |> 
    rename(NewLastName = 1)

hsdata2 <- hsdata |>
    bind_cols(sampled_first_names) |> 
    bind_cols(sampled_last_names) |> 
    mutate(NewLastName = stringr::str_to_title(NewLastName))
slice(hsdata2, 1:10)

```

## categorical probability and Venn diagrams

#### Dodge Chargers and FHP Cruisers

> In 2024, the Florida Highway Patrol won a national competition for "best looking cruiser." The winning car was a Dodge Charger.

![](images/FloridaCar.jpg)

> Not all FHP cruisers are Dodge Chargers, but some are. Assume that there are 8 million registered cars in Florida, that all cars (including all FHP cruisers) are registered, and that 80,000 of these (or 1% of all cars) are Dodge Chargers.
>
> a)  On the basis of the above information, if you see a Dodge Charger on the road, can you compute the probability that it is an FHP cruiser (i.e., P(FHP cruiser \| Dodge Charger)?
>
> b)  If you can compute this, what is the probability? If you cannot compute this, what is the minimum additional information would you need to compute this probability (P(FHP cruiser \| Dodge Charger)?
>
> c)  Provide a reasonable estimate of this additional value, then compute (P(FHP cruiser \| Dodge Charger).
>
> d)  Working with your own numbers, what is P(Dodge Charger \| FHP cruiser)?
>
> e)  How confident are you in these results? Are there any additional assumptions that you might make that would make you more confident about your results?

Let A = P(FHP cruiser) B = P(Dodge Charger)

a)  "not yet."

b)  Rule 4 gives us P(A\|B) = P(A and B)/P(B)\
    We have P(B) = .01 (80,000 / 8,000,000). We don't have P(A and B): We need to know how many FHP Dodge Chargers there are out of the 8 million cars in Florida.

c)  We will estimate the number of FHP Dodge Chargers as 800, or .0001 of the cars on the road.[^98-exercises-1]\
    So, P(A\|B) = P(A and B)/P(B) = .0001/.01 = .01.\
    That is, 1% of the Dodge Chargers on the road are FHP.

d)  Here, we want the reverse probability P(B\|A) [p(Dodge Charger \| FHP cruiser)].\
    We can use rule 4 again, just flipping around A and B:\
    P(B\|A) = P(A and B) / P(A) - but we first need to know the number of FHP Cruisers. This is the other missing number. We'll estimate 1,600.[^98-exercises-2]\
    So P(A) = 1,600/8,000,000 = .0002\
    So, P(B\|A) = P(A and B)/P(A) = .0001/.0002 = .5.\
    That is, half of 1% of the FHP cars on the road are Dodge Chargers.

e)  Actually, we have been working with the number of registered cars - not the number that's likely to be on the road at a given time. For this, FHP cruisers would need to be on the road with the same frequency as other vehicles - I'm guessing that they aren't.

[^98-exercises-1]: Microsoft Copilot estimates this number as between 100 and 300. Claude estimates 800 to 1200. The midpoints of these are 200 and 1000; we'll guess 800 (600 might be better, but 800 makes the math easier here, and we are really speculating).

[^98-exercises-2]: Claude ("1,400-1,600") and CoPilot ("several thousand") are in relative agreement here.

#### **(Asymmetrical) Venn Di**agrams in R

Problems about probability are often easier if we draw Venn diagrams.

> a)  Sketch out a Venn Diagram that accurately reflects the relationships you described in exercise 9.1.
>
> b)  Use R to generate your Venn Diagram.
>
> c)  Look at your figure. In general, if P(A\|B) \< P(B\|A), what must be true of the relationship of P(A) to P(B)?

```{r Venn1, fig.width = 3, fig.height = 3}
# Load required packages
library(VennDiagram)
library(grid)


# Create first Venn diagram
venn1 <- draw.pairwise.venn(
  area1 = 80000,
  area2 = 1600,
  cross.area = 800,
  category = c("Charger", "FHP"),
  fill = c("gray", "orange"),
  lty = "blank",
  cex = 1,#1.5,
  cat.cex = 1,
  cat.pos = c(-20, 40),
  cat.dist = 0.05,
  ind = FALSE
)

# To print Venn1, you need to wrap the figure using the grid package. The push and popViewport commands set the margins so it works right.
grid.newpage()
pushViewport(viewport(x = 0.5, y = 0.5, width = 0.9, height = 0.9))
grid.draw(venn1)
popViewport()

```

> Here, P(A\|B) is 800/1600 or .5, and P(B\|A) is 800/80000 or .01.
>
> If the base rate of A is greater than that of B, and there is at least some overlap, then P(A\|B) \> P(B\|A).

<!--chapter:end:98-Exercises.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:99-references.Rmd-->

