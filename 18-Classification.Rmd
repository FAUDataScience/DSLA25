---
title: "18-Classification"
author: "Lanning"
date: "March 26, 2019"
output: html_document
---
```{r, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warn = FALSE, message = FALSE) # defaults for all chunks after this one
library(tidyverse)
# library(dslabs)
# library(broom) 
library(modelr)
library(Ecdat) 
# library (class) # for knn
library (caret) # machine learning package
library (caTools) # ditto
# ds_theme_set()
set.seed(0)
```
# From regression to prediction and classification

## Revisiting the affairs data

Let's go back and do what we should have done earlier: Examine and think about "number of affairs". What does the distribution look like?

```{r}
data(Fair)
Fair %>% count(nbaffairs)
```
We note that the distibution is skewed - and we realize that perhaps we should think about it differently: The *meaning* of the difference between 0 and 1 is not the same as that between 1 and 2, or perhaps even 1 and 12.

We can transform the data in several ways. We might begin with a root transform, rounded as integer:
```{r}
Fair2 <- Fair %>% 
    mutate(rootAffair = 
              as.integer(sqrt(nbaffairs)))
Fair2 %>% count(rootAffair)
```
Or we could simply distinguish between those that do and don't have affairs. We are going to do this on our initial (Fair) data here as we will be examining this further:
```{r}
Fair <- Fair %>% 
    mutate(affairYN =
               ifelse (nbaffairs > 0,1,0))
Fair %>% count(affairYN)
```
With this change, the regression problem becomes more clearly a classification problem. How can we best predict which 'type' (affair-ers vs. not) a given person falls in to?

## Avoiding capitalizing on chance

One lesson from the last class was that correlations (and regression coefficients) drawn from small samples were not stable. In regression analysis, as progressively smaller samples were drawn from the Fair data, the ability to predict the outcome increased.  In the limiting case, when the number of predictors (variables) was equal to the number of cases in the sample (rows), prediction becomes perfect.
```{r}
# insert code here to show this
```
The problem of capitalizing on chance is a significant one in prediction, and one should always be skeptical of models which are untested beyond the sample from which they were derived.

### Splitting the data into training and test subsamples

The most basic solution to this problem is to split the data into two groups, a *training* sample from which we extract our model, and a *test* sample on which you will assess it. (Often, the logic of this will be extended to include a third group, a *validation* sample which would be used to tune or select the results of the training run before the test data are examined).  Here, we will consider the simpler approach, splitting the Fair data into training and test samples.  The critical feature of this analysis is that we will hold out the test data, and not even look at it until after our model building is complete.
```{r}
# establish a seed for your data-split
# so that your results will be reproducible
set.seed(33458)
# drop the nbaffairs variable as we
# are no longer using this
Fair <- Fair %>% select (-nbaffairs)
# create a set of line numbers 
# of size corresponding to the 
# desired training sample 
n <- nrow(Fair)
trainIndex <- sample(1:n, 
size = round(0.6*n), replace=FALSE)
# create training and test samples
trainFair <- Fair[trainIndex ,]
testFair <- Fair[-trainIndex ,]
```

## applying logistic regression analysis to the training data

Last class, we used the lm command in our regression analysis which predicted the (continuous) outcome nbaffairs.  Here, the outcome is now dichotomous and therefore binomially distributed variable, and so the desired regression is a logistic one. We run this analysis using only the training data.

```{r}
model2 <- glm(affairYN ~ ., data = trainFair,
              family = "binomial")
summary (model2)
```

## From regression to classification

Our "predicted scores" are continuous, corresponding to the probability that a given person will have an affair. Here's how they are distributed (still on the training data here):

```{r}
predictTrain <- predict(model2, trainFair, type = "response")
summary (predictTrain)
```

If we want to classify people, we will need to create a decision threshold at which we will change our prediction from no to yes.

One approach is to create a threshold equal to the actual proportion of people who don't have affairs in our sample. 
```{r}
(threshold <- mean(trainFair$affairYN))
```


This is equal to both the mean of our predicted scores (above) and the mean of our actual scores, and, because this is a dichotomous variable, the proportion of people in the sample who have affairs. We'll predict that if a person has a predicted score more than this we'll predict that s/he will be unfaithful, else we will "PredictOK." Then we will create a *confusion matrix,* to compare our correct predictions (PredictOK and affairYN = 1, Predictunfaithful and affairYN = 0) with the remainder.

```{r 0404model1Threshold}
classification <- ifelse(predictTrain > threshold,
                         "Predictunfaithful", "PredictOK")
(b <- table(classification, trainFair$affairYN))
(correct <- b[1,1] + b[2,2])
(errors <- b[1,2] + b[2,1])
(accuracy <- correct/(correct + errors))
```

## Applying the model to new data: Assessing out-of-sample risk

We shouldn't trust these results, though, because the model is based on the same data that it is tested upon. Now we will apply the model to the new, independent test data. 

Typically (but not invariably), the percent of accurate classifications will decline, especially if the model is a complex one with many variables or if the number of observations is low.

```{r 0404confusionTestData}
predictTest <- predict(model2, testFair, type = "response")
classification <- ifelse(predictTest > threshold,
                         "Predictunfaithful", "PredictOK")
(b <- table(classification, testFair$affairYN))
(correct <- b[1,1] + b[2,2])
(errors <- b[1,2] + b[2,1])
(accuracy <- correct/(correct + errors))
```
We are correct `r 100*(round(accuracy,2))` percent of the time.

## Changing our decision threshold

In many decision problems, there is an asymmetry in the cost of different types of errors: if you are foraging for mushrooms, for example, an error of the form (you decide its safe and it is poisonous) is more costly than the converse (you decide its poisonous and it is safe).

This may be true in the present example as well. Consider someone who is looking for a spouse, but is really averse to the idea of getting hurt by an affair. That person might feel like the cost of marrying an unfaithful person is much greater than the cost of not marrying a faithful one. So we adjust the threshold downwards:

```{r}
threshold <- .05
classification <- ifelse(predictTest > threshold,
                         "Predictunfaithful", "PredictOK")
(b <- table(classification, testFair$affairYN))
(correct <- b[1,1] + b[2,2])
(errors <-  b[1,2] + b[2,1])
(accuracy <- correct/(correct + errors))
```

The "overall accuracy" - that is, number of correct classifications - drops. But that's not what we are really interested in, rather, we are interested in minimizing hurt.

Here's another example: Someone who is very lonely might feel the opposite, and be willing to accept greater substantially greater risk.

```{r}
threshold <- .5
classification <- ifelse(predictTest > threshold,
                         "Predictunfaithful", "PredictOK")
(b <- table(classification, testFair$affairYN))
(correct <- b[1,1] + b[2,2])
(errors <- b[1,2] + b[2,1])
(accuracy <- correct/(correct + errors))
```

Prediction is higher here - but not much higher than it would be if we raised the threshold even further, and just assumed that everyone can be trusted.  Then, our error rate would be `r 100*(round(mean(trainFair$affairYN),2))` percent.  When overall predictability is low, it's often the case that you can maximize correct predictions by simply predicting that everyone will be in the most popular category.  Predicting rare events, such as suicides, is particularly difficult.

## More confusion

There is a nice shortcut to generating confusion matrices such as those above using the caret package. 

This function describes outcomes in several ways, as there are many languages for describing outcomes in 2 x 2 tables, including Type I vs. Type II errors, Hits vs. False Alarms/False Positives, and Sensitivity vs. Specificity. 

In these data, it's been set up so that 
 - hit rate ~ sensitivity ~ ("no affair" | no affair)
 - correct rejection ~ specificity ~ ("affair" | affair)

```{r}
threshold <- .5
# syntax for classification in caret is a little different
# (the labels for the actual and predicted scores have to be the same)
#classification <- ifelse(predictTest > threshold,
#                         "Predictunfaithful", "PredictOK")
classification <- ifelse(predictTest > threshold, 1,0)
# caret package (newest) requires explicit matching of factors
classification <- as.factor(classification)
testFair$affairYN <- as.factor(testFair$affairYN)
confusionMatrix(classification, testFair$affairYN)
```
## ROCs and AUC

Each of these decision thresholds describes the performance of a model at a particular point.  We can combine the thresholds and plot them in Receiver Operating Characteristic (ROC) curves.  The area under the curve (AUC) is a great measure of model accuracy, in that it summarizes how effective a classifier is across all possible thresholds.

```{r 0414model1ROC, fig.width=5, fig.height=5}
# fig.width and fig.height specified to get square plots
# colAUC function gets stats etc
AUCModel2<-colAUC(predictTest, testFair[["affairYN"]], plotROC = TRUE)
AUCModel2
```

