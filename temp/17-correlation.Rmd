---
title: "17-correlation"
author: "Lanning"
date: "`r Sys.Date()`"
output: html_document
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warn = FALSE, message = FALSE) # defaults for all chunks after this one
library(tidyverse)
library(dslabs)
ds_theme_set()
set.seed(0)
```

# correlation and linear regression



In Chapter 22 of R4DS, Wickham introduces **modeling** as a complement to data visualization.  At the beginning of Chapter 23, he points out that: 

> *The goal of a model is to provide a simple low-dimensional summary of a dataset.*

In the remainder of his section on modeling (Chapters 23 - 25), he presents an innovative approach to introducing models. Feel free to explore these.  Our treatment, however, will deviate from this.

## correlation 

(This section is excerpted directly from From https://github.com/datasciencelabs)

[Francis Galton](https://en.wikipedia.org/wiki/Francis_Galton), a polymath and cousin of Charles Darwin was interested, among other things, in how well can we predict a son's height based on the parents' heights. 

We have access to Galton's family height data through the `HistData` package. We will create a dataset with the heights of fathers and the first son of each family. Here are the key summary statistics for the two variables of father and son height taken alone:

```{r}
library(HistData)
data("GaltonFamilies")
galton_heights <- GaltonFamilies %>%
  filter(childNum == 1 & gender == "male") %>%
  select(father, childHeight) %>%
  rename(son = childHeight)
galton_heights %>% 
  summarise(mean(father), sd(father), mean(son), sd(son))
```

This **univariate** description fails to capture the key characteristic of the data, namely, the idea that there is a relationship between the two variables: 

```{r scatterplot, fig.cap="Heights of father and son pairs plotted against each other."}
galton_heights %>% ggplot(aes(father, son)) + 
  geom_point(alpha = 0.5)
```

The correlation summarizes this relationship. In these data, the correlation (r) is about .50.

```{r}
galton_heights %>% summarize(cor(father, son))
```

Incidentally, correlations based on small samples can bounce around quite a bit.  Consider what happens when, for example, we sample just 25 cases from Galton's data, then repeat this in a Monte Carlo analysis 1000 times to see the distribution of these sample rs:

```{r}
B <- 1000
N <- 25
R <- replicate(B, {
  sample_n(galton_heights, N, replace = TRUE) %>% # sample with replacement here
    summarize(r=cor(father, son)) %>% .$r
})
data.frame(R) %>% ggplot(aes(R)) + geom_histogram(binwidth = 0.05, color = "grey")
```

### The regression line

If we are predicting a random variable $Y$ knowing the value of another $X=x$ using a regression line, then we predict that for every standard deviation, $\sigma_X$, that $x$ increases above the average $mu_X$, $Y$ increases $\rho$ standard deviations $\sigma_Y$ above the average $\mu_Y$, where $\rho$ is the correlation between $X$ and $Y$. 

To manually add a regression line to a plots, we will need the means and standard deviations of each variable together with their correlation:

```{r}
mu_x <- mean(galton_heights$father)
mu_y <- mean(galton_heights$son)
s_x <- sd(galton_heights$father)
s_y <- sd(galton_heights$son)
r <- cor(galton_heights$father, galton_heights$son)

m <-  r * s_y / s_x
b <- mu_y - m*mu_x

galton_heights %>% 
  ggplot(aes(father, son)) + 
  geom_point(alpha = 0.5) +
  geom_abline(intercept = b, slope = m ) 
```

If we first standardize the variables, then the regression line has intercept 0 and slope equal to the correlation $\rho$. Here, the slope, regression line, and correlation are all equal (I've made the plot square to better indicate this).

```{r  fig.width = 4, fig.height = 4}
galton_heights %>% 
  ggplot(aes(scale(father), scale(son))) + 
  geom_point(alpha = 0.5) +
  geom_abline(intercept = 0, slope = r) 
```

### Warning: there are two regression lines

In general, when describing bivariate relationships, we talk of predicting y from x. But what if we want to reverse this? Here, for example, what if we want to predict the father's height based on the son's? 

For this, the intercept and slope of the lines will differ

```{r}
# son from father
m_1 <-  r * s_y / s_x
b_1 <- mu_y - m_1*mu_x
# vice-versa
m_2 <-  r * s_x / s_y
b_2 <- mu_x - m_2*mu_y
```

We will still see regression to the mean (the prediction for the father is closer to the father average than the son heights $y$ is to the son average).

Here is a plot showing the two regression lines, with blue for the predicting son heights with father heights and red for predicting father heights with son heights.

```{r fig.width = 4, fig.height = 4}
galton_heights %>% 
  ggplot(aes(father, son)) + 
  geom_point(alpha = 0.5) + 
  geom_abline(intercept = b_1, slope = m_1, col = "blue") +
  geom_abline(intercept = -b_2/m_2, slope = 1/m_2, col = "red") 
```

## multiple regression

We extend regression (predicting one continuous variable from another) to the multivariate case (predicting one variable from many). We extend this logic to the cases where our variables are not normally distributed, as in the case of dichotomous variables (yes/no, true/false), as well as counts, which often are skewed.

(This section is drawn from Peng, Caffo, and Leek's treatment from Coursera/ the Johns Hopkins Data Science Program. You may need to install the packages "UsingR", "GGally" and/or"Hmisc")

```{r}
library(broom) # an older package for unlisting regression results
library(modelr)
library(GGally) # for scatterplot matrix
library(Ecdat) # Econometric data incl. affairs dataset 
```

## Swiss fertility data

To consider the multivariate case, we examine a second dataset. 

```{r swiss data}
data(swiss)
str(swiss)
```

Here's a *scatterplot matrix* of the Swiss data. Look at the first column of plots (or first row of the correlations). What is the relationship between fertility and each of the other variables?

```{r scatterplot matrix with ggpairs, swiss}
ggpairs (swiss,
        lower = list(
            continuous = "smooth"),
        axisLabels ="none",
        switch = 'both')
```

Here, we predict fertility from all of the remaining variables together in a single regression analysis, using the lm (linear model) command. As we saw in Chapter 15, the result of this analysis is a list.  We can pull out the key features of the data using the summary() command. How do you interpret this?

```{r multiple regression - swiss data}
swissReg <- lm(Fertility ~ ., data=swiss)
summary(swissReg)
```

## marital affairs data

Here's another dataset. We'll apply ggpairs here, but for clarity will show only half of the data at a time.  The dependent variable of interest (nbaffairs) will be included in both plots:

```{r out.width = "100%", warning= FALSE, message = FALSE}
Fair1 <- Fair %>% 
  select(sex:child, nbaffairs)
ggpairs(Fair1,  
# if you wanted to jam all 9 vars onto one page you could do this
#        upper = list(continuous = wrap(ggally_cor, size = 10)),
         lower = list(continuous = 'smooth'),
         axisLabels = "none",
        switch = 'both')
Fair2 <- Fair %>% 
  select(religious:nbaffairs) 
ggpairs(Fair2, lower = list(continuous = 'smooth'),
           axisLabels = "none",
           switch = 'both')
```

> Exercise
>
> 1) Study the data. Learn about the measures by looking at Fair in the help tab of R studio.
> 2) What do the scatterplot matrices shown above tell us?
> 3) Can you just run a regression on it as is? Try predicting nbaffairs from the remaining 8 variables in Fair (not Fair1 or Fair2), using the same syntax in the Swiss data. Does it work? What do the results suggest? 

```{r multiple regression - affairs}
affairReg <- lm(nbaffairs ~ ., data=Fair)
summary(affairReg)
```

```


```