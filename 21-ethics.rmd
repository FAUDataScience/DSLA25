---
title: "21-ethics"
author: "Lanning"
date: "April 9, 2019"
output: html_document
---

```{r}
knitr::opts_chunk$set(echo = TRUE, warn = FALSE, message = FALSE) 
```

# ethics: some topics for discussion

## status 40% {-}

There is much more that can be done, including links to articles in NYT on privacy as case studies. In addition, each of the examples at the end can be expanded upon.  Check citations. 

Primary sources @loukides2018ethics, @salganik2017bit



## Background

There are ethics guidelines in professional organizations of [computer scientists](https://www.acm.org/code-of-ethics) and [statisticians](https://www.amstat.org//asa/files/pdfs/EthicalGuidelines.pdf).  

But this is not enough - "It's not enough to say "don't be evil." You have to not be evil" (e.g., facebook's year in review).

Oaths (like the Hippocratic Oath) are less effective than checklists (Gawande, 2009).



## Principles

Consent

Clarity: It's not enough that people agree to share their data, they must also understand what they are agreeing to.

Consistency (and trust): Organizations may have data stolen (SSNs, passwords), or may be sold to other organizations where the prior rules may be unenforceable.

Control (and transparency): I have the right to control my data / the General Data Protection Regulation in the EU.

**Consequences** (and harm): Includes protection of [children](https://en.wikipedia.org/wiki/Children's_Online_Privacy_Protection_Act) and rules against discrimination on the basis of genetic information. But these laws may not keep up with changes in technology. There are missteps, as when [Strava](https://www.wired.co.uk/article/strava-heat-maps-military-app-uk-warning-security) and Netflix shared their data which could be de-anonymized, but also examples of sharing of data in the public interest.

> Many data sets that could provide tremendous benefits remain locked up on servers. Medical data that is fragmented across multiple institutions limits the pace of research. And the data held on traffic from ride-sharing and GPS/mapping companies could transform approaches for traffic safety and congestion. But opening up that data to researchers requires careful planning.

Improvements in UX (user experience) provide users with short-term incentives and long-term costs...

## Remedies

Here's a checklist proposed by @loukides2018ethics:

### a checklist for people who are working on data projects

❏ Have we listed how this technology can be attacked or abused? 

❏ Have we tested our training data to ensure it is fair and representative? 

❏ Have we studied and understood possible sources of bias in our data? 

❏ Does our team reflect diversity of opinions, backgrounds, and kinds of thought? 

❏ What kind of user consent do we need to collect to use the data? 

❏ Do we have a mechanism for gathering consent from users? 

❏ Have we explained clearly what users are consenting to? 

❏ Do we have a mechanism for redress if people are harmed by the results? 

❏ Can we shut down this software in production if it is behaving badly? 

❏ Have we tested for fairness with respect to different user groups? 

❏ Have we tested for disparate error rates among different user groups? 

❏ Do we test and monitor for model drift to ensure our software remains fair over time? 

❏ Do we have a plan to protect and secure user data?

### additional ideas

Integrate discussions of software security and ethics: Security vulnerabilities may be privacy threats. 

Go beyond simple oaths to more: [Google](https://ai.google/principles) has moved from "don't be evil" to a set of negative (dont's) and positive (do's) guidelines for work in Artificial Intelligence.  Is it enough?

Kairos, which develops face recognition software, has said that [they will not share their software with law enforcement agencies.](https://techcrunch.com/2018/06/25/facial-recognition-software-is-not-ready-for-use-by-law-enforcement/)

## some case studies

Here are a few examples from recent headlines:

1) In today's news, [health information of pregnant women is shared with their employers.](https://www.washingtonpost.com/technology/2019/04/10/tracking-your-pregnancy-an-app-may-be-more-public-than-you-think/)

2) Several years ago, [Facebook Year in Review](https://www.theguardian.com/technology/2014/dec/29/facebook-apologises-over-cruel-year-in-review-clips) ran in to problems.

3) Did Eventbrite step over the line here: [we have the right to record your event](https://arstechnica.com/information-technology/2018/04/eventbrite-rolls-back-policy-that-would-have-given-it-right-to-record-events/)?

Three more from [@salganik2017bit](https://www.bitbybitbook.com/en/1st-ed/ethics/three-examples/): 

4) The [emotional contagion study](https://doi.org/10.1073/pnas.1320040111) of Adam Kramer and others at Facebook.

5) A study which linked [Facebook profiles of Harvard students](https://doi.org/10.1007/s10676-010-9227-5) with their academic records without their consent.

6) A study which attempted to [monitor government censorship](http://techscience.org/a/2015121501/) by tracking web traffic 

For still further reflection, consider one or more of the [six hypothetical case studies](https://aiethics.princeton.edu/case-studies/case-study-pdfs/)  provided by Princeton's Center for Information Technology and Policy.



