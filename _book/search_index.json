[["index.html", "Data science for the liberal arts ", " Data science for the liberal arts Kevin Lanning 2021-01-05 "],["preface.html", "preface some features of the text the book is for you", " preface This work-in-progress will ultimately serve as a textbook for introductory undergraduate courses in data sciences. No prior knowledge of computer programming is presumed, though, ideally, students will have had college algebra (or its equivalent) and an introductory course in statistics, methods, or data analysis. Data science is still a new field of study, and there are multiple approaches to teaching it and to its place in the college curriculum. This book is intended to serve courses such as the Introduction to Data Science at the Wilkes Honors College of Florida Atlantic University which, in turn, has drawn from data science classes at the universities of North Carolina, British Columbia, Duke, Maryland, Wisconsin, Stanford, BYU, Harvard, Pennsylvania, and UC Berkeley At each of these schools, the Introduction to Data Science appears, to my eyes at least, closer to Statistics than to Computer Science. But if our approach is closer to statistics than to programming, it is particularly close to statistics in its most applied and pragmatic form. The choice of statistical methods should follow from the data and problem at hand - that is, statistics should serve the needs of the user rather than dictate them (Loevinger 1957). This pragmatic focus is driving the growth of data science in industry, and it is reflected in the way data science is taught at still other schools including Chicago, Georgia Tech, UC Santa Barbara, Princeton, UC Berkeley, at Berlins Hertie School of Governance, and in Columbias School of Journalism. some features of the text There are a number of different approaches to teaching data science. The present text includes several distinguishing features. R In a recent informal survey of introductory data science courses, I saw a pretty even split between those which begin with Python and those which begin with the statistical programming language R. This difference corresponds, very loosely, to the split noted above: Computer science based approaches to data science are frequently grounded in Python, while statistics-based approaches are generally grounded in R. Our course, like those for most of the syllabi and courses linked above, will be based in R. Reproducible science The course will provide an introduction to some of the methods and tools of reproducible science. We will consider the replication crisis in the natural and social sciences, and then consider three distinct approaches which serve as partial solutions to the crisis. The first of these is training in a notebook-based approach to writing analyses, reports and projects (using R markdown documents). The second is using public repositories (such as the Open Science Framework and GitHub) to provide snapshots of projects over time. Finally, the third is to consider the place of significance testing in the age of Big Data, and to provide training in the use of descriptive, exploratory techniques of data analysis. Good visualizations Part of Type C data science is communication, and this includes not just writing up results, but also designing data displays that incisively convey the key ideas or features in a flood of data. Well examine and develop data visualizations such as plots, networks and text clouds. More advanced topics may include maps, interactive displays, and animations. All Some of the data Its been claimed that in the last fifteen years, humans have produced more than 60 times as much information as existed in the entire previous history of humankind. (It sounds like hyperbole, but even if its off by an order of magnitude its still amazing). There are plenty of data sources for us to examine, and well consider existing datasets from disciplines ranging from literature to economics to public health, with sizes ranging from a few dozen to millions of data points. We will also clean and create new datasets. All A few of the latest tools One feature of Data Science is that it is changing rapidly. The tools, methods, data sources, and ethical concerns that face us in 2021 are different from those which shaped the field just one or two years ago. In fields that are undergoing rapid change, there is some trade-off between building expertise with existing (older) tools and trying the newer approaches. Partly because I want to equip you with skills which will not be obsolete, partly because some of these new approaches promise more accessibility, elegance, and/or power, and partly because of my own interest in staying current, well be using some of the latest packages and programs. In the last few years, Ive shifted the class from the standard R dialect (as I learned it from the Johns Hopkins-Coursera Data Science Specialization) to the Tidyverse. This year, for the first time, most of our work with the R language will take place online, using RStudio cloud, for this should help overcome some hiccups that arise when students are using different machines, operating systems, etc., and should facilitate collaboration among us as well. Well also be experimenting for the first time with the learnr package, and emphasizing other approaches to learning R, including Swirl less. In the past, Ive used the Slack platform for messaging, communication and collaboration; this year, Ive somewhat reluctantly moved to the Canvas LMS. And, in the past, Ive recommended using dedicated markdown editors such as Typora. While I still think that these are great for some text-editing and note-taking, well do our work instead with the editor in the latest variant of RStudio on our laptops, as this allows WYSIWIG (what you see is what you get) formatting of documents - such as this one - that are intended as publication-ready texts. Well continue to use spreadsheets such as Excel or Google Sheets as well. the book is for you Its my intention that this text should serve every college student, regardless of concentration or college major. The skills and insights that you will gain in this course will help you in graduate and professional schools, will help you in your careers, and will help you in your goal of making a better world. And it will help you train the next generation of data scientists as well. References "],["data-science-for-the-liberal-arts.html", "1 data science for the liberal arts 1.1 type C data science = data science for the liberal arts 1.2 the incompleteness of the data science Venn diagram 1.3 a dimension of depth 1.4 Google and the liberal arts 1.5 data sci and TMI 1.6 discussion: what will you do with data science?", " 1 data science for the liberal arts Hochster, in (Hicks and Irizarry 2018), describes two broad types of data scientists: Type A (Analysis) data scientists, whose skills are like those of an applied statistician, and Type B (Building) data scientists, whose skills lie in problem solving or coding, using the skills of the computer scientist. This view arguably omits a critical component of the field, as data science is driven not just by statistics and computer science, but also by domain expertise: Fig 1.1 - The iconic data science Venn diagram 1.1 type C data science = data science for the liberal arts The iconic Venn diagram model of data science shown above suggests what we will call Type C data science. It begins with domain expertise in your concentration in the arts, humanities, social and/or natural sciences, it both informs and can be informed by new methods and tools of data analysis, and it includes such things as communication (including writing and the design and display of quantitative data), collaboration (making use of the tools of team science), and citizenship (serving the public good, overcoming the digital divide, furthering social justice, increasing public health, diminishing human suffering, and making the world a more beautiful place). Its shaped, too, by an awareness of the fact that the world and workforce are undergoing massive change: This puts the classic liberal arts focus of learning how to learn (as opposed to memorization) at center stage. And Type C data science is shaped, not least, by the creepiness of living increasingly in a measured, observed world. Type C data science does not merely integrate domain expertise with statistics and computing, it places content squarely at the center. We can appreciate the compelling logic and power of statistics as well as the elegance of well-written code, but for our purposes these are means to an end. Programming and statistics are tools in the service of social and scientific problems and cultural concerns. Type C data science aims for work which is not merely cool, efficient, or elegant but responsible and meaningful. 1.2 the incompleteness of the data science Venn diagram Data visualizations are starting points which can provide insights, typically highlighting big truths or effects by obscuring other, presumably smaller ones. The Venn diagram model of data science is no exception: As with other graphs, figures, and maps, it allows us to see by showing only part of the picture. What does it omit? That is, beyond statistics, computing/hacking, and domain expertise, what other skills contribute to the success of the data scientist? The complexity of data science is such that individuals typically have expertise in some but not all facets of the area. Consequently, problem solving requires collaboration. Collaboration, even more than statistical and technical sophistication, is arguably the most distinctive feature of contemporary scholarship in the natural and social sciences as well as in the private sector (Isaacson 2014). Communication is central to data science because results are inconsequential unless they are recognized, understood, and built upon; facets of communication include oral presentations, written texts and, too, clear data visualizations. Reproducibility is related to both communication and collaboration. There has been something of a crisis in recent years in the social and natural sciences as many results initially characterized as statistically significant have been found not to replicate. The reasons for this are multiple and presently contentious, but one path towards better science includes the public sharing of methods and data, ideally before experiments are undertaken. Reproducible methods are a key feature of contemporary data science. Pragmatism refers to the relevance of work towards real-world goals. These real-world goals should be informed by ethical concerns including a respect for the privacy and autonomy of our fellow humans. 1.3 a dimension of depth Cutting across these various facets (statistics, computing, domain expertise, collaboration, communication, reproducibility, pragmatism, and ethics), a second dimension can be articulated. No one of us can excel in all of these domains, rather, we might aim towards goals ranging from literacy (can understand) through proficiency (can get by) to fluency (can practice) to leadership (can create new solutions or methods). That is, we can think of a continuum of knowledge, skills, interests, and goals, ranging from that which characterizes the data consumer to the data citizen to the data science contributor. A Type C data science includes this dimension of depth as well. 1.4 Google and the liberal arts Data science is at its core empirical, and all of this rhetoric would be meaningless if not grounded in real world findings. Although it was reported in late 2017 that soft skills rather than STEM training were the most important predictors of success among Google employees, its difficult to know whether these results would generalize to a less select group. Nonetheless, there is a clear need for individuals with well-rounded training in the liberal arts in data science positions and, conversely, learning data science is arguably a key part of a contemporary liberal arts education. 1.5 data sci and TMI One difference between traditional statistics and data science is that the former is typically concerned with making inferences from datasets that are too small, while the latter is concerned with extracting a signal from data that is or are too big (Donoho 2017). The struggle to extract meaning from a sea of information - of finding needles in haystacks, of finding faint signals in a cacophony of overstimulation - is arguably the question of the age. It is a question we deal with as individuals on a moment-by-moment basis. It is a challenge I face as I wade through the many things that I could include in this class and these notes. The primacy of editing or selection lies at the essence of human perception and the creation of art forms ranging from novels to film. And it is a key challenge that the data scientist faces as well. 1.6 discussion: what will you do with data science? Imagine it is ten years from today. You are working in a cool job (yay). How, ideally, would data science inform your professional contributions? More proximally (closer to today) - what are your own goals for progress in data science, in terms of the model described above? References "],["getting-started.html", "2 getting started 2.1 are you already a programmer and statistician? 2.2 setting up your machine: some basic tools 2.3 a modified 15-minute rule 2.4 discussion: who deserves a good grade?", " 2 getting started We begin with a brief self-assessment, asking you to reflect on your own knowledge of data science, including the necessary-but-not-sufficient areas of computer programming and statistics. We then move to a description of some rudimentary tools that we will be using. 2.1 are you already a programmer and statistician? Regarding programming, you may know more than you think you do. Heres a simple program - a set of instructions - for producing a cup of coffee: add water to the kettle and turn it on if its morning, put regular coffee in the French press, otherwise use decaf if the water has boiled, add it to the French press, else keep waiting if the coffee has steeped for four minutes, depress (smash) piston/plunger, else keep waiting pour coffee into cup enjoy As a post-millennial student from a WEIRD culture, or Western, Educated, Industrialized, Rich Democracy (Henrich, Heine, and Norenzayan 2010), youve programmed computers, too, if only to enter a password, open an app, and upload a photo on your cell phone. Statistics is of fundamental importance, not just for understanding abstract trends, but for making decisions about everyday life. Consider the case of Susie, a college senior: Exercise 2_1 Susie is applying to two med schools. At School A, 25% of students are accepted, and at School B, 25% are accepted as well. You are Susie. Are you going to get in to at least one of these programs? What is the probability? Does your estimate depend upon any assumptions? Questions such as these are important for us. If the combined probability is low, it likely (another probability concept) will make sense for Susie to spend time, money, and energy to apply to additional programs. If the probability is higher, it may not. But problems like this are hard - our estimates of probability are frequently poorly calibrated, and combining probability estimates is challenging (see, e.g., (Tversky and Kahneman 1974), and consider taking a course in Behavioral Economics or Thinking and Decision Making to learn more). You may have worked with data in spreadsheets such as Excel or Google Sheets. Exercise 2_2 Open the Google Sheet at http://bit.ly/dslaX2_1. Save a copy and edit it, entering the following in cell B7: =SUM (B2:B6) What is the result? Now copy cell B7 to C7 What happens? Is this the result you expected? Would another approach be more useful?** Spreadsheets are great tools - the first one, Visi-Calc, was the first killer app to usher in the personal computer revolution. But they have limitations as well. (Broman and Woo 2018) propose some best practices for using spreadsheets in data science such as, for example, including only data (and not calculations) in spreadsheets, using what we will recognize as a tidy format in which data are in a simple rectangle (avoiding the combination of cells and the use of multi-line headers), and saving spreadsheets as simple text files (often as csvs). When we sort data in spreadsheets, we risk chaos, for example, only certain columns may be sorted. When we manipulate data in spreadsheets, we typically will not have a record of what was (and wasnt) changed, this compromises the reproducibility of our work. The bottom line is that spreadsheets should generally be used to store data rather than to analyze it. 2.2 setting up your machine: some basic tools Collaboration and communication are integral to data science. In the world beyond universities, the most important messaging and collaboration platform is Slack. Slack is a commercial app, but we will use the free tier. Well use Slack for group work, class announcements, and help-seeking and help-providing. Slack includes a simple markdown editor (for posts). You can find an introduction to markdown syntax in Chapter 3 of (Freeman and Ross 2017). I use Typora (currently free for both Windows and Mac), but there are many alternatives. Install this or another Markdown editor on your laptop and play with it. Install R then R studio on your own Windows or Mac laptop. Well use R studio as a front end (an integrated development environment, or IDE) for R, and will write most of our code in R markdown which is, not surprisingly, a flavor of markdown. Well go into R in increasing depth beginning in the next chapter; if you want to get a head start, consider Carmichael (2017) Getting started and the first chapter of (Wickham and Grolemund 2017). (Those documents, like this one, are all written in R markdown). Eager to start coding in R? Go to Chapter 4 (draw the rest of the owl), and begin the exercises in swirl (swirlstats). Google Docs is free and is convenient for collaborative work. One other important feature of Google Docs is that it provides a framework for version control, a critical skill in information management. You can learn more about how to see and revert to prior versions of a project in Google Docs here. Version control can help you avoid the chaos and confusion of having a computer (or several computers) full of files that look like Chams (2012) comic: Fig 2.1: Never call anything final.doc. Version control is an important concept in data science. Collaboratively built programs and platforms, including most of the add-ons (libraries, packages) which make R so powerful, are open-source projects built by many individuals over time. For projects such as these, both the current build and its associated history are typically maintained on GitHub, a website for hosting code. When we contribute to these projects, we will first mirror the web-based GitHub site using a program on our own Macs or Windows PCs called Git, then upload our proposed changes. Keeping remote and local branches of files in sync can be challenging, however, and you will not be expected to use this technology in this class. But if you are curious, or want to learn more, an introduction to using Git and R together may be found here. 2.3 a modified 15-minute rule You will run into problems, if not here, then elsewhere. An important determinant of your success will be the balance you maintain between persistence and help-seeking. The 15-minute rule is one guideline for this balance: It has been cleverly summarized as You must try, and then you must ask. That is, if you get stuck, keep trying for 15 minutes, then reach out to others. I think that this rule is basically sound, particularly if it is applied with cognitive flexibility, social sensitivity, and reciprocity. So when you get stuck, make a note of the problem, then move to another part of your project (thats the cognitive flexibility part): This allows your problem to percolate and still make progress. When you ask others for help, ask in a way that shows an awareness of the demands on their time (social sensitivity): Part of this means that you should explain your problem in as detailed a fashion as possible - in technical terms, a reprex or reproducible example. Finally, you should be willing to provide as well as give help (reciprocity). 2.4 discussion: who deserves a good grade? In an introductory class in data science, students invariably come to class with different backgrounds. Should this be taken into account in assigning grades? That is, would it be possible (and desirable) to assign grades in a class based not just on what students know at the end of the term, but also on how much they have learned? A formal, statistical approach to this could use regression analysis. That is, one could predict final exam scores from pretest scores, and use the residuals - the extent to which students did better or worse than expected - as a contributor to final exam grades. Interestingly, there would be an unusual incentive for students on this pretest to do, seemingly perversely, as poorly as possible. How could this be addressed? Another problem with this approach is that there may be ceiling effects - students who are the strongest coming in to the class cant improve as much as those who have more room to grow. Again, how might this be addressed? Should it? References "],["r-stands-for.html", "3 R stands for  3.1 a few characteristics of R 3.2 finding help 3.3 Wickham and R for Data Science", " 3 R stands for  Historically, R grew out of S which could stand for Statistics. But what does R stand for? R is a system for Reproducible analysis, and reproducibility is essential. R markdown documents, like Jupyter notebooks in the Python world, facilitate reproducible work, as they include comments or explanations, code, links to data, and results. R is for Research. Research is not just an end-product, not just a published paper or book:  these documents are not the research [rather] these documents are the advertising. The research is the full software environment, code, and data that produced the results (Buckheit and Donoho 1995; Donoho 2010, 385). When we separate the research from its advertisement we are making it difficult for others to verify the findings by reproducing them (Gandrud 2013). R is a system for Representing data in cool, insight-facilitating ways, a tool for creating (reproducible) data visualizations which can provide insights and communicate results. R is Really popular, and this matters, because learning R will make you a more attractive candidate for many graduate programs as well as jobs in the private sector. Because R is popular, there are many Resources, including, for example - Online resources include the simple (and less simple) lessons of SwirlR, which offers the possibility of learning R in R, as well as DataCamp, the Data Science Certificate Program at Johns Hopkins, and other MOOCs. Books include (Peng 2014) - which includes not only videos of his lectures in the program at Hopkins, but also a brief list of still more resources - and (Wickham and Grolemund 2017). Youll also learn (more directly) from people, including your classmates, as well as the broader community of people around the world. There are hundreds if not thousands of people, young and old, who are on the road with you. I am as well, just a step or two (hopefully) ahead. R might stand for Relatively high level. Programming languages can be described along a continuum from high to low level, the former (like R) are more accessible to humans, the latter (like assembly language) more accessible to machines. Python, Java, and C++ are all more towards the middle of this continuum. R does not stand for [arggh](https://www.urbandictionary.com/define.php?term=ARGH), although you may proclaim this in frustration (arggh, why cant I get this to work?) or, perhaps, in satisfaction (arggh, matey, that be a clever way of doing this). But R does stand for Rewarding. A language is a way of thinking about the world, and this is true for computer languages as well. Youll be challenged by its complexity, its idiosyncracy, its alien logic. But you will succeed, and you will find that you can do things that you did not believe possible. 3.1 a few characteristics of R R includes the base together with packages. These packages (libraries) are customized add-ons which simplify certain tasks, such as text analysis. There are, at this writing, 15,373 available packages on the CRAN package repository - and though there is not yet an R package for ordering pizza (Peng 2014), there are many for most data tasks, including, for example, over 50 different packages for text analysis. So how do you choose, and where do you begin? We will start with the curated list of packages which jointly comprise the tidyverse (Wickham and Grolemund 2017), which is effectively a dialect of R. R is an object-oriented language - one conceptually organized around objects and data rather than actions and logic. In R, at the atomic level, objects include characters, real numbers, integers, complex numbers, and logical. These atoms are combined into vectors, which generally include objects of the same type (one kind of object, lists, is an exception to this; Peng 2014). Vectors can be further combined into data frames, which are two-dimensional tables or arrays. A tibble is a particular type of data frame which is used in the tidyverse. It is, in some ways, handier to work with than other data frames. Well be working extensively with data frames in general, and tibbles in particular, as we move forward. Objects have attributes. Attributes of R include such things as name, dimensions (for vectors and arrays), class (thats the type of object described in the previous paragraph), length, etc. Real world data sets are messy, and frequently have missing values. In R, missing values may be characterized by NA (not available) or NaN (not a number, implying an undefined or impossible value). RStudio, the environment we will use to write, test, and run R code, is a commercial enterprise whose business model, judged from afar, is an important one in the world of technology. Most of what RStudio offers is free (97% according to Garrett Grolemund in the video below). The commercial product they offer makes sense for a relative few, but it is sufficiently lucrative to fund the enterprise. The free product helps to drive the popularity of Rstudio; this widespread use, in turn, makes it increasingly essential for businesses to use. This mixed free/premium, or freemium, model characterizes Slack as well, but while the ratio of free to paid users of Slack is on the order of 3:1, for R it is, I am guessing, an order of magnitude higher than this. 3.2 finding help One does not simply learn R. Unlike, say, learning to ride a bicycle, fry an egg, or drive a car with a manual transmission, learning R is not a discrete accomplishment that one can be said to have mastered and from which one then moves on. Rather, R is an evolving, open system of applications and tools which is so vast that there is always more that one can achieve, new lessons that one can learn. And, the complexity of R syntax is such that, for almost all of us, we will need help for coding on any non-trivial task. For us, the key ideas in looking for help will include not just the tools on the RStudio IDE, but also (a) using Google searches wisely, and (b) reaching out to your classmates on Slack. Here, as in the real world, there is an etiquette for help-seeking which is based on consideration. Your search for help should begin by making sure that others will encounter the same result, then by stripping the problem down to its essence. Once you have reduced the problem to this minimal, reproducible essence, you will often be able to spot the problem yourself - and, if not, you will make it easier for others to help you. There is an R package (reprex) which will likely facilitate this, but I havent tried it yet. Here is a good introduction. Finally, to get a sense of some of the ways you can get help in RStudio (and to see how a master uses the R Studio interface), consider this video: Video 3.1: Garrett Grolemund of RStudio 3.3 Wickham and R for Data Science The first chapter of the Wickham text (Wickham and Grolemund 2017) provides a framework for his approach and a brief introduction to the tidyverse which will be the dialect of R we will study in the weeks ahead. Please read it now. References "],["now-draw-the-rest-of-the-owl.html", "4 now draw the rest of the owl 4.1 Carmichael 4.2 DataCamp 4.3 Swirl (Swirlstats) 4.4 Peng text and videos 4.5 Something else 4.6 An exercise 4.7 An introduction to R markdown", " 4 now draw the rest of the owl Fig 4.1: Draw the rest of the owl. There are many sources for learning the basics of R. A few of these follow. Please spend at least 90 mins exploring at least two of the following. Be prepared to discuss your progress next class (you will be asked which source(s) you used, what you struggled with, and whether you would recommend it to your classmates. (Note that all of these are free, though you may choose to make a donation to the author if you use the Peng text). Hint: If you find the material too challenging - if you feel like you are drawing the rest of the owl - take a break away from your machine and other screens, clear your head, then try a different approach. 4.1 Carmichael Iain Carmichael prepared the following for his Intro to Data Science course at UNC-Chapel Hill. I think it is a great place to start: https://idc9.github.io/stor390/notes/getting_started/getting_started.html 4.2 DataCamp Many folks swear by (and others, I presume, at) DataCamp, which kind of gamifies learning software. As a student in this class, you have access to all of their stuff free. You can even do lessons on your phone. 4.3 Swirl (Swirlstats) I, like thousands of others, learned R in the process of completing the Johns Hopkins Data Science Specialization offered through Coursera. The sequence can be challenging, but their introduction to R used an accessible, interactive R package called Swirl. You can read about swirl (learn R in R) at https://swirlstats.com/. Using Swirl. After loading R (and opening R studio), you will get to the Swirl lessons with the following steps: Install the Swirl package on your computer (you only need to do this once). Type the following into your console window in R studio (typically left hand side of your screen or lower left) install.packages(swirl) Then load the package into your workspace (youll need to do this at the beginning of every session you use Swirl) library (swirl) Then run it! swirl () Swirl will ask a few questions then give you the option of choosing one of several courses. Youll choose the R Programming option, which leads to 15 separate lessons. At the end of each lesson, youll be asked Would you like to receive credit for completing this course on Coursera.org? Answer no then do another lesson. 4.4 Peng text and videos Finally, consider the text and videos from the Coursera R class. Most of the material from that class can be found in Peng (2014). A slightly updated version of the text can be found at https://bookdown.org/rdpeng/rprogdatascience/, and the videos in the series may be found by clicking on the following: . Video 4.2: Roger Peng introducing R 4.5 Something else The something else category includes Datacarpentry.org, which is aimed at fostering data literacy and provides free lessons for learning and applying R in areas such as Ecology, Genomics and Geospatial data analysis. Of particular interest are the social science lessons, as well as a workshop in data science based on the \"Studying African Farmer-led Irrigation (SAFI)\" dataset. 4.6 An exercise Review Carmichaels Getting started with R. Open R studio, and create a new R script called myMovies.R. Using his code as a reference, do each of the following Work in your source window. On the first line, enter the command to install the tidyverse, using the chunk below - but omit the octothorpe. (If youve already installed the tidyverse, keep the #). # install.packages (&quot;tidyverse&quot;) Hit ctrl+enter to run this line. Now comment it out if you havent already done so (why)? Load the tidyverse into your workspace. Load the movies/IMDB dataset. Start exploring the data and the RStudio interface: Apply the str (structure), head, and summary commands. When are each of these useful? Double-click the movies dataset in your environment tab in R studio. Click on a few columns to sort the data. In the data, what does spilled mean? How did you find out? How many rows and columns are in the data? We can think about the movies dataset as a matrix with rows and columns, and subset it using the following. # data.frame[rownumber,colnumber] # data.frame[&quot;rowname&quot;, &quot;colname&quot;] # data.frame[rowname, c(&quot;colname, colname&quot;)] movies[&quot;title&quot;] movies[title] movies[title,] Ask an interesting question about the data, and enter it as a comment in your code, e.g., # how long was the movie 42-up? Try to find the answer, ideally using reproducible code, and be prepared to share it with the class. Save your work :) 4.6.1 An initial solution Heres a sample solution supplied by one group of students. Ive added some comments to make it useful to my present and future self. # notes from class 1/22/20 # exercise in Data Science for Liberal Arts # contributed by _________ _______ # ---------- # run this line just once to put package on machine # install.packages(&quot;tidyverse&quot;) library(tidyverse) # this loads a movie dataset from the web load(url(&#39;https://stat.duke.edu/~mc301/data/movies.Rdata&#39;)) # some ways to look at the dataset View(movies) str(movies) head(movies) summary(movies) head(movies$genre) head(movies$genre,100) # an initial plot ggplot(data = movies) + geom_point(mapping = aes(x = imdb_rating, y = critics_score)) # what does &quot;spilled&quot; mean? summary(movies$audience_rating) ggplot(data = movies) + geom_point(mapping = aes(x = audience_score, y = audience_rating)) # this displays the list of movies movies[&quot;title&quot;] # this writes the list of movies to a new vector b &lt;- movies[&quot;title&quot;] # which genre has the highest audience score? ggplot(data = movies) + geom_point(mapping = aes(x = genre, y = audience_score, color=mpaa_rating)) ggplot(data = movies) + geom_point(mapping = aes(x = genre, y = audience_score, color=audience_score)) # these commands fail - why? # view(genre,audience_score) # summary(genre,audience_score) # Summary(&quot;genre&quot;) # # this works! c &lt;- movies[&quot;genre&quot;] # but not this # d &lt;- movies(&quot;audience_score&quot;) d &lt;- movies[&quot;audience_score&quot;] # # subsetting a file # # base syntax to create a new file # with the 3rd,4th, and 7th columns in movies movies2 &lt;- movies[,c(3,4,7)] # tidyverse syntax to # create a new file with just the first variable in movies2 movies3 &lt;- movies2 %&gt;% select(1) 4.7 An introduction to R markdown The above script worked fine, but it is not a good record of my examination of the movie database. It tells only part of the story (it doesnt include the results), and it doesnt tell it particularly clearly (the comments that explain the logic and syntax of my work are mixed in with the code that executes it). In literate programming, these aspects of work are clearly articulated and retained. R markdown documents are tools for elegantly integrating comments, code, and results - they document our work. Well return to this in some detail later; for now, to create an R markdown (Rmd) document in R studio, begin with (File -&gt; New File -&gt; R Markdown). A window will open up with a file that begins with some YAML (Yet Another Markdown Language). You can edit this as needed: --- title: &quot;My first R Markdown Document&quot; author: &quot;Frankie McFrank Frank&quot; date: &quot;1/27/2020&quot; output: html_document --- Click on the clever knit icon in the bar just above the source window to create a sample document. Youll need to save the file with a new name, then R will finally create an HTML (Hyper Text Markdown Language - see the pattern?) page. Compare the R Markdown document (your code) with the result (the HTML). Now, try to knit the document into different output formats (click on the down arrow next to knit, and see if you can knit to PDF and Word). It may take a while the first time you do this, as you may need to load some additional formatting tools or packages (e.g., TinyTex) to successfully render these files. After you have admired the results, continue editing your .Rmd document. You will replace the chunks (everything but the YAML at the top of the document) with the code from the Carmichael project. You can insert a chunk of code using the insert then R commands at the top of your source window. 4.7.1 Step-by-step When I am writing an R markdown document in R studio, Ill generally work on one chunk at a time, testing each in turn (generally by clicking the run triangle at the top of the chunk). Once I am done, I knit the document as a whole as a complete record of my work. My first chunk loads the libraries and data. When I write this, Ill modify the top of the chunk to say {r message=FALSE} rather than just {r} so that I dont get too much chatter about this step: library(tidyverse) load(url(&#39;https://stat.duke.edu/~mc301/data/movies.Rdata&#39;)) So from here on, the shaded blocks include code and results. My next verbatim chunk looks like this when I write it and when I test it: ` ``{r whatever} str(movies) head(movies) summary(movies) head(movies$genre) # head(movies$genre,100) why is this commented out? ` `` When I knit it, the results are folded in as follows: str(movies) ## tibble [651 x 32] (S3: tbl_df/tbl/data.frame) ## $ title : chr [1:651] &quot;Filly Brown&quot; &quot;The Dish&quot; &quot;Waiting for Guffman&quot; &quot;The Age of Innocence&quot; ... ## $ title_type : Factor w/ 3 levels &quot;Documentary&quot;,..: 2 2 2 2 2 1 2 2 1 2 ... ## $ genre : Factor w/ 11 levels &quot;Action &amp; Adventure&quot;,..: 6 6 4 6 7 5 6 6 5 6 ... ## $ runtime : num [1:651] 80 101 84 139 90 78 142 93 88 119 ... ## $ mpaa_rating : Factor w/ 6 levels &quot;G&quot;,&quot;NC-17&quot;,&quot;PG&quot;,..: 5 4 5 3 5 6 4 5 6 6 ... ## $ studio : Factor w/ 211 levels &quot;20th Century Fox&quot;,..: 91 202 167 34 13 163 147 118 88 84 ... ## $ thtr_rel_year : num [1:651] 2013 2001 1996 1993 2004 ... ## $ thtr_rel_month : num [1:651] 4 3 8 10 9 1 1 11 9 3 ... ## $ thtr_rel_day : num [1:651] 19 14 21 1 10 15 1 8 7 2 ... ## $ dvd_rel_year : num [1:651] 2013 2001 2001 2001 2005 ... ## $ dvd_rel_month : num [1:651] 7 8 8 11 4 4 2 3 1 8 ... ## $ dvd_rel_day : num [1:651] 30 28 21 6 19 20 18 2 21 14 ... ## $ imdb_rating : num [1:651] 5.5 7.3 7.6 7.2 5.1 7.8 7.2 5.5 7.5 6.6 ... ## $ imdb_num_votes : int [1:651] 899 12285 22381 35096 2386 333 5016 2272 880 12496 ... ## $ critics_rating : Factor w/ 3 levels &quot;Certified Fresh&quot;,..: 3 1 1 1 3 2 3 3 2 1 ... ## $ critics_score : num [1:651] 45 96 91 80 33 91 57 17 90 83 ... ## $ audience_rating : Factor w/ 2 levels &quot;Spilled&quot;,&quot;Upright&quot;: 2 2 2 2 1 2 2 1 2 2 ... ## $ audience_score : num [1:651] 73 81 91 76 27 86 76 47 89 66 ... ## $ best_pic_nom : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ best_pic_win : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ best_actor_win : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 2 1 1 1 2 1 1 ... ## $ best_actress_win: Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ best_dir_win : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 2 1 1 1 1 1 1 ... ## $ top200_box : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ director : chr [1:651] &quot;Michael D. Olmos&quot; &quot;Rob Sitch&quot; &quot;Christopher Guest&quot; &quot;Martin Scorsese&quot; ... ## $ actor1 : chr [1:651] &quot;Gina Rodriguez&quot; &quot;Sam Neill&quot; &quot;Christopher Guest&quot; &quot;Daniel Day-Lewis&quot; ... ## $ actor2 : chr [1:651] &quot;Jenni Rivera&quot; &quot;Kevin Harrington&quot; &quot;Catherine O&#39;Hara&quot; &quot;Michelle Pfeiffer&quot; ... ## $ actor3 : chr [1:651] &quot;Lou Diamond Phillips&quot; &quot;Patrick Warburton&quot; &quot;Parker Posey&quot; &quot;Winona Ryder&quot; ... ## $ actor4 : chr [1:651] &quot;Emilio Rivera&quot; &quot;Tom Long&quot; &quot;Eugene Levy&quot; &quot;Richard E. Grant&quot; ... ## $ actor5 : chr [1:651] &quot;Joseph Julian Soria&quot; &quot;Genevieve Mooy&quot; &quot;Bob Balaban&quot; &quot;Alec McCowen&quot; ... ## $ imdb_url : chr [1:651] &quot;http://www.imdb.com/title/tt1869425/&quot; &quot;http://www.imdb.com/title/tt0205873/&quot; &quot;http://www.imdb.com/title/tt0118111/&quot; &quot;http://www.imdb.com/title/tt0106226/&quot; ... ## $ rt_url : chr [1:651] &quot;//www.rottentomatoes.com/m/filly_brown_2012/&quot; &quot;//www.rottentomatoes.com/m/dish/&quot; &quot;//www.rottentomatoes.com/m/waiting_for_guffman/&quot; &quot;//www.rottentomatoes.com/m/age_of_innocence/&quot; ... head(movies) ## # A tibble: 6 x 32 ## title title_type genre runtime mpaa_rating studio thtr_rel_year thtr_rel_month ## &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Fill~ Feature F~ Drama 80 R Indom~ 2013 4 ## 2 The ~ Feature F~ Drama 101 PG-13 Warne~ 2001 3 ## 3 Wait~ Feature F~ Come~ 84 R Sony ~ 1996 8 ## 4 The ~ Feature F~ Drama 139 PG Colum~ 1993 10 ## 5 Male~ Feature F~ Horr~ 90 R Ancho~ 2004 9 ## 6 Old ~ Documenta~ Docu~ 78 Unrated Shcal~ 2009 1 ## # ... with 24 more variables: thtr_rel_day &lt;dbl&gt;, dvd_rel_year &lt;dbl&gt;, ## # dvd_rel_month &lt;dbl&gt;, dvd_rel_day &lt;dbl&gt;, imdb_rating &lt;dbl&gt;, ## # imdb_num_votes &lt;int&gt;, critics_rating &lt;fct&gt;, critics_score &lt;dbl&gt;, ## # audience_rating &lt;fct&gt;, audience_score &lt;dbl&gt;, best_pic_nom &lt;fct&gt;, ## # best_pic_win &lt;fct&gt;, best_actor_win &lt;fct&gt;, best_actress_win &lt;fct&gt;, ## # best_dir_win &lt;fct&gt;, top200_box &lt;fct&gt;, director &lt;chr&gt;, actor1 &lt;chr&gt;, ## # actor2 &lt;chr&gt;, actor3 &lt;chr&gt;, actor4 &lt;chr&gt;, actor5 &lt;chr&gt;, imdb_url &lt;chr&gt;, ## # rt_url &lt;chr&gt; summary(movies) ## title title_type genre runtime ## Length:651 Documentary : 55 Drama :305 Min. : 39.0 ## Class :character Feature Film:591 Comedy : 87 1st Qu.: 92.0 ## Mode :character TV Movie : 5 Action &amp; Adventure: 65 Median :103.0 ## Mystery &amp; Suspense: 59 Mean :105.8 ## Documentary : 52 3rd Qu.:115.8 ## Horror : 23 Max. :267.0 ## (Other) : 60 NA&#39;s :1 ## mpaa_rating studio thtr_rel_year ## G : 19 Paramount Pictures : 37 Min. :1970 ## NC-17 : 2 Warner Bros. Pictures : 30 1st Qu.:1990 ## PG :118 Sony Pictures Home Entertainment: 27 Median :2000 ## PG-13 :133 Universal Pictures : 23 Mean :1998 ## R :329 Warner Home Video : 19 3rd Qu.:2007 ## Unrated: 50 (Other) :507 Max. :2014 ## NA&#39;s : 8 ## thtr_rel_month thtr_rel_day dvd_rel_year dvd_rel_month ## Min. : 1.00 Min. : 1.00 Min. :1991 Min. : 1.000 ## 1st Qu.: 4.00 1st Qu.: 7.00 1st Qu.:2001 1st Qu.: 3.000 ## Median : 7.00 Median :15.00 Median :2004 Median : 6.000 ## Mean : 6.74 Mean :14.42 Mean :2004 Mean : 6.333 ## 3rd Qu.:10.00 3rd Qu.:21.00 3rd Qu.:2008 3rd Qu.: 9.000 ## Max. :12.00 Max. :31.00 Max. :2015 Max. :12.000 ## NA&#39;s :8 NA&#39;s :8 ## dvd_rel_day imdb_rating imdb_num_votes critics_rating ## Min. : 1.00 Min. :1.900 Min. : 180 Certified Fresh:135 ## 1st Qu.: 7.00 1st Qu.:5.900 1st Qu.: 4546 Fresh :209 ## Median :15.00 Median :6.600 Median : 15116 Rotten :307 ## Mean :15.01 Mean :6.493 Mean : 57533 ## 3rd Qu.:23.00 3rd Qu.:7.300 3rd Qu.: 58301 ## Max. :31.00 Max. :9.000 Max. :893008 ## NA&#39;s :8 ## critics_score audience_rating audience_score best_pic_nom best_pic_win ## Min. : 1.00 Spilled:275 Min. :11.00 no :629 no :644 ## 1st Qu.: 33.00 Upright:376 1st Qu.:46.00 yes: 22 yes: 7 ## Median : 61.00 Median :65.00 ## Mean : 57.69 Mean :62.36 ## 3rd Qu.: 83.00 3rd Qu.:80.00 ## Max. :100.00 Max. :97.00 ## ## best_actor_win best_actress_win best_dir_win top200_box director ## no :558 no :579 no :608 no :636 Length:651 ## yes: 93 yes: 72 yes: 43 yes: 15 Class :character ## Mode :character ## ## ## ## ## actor1 actor2 actor3 actor4 ## Length:651 Length:651 Length:651 Length:651 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## ## actor5 imdb_url rt_url ## Length:651 Length:651 Length:651 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## head(movies$genre) ## [1] Drama Drama Comedy Drama Horror Documentary ## 11 Levels: Action &amp; Adventure Animation Art House &amp; International ... Science Fiction &amp; Fantasy # head(movies$genre,100) why is this commented out? 4.7.2 Spilled??? In class, we were asked what doesspilled\" mean?\" We looked at the audience_rating variable (spilled) to see if it was related to audience_score; summary(movies$audience_rating) ## Spilled Upright ## 275 376 ggplot(data = movies) + geom_point(mapping = aes(x = audience_score, y = audience_rating)) Bingo! Eyeballing this, it looks like scores 60 and above are upright, those below 60 are Spilled. What would I need to do to test this more explicitly? 4.7.3 Your turn Now, add sections which describe and examine the interesting question you asked about the movie dataset. I dont expect statistical tests but I do ask that you think about your work and that, after your last chunk of code, you include a section of text which describes the next steps that you feel would be needed on the project to give you confidence in your results. Knit the result to a PDF, and rename this as YourLastNameMovie1.pdf. Ill collect in the next class. References "],["references.html", "References", " References Broman, Karl W., and Kara H. Woo. 2018. Data Organization in Spreadsheets. The American Statistician 72 (1): 210. https://doi.org/10.1080/00031305.2017.1375989. Buckheit, Jonathan B., and David L. Donoho. 1995. Wavelab and Reproducible Research. In Wavelets and Statistics, 5581. Springer. Donoho, David. 2017. 50 Years of Data Science. Journal of Computational and Graphical Statistics 26 (4): 74566. https://doi.org/10.1080/10618600.2017.1384734. Donoho, David L. 2010. An Invitation to Reproducible Computational Research. Biostatistics 11 (3): 38588. https://doi.org/10.1093/biostatistics/kxq028. Freeman, Michael, and Joel Ross. 2017. Technical Foundations of Informatics, U Washington INFO 201. Gandrud, Christopher. 2013. Reproducible Research with R and R Studio. CRC Press. Henrich, Joseph, Steven J. Heine, and Ara Norenzayan. 2010. The Weirdest People in the World? Behavioral and Brain Sciences 33 (2-3): 6183. https://doi.org/10/c9j35b. Hicks, Stephanie C., and Rafael A. Irizarry. 2018. A Guide to Teaching Data Science. The American Statistician 72 (4): 38291. https://doi.org/10/gfr5tf. Isaacson, Walter. 2014. The Innovators: How a Group of Hackers, Geniuses, and Geeks Created the Digital Revolution. Simon and Schuster. Loevinger, Jane. 1957. Objective Tests as Instruments of Psychological Theory. Psychological Reports 3 (3): 63594. https://doi.org/10.2466/pr0.1957.3.3.635. Peng, Roger D. 2014. R Programming for Data Science. Tversky, Amos, and Daniel Kahneman. 1974. Judgment Under Uncertainty: Heuristics and Biases. Science 185 (4157): 112431. https://doi.org/10.1126/science.185.4157.1124. Wickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. "]]
