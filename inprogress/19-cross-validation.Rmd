```{r, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warn = FALSE, message = FALSE) # defaults for all chunks after this one
library(tidyverse)
library(modelr)
library(Ecdat) 
library (caret)# machine learning package
library (caTools) # ditto
```

# cross-validation

*The problem of capitalizing on chance is a significant one in prediction, and one should always be skeptical of models which are untested beyond the sample from which they were derived.*

## revisiting the affairs data

Let's go back and do what we should have done earlier, that is, examine and think about the "number of affairs" variable. What does the distribution look like?

```{r}
data(Fair)
Fair %>% count(nbaffairs)
```

We note that the distribution is skewed - and we realize that perhaps we should think about it differently: The *meaning* of the difference between 0 and 1 is not the same as that between 1 and 2, or perhaps even 1 and 12.

We can transform the data in several ways. We might perform a *root transform*, which we could then round to an integer:

```{r}
Fair <- Fair %>% 
    mutate(rootAffair = 
              as.integer(sqrt(nbaffairs)))
Fair %>% count(rootAffair)
```

Or we could simply distinguish between those that do and don't have affairs.

```{r}
Fair <- Fair %>% 
    mutate(affairYN =
               ifelse (nbaffairs > 0,1,0))
Fair %>% count(affairYN)
```

We'll examine each of these in the following paragraphs.

## avoiding capitalizing on chance

One lesson from the last class was that correlations (and regression coefficients) drawn from small samples were not stable. In regression analysis, as progressively smaller samples were drawn from the Fair data, the ability to predict the outcome increased. In the limiting case, when the number of predictors (variables) was equal to the number of cases in the sample (rows), prediction became perfect.

### splitting the data into training and test subsamples

The most basic solution to this problem is to split the data into two groups, a *training* sample from which we extract our model, and a *test* sample on which you will assess it. (Often, the logic of this will be extended to include a third group, a *validation* sample which would be used to tune or select the results of the training run before the test data are examined). Here, we will consider the simpler approach, splitting the Fair data into training and test samples. The critical feature of this analysis is that we will hold out the test data, and not even look at it until after our model building is complete.

```{r}
# establish a seed for your data-split
# so that your results will be reproducible
set.seed(33458)
n <- nrow(Fair)
# create a set of line numbers 
# of size corresponding to the 
# desired training sample 
trainIndex <- sample(1:n, size = round(0.6*n), replace=FALSE)
# create training and test samples
trainFair <- Fair[trainIndex ,]
testFair <- Fair[-trainIndex ,]
```

## an example of cross-validated linear regression

We first predict the variable "rootAffair," using just the training data:

```{r}
trainFair2 <- trainFair %>%
    select(-nbaffairs, -affairYN)
testFair2 <- testFair %>%
    select(-nbaffairs, -affairYN)
model2 <- lm(rootAffair ~ ., data=trainFair2)
summary(model2)
```

To assess the effectiveness of this model on an independent sample, we write a simple function, which assesses R2, then applies it to both the training data and the test data:

```{r}
R2.model.dep.data <- function(myModel,myDep,myData) {
    errorscores <- myDep - predict(myModel,myData)
    SS.error <- sum(errorscores^2)
    deviations <- scale(myDep,scale = FALSE)#)
    SS.total <- sum(deviations^2)
    1 - (SS.error/SS.total)
}    

R2Train <- R2.model.dep.data(model2,trainFair2$rootAffair,trainFair2)
(R2Test <- R2.model.dep.data(model2,testFair2$rootAffair,testFair2))
```

The R2 on the training sample is `r R2Train`, on the test sample it is `r R2Test`. The difference between these is sometimes referred to as shrinkage. Shrinkage will be problematic particularly when there are a small number of observations, a large number of predictors (and consequently a complex model), or both of these.

### applying logistic regression analysis to the training data

Let's go back to the "number of affairs" variable and consider a second way of thinking about it, that is, simply comparing those who do and don't have affairs:

```{r}
Fair <- Fair %>% 
    mutate(affairYN =
               ifelse (nbaffairs > 0,1,0))
Fair %>% count(affairYN)
```

With this change, the regression problem becomes more clearly a problem in classification. How can we best predict which 'type' (affair-ers vs. not) a given person falls in to?

With this change, we have moved from a variable which is essentially continuous (nbaffairs) to one which is dichotomous and therefore distributed binomially. The desired regression is now a logistic one.

We begin by running this analysis using only the training data.

```{r}
model2 <- glm(affairYN ~ ., data = trainFair,
              family = "binomial")
summary (model2)
```

Our "predicted scores" are continuous, corresponding to the probability that a given person will have an affair. Here's how they are distributed (still on the training data here):

```{r}
predictTrain <- predict(model2, trainFair, type = "response")
summary (predictTrain)
```

As scores on predictTrain increase, the predicted likelihood of an affair increases. If you are considering marriage to a potential partner, what score would be too high? How do we distinguish "ok" from "not"?
