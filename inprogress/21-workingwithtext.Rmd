---
title: "workingwithtext"
output: html_document
date: "2025-02-23"
---

# working with text

In this chapter, I consider some rudimentary approaches to text analysis using R, leading to the creation of comparison word-clouds and the creation of tables to describe differences in different categies of language use, in particular, the Linguistic Inquiry and Word Count (LIWC) categories [LIWC; @boyd2022]. The focus continues to be on the tidyverse, specifically the `tidyjson` package, which allows for the extraction of data from JSON files, and the `tidytext` package, which re-expresses a body of text in a tidy (i.e., rectangular) format for easy processing.

This chapter is light on the grunt work of data wrangling and preprocessing (which were introduced in Chapter 14). I instead consider some examples from my own work, particularly with Reddit data.

## getting text off of the web

In this block, I take a set of posts that I have downloaded from Reddit. Although Reddit data became less accessible beginning in 2023, archives of Reddit posts and comments some quite comprehensive) remain readily available following some simple Web sleuthing.

The file is large, so it is read in through streaming

was reduced

There are several approaches to doing this that appear to be legal

## for very large files, may need to read in piecemeal.

```{r }
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidyjson)
library(jsonlite)
start.time <- Sys.time()
posts <- readLines(
    "r_applyingtocollege_posts.jsonl")[
        1:100000] # for file a1
#        200001:400000] # for file b
#       400001:500000] # for file c
# SELECTION IS done here rather than in a separate step
a1 <- stream_in(textConnection(
     gsub("\\n","", posts))) |> 
     select(author, title, selftext, id, link_flair_text, num_comments,
            permalink, score, subreddit, created_utc) 
write_csv(a1, "applyingposts1.csv")
end.time <- Sys.time()
```

# for smaller files

```{r}
# posts <- ("r_firstgenstudents_posts.jsonl")
posts2 <- ("r_applyingtocollege_posts1.jsonl")
a <- stream_in(file(posts))
earliesta <-
    as.POSIXct(min(a$created_utc))
latesta <-
    as.POSIXct(max(a$created_utc))
etc. 
# this file is a list - can't write as a csv file
# so save to disk as an rds
# save(c, file = "applyingposts20210118to20211125.rds")
# then open rds file, save appropriate columns, save as dataframe/tibble
```

for comments

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidyjson)
library(jsonlite)
comments <- ("r_ApplyingToCollege_comments.jsonl")
a <- stream_in(file(comments))
# fails a <- fromJSON(file(comments))
# fails a <- read_json("r_firstgenstudents_comments.jsonl")
comments <- a %>% 
    select(author, body, created_utc, id, score, subreddit) %>% 
    as_tibble %>% 
    write_csv("ApplyingToCollegeComments.csv")
```

some data cleaning may be appropriate here, but the file should be readable by the LIWC software

```{r}
```

This is adapted from my 2018 paper on Ego Level available from [OSF](https://osf.io/jw7dy/). In that paper, the groups were ordered on a continuum, here they are not.

The size of each word in the plots corresponds to its ((proportion within the group) / (total proportion)).

The first chunk of the code extracts simulated text from three 'groups' of different sizes. The second chunk constructs the differential word clouds.

```{r echo=TRUE, suppressPackageStartupMessages=TRUE, setup}
library(randomizr)
library(tidyverse)
library(tidytext)
library(wordcloud)
```

## Creation of simulated data to test code

The corpus has three groups of different sizes. The first group has 100 documents, the second group has 300 documents, and the third group has 600 documents. Each document has between 50 and 500 words. The words in the corpus are randomly selected from a sample of sentiment words. The code below creates a simulated corpus for testing purposes.

```{r echo=TRUE,  warning = TRUE, simdata}
sampledata <- read_csv(url(
  "https://raw.githubusercontent.com/totalgood/hope/master/data/corpora/sentiment-words-DFE-785960.csv")) |> 
    select(w1:w4) |> 
    na.omit()
ndocs <- 1000
minDocLength <-  50
maxDocLength <- 500
doc <- vector(mode="character", length=ndocs)
set.seed(33458)
# get a random word from the sampledata
getword <- function() {
      rowid <- sample(1:nrow(sampledata), 1)
      colid <- sample(1:ncol(sampledata), 1)
      word<-sampledata[rowid,colid]
      word                 
}
# combine words into docs
# initialize
docLength <- sample(minDocLength:maxDocLength,1)
sampleCorpus <- getword()
for (i in 1:docLength){
      addWord<-getword()
      sampleCorpus<-paste(sampleCorpus, addWord)
}
#add additional simulated documents
for (j in 2:ndocs) {
      docLength <-sample(minDocLength:maxDocLength,1)
      newdoc<-getword()
      for (i in 1:docLength-1){
            addWord<-getword()
            newdoc<-paste(newdoc, addWord)
      }
      sampleCorpus<-rbind(newdoc,sampleCorpus)
}
#row.names(sampleCorpus) <- NULL
workingCorpus <- sampleCorpus |> 
    as_tibble() |>
    mutate(myGroups = complete_ra(N = ndocs, 
                                  m_each = c(100,300,600),
                                  conditions = c("sharks",
                                                 "jets","nerds")))
head(workingCorpus)
```

## Construction of the differential word cloud

This takes the simulated corpus and reshjapes it to a matrix with row names (words) and counts (one column for each group). This is then re-expressed as a matrix of differential proportions. These proportions are then scaled to eliminate negative values. The `comparison.cloud` function from the `wordcloud` package is used to show the relative proportions of words in each group (in that function, the scale parameters will need to be adjusted to get the best visual representation).

```{r}
tidy1 <- workingCorpus |> 
    unnest_tokens(word, V1) |> 
    count(myGroups, word, sort = TRUE) #|>
wideCorpus <- tidy1 |>
    pivot_wider(names_from = myGroups, values_from = n, 
                values_fill = 0) |> 
    column_to_rownames(var = "word") |>
    as.matrix(rownames = TRUE)
groupSums<-margin.table(as.matrix(wideCorpus), 2) # Column sums
groupProportions<- sweep(wideCorpus, 2, groupSums, "/")
wordProportions <- margin.table(as.matrix(groupProportions), 1) # Row sums
rawProportions<- sweep(groupProportions, 1, wordProportions, "/")

scaledProportions <- 50 + (10 * scale(rawProportions))
document_tm_mat <- 50 + (10*scaledProportions)


comparison.cloud(document_tm_mat,
#       scale = c(2.25,1.25),
        scale = c(1.75,.5),
        max.words=50,
        random.order=FALSE,
        rot.per = 0,
#       colors=myPalette,
#       colors= c("blue","red", "green"),
        use.r.layout=FALSE,
        family="sans",
        title.size=1)
   

```

## Code for assessing LIWC effect sizes between 2 groups

This uses data from my OSF page (<https://osf.io/zpk84/files/osfstorage>), which are downloaded to my drive prior to running. Code is mostly Base R but works fine. Number of LIWC categories has increased since the original paper, so the code has been updated to reflect that.

Consider filtering data by WC (word count) to remove posts with fewer than 250 words. This is done in the code below.

The Bonferroni adjustment is used to correct for multiple comparisons. The number of LIWC categories is 117, so the Bonferroni p-value should be .05/117 = .000427.

### Libraries and initial csv set up.

```{r setup, include=FALSE}
library(psych)
library(tidyverse)
library(compute.es)
myDownloads <- ("C:/Users/the_l/OneDrive - Florida Atlantic University/DownloadsOneDrive/")
Clintonposts<-read.csv(paste0(myDownloads,
                              "LIWCClintonposts08011108.csv")) # |> 
#    filter(WC > 249)

Trumpposts<-read.csv(paste0(myDownloads,
                            "LIWCTrumpposts08011108.csv"))
BonferroniP <- (.05/117) # 117 is the number of LIWC categories
```

## Generate effect sizes for all LIWC categories.

This uses the describe function from the psych package to get means, standard deviations, and sample sizes for each LIWC category. Then it computes the effect sizes and associated statistics using the mes function from the compute.es package and makes tables for the LIWC variables with the largest effects.

```{r}
# simple descriptives
CP <- Clintonposts |> 
    select(WC:OtherP) |> 
    describe() |>
    select(ClintonN=n, ClintonMean = mean, ClintonSD = sd) |> 
    round(3)
TP <- Trumpposts |>
    select(WC:OtherP) |> 
    describe() |>
    select(TrumpN=n, TrumpMean = mean, TrumpSD = sd) |> 
    round(3)
poststats <- CP |> 
    bind_cols (TP)
# effect size stats. tidy syntax fails here.
Effects <- mes(poststats[,2], poststats[,5],
               poststats[,3], poststats[,6],
               poststats[,1],poststats[,4],
               level = BonferroniP,
               verbose = FALSE) |> 
    select(d, l.d, u.d, pval.d)
LIWCSummary <- poststats |> 
    bind_cols(Effects) |>
    write_csv("LIWCeffects.csv")

LIWCSummary |> 
    arrange(d) |> 
    head(10) |> 
    knitr::kable()

LIWCSummary |> 
    arrange(desc(d)) |> 
    head(10) |> 
    knitr::kable()


```
