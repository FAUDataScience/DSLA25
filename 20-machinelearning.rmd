---
title: "20-machineLearning"
author: "Lanning"
date: "March 21, 2019"
output: html_document
---

```{r}
knitr::opts_chunk$set(echo = TRUE, warn = FALSE, message = FALSE) # defaults for all chunks after this one
library(tidyverse)
# library(ggplot2)
library(Ecdat) # for Swiss data
# library (class) # for knn
# library(caret) # confusion matrix
# library(magrittr) # allows bidirectional pipe for updating file easily %<>% 
```

# Machine learning: some distinctions and ideas

In the last few chapters, we have considered regression and k-nearest neighbor analysis as tools for prediction and classification.  Here, we extend this treatment to understand some key ideas in the study of *machine learning*. My approach is drawn largely from [@james2013introduction], which is available freely on the web and includes links to additional materials for those who would like to study this further.

## supervised versus unsupervised problems

The distinction between supervised and unsupervised problems is the most fundamental distinction in machine learning.

In both the Swiss and the Fair problems, we had a known outcome (fertility, infidelity) which we were trying to predict from a set of independent variables.  In these problems, we have an a priori split of the variables into two sets (outcomes and predictors). These are considered **supervised** problems.

There is a second type of problem in which we don't have an outcome, which would guide or supervise our model. In this alternate case, we have one set of variables, not two, and we want to assess its internal structure.  These are considered **unsupervised** problems. Methods used to address unsupervised problems include cluster analysis (of which there are many subtypes), component analysis, and exploratory factor analysis.  

In the unsupervised approach, objectives include finding unknown patterns, developing a set of types (or a taxonomy), and assessing the dimensionality of a latent set of variables. This approach is particularly important in my own field of personality psychology, where a focal problems involves assessing the factor structure of personality: If you have taken introductory psychology, you are likely familiar with the five-factor (Big Five) model of personality.  The unsupervised approach is also used to solve problems of community detection in the study of social and scientific networks (see Figure 20.1). Questions about dimensionality and internal structure can be compelling (@lanning1994dimensionality, @lanning1996robustness), but we will not consider them further here.

![Figure 20.1. Part of the structure of personality research. From [@lanning2018network] ](SoniaFigure.PNG)



## prediction versus classification

Within the category of supervised problems, we can distinguish problems in classification (those in which the outcome is nominal or discrete) from problems where the outcome is ordered or numeric.  We saw this in the Fair data, when we shifted from treating the outcome as the number of reported affairs to the distinction between those who did and did not have affairs.

## understanding versus prediction

In both the Swiss and Fair data, we were concerned with the problem of predicting an outcome (fertility or infidelity) from a set of independent variables.  

In the Swiss data, we were particularly concerned with the nature of the function, the equation which best predicts fertility from the remaining predictors.

In the Fair data, we initially were concerned with the nature of this **function** (what predicts infidelity?), but, with the k-nn analysis, became particularly concerned with increasing our hit rate or overall accuracy.  We moved, in short, from a position where we were concerned with inference and understanding to a position where we were satisfied to treat the algorithm as a **black box** from which we were only concerned with the accuracy of outputs.

The two objectives of, on the one hand, "understanding and thinking in terms of equations and models" and "prediction and thinking only in terms of optimization" can be thought of as two points on a continuum of **interpretability.** Some techniques used in machine learning give results that are quite interpretable (including multiple regression, particularly restricted regression techniques such as the **lasso**). Others, including non-linear approaches such as **support vector machines** and, in particular, **deep neural networks** sacrifice interpretability in the service of prediction.  For complex approaches in image recognition, such as the chihuahua versus muffin problem, deep neural networks provide the best solutions, but are particularly challenging to understand [@kumar2017explaining].

![Fig 20.1: The question "Is this a chihuahua or a muffin?" is best solved using a deep neural network approach that can't be reduced to a simple equation.](chihuahuamuffin.jpg)

## the bias-variability tradeoff

If you read further about machine learning, you are likely to encounter the phrase "bias-variability tradeoff." You may remember that, in our initial analyses of the Swiss fertility data, we discussed how reducing the number of observations increases the fit of the model on the (training) data - and that, ultimately, the fit of the model would become perfect when we reduce the number of observations to the number of variables in the model. 

```{r}
set.seed (33458)
FairSample1 <- Fair %>% sample_n(9)
affairReg1 <- lm(nbaffairs ~ ., data=FairSample1)
summary(affairReg1)$r.squared
```
If, however, we were to run the same analysis on a different sample of the same size, we would still get perfect predictability, but would get a very different set of predictors:

```{r}
options(digits = 2)
set.seed (94611)
FairSample2 <- Fair %>% sample_n(9)
affairReg2 <- lm(nbaffairs ~ ., data=FairSample2)
summary(affairReg2)$r.squared
cbind(unlist (affairReg1[["coefficients"]]),
unlist (affairReg2[["coefficients"]]))
```
The difference between these two sets of coefficients is an illustration of how coefficients in overfit models will vary from one sample to another. At the opposite extreme are underfit models, which are likely to provide relatively stable coefficients across samples, but which are based on a too-simple model.  

## dealing with the structure of the data

### preprocessing

The problem that we encountered in the tiny samples of the Swiss data exists in bigger datasets as well, where a large number of predictor variables can make it difficult to find a model which is robust (low variability) and effective (low bias).

In areas such as artificial intelligence (including image recognition) and bioinformatics (including statistical genetics), this **curse of dimensionality** can be quite challenging.  Approaches to minimizing its effects include scaling predictors (e.g., standardizing them), imputing missing values (to increase the effective number of rows in the data), and  component analysis (to decrease the number of columns).

### resampling

We've considered one approach to avoiding chance-inflated models and prediction estimates, and that is the approach of 'holding out' test (and possibly validation) samples.

An extension of this approach is **k-fold cross validation**, in which the sample is divided into k (e.g., ten) parts, each of which is used as a validation sample in k different analyses.  To assess the overall performance of the model, the results of these are averaged.  This is a sophisticated and relatively easy to implement approach which can be used, for example, to assess the relative performance of different models, such as linear versus non-linear models 

![egoregressions2018](C:\Users\lanning\Dropbox\0DataSciLibArts\egoregressions2018.PNG)



Another approach to resampling is **bootstrapping**, in which model parameters (e.g., regression coefficients) are taken as the average of many (for example 1,000) analyses of subsamples of the data.  In the bootstrap, sampling is done with replacement.  The averaged coefficients arrived at using bootstrapping are less variable than the results of a single analysis.

When the bootstrap is applied to a set of training samples in the context of a decision-tree, the approach is called bootstrap aggregating or **bagging.** 

## approaches to ml

decision trees, boosting, and random forests

support vector machines

neural networks

