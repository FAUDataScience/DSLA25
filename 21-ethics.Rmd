---
title: "21-ethics"
author: "Lanning"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
knitr::opts_chunk$set(echo = TRUE, warn = FALSE, message = FALSE) 
```

# ethics: some topics for discussion

## status 40% {-}

There is much more that can be done, including links to articles in NYT on privacy as case studies. In addition, each of the examples at the end can be expanded upon.  Check citations. 

Primary sources @loukides2018ethics, @salganik2017bit



## discussion: should digital tracking be used to ... coronavirus...

## is open-source software secure?

## discussion: is open-source software secure?

Perhaps the most important feature of R is that it is open-source software. This is important not just because it saves you money, but because contributing to the world of R is an act of digital democracy. In using and contributing to the world of R we open up knowledge to others who may lack our privileges. R, like Android or Wikipedia, is a tool for all of us, maintained and continually improved upon by the crowd.

But is open-source software safe? More generally, in a data-dependent world, who should be the guardians of the code that connects us? 

**Securing the Internet of Vehicles**. To consider just one example, the computer systems in modern cars typically run millions of lines of code. As cars become increasingly autonomous, this complexity will only increase. (Incidentally, the Society of Automotive Engineers, or SAE, describes 6 levels of 'auto autonomy.' At this writing, the most sophisticated systems available to consumers, such as Tesla Autopilot, are at level 2. What lies ahead are cars which are self-driving on carefully selected, geo-fenced roads, and ultimately cars "[which can operate on any road... a human driver could negotiate](https://www.caranddriver.com/features/a15079828/autonomous-self-driving-car-levels-car-levels/)"). Our roads and highways will become an Internet of Vehicles (IOV), which will include not just connections between cars and an intelligent *cloud* 'above us' but also direct links between a distributed system of intelligent cars, stoplights, and road sensors in a *fog* 'around us' [@bonomi2012fog]. **Fog computing** and the IOV will reduce travel times and increase both fuel efficiency and automotive safety.

Obviously, there are **cybersecurity** concerns. While the prospects for [a chaotic, choreographed hack of hundreds of vehicles on the streets of Manhattan](https://www.youtube.com/watch?v=OvewYslou9g), such as that in the 2017 movie "The Fate of the Furious", are remote at best (or worst), there have been examples of "white-hat hackers" who have successfully infiltrated (and thereby helped secure) car information systems.

As the IOV develops, there will be vulnerabilities to privacy as well as safety, and the security of the system will be paramount. Different car manufacturers are taking different approaches to developing secure information systems, with many using a closed or proprietary approach. But the scope of the problem is so large that there is a movement towards pooling resources and encouraging collaboration among industry partners, academics, and citizen scientists in the development of an open-source autonomous driving platform, such as [Apollo](http://apollo.auto/). Perhaps counterintuitively, there may be significant security advantages to using source code that is open to all [@clarke2009open; @fitzgerald2016open].


## background

There are ethics guidelines in professional organizations of [computer scientists](https://www.acm.org/code-of-ethics) and [statisticians](https://www.amstat.org//asa/files/pdfs/EthicalGuidelines.pdf).  

But this is not enough - "It's not enough to say "don't be evil." You have to not be evil" (e.g., facebook's year in review).

Oaths (like the Hippocratic Oath) are less effective than checklists (Gawande, 2009).



## principles

Consent

Clarity: It's not enough that people agree to share their data, they must also understand what they are agreeing to.

Consistency (and trust): Organizations may have data stolen (SSNs, passwords), or may be sold to other organizations where the prior rules may be unenforceable.

Control (and transparency): I have the right to control my data / the General Data Protection Regulation in the EU.

**Consequences** (and harm): Includes protection of [children](https://en.wikipedia.org/wiki/Children's_Online_Privacy_Protection_Act) and rules against discrimination on the basis of genetic information. But these laws may not keep up with changes in technology. There are missteps, as when [Strava](https://www.wired.co.uk/article/strava-heat-maps-military-app-uk-warning-security) and Netflix shared their data which could be de-anonymized, but also examples of sharing of data in the public interest.

> Many data sets that could provide tremendous benefits remain locked up on servers. Medical data that is fragmented across multiple institutions limits the pace of research. And the data held on traffic from ride-sharing and GPS/mapping companies could transform approaches for traffic safety and congestion. But opening up that data to researchers requires careful planning.

Improvements in UX (user experience) provide users with short-term incentives and long-term costs...

## remedies

Here's a checklist proposed by @loukides2018ethics:

### a checklist for people who are working on data projects

❏ Have we listed how this technology can be attacked or abused? 

❏ Have we tested our training data to ensure it is fair and representative? 

❏ Have we studied and understood possible sources of bias in our data? 

❏ Does our team reflect diversity of opinions, backgrounds, and kinds of thought? 

❏ What kind of user consent do we need to collect to use the data? 

❏ Do we have a mechanism for gathering consent from users? 

❏ Have we explained clearly what users are consenting to? 

❏ Do we have a mechanism for redress if people are harmed by the results? 

❏ Can we shut down this software in production if it is behaving badly? 

❏ Have we tested for fairness with respect to different user groups? 

❏ Have we tested for disparate error rates among different user groups? 

❏ Do we test and monitor for model drift to ensure our software remains fair over time? 

❏ Do we have a plan to protect and secure user data?

### additional ideas

Integrate discussions of software security and ethics: Security vulnerabilities may be privacy threats. 

Go beyond simple oaths to more: [Google](https://ai.google/principles) has moved from "don't be evil" to a set of negative (dont's) and positive (do's) guidelines for work in Artificial Intelligence.  Is it enough?

Kairos, which develops face recognition software, has said that [they will not share their software with law enforcement agencies.](https://techcrunch.com/2018/06/25/facial-recognition-software-is-not-ready-for-use-by-law-enforcement/)

## some case studies

Here are a few examples from recent headlines:

1) In today's news, [health information of pregnant women is shared with their employers.](https://www.washingtonpost.com/technology/2019/04/10/tracking-your-pregnancy-an-app-may-be-more-public-than-you-think/)

2) Several years ago, [Facebook Year in Review](https://www.theguardian.com/technology/2014/dec/29/facebook-apologises-over-cruel-year-in-review-clips) ran in to problems.

3) Did Eventbrite step over the line here: [we have the right to record your event](https://arstechnica.com/information-technology/2018/04/eventbrite-rolls-back-policy-that-would-have-given-it-right-to-record-events/)?

Three more from [@salganik2017bit](https://www.bitbybitbook.com/en/1st-ed/ethics/three-examples/): 

4) The [emotional contagion study](https://doi.org/10.1073/pnas.1320040111) of Adam Kramer and others at Facebook.

5) A study which linked [Facebook profiles of Harvard students](https://doi.org/10.1007/s10676-010-9227-5) with their academic records without their consent.

6) A study which attempted to [monitor government censorship](http://techscience.org/a/2015121501/) by tracking web traffic 

For still further reflection, consider one or more of the [six hypothetical case studies](https://aiethics.princeton.edu/case-studies/case-study-pdfs/)  provided by Princeton's Center for Information Technology and Policy.



