---
title: "Data science for the liberal arts"
author: "Kevin Lanning"
date: "`r Sys.Date()`"
site: "bookdown::bookdown_site"
output: 
    bookdown::gitbook:
        config:
            fontsettings:
                theme: sepia
documentclass: "book"
bibliography: 
    ../9Bibliography/MyLibrary.bib
biblio-style: apalike
description: "Test"
link-citations: yes
github-repo: kevinlanning/DataSciLibArts
# classoption: "openany"
cover-image: chihuahuamuffin.png
# urlcolor: blue
---
# {-} 

<!--chapter:end:index.Rmd-->

# preface {.unnumbered}

This work-in-progress will ultimately serve as a textbook for introductory undergraduate courses in data sciences. No prior knowledge of computer programming is presumed, though, ideally, students will have had college algebra (or its equivalent) and an introductory course in statistics, methods, or data analysis.

Data science is still a new field of study, and there are multiple approaches to teaching it and to its place in the college curriculum. This book is intended to serve courses such as the [*Introduction to Data Science*](https://kevinlanning.github.io/DataSciSpring2020/) at the Wilkes Honors College of Florida Atlantic University which, in turn, has drawn from data science classes at the universities of [North Carolina](https://idc9.github.io/stor390/), [British Columbia](https://stat545.com/), [Duke](http://www2.stat.duke.edu/courses/Fall18/sta112.01/), [Maryland](http://www.hcbravo.org/IntroDataSci/calendar/), [Wisconsin](http://pages.stat.wisc.edu/~yandell/R_for_data_sciences/syllabus.html), [Stanford](https://github.com/dcl-2017-04/curriculum), [BYU](https://byuistats.github.io/M335/syllabus.html), [Harvard](http://datasciencelabs.github.io/), and [UC Berkeley](http://www.gastonsanchez.com/stat259/) At each of these schools, the Introduction to Data Science appears, to my eyes at least, closer to Statistics than to Computer Science.

But if our approach is closer to statistics than to programming, it is particularly close to statistics in its most applied and pragmatic form. The choice of statistical methods should follow from the data and problem at hand - that is, statistics should serve the needs of the user rather than dictate them [@loevinger1957]. This pragmatic focus is driving the growth of data science in industry, and it is reflected in the way data science has been taught at still other schools including [Chicago](https://github.com/UC-MACSS/persp-analysis), [Georgia Tech](https://github.com/jacobeisenstein/gt-css-class), [UC Santa Barbara](https://github.com/raviolli77/dataScience-UCSBProjectGroup-Syllabus), [Princeton](http://www.princeton.edu/~mjs3/soc596_f2016/), [UC Berkeley](https://github.com/rochelleterman/PS239T), at [Berlin's Hertie School of Governance](https://github.com/HertieDataScience/SyllabusAndLectures), and in [Columbia's School of Journalism](https://github.com/tommeagher/data1-fall2015).

## some features of the text {.unnumbered}

There are a number of different approaches to teaching data science. The present text includes several distinguishing features.

**R**

In a recent informal survey of introductory data science courses, I saw a pretty even split between those which begin with Python and those which begin with the statistical programming language R. This difference corresponds, very loosely, to the split noted above: Computer science based approaches to data science are frequently grounded in Python, while statistics-based approaches are generally grounded in R. Our course, like those for most of the syllabi and courses linked above, will be based in R.

**Reproducible science**

The course will provide an introduction to some of the methods and tools of reproducible science. We will consider the replication crisis in the natural and social sciences, and then consider three distinct approaches which serve as partial solutions to the crisis. The first of these is training in a notebook-based approach to writing analyses, reports and projects (using R markdown documents). The second is using public repositories (such as the [Open Science Framework](https://osf.io/) and, to a limited extent, [GitHub](https://github.com/)) to provide snapshots of projects over time. Finally, the third is to consider the place of significance testing in the age of Big Data, and to provide training in the use of descriptive, exploratory techniques of data analysis.

**Good visualizations**

Part of Type C data science is communication, and this includes not just writing up results, but also designing data displays that incisively convey the key ideas or features in a flood of data. We'll examine and develop data visualizations such as plots, networks and text clouds. More advanced topics may include maps, interactive displays, and animations.

~~**All**~~ ***Some of*** **the data**

It's been claimed that in the last fifteen years, humans have produced more than 60 times as much information as existed in the entire previous history of humankind. (It sounds like hyperbole, but even if it's off by an order of magnitude it's still amazing). There are plenty of data sources for us to examine, and we'll consider existing datasets from disciplines ranging from literature to economics to public health, with sizes ranging from a few dozen to millions of data points. We will also clean and create new datasets.

~~**All**~~ ***A few of*** **the latest tools**

One feature of Data Science is that it is changing rapidly. The tools, methods, data sources, and ethical concerns that face us in 2021 are different from those which shaped the field just one or two years ago.

In fields that are undergoing rapid change, there is some trade-off between building expertise with existing (older) tools and trying the newer approaches. Partly because I want to equip you with skills which will not be obsolete, partly because some of these new approaches promise more accessibility, elegance, and/or power, and partly because of my own interest in staying current, we'll be using some of the latest packages and programs.

In the last few years, I've shifted the class from the standard R dialect (as I learned it from the [Johns Hopkins-Coursera Data Science Specialization](https://www.coursera.org/specializations/jhu-data-science)) to the Tidyverse. This year, for the first time, much of our work with the R language will take place online, using [RStudio cloud](https://rstudio.cloud/), for this should help overcome some hiccups that arise when students are using different machines, operating systems, etc., and should facilitate collaboration among us as well. We'll also be experimenting for the first time with the [learnr](https://rstudio.github.io/learnr/) package, and emphasizing other approaches to learning R, including [Swirl](https://swirlstats.com/), less.

In the past, I've used the Slack platform for messaging, communication and collaboration; this year, I've somewhat reluctantly moved to the Canvas LMS. And, in the past, I've recommended using dedicated markdown editors such as [Typora](https://typora.io/). While I still think that these are great for some text-editing and note-taking, we'll do our work instead with the editor in the latest variant of RStudio on our laptops, as this allows WYSIWIG (what you see is what you get) formatting of documents - such as this one - that are intended as "publication-ready" texts.

We'll continue to use spreadsheets such as Excel or Google Sheets as well.

## the book is for you {.unnumbered}

It's my intention that this text should serve every college student, regardless of concentration or college major. The skills and insights that you will gain in this course will help you in graduate and professional schools, will help you in your careers, and will help you in your goal of making a better world. And it will help you train the next generation of data scientists as well.

<!--chapter:end:00-Preface.Rmd-->

# (PART) Introduction {.unnumbered}

# data science for the liberal arts

Hochster, in [@hicks2018], describes two broad types of data scientists: Type A (Analysis) data scientists, whose skills are like those of an applied **statistician**, and Type B (Building) data scientists, whose skills lie in problem solving or coding, using the skills of the **computer scientist**. This view arguably omits a critical component of the field, as data science is driven not just by statistics and computer science, but also by "domain expertise:"

![*Fig 1.1 - The iconic data science Venn diagram*](images/dataVenn.png)

## type C data science = data science for the liberal arts

The iconic [Venn diagram model of data science](https://www.google.com/search?q=venn+diagram+model+of+data+science&newwindow=1&safe=active&rlz=1C1CHBF_enUS762US763&tbm=isch&tbo=u&source=univ&sa=X&ved=0ahUKEwiM_abBtY7XAhXDQCYKHdgyB58QsAQIOg&biw=1378) shown above suggests what we will call "Type C data science." It begins with "domain expertise" in your **concentration** in the arts, humanities, social and/or natural sciences, it both informs and can be informed by new methods and tools of data analysis, and it includes such things as **communication** (including writing and the design and display of quantitative data), **collaboration** (making use of the tools of team science), and **citizenship** (serving the public good, overcoming the digital divide, furthering social justice, increasing public health, diminishing human suffering, and making the world a more beautiful place). It's shaped, too, by an awareness of the fact that the world and workforce are undergoing massive **change**: This puts the classic liberal arts focus of "learning how to learn" (as opposed to memorization) at center stage. And Type C data science is shaped, not least, by the **creepiness** of living increasingly in a measured, observed world.

Type C data science does not merely integrate 'domain expertise' with statistics and computing, it places content squarely at the center. We can appreciate the compelling logic and power of statistics as well as the elegance of well-written code, but for our purposes these are means to an end. Programming and statistics are tools in the service of social and scientific problems and cultural concerns. Type C data science aims for work which is not merely cool, efficient, or elegant but responsible and meaningful.

## the incompleteness of the data science Venn diagram

Data visualizations are starting points which can provide insights, typically highlighting big truths or effects by obscuring other, presumably smaller ones. The Venn diagram model of data science is no exception: As with other graphs, figures, and maps, it allows us to see by showing only part of the picture. What does it omit? That is, beyond **statistics**, **computing/hacking**, and **domain expertise**, what other skills contribute to the success of the data scientist?

The complexity of data science is such that individuals typically have expertise in some but not all facets of the area. Consequently, problem solving requires **collaboration**. Collaboration, even more than statistical and technical sophistication, is arguably the most distinctive feature of contemporary scholarship in the natural and social sciences as well as in the private sector [@isaacson2014].

**Communication** is central to data science because results are inconsequential unless they are recognized, understood, and built upon; facets of communication include oral presentations, written texts and, too, clear data visualizations.

**Reproducibility** is related to both communication and collaboration. There has been something of a crisis in recent years in the social and natural sciences as many results initially characterized as "statistically significant" have been found not to replicate. The reasons for this are multiple and presently contentious, but one path towards better science includes the public sharing of methods and data, ideally before experiments are undertaken. Reproducible methods are a key feature of contemporary data science.

**Pragmatism** refers to the relevance of work towards real-world goals.

These real-world goals should be informed by **ethical concerns** including a respect for the privacy and autonomy of our fellow humans.

## a dimension of depth

Cutting across these various facets (statistics, computing, domain expertise, collaboration, communication, reproducibility, pragmatism, and ethics), a second dimension can be articulated. No one of us can excel in all of these domains, rather, we might aim towards goals ranging from **literacy** (can understand) through **proficiency** (can get by) to **fluency** (can practice) to **leadership** (can create new solutions or methods).

That is, we can think of a *continuum* of knowledge, skills, interests, and goals, ranging from that which characterizes the data *consumer* to the data *citizen* to the data science *contributor.* A Type C data science includes this dimension of 'depth' as well.

## Google and the liberal arts

Data science is at its core empirical, and all of this rhetoric would be meaningless if not grounded in real world findings. Although it was reported in late 2017 that [soft skills rather than STEM training were the most important predictors of success among Google employees](https://www.washingtonpost.com/news/answer-sheet/wp/2017/12/20/the-surprising-thing-google-learned-about-its-employees-and-what-it-means-for-todays-students/?sw_bypass=true&utm_term=.23e48235d66e), it's difficult to know whether these results would generalize to a less select group. Nonetheless, there is a clear need for individuals with well-rounded training in the liberal arts in data science positions and, conversely, learning data science is arguably a key part of a contemporary liberal arts education.

## data sci and TMI

One difference between traditional statistics and data science is that the former is typically concerned with making inferences from datasets that are too *small*, while the latter is concerned with extracting a signal from data that is or are too *big* [@donoho2017].

The struggle to extract meaning from a sea of information - of finding needles in haystacks, of finding faint signals in a cacophony of overstimulation - is arguably the question of the age. It is a question we deal with as individuals on a moment-by-moment basis. It is a challenge I face as I wade through the many things that I could include in this class and these notes.

The *primacy of editing* or selection lies at the essence of human perception and the creation of art forms ranging from novels to film. And it is a key challenge that the data scientist faces as well.

## discussion: what will you do with data science?

Imagine it is ten years from today. You are working in a cool job (yay). How, ideally, would 'data science' inform your professional contributions?

More proximally (closer to today) - what are your own goals for progress in data science, in terms of the model described above?

<!--chapter:end:01-DataSci.Rmd-->

# getting started

------------------------------------------------------------------------

We begin with a brief self-assessment, asking you to reflect on your own knowledge of data science, including the necessary-but-not-sufficient areas of computer programming and statistics. We then move to a description of some rudimentary tools that we will be using.

## are you already a programmer and statistician?

Regarding **programming**, you may know more than you think you do. Here's a simple program - a set of instructions - for producing a cup of coffee:

> add water to the kettle and turn it on
>
> if it's morning, put regular coffee in the French press, otherwise use decaf
>
> if the water has boiled, add it to the French press, else keep waiting
>
> if the coffee has steeped for four minutes, depress (smash) piston/plunger, else keep waiting
>
> pour coffee into cup
>
> enjoy

As a post-millennial student from a WEIRD culture, or Western, Educated, Industrialized, Rich Democracy [@henrich2010], you've 'programmed' computers, too, if only to enter a password, open an app, and upload a photo on your cell phone.

**Statistics** is of fundamental importance, not just for understanding abstract trends, but for making decisions about everyday life. Consider the case of Susie, a college senior:

> **Exercise 2\_1** *Susie is applying to two med schools. At School A, 25% of students are accepted, and at School B, 25% are accepted as well. You are Susie. Are you going to get in to at least one of these programs? What is the probability? Does your estimate depend upon any assumptions?*

Questions such as these are important for us. If the combined probability is low, it will *likely* (another probability concept) make sense for Susie to spend time, money, and energy to apply to additional programs. If the probability is higher, it may not. But problems like this are hard - our estimates of probability are frequently poorly calibrated, and combining probability estimates is challenging (see, e.g., [@tversky1974], and consider taking a course in *Behavioral Economics* or *Thinking and Decision Making* to learn more).

You may have worked with data in spreadsheets such as Excel or Google Sheets.

> **Exercise 2\_2** Open the Google Sheet at <http://bit.ly/dslaX2_1>. Save a copy and edit it, entering the following in cell B7:
>
> *=SUM (B2:B6)*
>
> What is the result?
>
> Now **copy cell B7 to C7**
>
> What happens? Is this the result you expected? Would another approach be more useful?

## some best practices for spreadsheets

**Spreadsheets** are great tools - the first one, Visi-Calc, was the first "killer app" to usher in the personal computer revolution. But they have limitations as well. **Best practices** have been proposed for using spreadsheets in data science such as, for example, include only data (and not calculations) in spreadsheets, use what we will recognize as a 'tidy' format in which data are in a simple rectangle (avoid combining cells and using multi-line headers), and save spreadsheets as simple text files, typically in comma-delimited or CSV format [@broman2018]. Typically, this means that each worksheet will be in a separate file, rather than in workbooks with multiple tabs.

There are good reasons for these recommendations: For example, when we sort data in spreadsheets, we risk chaos, for example, only certain columns may be sorted. When we manipulate data in spreadsheets, we typically will not have a record of what was (and wasn't) changed, and this compromises the reproducibility of our work.

**The bottom line is that spreadsheets should generally be used to store data rather than to analyze it.** But don't be a slave to this - if you are making some quick calculations that are inconsequential and/or to be used on just one occasion, working in Excel or Google Sheets is often the way to go.

## setting up your machine: some basic tools

Collaboration and communication are integral to data science. In the world beyond universities, the most important messaging and collaboration platform is [Slack](https://slack.com/). Slack is a commercial app, but it has a free tier. Slack is handy for group work of all forms, but most students in full-time programs will be expected to use a Learning Management System such as Canvas so that all of their classes are on the same platform.

Documents produced in word processors typically include invisible characters used for formatting, security, etc. These characters, when fed into a computer program, lead to unpredictable results. *Markdown* is a simple syntax for producing documents that are easily read by both humans and machines. Most of the documents you will produce and work with this term, including the present chapter, are and will be written in some version of Markdown.

You can find an introduction to markdown syntax in Chapter 3 of [@freeman2017]. I use [Typora](https://www.typora.io/) (currently free for both Windows and Mac), but there are many alternatives. Install this or another Markdown editor on your laptop and play with it.

**Google Docs** is free and is convenient for collaborative work. One other important feature of Google Docs is that it provides a framework for *version control,* a critical skill in information management. You can learn more about how to see and revert to prior versions of a project in Google Docs [here](https://sites.google.com/site/scriptsexamples/home/announcements/named-versions-new-version-history-google-docs). Version control can help you avoid the chaos and confusion of having a computer (or several computers) full of files that look like Cham's (2012) comic:

![*Fig 2.1: Never call anything 'final.doc'.*](images/final.jpg)

Version control is an important concept in data science. Collaboratively built programs and platforms, including most of the add-ons (libraries, packages) which make R so powerful, are open-source projects built by many individuals over time. For projects such as these, both the current build and its associated history are typically maintained on [GitHub](https://github.com/), a website for hosting code. When we contribute to these projects, we will first mirror the web-based GitHub site using a program on our own Macs or Windows PCs called [Git](https://git-scm.com/), then upload our proposed changes. Keeping remote and local branches of files in sync can be challenging, however, and you will not be expected to use this technology in this class. But if you are curious, or want to learn more, an introduction to using Git and R together may be found [here](https://happygitwithr.com/).

## a modified 15-minute rule

You will run into problems, if not here, then elsewhere. An important determinant of your success will be the balance you maintain between persistence and help-seeking.

The 15-minute rule is one guideline for this balance: It has been cleverly summarized as "[You must try, and then you must ask](https://blogs.akamai.com/2013/10/you-must-try-and-then-you-must-ask.html)." That is, if you get stuck, keep trying for 15 minutes, then reach out to others. I think that this rule is basically sound, particularly if it is applied with cognitive flexibility, social sensitivity, and reciprocity. So when you get stuck, make a note of the problem, then move to another part of your project (that's the cognitive flexibility part): This allows your problem to percolate and still make progress. When you ask others for help, ask in a way that shows an awareness of the demands on their time (social sensitivity): Part of this means that you should explain your problem in as detailed a fashion as possible - in technical terms, a "reprex" or reproducible example. Finally, you should be willing to provide as well as give help (reciprocity).

## discussion: who deserves a good grade?

In an introductory class in data science, students invariably come to class with different backgrounds. Should this be taken into account in assigning grades? That is, would it be possible (and desirable) to assign grades in a class based not just on what students know at the end of the term, but also on how much they have learned?

A formal, statistical approach to this could use regression analysis. That is, one could predict final exam scores from pretest scores, and use the residuals - the extent to which students did better or worse than expected - as a contributor to final exam grades. Interestingly, there would be an unusual incentive for students on this 'pretest' to do, seemingly perversely, as poorly as possible. How could this be addressed?

Another problem with this approach is that there may be 'ceiling effects' - students who are the strongest coming in to the class can't improve as much as those who have more room to grow. Again, how might this be addressed? Should it?

<!--chapter:end:02-Setup.Rmd-->

# welcome to R world

In this chapter, we'll learn how to install and use R in two ways - in the cloud and on your own laptop. For each of these, we'll use RStudio as a front end (an 'integrated development environment', or IDE).

## Using RStudio cloud

Using the link provided to you by your instructor, set up a new account in RStudio (it is free for you). Then click on the Join Space button. It will bring you to the Welcome page. (Not much to see here as yet). Click on the [hamburger menu](https://en.wikipedia.org/wiki/Hamburger_button). You should see, on the left side of the screen, a menu with three sections: Spaces, Learn, and Help.

The **Spaces** menu will likely include three options. The first will be "Your Workspace," the second will be the workspace for this class (e.g., IntroDataSciSpring21), and the third will allow you to create a "New Space."

Click on the class workspace, then on the first assignment. After a few seconds, you'll see a screen that looks something like the following:

![Fig 3.1 - R Studio Cloud.](images/03-R_insertimage_4.png)

**Click on the assignment**, and you'll see that there are now four windows on the screen (note that these windows are fully customizable, so that the locations and characterizations I am giving here are just a starting point):

In the upper left is the **source** window - the code you are working on.. This is where we will be doing much of our work, typically in R markdown documents like this one. R markdown is, not surprisingly, a 'flavor' of markdown. Other tabs in this quadrant may include displays of your data and other scripts that you may be working on.

The **console**, by default in the lower left quadrant, can be used to execute single lines of code. You may occasionally use the terminal tab here as well.

The **environment**, in the upper right, includes datasets, variables, and functions that are available to us. The Build and Git tabs are useful for producing documents and version control/collaboration, respectively. The history tab is useful to go back and look at the syntax you used in a prior chunk of code.

The **files** in our directory are listed in the lower right; additional tabs include a list of available packages (libraries) and some resources for help.

Now read the code in the console, and follow the instructions to complete your first homework assignment. Congratulations, you've just run your first code in R.

## Using R Studio on your laptop

Install the latest version of [R](https://www.r-project.org/) on your own Windows or Mac laptop. Then install the [preview version of R studio](https://rstudio.com/products/rstudio/download/preview/), as this has a visual (WYSIWYG) editor that is a significant advance over the existing version.

**Feeling ambitious?** Once this is loaded, (a) set up a directory for this class, then (b) a subdirectory you can call Assignment01.

Go back to your Rstudio.cloud / Assignment01, and check the boxes next to Assiignment01.Rmd, movies.Rdata, and project.rproj. Then click on 'More', then 'Export...' and put these in your new subdirectory.

Now go back to your desktop environment, and try to run the same code on your home machine. (Note that to be successful, you will need to first execute the command on your laptop).

> install.packages("tidyverse")

------------------------------------------------------------------------

Get as far as you can, but remember the 15 minute rule. How far did you get? Be prepared to share your frustrations, discoveries, and accomplishments with your classmates.

<!--chapter:end:03-welcometoR.Rmd-->

# R stands for ...

Historically, R grew out of S which could stand for Statistics. But what does R stand for?

R is a system for **Reproducible** analysis, and reproducibility is essential. R markdown documents, like Jupyter notebooks in the Python world, facilitate reproducible work, as they include comments or explanations, code, links to data, and results.

R is for **Research.** Research is not just an end-product, not just a published paper or book:

> ... these documents are not the research [rather] these documents are the "advertising". The research is the "full software environment, code, and data that produced the results"

[@buckheit1995; @donoho2010, 385]. When we separate the research from its advertisement we are making it difficult for others to verify the findings by reproducing them [@gandrud2013].

R is a system for **Representing** data in cool, insight-facilitating ways, a tool for creating (reproducible) data visualizations which can provide insights and communicate results.

R is **Really popular**, and this matters, because learning R will make you a more attractive candidate for many graduate programs as well as jobs in the private sector.

Because R is popular, there are many **Resources**, including, for example -

-   Online resources include the simple (and less simple) lessons of [SwirlR](http://swirlstats.com/), which offers the possibility of "learning R in R," as well as [DataCamp](https://www.datacamp.com/home), the [Data Science Certificate Program at Johns Hopkins,](https://www.coursera.org/specializations/jhu-data-science) and other MOOCs.\
-   Books include [@peng2014] - which includes not only videos of his lectures in the program at Hopkins, but also a brief list of still more resources - and [@wickham2016].
-   You'll also learn (more directly) from people, including your classmates, as well as the broader community of people around the world. There are hundreds if not thousands of people, young and old, who are on the road with you. I am as well, just a step or two (hopefully) ahead.

R might stand for *Relatively high level.* Programming languages can be described along a continuum from high to low level, the former (like R) are more accessible to humans, the latter (like assembly language) more accessible to machines. Python, Java, and C++ are all more towards the middle of this continuum.

R does not stand for '**[arggh]**(<https://www.urbandictionary.com/define.php?term=ARGH>),' although you may proclaim this in frustration ('arggh, why can't I get this to work?) or, perhaps, in satisfaction ('arggh, matey, that be a clever way of doing this').

But R does stand for ***Rewarding***. A language is a way of thinking about the world, and this is true for computer languages as well. You'll be challenged by its complexity, its idiosyncracy, its alien logic. But you will succeed, and you will find that you can do things that you did not believe possible.

## a few characteristics of R

R includes the base together with **packages**. These packages (libraries) are customized add-ons which simplify certain tasks, such as text analysis. There are, at this writing, 15,373 available packages on the CRAN package repository - and though there is not yet an R package for ordering pizza [@peng2014], there are many for most data tasks, including, for example, [over 50 different packages for text analysis](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html). So how do you choose, and where do you begin? We will start with the curated list of packages which jointly comprise the **tidyverse** [@wickham2016], which is effectively a dialect of R.

R is an **object-oriented** language - one conceptually organized around objects and data rather than actions and logic. In R, at the atomic level, objects include *characters, real numbers, integers, complex numbers, and logical.* These atoms are combined into vectors, which generally include objects of the same type [one kind of object, 'lists,' is an exception to this; @peng2014]. Vectors can be further combined into **data frames**, which are two-dimensional tables or arrays. A **tibble** is a particular type of data frame which is used in the tidyverse. It is, in some ways, handier to work with than other data frames. We'll be working extensively with data frames in general, and tibbles in particular, as we move forward.

Objects have **attributes**. Attributes of R include such things as name, dimensions (for vectors and arrays), class (that's the type of object described in the previous paragraph), length, etc.

Real world data sets are messy, and frequently have **missing values.** In R, missing values may be characterized by NA (not available) or NaN (not a number, implying an undefined or impossible value).

**RStudio,** the environment we will use to write, test, and run R code, is a commercial enterprise whose business model, judged from afar, is an important one in the world of technology. Most of what RStudio offers is free (97% according to Garrett Grolemund in the video below). The commercial product they offer makes sense for a relative few, but it is sufficiently lucrative to fund the enterprise. The free product helps to drive the popularity of Rstudio; this widespread use, in turn, makes it increasingly essential for businesses to use. This mixed free/premium, or 'freemium,' model characterizes Slack as well, but while [the ratio of free to paid users of Slack is on the order of 3:1](https://www.statista.com/statistics/652779/worldwide-slack-users-total-vs-paid/), for R it is, I am guessing, an order of magnitude higher than this.

## finding help

**One does not simply 'learn R.'** Unlike, say, learning to ride a bicycle, fry an egg, or drive a car with a manual transmission, learning R is not a discrete accomplishment that one can be said to have mastered and from which one then moves on. Rather, R is an evolving, open system of applications and tools which is so vast that there is always more that one can achieve, new lessons that one can learn. And, the complexity of R syntax is such that, for almost all of us, we will need help for coding on any non-trivial task.

For us, the key ideas in "looking for help" will include not just the tools on the RStudio IDE, but also (a) using Google searches wisely, and (b) reaching out to your classmates on Slack.

Here, as in the real world, there is an etiquette for help-seeking which is based on consideration. Your search for help should begin by making sure that others will encounter the same result, then by stripping the problem down to its essence. Once you have reduced the problem to this *minimal, reproducible* essence, you will often be able to spot the problem yourself - and, if not, you will make it easier for others to help you. There is an R package (reprex) which will likely facilitate this, but I haven't tried it yet. [Here](https://www.tidyverse.org/help/) is a good introduction.

Again, remember the 15 minute rule - we all need [somebody to lean on](https://youtu.be/2YapAxPfRyI).

Finally, to get a sense of some of the ways you can get help in RStudio (and to see how a master uses the R Studio interface), consider this video:

[![](https://embed-fastly.wistia.com/deliveries/85f90f89c20cf329c8e6091508fe44c045e70167.jpg?image_play_button_size=2x&image_crop_resized=960x585&image_play_button=1&image_play_button_color=4287c7e0)](https://rstudio.com/resources/webinars/programming-part-1-writing-code-in-rstudio/?wvideo=k8kz4e0p2v)

*video 4.1: Garrett Grolemund of [RStudio](https://www.rstudio.com/)*

## Wickham and R for Data Science

[The first chapter of the Wickham text](https://r4ds.had.co.nz/introduction.html) [@wickham2016] provides both a framework for his approach and a brief introduction to the *tidyverse* which will be the dialect of R we will study in the weeks ahead.

Please read it now.

<!--chapter:end:04-RStandsFor.Rmd-->

# now draw the rest of the owl

![Fig 5.1: Draw the rest of the owl.](images/InkedrCr9A_LI.jpg)

There are many sources for learning the basics of R. A few of these follow. Please **spend at least 90 mins exploring at least two of the following.** Be prepared to discuss your progress next class (you will be asked which source(s) you used, what you struggled with, and whether you would recommend it to your classmates. (Note that all of these are free, though you may choose to make a donation to the author if you use the Peng text).

> **Hint:** If you find the material too challenging - if you feel like you are drawing the rest of the owl - take a break away from your machine and other screens, clear your head, then try a different approach.

## Review the RStudio cloud primers

A series of up-to-date, interactive tutorials may be found for working with RStudio.cloud [here](https://rstudio.cloud/learn/primers). There are two basic tutorials - one on data visualization, the other on programming.

## Play with RStudio on your laptop

Last week, we began exploring [Mine Çetinkaya-Rundel](http://www2.stat.duke.edu/~mc301/)'s movies dataset in RStudio.cloud. Were you successful in getting this to run on your laptop as well?

### Create a new R Markdown document and knit it to a PDF or a Word doc

In *literate programming*, comments, code, and results are integrated in a clear and reproducible way - they *document* our work. R markdown is built around this idea. To create an R markdown (Rmd) document in Rstudio (on your laptop) or Rstudio.cloud, begin with (**File -\> New File -\> R Markdown**). A window will open up with a file that begins with some *YAML* (Yet Another Markdown Language). You can edit this as needed:

    ---
    title: "Yo movies in R Markdown Document"
    author: "Frankie McFrank Frank"
    date: "1/27/2020"
    output: html_document
    ---

Go ahead and click on the clever "knit" icon in the bar just above the source window to create a sample document. You'll need to save the file with a new name, then R will finally create an HTML (Hyper Text Markdown Language - see the pattern?) page. Compare the R Markdown document (your code) with the result (the HTML). Now, try to knit the document into different output formats (click on the down arrow next to knit, and see if you can knit to PDF and Word). It may take a while the first time you do this, as you may need to load some additional formatting tools or packages (e.g., TinyTex) to successfully render these files.

> Can you knit directly to a PDF document on your laptop? A Word document? If you struggle, you are not alone here - an advantage of RStudio.cloud over RStudio on the desktop is that, on your own machine, you may run into system-specific problems associated with the pandoc document converter.
>
> If, after spending 15 minutes on this problem, you cannot knit directly to a PDF, stop and think about the problem.

### Play with and explore the movies data

Iain Carmichael used these data to introduce R for his students several years ago; you may wish to use his [code](https://idc9.github.io/stor390/notes/getting_started/getting_started.html) as a starting point. Continue to ask interesting questions about the data - can you see any way in which movies appear to have changed over time? What does "spilled" mean? Write up your results as an R markdown document.

## Take a DataCamp class

Many folks swear by (and others, I presume, at) DataCamp, which kind of gamifies learning software. As a student in this class, you have access to all of their stuff... free. You can even do lessons on your phone. Use the link given to you in class to enroll.

## Swirl (Swirlstats)

I, like thousands of others, learned R in the process of completing the Johns Hopkins [Data Science Specialization](https://jhudatascience.org/courses.html) offered through [Coursera](https://www.coursera.org/). The sequence can be challenging, but their introduction to R used an accessible, interactive R package called *Swirl.* You can read about swirl ("learn R in R") at <https://swirlstats.com/>.

**Using Swirl.** After loading R (and opening R studio), you will get to the Swirl lessons with the following steps:

1)  Install the Swirl package on your computer (you only need to do this once). Type the following into your console window in R studio (typically left hand side of your screen or lower left)

> install.packages("swirl")

2)  Then load the package into your workspace (you'll need to do this at the beginning of every session you use Swirl)

> library (swirl)

2)  Then run it!

> swirl ()

Swirl will ask a few questions then give you the option of choosing one of several courses. You'll choose the R Programming option, which leads to 15 separate lessons.

At the end of each lesson, you'll be asked

> *Would you like to receive credit for completing this course on Coursera.org?*

Answer no... then do another lesson.

## Read Peng's text and/or watch the associated videos

Finally, consider the text and videos from the Coursera R class. Most of the material from that class can be found in the Peng text [-@peng2014]. A slightly updated version of the text can be found at <https://bookdown.org/rdpeng/rprogdatascience/>, and the videos in the series may be found by clicking on the following:

[![Roger Peng](https://img.youtube.com/vi/wy0h1f5awRI/0.jpg)](https://youtu.be/wy0h1f5awRI).

*Video 5.2: Roger Peng introducing R*

## Code along with a tidy webinar, or attend a virtual conference

Among these [resources for R studio beginners](https://education.rstudio.com/learn/beginner/), you will find [Thomas Mock's introduction to Tidy statistics in R](https://rstudio.com/resources/webinars/a-gentle-introduction-to-tidy-statistics-in-r/). It includes a link to the code used in the seminar at the bottom of the launch screen.

The second chapter of [Healy's online book about Data visualizations](https://socviz.co/) provides an introduction to R and R studio which largely parallels the discussions here and in the [Wickham and Grolemund text](https://r4ds.had.co.nz/index.html) [@healy2017; @wickham2016]

The Grolemund video linked in the prior chapter is part of [a series of videos on programming in RStudio](https://rstudio.com/resources/webinars/programming-part-1-writing-code-in-rstudio/?wvideo=k8kz4e0p2v). It's a little out of date now (from 2015), but still likely to be useful.

Most of the offerings at the Rstudio::global conference are aimed at more experienced R users, some are aimed at (relative) noobs, including [Learning R with Humorous Side Projects](https://gbr01.safelinks.protection.outlook.com/?url=https%3A%2F%2Finfo.rstudio.com%2Ff00xY0S0YZ017X2eNCNtb0r&data=04%7C01%7Cdavid.a.egan%40merseyside.police.uk%7C0bcfc46873594beb93b408d8b7002ba3%7Cf3955ea24c5d4e27ab8df6f577fa122d%7C0%7C0%7C637460558642128429%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=m1xysQw%2F9Y1VwGVgSa0Q0DS2Y2UAdCbtE245ccMo1GE%3D&reserved=0). The 2021 conference, to be held on January 21, 2021, is free; you can enroll and explore [here](https://global.rstudio.com/student/catalog)

## Something else

The "something else" category includes Datacarpentry.org, which is aimed at fostering data literacy and provides free lessons for learning and applying R in areas such as [Ecology](https://datacarpentry.org/R-ecology-lesson/), [Genomics](https://datacarpentry.org/genomics-r-intro/) and [Geospatial data analysis](https://datacarpentry.org/r-intro-geospatial/). Of particular interest are the [social science](https://datacarpentry.org/r-socialsci/) lessons, as well as a workshop in data science based on the "[Studying African Farmer-led Irrigation (SAFI)" dataset](https://datacarpentry.org/socialsci-workshop/).

<!--chapter:end:05-owl.Rmd-->

# (PART) Part II Towards data literacy {.unnumbered}

# principles of data visualization

## some opening thoughts

Graphs aren't just to inform, but to make you reflect.

We are concerned not just with the design of graphs, but with our role in society as leaders, as arbiters of facts, as educators. We want to foster data literacy.

How do people understand graphs? How can you use graphs to tell people the truth in a way that they understand? How can you convey uncertainty in a graph?

When you see a graph, what do you notice, what do you wonder, and what is the story? Is "story telling" what visualizations should be about?

A graph might begin with perception and understanding (the consumer), with knowledge and design values (the producer), but it also reflects the truth of the data. How much is each?

## some early graphs

Visual displays of information reach back to prehistoric times; modern graphs date back, arguably, to [Playfair's 1786 Political Atlas](https://robots.thoughtbot.com/analyzing-minards-visualization-of-napoleons-1812-march) - in which

> "... spatial dimensions were used to represent nonspatial, quantitative, idiographic, empirical data. Although it now seems natural to represent, for example, rising and falling imports over time as a rising and falling line, it does not seem to have been done before that time and was quite an accomplishment. Notably, in addition to the statistical line chart, Playfair at one fell swoop single-handedly invented most of the remaining forms of the statistical graphic repertoire used today-the bar chart and histogram, the surface chart, and the circle diagram or "pie chart" [@wainer1981].

![*Fig 6.1: Playfair's 1786 analysis of trade deficits*](images/playfair1786.PNG)

------------------------------------------------------------------------

The most celebrated early graph is that of [Minard](https://datavizblog.com/2013/05/30/dataviz-history-charles-minards-flow-map-of-napoleons-russian-campaign-of-1812-polotsk-smolensk-and-on-to-borodino/):

![*Fig 6.2: Minard's display of Napoleon's catastrophic assault on Moscow, 1812*](images/minard1812.PNG)

The visualization depicts the size, latitude, and longitude of Napoleon's army as they moved towards (tan line) then away (black line) from Moscow; temperature during the retreat is plotted as well. Further, vertical lines linking the temperature display to the number of troops indicate the often perilous river crossings which further decimated Napoleon's troops). [Cheng (2014)](https://robots.thoughtbot.com/analyzing-minards-visualization-of-napoleons-1812-march) decomposes the graph and provides some simpler visualizations; she also provides the following background:

> "*Czar Alexander of Russia sees that Napoleon was becoming too powerful, so he refuses to participate in this embargo [against the UK]. Angry at Czar Alexander's decision, Napoleon gathers a massive army of over 400,000 to attack Russia in June of 1812. While Russia's troops are not as numerous as France's, Russia has a plan. Russian troops keep retreating as Napoleon's troops move forward, burning everything they pass, ensuring that the French forces could not take anything from their environment. Eventually the French army follows the Russian army all the way to Moscow during October, suffering major losses from lack of food. By the time Napoleon gets to Moscow, he knows he has to retreat. As winter settles into Europe and the temperature drops, Napoleon's troops suffer even more losses, returning to France from lack of food, disease, and weather conditions."*

Of course, the casualties and retreat of Napoleon's army are immortalized not just in this graph, but also in Russian literature (Tolstoy's *War and Peace*) and music (Tchaikovsky's 1812 overture, in which five cannon shots mark the battle of Borodino and eleven more mark the arrival of Napoleon in the now-razed city of Moscow).

## Tukey and EDA

For [@donoho2017], the publication of John Tukey's "Future of Data Analysis" [@tukey1962] arguably marks the beginning of data science. As the first data scientist, Tukey embraced a descriptive and exploratory approach to data analysis, particularly in his publication of *Exploratory Data Analysis* [@tukey1977].

In EDA, Tukey presented an idiosyncratic, coherent approach to looking at data, beginning with tallying. The distributions of small counts of single variables, for Tukey, could best be presented in **stem and leaf displays.** Comparisons between groups can be presented in **box plots.** To examine relationships between variables and the adequacy of simple models, he argues for thoughtfully transforming data to uncover linear trends, then to examine **residuals** to find where these trends do not hold.

## approaches to graphs

A graph might begin with perception and understanding (the consumer), with knowledge and design values (the producer), but it also reflects the truth of the data. How much is each?

In thinking about how to design graphs, we can begin with abstract theory, with principles of design informed by our understanding of perception, or with empirical analyses of understanding and memory.

## Tufte: first principles

[@tufte2001] describes **Graphical Excellence**. Graphs should, among other things, "Induce the viewer to think about the substance, rather than about methodology, graphic design, the technology of graphic productions, or something else." Graphs should "Present many numbers in a small space, make large data sets coherent, and encourage the eye to compare different pieces of data." Graphs should "serve a reasonably clear purpose: description, exploration, tabulation, or decoration [and] be closely integrated with the statistical and verbal descriptions of a data set." Tufte concludes with the following Principles of Graphical Excellence, which I quote verbatim:

-   Graphical excellence is the well-designed presentation of interesting data---a matter of substance, of statistics, and of design.
-   Graphical excellence consists of complex ideas communicated with clarity, precision and efficiency.
-   Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space.
-   Graphical excellence is nearly always multivariate.
-   And graphical excellence requires telling the truth.

## the politics of data visualization

On a cold morning in January, 1986, Space Shuttle Challenger lifted off from Cape Canaveral. Because of the cold weather, engineers at Morton Thiokol, who designed the rocket boosters, considered the possibility that the O-rings which sealed joints on the rockets would be too hard and brittle to prevent the release of potentially explosive fuel. They examined the relation between temperature and o-ring damage on prior flights, using the following visualization:

![*Figure 6.3: What Morton Thiokol engineers saw prior to deciding to launch the Challenger Space Shuttle in January, 1986*](images/mortonthiokolChallenger.PNG)

What, if anything, can we see here? The graphic was chaotic and poorly designed; if the engineers had wanted to systematically explore the relation between o-ring damage and temperature they could have removed the rockets and arranged them on a single axis by liftoff-temperature rather on several rows by liftoff-date. Here's what they would have seen:

![*Figure 6.4: What the engineers could have seen, perhaps, with a better graph*.](images/tufteChallenger.PNG)

The forecast for the morning of the launch was far colder than that for prior launches, and within the set of prior launches a clear relationship between temperature and o-ring damage is evident. But the Challenger did launch, exploding shortly after takeoff, killing the seven astronauts on board. With a clearer display of the data, the Challenger launch would likely have been postponed [@tufte2001].

### poor design leads to an uninformed or misinformed world

In the flood of information that surrounds us, our peripheral and sometimes our focal attention will be drawn to pictorial summaries of consequential events and effects. But while data visualizations and data journalism has become more responsible, poorly designed graphs - what Tufte has described as "chartjunk" - are still common. Consider the following graph, which recently appeared in a number of newspapers or feeds in the United States (including the *Palm Beach Post,* on January 12, 2019).

![*Figure 6.5: Which smartphone manufacturers are doing well?*](images/badgraph.PNG)

> **Exercise 6.1** *Examine the graph shown above.*
>
> *Look at it for just a moment, as if you might while reading something else on the screen, or as your ten-year old little sister might. What does the graph tell you?*
>
> *Look at it more closely. What does it actually say? How could you improve it to make the content of the graph more informative about its title?*
>
> *Why was the graph designed in this way?*
>
> *Does this matter?*

Poorly designed graphs don't just confuse rocket scientists. They reach us, mislead us, and make fools of all of us as well. With better graphs, we become a better informed, better educated citizenry.

### poor design can be a tool to deceive

![*Figure 6.6: Trump as "the first datavis President" ([Meeks, 2019](https://datastori.es/152-year-in-review-2019/#t=1:15:06.516)).*](images/trumpDorian.jpg)

The apps on our smartphones and electronic devices are meticulously designed to maintain our attention. They are largely successful, but, for many of us, made it more difficult to read in depth (Wu, *The Attention Merchants*). With information all around us demanding our attention, visual representations of data have become particularly important. The data visualization is, increasingly, not just a supplement to the story, but the story itself. When this premise is coupled with the idea that the truth is malleable, the product of what our leaders say rather than empirical statements of fact, we approach a dangerous situation, one in which real threats are misrepresented and lives are again endangered.

Presenting information in self-promoting ways includes so-called "Sharpie-gate," where President Trump simply altered a hurricane prediction map in defense of a misstatement. It also includes more subtle misrepresentations: Does stand-your-ground really make us safer?

![*Figure 6.7: Stand your ground makes us all safer. Or not.*](images/standground.PNG)

## the psychology of data visualization

Speaking of America, consider the following:

![*Figure 6.8: Chernoff's too-clever faces*](images/Chernoff.PNG)

In this figure, from [@wainer1981], (Chernoff's) faces are used to represent multivariate data. The display is loosely based on the (psychological) premise that, from infancy, we are inordinately sensitive to facial features. Consequently, the most powerful, effective, or informative data displays should manipulate facial features. Does this graph succeed? Why or why not? Would an updated model, based on richer, more complex, and more realistic simulations of facial expressions [@thies2015] be more successful?

### the power of animation

Animated data displays bring the dimension of time into data visualization. Here are two brief (\< 5 minutes) animated data displays, each powerful in its own way, and each of which gives the viewer an appreciation of an important social phenomenon.

The first is from the late Hans Rosling, and summarizes a chunk of the Gapminder data. There is a Gapminder package in R if you would like to explore the data further, and perhaps become the next Rosling:

[![](https://img.youtube.com/vi/jbkSRLYSojo/0.jpg)](https://youtu.be/jbkSRLYSojo)

*Video 6.9: Rosling and social progress*

The second is from Kim Rees and her ex-colleagues at [Periscopic](https://periscopic.com/) (Rees is now at CapitalOne). For me, it's an important graphic because it tries to overcome what has been called "psychic numbing" - we are sensitive to the individual case, but lose our perspective when faced with large-scale tragedies, so that effectively the more lives lost... the less we care [@slovic2013].

[![](https://img.youtube.com/vi/8R8UOjMy-5k/0.jpg)](https://youtu.be/8R8UOjMy-5k)

*Video 6.10: Rees and stolen years*

### telling the truth when the truth is unclear

We live in an uncertain world, and a major challenge in data visualization is how to convey this uncertainty in a way that people understand readily. Examples of this are familiar. Here in Florida, projected hurricane paths are frequently represented by a "cone of uncertainty" surrounding a single track. An alternative approach is to use a display which shows a distribution of possible tracks.

![*Figure 6.11: Two approaches to displaying hurricane paths*](https://i1.wp.com/datastori.es/wp-content/uploads/2019/01/02.jpg?w=650&h=434)

### visualizing uncertainty

To further illustrate the idea of uncertainty, data visualizations may be animated. To display hurricane outcomes, for example, individual lines in a spaghetti plot might fade in and out [@cox2013a]. Another use of animation is suggested by [@hullman2015] who use [hypothetical outcome plots](https://cdn-images-1.medium.com/max/600/1*vol7-537cqnpucRgBP-j9A.gif) rather than simple error bars to illustrate sampling distributions. Empirically, subjects have a better understanding of the meaning of statistical effects when these are represented by dynamic displays.

During the 2016 US Presidential Election, the NY Times graphics presented the range of likely possibilities (here, between the 25th and 75th percentiles) as a dynamically changing ["jittery gauge"](https://www.vis4.net/blog/images/old/jitter4.gif) . Following the election, there was some criticism of these gauges, as readers claimed that they tended to make them anxious. However, it appears that at least some of the anxiety was attributable not to the gauges themselves, but to the [electoral outcome itself](https://www.vis4.net/blog/2016/11/jittery-gauges-election-forecast/). The gauges were back in 2018, and will likely be used again in the future.

Finally, animation may be used not just to display uncertainty or a possible range, but also data displays which both carry a great deal of information and unfold over time, such as [the language of personality development](http://wise.fau.edu/~lanning/EgoDevelopmentsmallest.gif) in my own work.

## further reading and resources

If you'd like to learn more, [@tufte2001] and his other books are beautiful and thought provoking. [@cleveland1985] examines graphs from a more rigorous psychological and empirical viewpoint. The Data Stories podcasts are often excellent, despite the challenge of an auditory medium for visual display ([especially the episode on Hans Rosling](http://datastori.es/92-a-tribute-to-hans-rosling/)). And [@healy2018] provides a comprehensive introduction that, in essence, provides the basis for a semester-long course based in the ggplot2 package in R that we will be working with in the next chapter [@wickham2016].

Finally, several years ago Alberto Cairo of the University of Miami offered an online MOOC (massive open online course) on data visualizations. You can find the course materials [here](https://journalismcourses.org/course/data-visualization-for-storytelling-and-discovery/).

<!--chapter:end:06-Visualization.Rmd-->

# visualization in R with ggplot

In the last chapter, we introduced data visualization, citing "vision-aries" including Edward Tufte and Hans Rosling, inspired works such as Minard's *Carte Figurative* and Periscopic's *stolen years*, as well as a few cautionary tales of misleading and confusing graphs.

Here, in playing with and learning the R package **ggplot**, we begin to move from consumers to creators of data visualizations.

As the first visualization in [@wickham2016] reminds us, data visualization is at the core of exploratory data analysis:

![Fig 7.1: Data visualization is at the core of data analysis ([@wickham2016])](images/dataviscycle.PNG)

In the world of data science, statistical programming is about discovering and communicating truths within your data. This **exploratory data analysis is the corner of science, particularly at a time in which confirmatory studies are increasingly found to be unreproducible**.

Most of your reading will be from Chapter 3 of [@wickham2016], this is intended only as a supplement.

## a picture \> (words, numbers)?

The chapter begins with a quote from John Tukey about the importance of graphs. Yet there is a tendency among some statisticians and scientists to discount graphs, to consider graphic representations of data as less valuable than statistical ones. It is true that, because there are many ways to graph data, and because scientists and data journalists are humans with pre-existing beliefs and values, a graphical displays should not be assumed to simply depict a singular reality. But the same can be said about statistical analyses (see Chapter 8).

To consider the value of statistical versus graphical displays, consider 'Anscombe's quartet' (screenshot below, live at <http://bit.ly/anscombe2019>):

![Table 7.1: An adaptation of Anscombe's "quartet" [@anscombe1973]](images/spreadsheet61.PNG)

> **Exercise 7\_1** *Consider the spreadsheet chunk presented above, which I am characterizing as data collected on a sample of ten primary school children at recess on four consecutive days. Working with your classmates, compute the mean, standard deviation, and correlation between the two measures for one day. Share your results with the class.*

The four pairs of variables in [@anscombe1973] appear statistically "the same," yet the data suggest something else. Later, we'll try to plot these. Perhaps graphs can reveal truths that statistics can hide.

> **Exercise 7\_2** *The anscombe data is included as a library in R. Can you find, load, and explore it?*

## Read Hadley ggplots

In class, we will review and recreate the plots in section 3.2 of [@wickham2016] and exercises through 3.4.

Savor this section, reading slowly, and playing around with the RStudio interface. For example, read about the mpg data in the 'help' panel, pull up the mpg data in a view window, and sort through it by clicking on various columns.

![Fig. 7.2: A screenshot from RStudio, showing the mpg dataset](images/rstudio62.PNG)

## exploring more data

Explore the Gapminder data <https://cran.r-project.org/web/packages/gapminder/README.html>

Choose one of the datasets in R, pull out a few variables, and explore these.

Try to make a cool graph - one that informs the viewer, and, to paraphrase Tukey, helps us see what we don't expect.

Try several different displays. Which fail? Which succeed? **Be prepared to share your efforts.**

Don't be afraid to screw up. Each mistake you wisdom.



<!--chapter:end:07-ggplot.Rmd-->

# examining local COVID data in R

In Winter 2020, as the COVID-19 pandemic began to take hold, I began chronicling the course of the virus. What follows is an archive, followed by a consideration of some possible new directions.

## tracking the Novel Coronavirus (from Feb 2020)

Here, I want to consider a timely (but challenging) dataset.

The Novel Coronavirus is an emerging health crisis, particularly in Wuhan (a Chinese city larger than New York City) and the surrounding province of Hubei. **It is not yet a threat in the United States - there have, at this writing (02/11/20) been zero cases in Florida.** Still, tracking the spread of the virus - the unfolding number of people infected and recovered, as well as the number of deaths - is a fascinating exercise.

This is an educational script for students learning R with the Tidyverse. It reads data provided by the Johns Hopkins Center for Systems Science and Engineering (JHU/CSSE).

It was modified February 3 because of a new GoogleSheet link and altered variable names, on Feb 5 because of a new URL for the data and additional changes in the variable name for date, and Feb 7 to (a) remove need for OAuth and (b) separate Wuhan from 'other China.' On Feb 9, additional data cleaning was performed and interactive plots were added. On February 11, the code was rewritten to read files from a Github repo rather than Google Sheets. Consequently, this does not use an API or require authorization from Github.

```{r coronasetup, message = FALSE}
library(tidyverse)
library(magrittr)
library(lubridate)
library(htmlwidgets)
library(httr)
library(purrr)
# get list of files
filelist <- GET("https://api.github.com/repos/CSSEGISandData/2019-nCoV/git/trees/master?recursive=1") %>% 
  content() %>% 
# there is probably a more efficient way to reduce this
# list to a set of filenames
  flatten() %>% 
  map ("path") %>%
  flatten() %>%
  tibble() %>% 
  rename(filename = 1) %>% 
  filter(str_detect(filename,".csv") &
           str_detect(filename,"daily")) 
nsheets <- nrow(filelist)
rawGitFiles <- "https://raw.githubusercontent.com/CSSEGISandData/2019-nCoV/master/"
```

### reading the data (Feb 2020)

The Novel Coronavirus data consists of a series of csv files in a Github repository. This combines them into a single sheet in R.

```{r readcoronadata, message = FALSE}
# variables to retain or create
numvars <- c("Confirmed", "Deaths", "Recovered")
varlist <- c("Province/State", "Country/Region",
             "Last Update", numvars) 
# one cool trick to initialize a tibble
coronaData <- varlist %>%
     map_dfr( ~tibble(!!.x := logical() ) )
# add data from files to tibble
for (i in 1:nsheets) {
  j <- read_csv(paste0(rawGitFiles,filelist$filename[i]))
# if a variable doesn't exist in sheet, add it
  j[setdiff(varlist,names(j))] <- NA
# datetime is formatted inconsistently
# across files, this must be done before merging  
  j %<>% mutate(`Last Update` = 
           parse_date_time(`Last Update`,
                           c('mdy hp','mdy HM',
                             'mdy HMS','ymd HMS'))) %>% 
  select(varlist) 
  coronaData <- rbind(coronaData, j) 
}
head(coronaData)
str(coronaData)
```

### cleaning (wrangling, munging) the data (Feb 2020)

Cleaning the data includes not just finding "errors," but adapting it for our own use. It's generally time consuming, as was the case here. The following letters refer to sections of the code below.

-   a - fix a few missing values outside of China for province and country
-   b - the earliest cases, all in China, did not include country
-   c - because province/state is included inconsistently, an unambiguous place variable is created
-   d - reportdate is standardized (above) and renamed
-   e - in some cases, multiple reports are issued for each day. only the last of these is used for each place.
-   f - for dates where no data was supplied, the most recent (previous) data are used
-   g - values of NA for Deaths, Confirmed, and Recovered cases are replaced by zero.
-   h - Prior to Feb 1, 2020 reporting for US included only state, since then, city and state. This drops the (duplicated) province/state-only values beginning Feb 1.

```{r cleaningcorona}

coronaData <- coronaData %>% 
# a 
  mutate (`Province/State` = case_when(
    (is.na(`Province/State`) & 
       (`Country/Region` == "Australia")) ~ "New South Wales",
    (is.na(`Province/State`) & 
       (`Country/Region` == "Germany")) ~ "Bavaria", 
    TRUE ~ `Province/State`)) %>% 
  mutate (`Country/Region` = case_when(
    `Province/State` == "Hong Kong" ~ "Hong Kong",
    `Province/State` == "Taiwan" ~ "Taiwan",
    `Province/State` == "Washington" ~ "US",
# b
    is.na (`Country/Region`) ~ "Mainland China",
    TRUE ~ `Country/Region`)) %>% 
# c 
  mutate(place = ifelse(is.na(`Province/State`),
                        `Country/Region`,
                        paste0(`Province/State`,", ",
                               `Country/Region`))) %>% 
  mutate(reportDate = 
           date(`Last Update`)) %>% 
  group_by(place,reportDate) %>% 
# e
  slice(which.max(`Last Update`)) %>% 
  ungroup() %>%
# fill in missing dates for each place for time series
# f
  group_by(place) %>% 
  complete(reportDate = seq.Date(min(reportDate),
                                 today(),
                                 by="day")) %>% 
  fill(c(Confirmed,Deaths,Recovered,
         `Country/Region`,`Province/State`)) %>% 
# g
  ungroup() %>% 
  mutate_if(is.numeric, ~replace_na(., 0)) %>% 
# h
  mutate(dropcase = ((!str_detect(`Province/State`,",")) & 
                       (reportDate  > "2020-01-31") &
                       (`Country/Region` == "Canada" | `Country/Region` == "US"))) %>% 
# dplyr called explicitly here because plotly has taken over 'filter'
  dplyr::filter (!dropcase) %>% 
  select(-c(`Last Update`,`Province/State`,`Country/Region`,dropcase)) 
head(coronaData)
write_csv(coronaData,"coronaData.csv")
```

### eleven months later: the code still runs!

The above code runs without apparent error, and leads to a dataset of `r nrow(coronaData)` rows by `r ncol(coronaData)` columns. Here's a quick peek (Note that I use the group\_by and summarize functions to collapse the file to one line per date):

```{r}
coronaData %>% group_by(reportDate) %>% 
  summarize(Confirmed = sum(Confirmed)) %>% 
  head() 
```

### shall we graph it? (Feb 2021)

So far, so good. Let's plot the whole range. What do we see? This will generate a quick graph in ggPlot which shows the global incidence of confirmed cases.

```{r}
coronaData %>% 
  group_by(reportDate) %>% 
  summarize(Confirmed = sum(Confirmed)) %>% 
  ggplot(aes(x=reportDate)) +
  geom_line(aes(y=Confirmed)) 
```

### too bad

According to the above graph, there have been no additional COVID cases since mid-March or so. Too bad that the data aren't right here - for us and especially the world.

> **Exercise 8\_1** *How would we find the exact date when the file stopped updating? In class, we'll consider this question (we'll use pre-downloaded data to save time and computer resources). Use whatever method you like for this - kludgy or clever - but you should describe your results in explicit language or code so that anyone else would get the same results.*

### what now?

I want to make sure that the rest of my code still works, so I'll look more closely at the valid data. I'll generate a new variable ('firstbaddate,' which sounds like a corny romcom), and filter by it.

When the code was written a year ago, the COVID outbreak was largely contained to the Hubei province (which includes the city of Wuhan). So I tried breaking down the data into three locations, breaking down China into Hubei, other China, and the rest of the world.

And I generated summaries for three measures - Confirmed, Deaths, and Recovered cases

```{r threelocationscorona}

firstbaddate <- '2020-03-22' # This line is added in 2021
coronaDataSimple <- coronaData %>% 
  filter(reportDate < firstbaddate) %>%  # added in 2021
  mutate(country = case_when(
    str_detect(place,"China") ~ "China",
    TRUE ~ "Other countries")) %>% 
  mutate(location = case_when(
    place == "Hubei, Mainland China" ~ "Hubei (Wuhan)",
    country == "China" ~ "Other China",
# what happens when this line is not commented out?
# why is it written this way?
# str_detect(place, "ruise") ~ "Cruise Ship", 
    TRUE ~ "Elsewhere")) %>% 
  group_by(location,reportDate) %>% 
  summarize(Confirmed = sum(Confirmed),
            Deaths = sum(Deaths),
            Recovered = sum(Recovered)) %>% 
  ungroup()
```

### an initial plot (Feb 2020)

The first plot is simple, including data for only deaths. A caption is added to show the source of the data. Here's what the data looked like last February 11:

```{r simpleplot}
myCaption <- " Data courtesy JHU/CSSE http://bit.ly/ncvdata"
oldData <- coronaDataSimple %>% 
  filter(reportDate < '2020-02-12')
coronaPlot0 <- oldData %>% 
#  filter(reportDate < '2020-02-12')
  ggplot(aes(x=reportDate)) +
  geom_line(aes(y=Deaths, color = location)) +
  labs(caption = myCaption)
coronaPlot0
```

### five weeks later (Mar 2020)

About five weeks later our data would stop updating. But the world had already changed: Here's the same graph, through March 21, 2020:

```{r simpleplot2 }
coronaPlot0 <- coronaDataSimple %>% 
  ggplot(aes(x=reportDate)) +
  geom_line(aes(y=Deaths, color = location)) +
  labs(caption = myCaption)
coronaPlot0
```

It's apparent that the coding for Wuhan versus the rest of China is off for some of the newer data, as one of these increases while the other reaches a plateau. Still, the overall picture is clear.

### adding recovered cases (code from Feb, data through Mar 2020)

Similar results are obtained if we examine two other populations of interest, recovered and confirmed cases. Here, recovered cases and deaths are included on a single plot, as these are roughly on the same scale. Additional changes to the graph are self-evident.

```{r deathsrecovered}
mySubtitle <- paste0(
         "Recovered cases (solid line) and deaths (dotted) by region through ",
         firstbaddate)
 #        (month(today())), "/",
 #         (day(today())),"/",
 #         (year(today())),".")
myCaption <- " Data courtesy JHU/CSSE http://bit.ly/ncvdata"
coronaPlot1 <- coronaDataSimple %>% 
  ggplot(aes(x=reportDate)) +
  geom_line(aes(y=Recovered, 
                color = location), 
            linetype = "solid") + 
  geom_line(aes(y=Deaths, 
                color = location), 
            linetype = "dotted") +
  theme(axis.title.y = 
        element_text(angle = 90,
                     vjust = 1,size = 14),
        legend.position = (c(.2,.8))) +
  labs(title = "Novel coronavirus",
       subtitle = mySubtitle,
       y = "Cases", 
       caption = myCaption)
coronaPlot1
```

## plotting confirmed cases (Feb-Mar 2020)

Finally, confirmed cases are plotted on a different scale:

```{r confirmed}
mySubtitle <- paste0(
         "Confirmed cases (solid line)  through ",
         firstbaddate)
 #        (month(today())), "/",
 #         (day(today())),"/",
 #         (year(today())),".")
myCaption <- " Data courtesy JHU/CSSE http://bit.ly/ncvdata"
coronaPlot1 <- coronaDataSimple %>% 
  ggplot(aes(x=reportDate)) +
  geom_line(aes(y=Confirmed, 
                color = location), 
            linetype = "solid") + 
  theme(axis.title.y = 
        element_text(angle = 90,
                     vjust = 1,size = 14),
        legend.position = (c(.2,.8))) +
  labs(title = "Novel coronavirus",
       subtitle = mySubtitle,
       y = "Cases", 
       caption = myCaption)
coronaPlot1
```

## status (Feb 2021)

The code above works ok, but the March 2020 data need further cleaning, and no new data have been added in 11 months. In that time, as the pandemic has spread, numerous other resources for tracking COVID have been developed, most of which are far more sophisticated and less cumbersome than the code above.

There are new datasets, new R packages, and, perhaps most importantly, new questions that we might consider about variables such as vaccination rates, test-positivity rates, and hospital capacity.

### an assignment

**for Wednesday, please study and be prepared to report on at least one of the following packages for studying COVID data using R:**

> If the last digit of your Z number is 1 or 2, begin with [covid19datahub/COVID19: Unified dataset for a better understanding of COVID-19 (github.com)](https://github.com/covid19datahub/COVID19)
>
> If the last digit of your Z number is 3 or 4, begin with [aedobbyn/covid19us: R package for the COVID Tracking Project API providing US COVID-19 data (github.com)](https://github.com/aedobbyn/covid19us)
>
> If the last digit of your Z number is 5 or 6, begin with [RamiKrispin/coronavirus: The coronavirus dataset (github.com)](https://github.com/RamiKrispin/coronavirus)
>
> If the last digit of your Z number is 7 or 8, begin with [Covid19R/covid19nytimes: Pulls the covid-19 data from the New York Times Public Data Source (github.com)](https://github.com/Covid19R/covid19nytimes)
>
> If the last digit of your Z number is 9 or 0, begin with [joachim-gassen/tidycovid19: {tidycovid19}: An R Package to Download, Tidy and Visualize Covid-19 Related Data (github.com)](https://github.com/joachim-gassen/tidycovid19)

Review the GitHub page, especially the README.md section to get a sense of what is available in the data. How current is it (when was the code last modified?). How popular is it (look at the number of 'watches' 'stars' and 'forks' in the upper right hand corner of the webpage)? How buggy is it (look at the 'issues' tab, then look at 'Open' and 'Closed' issues? Is it worth looking further at this library)?

If you get this far, consider looking at a second package, Googling 'R COVID data', and/or installing and playing with a package on your home machine. We'll discuss your findings and work together on this in class on Wednesday.

## how to create new knowledge

One thing to keep in mind as we move forward is that, despite the fact that hundreds if not thousands of statisticians, data scientists, epidemiologists, policy makers, and journalists have looked at COVID data, there are still unanswered questions.

> For example, here in Florida, there was recently some controversy as the Governor chose to disperse COVID vaccinations at Publix pharmacies; the controversy arose, in large part, because poorer communities (including those in the Western part of Palm Beach County) appear less likely to have a Publix supermarket. Is it the case that there is a relationship between COVID disease rates and the number of Publix/per capita by city or zip code (Florida data by Zip code is available at [https://covid19-usflibrary.hub.arcgis.com/datasets/)?](https://covid19-usflibrary.hub.arcgis.com/datasets/)?) If so, what would this tell us?

In data science, we can often create new knowledge by putting together two data sources which haven't been combined before, together with an interesting and potentially important question.

<!--chapter:end:08-COVID.Rmd-->

# on probability and statistics

Last week, we considered [@anscombe1973] and his quartet, and how visualizing data is valuable. This week, we move to a brief discussion of principles of statistics.

## on probability

**Discrete** probability is used to understand the likelihood of categorical events. We can think of initial estimates of probability as subjective or personal. For some events (*what is the probability this plane will crash?*), an estimate of probability can be drawn from a base rate or relative frequency (e.g., *p(this plane will crash) = (number of flights with crashes/ number of flights)*). For other events (what is the probability that a US President will resign or be impeached before completing his term of office?), it may be hard to arrive at a suitable base rate. Here, a number of subjective beliefs or principles may be combined to arrive at a subjective or personal probability. In a sense, all probability estimates begin with a personal belief such as this, in part because the choice of the most informative base rate is often not self-evident - in the plane crash example, maybe we should consider a reference group rates such as 'for this airline' etc. [@lanning1987]. The personal origins of probability estimates should become less important as we are exposed to data and revise our estimates in accordance with Bayes theorem. But over the last 45 years, a substantial body of evidence has demonstrates that, under at least some circumstances, we don't make estimates of probability in this way. (This material is discussed in the Thinking and Decision Making/Behavioral Economics class).

## the rules of probability

Here's an introduction to the principles of probability. These are presented, with examples and code, in this [R markdown document](https://github.com/datasciencelabs/2020/blob/master/06_probability/01_discrete-probability.Rmd) at Harvard's datasciencelabs repository:

> **I. For any event A, 0 \<= P (A) \<= 1**
>
> **II. Let S be the sample space, or set of all possible outcomes. Then P(S) = 1, and P (not S) = 0.**
>
> **III. If P (A and B) = 0, then P (A or B) = P (A) + P (B).**
>
> **IV. P (A\|B) = P (A and B)/ P (B)**

Principle III applies for **mutually exclusive** events, such as A = you are here this morning, B = you are at the beach this morning. For mutually exclusive (disjoint, disjunctive) events, the union is the sum of the two events. This is called the addition rule for disjoint events.

A different rule applies for events that are **mutually independent**, such as (A = I toss a coin and it lands on 'Heads') and (B = it will rain tomorrow). What we mean by independent is that our estimates of the probability of one don't change based on the state of the other - your estimate of the likelihood of rain shouldn't depend on my coin flip. Here, you multiply rather than add:

> **If P (A\|B) = P (A), then P (A and B) = P (A) P (B).**

In words - if the probability of A given B equals the probability of A, then the probability of both A and B equals the probability of A times the probability of B.

Ask yourself - are mutually exclusive events independent? Come up with your own examples.

This **multiplication rule** is handy for estimating the probability of an outcome that happens following a chain of independent events, such as the probability that the next eight times I toss a coin it will land on "tails" every time:

> P (TTTTTTTT) = P (T) P (T) P (T) P (T) P (T) P (T) P (T) P (T). = .58 = 1/256.

Many sets of events are neither disjoint nor independent, so we need more general ways of thinking about pairs of events. For most of us, Venn diagrams are useful to think about combining probabilities. The **union or P (A U B)** describes the probability that A, B, or both of these will occur. Here, you will use the **general addition rule:**

> **P (A or B) = P (A) + P (B) - P (A and B)**

(the probability of A or B is the probability of A plus the probability of B minus the probability of both A and B).

For the **intersection or P (A ∩ B)**, we need to consider **conditional probabilities**. Think of the probability of two events sequentially: First, what's the probability of A? Second, what's the probability of B, given that A has occurred? Multiply these to get the likelihood of A and B:

> **P (A and B) = P (A) P (B\|A).**

*Example: The probability of you and your roommate both getting mononucleosis equals the probability of your getting mono times the probability that your roommate gets it, given that you have it also.*

This is the **general multiplication rule**. In this abstract example, the order is irrelevant. To estimate the likelihood of A and B, we could as easily take the probability of B, and multiply it by the conditional probability of A given B

> **P (A and B) = P (B) P (A\|B).**

*Use the mono example again. What are A and B here? Does it still make sense? When might P (B\|A) make more sense than P (A\|B)?*

We are often interested in estimating conditional probabilities, in which case we'll use the same equation, but solve instead for P (A\|B). This leads us back to principle IV:

> **IV. P (A\|B) = P (A and B)/ P (B)**

> *What is the likelihood of getting in an accident (A), given that one is driving on I-95 (B)? How would you estimate this? If there are fewer accidents on Military Trail, does this mean that you would be safer there?*

### keeping conditional probabilities straight

In general, P (B\|A) and P (A\|B) are not equivalent. Moore's (2000) *Basic Practice of Statistics* gives the example of

> P (Police car \| Crown Victoria) = .7, and P (Crown Vic \| Police car) = .85.

Here, one could use a Venn diagram to model this asymmetry. There are several R packages for doing this, including Venn and VennDiagram. Consider exploring these, or at the very least make a rough sketch that can help you answer the following question: \> In general, if P (A\|B) \< P (B\|A), what must be true of the relationship of P (A) to P (B)?

## continuous probability distributions

We can also use probability with **continuous** variables such as systolic blood pressure (that's the first one), which has a mean of approximately 120 and a standard deviation of 15. Continuous probability distributions are handy tools for thinking about the meaning of scores, particularly when we express scores in standard deviations from the mean (z scores). More to the point, this way of thinking about probability is widely used in questions of scientific inference, as, for example, in testing hypotheses such that "the average systolic blood pressure among a group of people studying at Crux (hence caffeinated) will be significantly greater than that of the population as a whole."

This is part of the logic of **Null Hypothesis Significance Testing (NHST)** - if the result in my Crux sample is sufficiently high, then I say that I have rejected the null hypothesis, and found data which support the hypothesis of interest.

## the most dangerous equation

Just as [@tufte2001] demonstrated that poor data visualizations can be dangerous, leading, for example, to the loss of life in the Challenger disaster, [@wainer2007] shows that a lack of statistical literacy is also "dangerous."

He argues that deMoivre's equation is the most dangerous equation - this equation (for the standard error) shows that variability decreases with the square root of sample size. Other nominees include the linear regression equation (and, in particular, how coefficients may change or reverse when new variables are added) and regression to the mean. Regarding linear regression, Simpson's paradox shows how the direction of regression coefficients may change when additional variables are added to a model.

I would argue that, from the standpoint of psychology, ignorance of regression to the mean was arguably more 'dangerous' than ignorance about the central limit theorem and standard error, in particular because regression effects contribute to an overestimate of the effectiveness of punishment and an under-appreciation of the effectiveness of positive reinforcement as tools for behavior change [@hastie2010].


<!--chapter:end:09-probabilityAndInference.Rmd-->

# reproducibility and the replication crisis

Probability theory is elegant, and the logic of **Null Hypothesis Significance Testing (NHST)** is compelling. But philosophers of science have long recognized that this is not really how science works [@lakatos1969]. That is, science is not primarily built by the testing and rejecting of null hypotheses. (Think, for example, of how you might describe an experiment for testing the hypothesis that gravity exists, and whether you would ever reject this hypothesis).

The problem is a multifaceted one. It arises partly because, in any experiment, there is a large (infinite?) number of conditions which might be invoked to explain-away results of a failure of our work. It arises because we are human, and prone to various biases of decision making and information integration (that is, we are poor Bayesians). It arises, too, because scientific institutions such as journals, funding agencies, and universities are competitive environments which incentivize positive findings.

In recent years, the tension between **the false ideal of NHST** and the real world of science has become increasingly evident. Within psychology, experimental studies have often - even typically - failed to replicate well-known results [@opensciencecollaboration2015]. It's not just psychology [@baker2016]. One of the first important papers to shine light in the area [@ioannidis2005] came from medicine; it suggested six contributing factors, which I quote verbatim here:

> *The smaller the studies conducted in a scientific field, the less likely the research findings are to be true.*
>
> -   This stems directly from our discussion of the central limit theorem and the instability of results from small samples.
>
> *The smaller the effect sizes in a scientific field, the less likely the research findings are to be true*
>
> -   We'll talk about effect size below.
>
> *The greater the number and the lesser the selection of tested relationships in a scientific field, the less likely the research findings are to be true.* (and) *The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true.*
>
> -   The "problem" of analytic flexibility leads to 'p-hacking'
>
> *The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true* and *The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true.*
>
> -   Positive findings rise, and negative ones are ignored. And scientists are human, and subject to incentives.

Here's a video which provides some more context for the crisis:

[![Reproducibility](images/reprostill.png)](https://youtu.be/42QuXLucH3Q).

\*Video 10.1: On the reproducibility crisis (12 mins)

## Answers to the reproducibility crisis

For scientific progress to occur, there needs to be more than a simple rejection of what is wrong with current methods and research programs: Here, as elsewhere, criticism must be constructive to be valuable.

There have been a number of solutions proposed to the reproducibility crisis.

### Tweak or abandon NHST

The first cluster of responses to the reproducibility crisis is concerned with statistics, specifically problems with Null Hypothesis Significance Testing (NHST). These include (a) justifying one's alpha - making it more stringent, for example, for counter-intuitive claims [@grange2018], (b) changing the default p value from .05 to .005 [@benjamin2018], and (c) abandoning significance testing altogether [@mcshane2017].

[@szucs2017] goes into some of these issues in more detail and discusses other limitations of significance testing, including the dichotomous, all-or-none silliness of the accept/reject decision. (If you play the NHST game, there is no 'almost' significant, 'approached significance,' 'highly significant', etc.).

[@leek2015] argue that the problems are not merely with NHST, but with the whole of data analysis. They maintain that better training in data science - courses like ours, perhaps, are part of the answer.

![leek2015pipeline](images/leek2015pipeline.PNG) (figure)

[@munafo2017] also argue that threats to reproducible science occur at a number of places in science, not just with the evaluation of hypotheses.

![munafo2017threats](images/munafo2017threats.PNG)

### keep a log of every step of every analysis in R markdown or Jupyter notebooks

A second cluster of responses is concerned with keeping good records. Let's say that you are running a study, say, which looks at the hypothesis of differential variation of females and males in a cognitive measure; your interest is to critically examine the hypothesis discussed by [@wainer2007] that males show more variability.

There have been *a lot* of studies related to this over the years, so that rather than collect new data you decide that you will work with existing data from several online archives. You find and download spreadsheets from two studies: In the first, gender is coded '1' for male, '2' for female. In the second, gender is coded '1' for female, '2' for male, and '3' for other. There are, in essence, two ways that you can combine the variables into a common format: The first would be to take one of the spreadsheets and do a few find-and-replace commands on the appropriate column of the data. This is quick and easy - but when someone else, or even future you, returns to this data, you will not remember if you have recoded it.

The alternative is to keep a record of your work in R markdown. This is more time consuming, and can sometimes be clumsy. But it is ~~virtuous~~ useful and clear - and when you screw up, you will have a full record of what happened.

Part of the problem of scientific reproducibility is to keep comprehensive records. This record-keeping and research transparency is at the heart of R markdown documents.

## answers to the reproducibility crisis III: Pre-registration

The third answer to the reproducibility crisis is the most comprehensive; it involves not merely keeping a record of what you have done, but preregistering your work, that is, fully specifying your planned analyses beforehand [@miguel2014]. The author, an economist, outlines his argument in a five-minute video [here](https://www.futurelearn.com/courses/open-social-science-research/0/steps/31436). For randomized controlled trials, consider socialscienceregistry.org, and for more general use, use the open science framework page.

## further readings

Finally, if you would like to learn more about the reproducibility crisis, there is a collection of papers in Nature [here](https://www.nature.com/collections/prbfkwmwvz/).

<!--chapter:end:10-Reproducibility.Rmd-->

# (PART) Part III Towards data proficiency {.unnumbered}

# literate programming with R markdown

Showing your work, to (future) you as well as others, is a key part of reproducible science. R Markdown documents facilitate this, as they allow you to include comments, code, and results in a single place. But before we consider R markdown, we begin with two more elemental ideas: *scripts* (R4DS, Chapter 6) and *projects* (Chapter 8).

## scripts are files of code

We begin with R4DS Chapter 6, which shows the R studio interface and encourages you to save your work using scripts, written in the source (editor) window in the upper left quadrant of the default R studio screen.

Note the recommendations - for example, include packages (libraries) at the beginning of your code. One more thing - in setting up R studio, consider adjusting the "insert spaces for tab" setting to something more than 2. This will allow you to more easily see the nested structure of functions, loops, etc. - and will create a modest disincentive against making these nested structures too deep or complex:

![Fig 11.1: I use 4 spaces here. YMMV.](images/RStudioOptions.jpg)

Note, too, the [code diagnostics](https://support.rstudio.com/hc/en-us/articles/205753617-Code-Diagnostics) in R. Consider enabling all of these, including the R style diagnostics, to help you keep your code readable:

![Fig 11.2: Enable code diagnostics](images/codediagnostics.PNG)

### some elements of coding style

Good coding is often a combination of several skills ranging from puzzle-solving to communication. I can't claim that these are *the* elements of coding style (apologies to Strunk & White), but rather that these are merely some of the elements.

Good coding is **clear** and thus commented. You are writing for your future self as well as others, so be explicit about the purpose of each chunk of code.

Good coding is **concise**. When you can write code in 3 lines instead of 30, your code may be more clear and efficient. Take pleasure in writing parsimonious, efficient code. But where efficiency and clarity conflict, choose the latter.

Good code should be **complete**, including all steps from reading the data to producing output. Where appropriate, comment-out (rather than delete) informative errors, again for the future you.

Good code may be **creative**. The coolest solutions are those which pull from and synthesize a number of ideas. Creativity often requires walking away from a problem in order to ultimately arrive at a solution (Wertheimer's *Productive Thinking*).

Finally, good code should be **considered**. Reflect on the impacts of your work - just because you can analyze something doesn't mean that you should.

## projects are directories containing related scripts

You will save your work in *projects* - which isolate your data and scripts into different directories. (See r4ds, Chapter 8). To reinforce the idea that your unit of analysis in R is "the project" rather than "the script", consider associating your Rmd filetype (see next section) with your markdown editor, and only your Rproj filetype with R studio.

Soon, it is likely that you will soon be working on R for different things in parallel - for this and another class, for this class and your thesis, or perhaps for two distinct types of analysis within your thesis. When you open up an R project, you'll be in the right directory, with the relevant files (and only the relevant files) at your fingertips in the files pane.

## R markdown documents integrate rationale, script, and results

R Markdown documents allow you to include comments, scripts, and results in a single place. The basics of R markdown are presented in Chapter 27 of R4DS. I encourage you to use R markdown for nearly everything you do in R.

Within R studio, open up a new R markdown document. There are as many as four parts of an R markdown document:

-   A YAML (yet another markdown language) header
-   Text formatted in markdown
-   R code (chunks) surrounded by code fences
-   and, occasionally, inline code

There is a handy [R Markdown cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf) which can give you a sense of what R markdown is about. It describes eight steps, from "workflow" to "publish" (and a ninth, "learn more"). Don't worry about all of the detail here, but do get a sense of how it works.

> Exercise 11.1:
>
> Working in groups, do the exercises in section 27.4.7 of R4DS.
>
> Begin with the R markdown file that is included at the beginning of Chapter 27. You can download it [here](https://raw.githubusercontent.com/hadley/r4ds/master/rmarkdown/diamond-sizes.Rmd).
>
> Study the code, and annotate it so that you have a better sense of how it works. For example, "this block loads needed libraries, then takes the \_\_\_\_\_ dataset and \_\_\_\_\_\_\_\_\_\_\_ ."
>
> Play with the graph. Change one or more parameters of it to make it more useful. Again, annotate your changes.

## What to do when you are stuck

-   Google. pay attention to your error messages

-   Ask for help, make your questions clear and reproducible (see R4DS Chapter 1)

-   Take a break, think outside the box and [kludge](https://www.google.com/search?newwindow=1&safe=active&rlz=1C1SQJL_enUS782US782&q=Dictionary#dobs=kludge) something together if you have to

-   Document your struggle and your cleverness for a future you

<!--chapter:end:11-LiterateRmd.Rmd-->

------------------------------------------------------------------------

# the tidyverse

------------------------------------------------------------------------

> *The tidyverse is an opinionated collection of R packages designed for data science -* <https://www.tidyverse.org/>

R had its origins in S, a system designed for engineers at Bell Labs. This audience meant that R would be more accessible to those with programming backgrounds, more aimed at "developers" than users approaching data science from an applied or statistical perspective than one in programming. As the popularity of R increased, it would become more flexible and versatile for these power users, but there was less progress in making R accessible to and tailored for data scientists. To this day, "base-R" is, for most users, more challenging than SPSS or Stata. The **tidyverse** was born partly to address these issues [@peng2018].

The tidyverse is a growing set of interconnected packages which share a common syntax; it can be seen as a dialect of R. More precisely,

> *...the tidyverse is a lucid collection of R packages offering data science solutions in the areas of data manipulation, exploration, and visualization that share a common design philosophy. It was created by R industry luminary Hadley Wickham, the chief scientist behind [RStudio](https://www.rstudio.com/). R packages in the tidyverse are intended to make statisticians and data scientists more productive. Packages guide them through workflows that facilitate communication and result in reproducible work products. The tidyverse essentially focuses on the interconnections of the tools that make the workflow possible [@gutierrez2018].*

The workflow is one that you have seen here and in R4DS. In this 2017 slide, the main processes of data analysis are accompanied by the packages in the tidyverse. (As of 2019, there have been a few small changes in the packages associated with modeling). All of these are installed on your computer with install.packages("tidyverse"), but only those in bold are loaded into memory when you issue the command library(tidyverse):

![Fig 10.1: Schematic of the tidyverse. From Wickham's 2017 rstudio:conf keynote](images/tidyworld.PNG)

## some simple principles

1)  **search for tidyverse solutions.** When you have a problem in your code, for example, "how do I compute the mean for different groups of a variable," search for *R mean groups tidyverse*, not just *R mean groups.* This will get you in the habit of working with tidy solutions where they can be found.

```{r echo=TRUE, results="hide", message=FALSE}
library(tidyverse)
mtcars %>%
  group_by(cyl) %>%
  summarise(mean = mean(disp), n = n())
```

(Another suggestion: Because R and the tidyverse are constantly evolving, consider looking at recent pages first. In your Google search bar, click on Tools -\> Past year).

2)  **talk the talk**. Recognize that **%\>%** (the pipe) means **then.** Statements with pipes begin with data, may include **queries** (extract, combine, arrange), and finish with a **command.**

3)  **annotate your work**. Assume that you will come back to it at a later date, while working on a different project, and use portions of your current code. Your R markdown documents should be a log. When you run in to a significant, challenging problem, don't delete your mistakes, but \#\# comment them out.\

```{r echo=TRUE, results="hide", message=FALSE}
library(gapminder)
b <- gapminder %>% 
## when should you comment out an error
## instead of deleting it? for me, I'll 
## comment out errors that took me a long time 
## to solve, and/or that I'll learn from. 
## Probably not here, in other words...
##  filter(lifeExp) > 70 bad parens
    filter(lifeExp > 70)
```

4)  **work with tidy data.** Make each row an observation, and each column a variable. Complex data sets, such as samples of text, become much more manageable when reduced to simple arrays.

5)  **write functions.** If you repeat a section of code, rewrite it as a function. (We'll come back to this later).

6)  **adhere to good coding style.** Well-written code is reasonably parsimonious, readable, and easily debugged. There are a few style manuals for R, including one from [Hadley](http://adv-r.had.co.nz/Style.html), and this [Rchaeological Commentary] (<https://cran.r-project.org/web/packages/rockchalk/vignettes/Rstyle.pdf>).

7)  **but maintain perspective.** Your need to solve problems (how to analyze x, etc.) should not take a back seat to your desire to write the best code. There is almost always a better way to do things. Strive reasonably to accomplish this, but be prepared to kludge.

<!--chapter:end:12-tidyverse.Rmd-->

# finding, exploring, and cleaning data

**Data science, oddly enough, begins not with R... but with *data*.** There is no shortage of datasets available to analyze, and each can give rise to a host of interesting analyses and insights.

What do you want to study?

## Data in R libraries

A few weeks back, we explored five different R packages (libraries) for looking at COVID data. There are a number of other R packages which provide 'easy' access to data - for example, consider the [**babynames package**](https://cran.r-project.org/web/packages/babynames/babynames.pdf). Install it on your machine, load the library, and View it.

> Exercise 13.1 Come up with an interesting question about the babynames dataset. How would you go about examining it?

Libraries in R with multiple datasets include the built-in **R dataset library**, which includes, at this writing, `r length((library(help = "datasets"))$info[[2]])` different datasets. Another is the **R dslabs** library, with `r length((library(help = "dslabs"))$info[[2]])` additional datasets. And the **R fivethirtyeight library** provides access to `r length((library(help = "fivethirtyeight"))$info[[2]])` clever, clean, and largely manageable datasets, each of which underlies the empirical analyses and reports of Nate Silver and his team (You can learn more at <https://data.fivethirtyeight.com/>).

Less tidy, somewhat more ambitious, and more far-ranging datasets include these [**Nineteen datasets and sources**](https://www.springboard.com/blog/free-public-data-sets-data-science-project), ranging from Census data to Yelp reviews.

## Other prepared datasets

The datasets in the libraries described above should be relatively easy to work with, requiring minimal munging prior to analysis. If you are up for something a little more ambitious, read on.

[**Kaggle**](https://www.kaggle.com)is a noun (a community, a website, a challenge), and a verb (to kaggle is to participate in a data challenge) which describes a crowdsourced competition to improve on a problem in prediction. Perhaps the first and best known example of this was the Netflix prize [@jackson2017], which, in 2006, promised one million dollars to the first team to improve the algorithm by which that company recommended movies to its customer base. The competition took several years, and inspired substantial improvements in machine learning as well as in crowdsourced science. At this writing, Kaggle hosts many active competitions - including a \$1,500,000 award offered by the United States Department of Homeland Security known as the "passenger screening algorithm challenge." (Good luck!) Kaggle also hosts hundreds if not thousands of datasets. A good place to start is with their datasets stored in comma separated value format (.csv); you can find them [here](https://www.kaggle.com/datasets?fileType=csv). Kaggling is an important feature of data science culture.

If you are into psychology and behavioral science, the **Open Science Framework** (OSF) provides a system for hosting and sharing code and data from research articles. One OSF page is a compilation of many [**datasets from prominent papers in psychology and psychiatry**](https://osf.io/r38qu/) Incidentally, almost all of the data and code from papers I have published is on the OSF as well. Beyond OSF, there is some **[structured personality test dat](https://openpsychometrics.org/_rawdata/)**[a](https://openpsychometrics.org/_rawdata/) available, too.

Outside of psychology, repositories of data from many disciplines may be found at **Re3data** <https://www.re3data.org/>.

There are many datasets about **music** - songs, artists, lyrics, etc. - at [**millionsongdataset**](http://millionsongdataset.com/pages/additional-datasets/){.uri}. Note that many of these are quite large, but there are "smaller" files of \~ 10000 songs that are available.

**Github** is the primary site for coders to share and improve upon their work. Git is a system in which one can upload (*push*) one's work from a local computer to the cloud in a repository (*repo*), share this with collaborators who copy (*fork*) the repo, *pull* it down to their computers, and possibly make changes which will appear as a separate branch of the repo. Each change is time-stamped, and efficiently stored as only its difference from the prior edit (or *commit*). There are, in all of these pushes and pulls, opportunities for collisions and problems, but learning Git remains a critical part of the data scientist's toolkit. You can set up an account on Github if you like, but even without this you can access some of the datasets that are stored there, including a set of [**curated datasets on topics such as economics, demographics, air quality, flights and house prices**](https://github.com/datasets/). Perhaps the easiest way to access these is to click through repos until you find a data directory, open the files up as 'raw' files, and paste them into a spreadsheet or notepad program of your choice. Github also hosts the '[**awesome public datasets**](https://github.com/awesomedata/awesome-public-datasets)**'** (many of which probably are). You can work with R repositories straight from R studio.

Or just **[Google datasets](https://toolbox.google.com/datasetsearch)**.

### Keep it manageable

Proceed with caution - many of these datasets are likely to be quite large (for example, analyses of images) and/or in formats that for now are too challenging (JSON). I encourage you to stick with data that are available in a .csv format and that don't have more than, say, a million data points (e.g., 50,000 observations \* 20 variables). And probably avoid, for the time being, datasets consisting primarily of natural language samples or networks - for those who are interested, we will look at these in the Computational Social Science class in the Fall.

## Make/extract/combine your own data

Despite the petabytes (exabytes? zettabytes? [yottabytes](https://en.wikipedia.org/wiki/Yottabyte)?) of data in the datasets described above, it's possible that the dataset that you want to examine does not yet exist. But you may be able to create it, for example, by **scraping** data from the Web. Typically, you would use an Application Programming Interface (API) to pull data down from platforms such as Twitter or Reddit. For these and other major social media and news platforms, there are R packages which will walk you through the process of getting the data from webpage to tidy dataset. (Be aware, though, that the methods for data access on these platforms frequently changes, so that code that worked a year ago might not work today).

Another source of data is ... your own life. If you wear a pedometer or sleep tracker, are a calorie counter or keep other logs as a member of the **quantified self** movement, consider how such data might relate to aspects of the physical environment (such as temperature, or the time between sunrise and sunset) and/or the broader social and cultural context (a measure, perhaps of the sentiment, or mood, of news articles from papers like the *NY Times*).

Finally, you might want to **combine multiple datasets**, such as county-level home pricing data from Zillow (<https://www.zillow.com/research/data/>), county-level elections data from, for example, here: <https://github.com/tonmcg/US_County_Level_Election_Results_08-16>, and the boundaries of Woodard's 11 American Nations (see Lanning). In joining different datasets, or data from different sources, we can go beyond a pedagogical exercise (learning about learning) and contribute new and meaningful knowledge.

## exploring data

To look at the datasets in the prior section, remember that there are a few key commands, which are here applied to the gapminder dataset:

```{r echo=TRUE, results="hide", message=FALSE}
library(gapminder)
library(tidyverse)
str(gapminder)
head(gapminder)
summary(gapminder)
```

To simplify your data, you'll want to select certain columns or rows, or possibly create new variables based on existing scores:

```{r echo=TRUE, results="hide", message=FALSE}
gapminder %>% 
    filter(gdpPercap < 100 & year > 2000)
gapminder %>%
    select(-(c(continent,pop)))
gapminder %>% 
    mutate(size = ifelse(pop < 10e06, "small", "large"))
```

## messy data: Cleaning and curation

------------------------------------------------------------------------

Between 50 and 80% of the work of the data scientist consists of the compiling, cleaning and curation of data, or what is called **data munging or wrangling**.

One part of data wrangling is looking for and dealing with encoding inconsistencies, missing values, and errors. Consider the following:

> Exercise 13.2
>
> Run the following code in an R markdown document. You'll need to add a library beforehand.
>
> car2019 \<- tibble("model" = c("Corolla", "Prius", "Camry", "Avalon"), "price" = c(22.5, "about 25K" , 24762, "33000-34000"))
>
> Inspect the data frame. Add and annotate code to fix any problems that you believe exist. Summarize the results.

Another part of data wrangling is about data rectangling [@bryan2017], that is, getting diverse types of data into a data frame (specifically, a tibble). This is likely to be particularly challenging when you are combining data from different sources, including Web APIs. We'll consider this further down the road when we talk about lists.

A third part of data wrangling occurs when we join data from different sources. There are many ways to do this, but attention must be paid to insure that the observations line up correctly, that the same metrics are used for different datasets (for example, inflation adjusted dollars vs raw), that dates are interpreted as dates, that missing values are recognized as missing and not scored as zero, and so forth. We'll talk about this in the weeks ahead, particularly when we consider relational data.

<!--chapter:end:13-data.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:99-references.Rmd-->

